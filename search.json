[
  {
    "objectID": "posts/shelton_HW2.html",
    "href": "posts/shelton_HW2.html",
    "title": "Homework 2 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(warning= FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/shelton_HW2.html#homework-2",
    "href": "posts/shelton_HW2.html#homework-2",
    "title": "Homework 2 Solution",
    "section": "Homework 2",
    "text": "Homework 2\n\nQ1Q2Q3Q4Q5Q6\n\n\n\n90% Confidence Intervals\n\n\nCode\n# Bypass\nn_bp <- 539\nmean_bp <- 19\nsd_bp <- 10\nt_90 <- qt(.05, (n_bp-1), lower.tail=F)\n\n#CI\nupper_bp <- mean_bp + ((sd_bp/sqrt(539))*t_90)\nlower_bp <- mean_bp - ((sd_bp/sqrt(539))*t_90)\n\nci90_bp <- c(lower_bp,upper_bp)\nprint(c(\"90% CI For Mean Bypass Wait\", ci90_bp))\n\n\n[1] \"90% CI For Mean Bypass Wait\" \"18.2902893200424\"           \n[3] \"19.7097106799576\"           \n\n\nCode\n# Angiography\nn_ag <- 847\nmean_ag <- 18\nsd_ag <- 9\nt_90 <- qt(.05, (n_ag-1), lower.tail=F)\n\n#CI\nupper_ag <- mean_ag + (sd_ag/sqrt(539)*t_90)\nlower_ag <- mean_ag - (sd_ag/sqrt(539)*t_90)\n\nci90_ag <- c(lower_ag,upper_ag)\n\nprint(c(\"90% CI For Angiography Wait\", ci90_ag))\n\n\n[1] \"90% CI For Angiography Wait\" \"17.3616612514732\"           \n[3] \"18.6383387485268\"           \n\n\nCode\nprint(c(\"Width Bypass\", upper_bp-lower_bp))\n\n\n[1] \"Width Bypass\"     \"1.41942135991513\"\n\n\nCode\nprint(c(\"Width Angiography\", upper_ag-lower_ag))\n\n\n[1] \"Width Angiography\" \"1.27667749705367\" \n\n\nThe 90% Confidence interval is narrower for the mean Angiography wait time (days) than mean Bypass wait due to the larger sample and smaller standard deviation.\n\n\n\n\nOne Prop Confidence Interval\n\n\nCode\n# College 95% CI\nprop.test(567,1031,conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the true proportion of Americans who believe a college education is essential for success is (0.52,0.58). 95% confidence is not a comment on the proportion itself, rather our method. If we took several samples and created a confidence interval for proportion p, 95% of the intervals would contain the true population proportion.\nBecause our confidence interval does not include .5, we can conclude that at .05 significance, the majority (>0.5) of Americans believe that a college educcation is essential for success.\n\n\n\n\nMargin of Error Calculation\n\n\nCode\n# Margin of Error Calculation\n\nci_95 <- qnorm(.025, lower.tail=F)\n\n# 5 = (170*.25)/sqrt(x)*1.96\n\n(x <- ((170*.25)/5)*ci_95)^2\n\n\n[1] 277.5454\n\n\nTo estimate mean textbook cost per semester within $5 of true value at .05 significance level the financial aid office would need 278 students in their sample.\n\n\n\n\nOne Sample T-Test\n\n\n\n\n\n\na\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees != $500/wk\n\n\nCode\n# womens data\nw_xbar <- 410\nw_n <- 9\nw_sd <- 90\nw_se <- 90/sqrt(w_n)\ntest_stat <- (w_xbar-500)/w_se\ncrit_2sided <- abs(qt(.025,w_n-1))\ncrit_less <- qt(.05, w_n-1,lower.tail=T)\ncrit_greater <- qt(.95,w_n-1,lower.tail=T)\np_value <- pt(test_stat, df=w_n-1, lower.tail=T)\npval_greater <- pt(test_stat, df=w_n-1, lower.tail=F)\n\n# Two Sided 2 Test\nprint('Two-Sided T-Test')\n\n\n[1] \"Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"-3\"                                  \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"2.30600413520417\" \n\n\nCode\nprint(c('p-value', 2*p_value))\n\n\n[1] \"p-value\"            \"0.0170716812337826\"\n\n\np-value = .017; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages differ from $500/week. If female weekly income was equal to 500, we would expect 1.7% of samples to produce a sample mean of 410$ or more extreme.\n\n\n\n\n\n\n\n\nb\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is less than $500/wk\n\n\nCode\nprint('Left-Sided T-Test')\n\n\n[1] \"Left-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_less))\n\n\n[1] \"rejection-region:\" \"-1.8595480375309\" \n\n\nCode\nprint(c('p-value',p_value))\n\n\n[1] \"p-value\"             \"0.00853584061689132\"\n\n\np-value = .009; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages are less than $500/week. If mean female weekly income was equal to 500 , we would expect less than one percent of samples to produce a mean equal to or more extreme (less) than 410.\n\n\n\n\n\n\n\n\nc\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is greater than $500/wk\n\n\nCode\nprint('Right-Sided T-Test')\n\n\n[1] \"Right-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_greater))\n\n\n[1] \"rejection-region:\" \"1.8595480375309\"  \n\n\nCode\nprint(c('p-value',pval_greater))\n\n\n[1] \"p-value\"           \"0.991464159383109\"\n\n\np-value = .991; Fail to reject the null, at alpha=.05 we do nothave sufficient evidence to conclude female employees’ wages are greater than $500/week. If female weekly income was equal to 500, we would expect 99 percent of samples to produce a mean equal to or greater than 410.\n\n\n\n\n\n\n\n\n\n\n\na & b\n\n\n\n\n\nCode\n# jones data\nj_xbar <- 519.5\nj_n <- 1000\nj_se <- 10\nj_test_stat <- (j_xbar-500)/j_se\ncrit_2sided <- abs(qt(.025,j_n-1))\nj_p_value <- pt(j_test_stat, df=j_n-1, lower.tail=F)\n\n\n# Jones Two Sided 2 Test\nprint('Jones Two-Sided T-Test')\n\n\n[1] \"Jones Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', j_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.95\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*j_p_value))\n\n\n[1] \"p-value\"            \"0.0514555476459477\"\n\n\nCode\nprint(c('insignificant at alpha = 0.05'))\n\n\n[1] \"insignificant at alpha = 0.05\"\n\n\nCode\n# smith data\ns_xbar <- 519.7\ns_n <- 1000\ns_se <- 10\ns_test_stat <- (s_xbar-500)/s_se\ncrit_2sided <- abs(qt(.025,j_n-1))\ns_p_value <- pt(s_test_stat, df=s_n-1, lower.tail=F)\n\n\n# Smith Two Sided 2 Test\nprint('Smith Two-Sided T-Test')\n\n\n[1] \"Smith Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', s_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.97\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*s_p_value))\n\n\n[1] \"p-value\"            \"0.0491142565416521\"\n\n\nCode\nprint(c('significant at alpha = 0.05'))\n\n\n[1] \"significant at alpha = 0.05\"\n\n\n\n\n\n\n\n\n\n\nc\n\n\n\nBy not reporting the p-value, we do not understand the strength of the test - how extreme are the findings? In in an example like this, we see nearly identical results produce opposite significance results; language like “statistically significant” can get especially dangerous here to someone who is unfamiliar with basic statistical theory.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, \n               41.95, 28.61, 41.29, \n               52.19, 49.48, 35.02, \n               48.13, 39.28, 54.41, \n               41.66, 30.28, 18.49, \n               38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes; at the 95% confidence level, we have sufficient evidence to reject the null hypothesis mu=45. 45 is not included in our left sided confidence interval, favoring the alternative hypothesis that the average tax on gas in the United States in 2005 was less than 45 cents per gallon."
  },
  {
    "objectID": "posts/Quarkume HW1.html#lungcapdate",
    "href": "posts/Quarkume HW1.html#lungcapdate",
    "title": "HW1 Quat",
    "section": "LungCapDate",
    "text": "LungCapDate\n\nUse the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\nInstall Libraries\n\n#install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n#install.packages(\"readxl\")\nlibrary(readxl)\n#install.packages(\"magrittr\")\nlibrary(magrittr)\n\n\nWhat does the distribution of LungCap look like?\nThe distribution of Lung Capacity in the data set looks normally distributed.\n\n\n#histogram of LungCap\nhist(LungCapData$LungCap, xlab = 'LungCap', main = '', freq = F)\n\nError in hist(LungCapData$LungCap, xlab = \"LungCap\", main = \"\", freq = F): object 'LungCapData' not found\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\nLooking at the comparative boxplot males have a higher lung capacity than females.\n\n\nboxplot(LungCapData$LungCap ~ LungCapData$Gender,\n        col = c(\"#FFE0B2\", \"#FFA726\"))\n\nError in eval(predvars, data, env): object 'LungCapData' not found\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense? In comparing the means, the lung capacity for smokers is higher than for nonsmokers.\n\n#Mean Lung capacities of smokers\nLungCapData %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"yes\"): object 'LungCapData' not found\n\n#Mean Lung capacities of non-smokers\nLungCapData %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"no\"): object 'LungCapData' not found\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#new var for Age Groups\nLungCapData$Age_Cat <- cut(LungCapData$Age,\n                           breaks = c(0,13,15,17,25),\n                           labels = c('less than or equal to 13','14 to 15','16 to 17','greater than or equal to 18'))\n\nError in cut(LungCapData$Age, breaks = c(0, 13, 15, 17, 25), labels = c(\"less than or equal to 13\", : object 'LungCapData' not found\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age_Cat, scale=\"free\")\n\nError in ggplot(LungCapData, aes(x = Smoke, y = LungCap)): object 'LungCapData' not found\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here? We see an intervening relationship with age. Where most young children either don’t smoke ar all and have smaller lung capacities because of their size.\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age, scale=\"free\")\n\n--\n  \n\nError: <text>:7:0: unexpected end of input\n5: --\n6:   \n  ^\n\n\nf.Calculate the correlation and correlation between Lung Capacity and Age. (use the cov() and cor() functions in R).\n\n#correlation\nLungCapData %>% \n  summarize(correlation = cor(LungCap, Age))\n\nError in summarize(., correlation = cor(LungCap, Age)): object 'LungCapData' not found\n\n#correlation\nLungCapData %>% \n  summarize(covariance = cov(LungCap, Age))\n\nError in summarize(., covariance = cov(LungCap, Age)): object 'LungCapData' not found"
  },
  {
    "objectID": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "href": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "title": "HW1 Quat",
    "section": "1. Examination of Prison Convictions",
    "text": "1. Examination of Prison Convictions"
  },
  {
    "objectID": "posts/Quarkume HW1.html#prisondata",
    "href": "posts/Quarkume HW1.html#prisondata",
    "title": "HW1 Quat",
    "section": "PrisonData",
    "text": "PrisonData\nData\n\nPrisonData <- tibble(\n  prior_convictions = c(0,1,2,3,4),\n  freq = c(128,434,160,64,24))\n\nPrisonData\n\n# A tibble: 5 × 2\n  prior_convictions  freq\n              <dbl> <dbl>\n1                 0   128\n2                 1   434\n3                 2   160\n4                 3    64\n5                 4    24\n\nnum <- sum (PrisonData$freq)\nnum\n\n[1] 810\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nPrisonData %>% \n  filter(prior_convictions == 2) %>% \n  pull (freq) %>% \n  divide_by (num)\n\n[1] 0.1975309\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions < 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.6938272\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions <= 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.891358\n\n\nd.What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions > 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.108642\n\n\ne. What is the expected value for the number of prior convictions?\n\nsum(prior_convictions*freq)\n\nError in eval(expr, envir, enclos): object 'prior_convictions' not found\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html",
    "href": "posts/FinalPart1_ToryBarteloni.html",
    "title": "DACSS 603: Final Part 1",
    "section": "",
    "text": "The concept of political trust has been researched in great depth for decades. That research indicates that a number of factors have at least some impact on a group’s level of trust or confidence in their government. Most of the factors studied are related to the public’s perception of government performance including control over crime, the economy, and the appearance of corruption and scandal. To this point there has been no consensus or holistic model that produces a satisfactory answer to the question why do groups trust and have confidence in their government? In this project we will try to bring us one step closer by examining a model that takes into account several factors."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#setup",
    "href": "posts/FinalPart1_ToryBarteloni.html#setup",
    "title": "DACSS 603: Final Part 1",
    "section": "Setup",
    "text": "Setup\nLoading packages and reading in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_final <- read.csv(\"_data/FinalPart1_ToryBartelloni_data.csv\")"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "href": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "title": "DACSS 603: Final Part 1",
    "section": "Summary of Data",
    "text": "Summary of Data\nFirst things first, I will include a brief look at the data set and then we will look at the specifics.\n\n\nCode\nstr(data_final)\n\n\n'data.frame':   91 obs. of  27 variables:\n $ X                           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Power_Distance              : int  NA NA 49 NA 38 11 NA 80 NA NA ...\n $ Individualism               : int  NA NA 46 NA 90 55 NA 20 NA NA ...\n $ Masculinity                 : int  NA NA 56 NA 61 79 NA 55 NA NA ...\n $ Uncertainty_Avoidance       : int  NA NA 86 NA 51 70 NA 60 NA NA ...\n $ Time_Perspective            : int  61 NA 20 61 21 60 61 47 81 70 ...\n $ Indulgence                  : int  15 65 62 NA 71 63 22 20 15 44 ...\n $ Country_Final               : chr  \"Albania\" \"Andorra\" \"Argentina\" \"Armenia\" ...\n $ CPI.Score.2018              : int  36 NA 40 35 77 76 25 26 44 38 ...\n $ GDP_per_Capita              : num  13653 NA 22066 13654 49309 ...\n $ Homicides_per_100K          : num  2.256 NA 5.143 2.468 0.893 ...\n $ Gov_Exp_Employees           : num  9.35e+10 NA 5.52e+11 3.01e+11 5.37e+10 ...\n $ Gov_Exp_GoodsAndServices    : num  3.66e+10 NA 1.74e+11 2.06e+11 5.37e+10 ...\n $ Gov_Exp_Total               : num  3.92e+11 NA 4.75e+12 1.42e+12 5.09e+11 ...\n $ Gov_Exp_Interest            : num  3.50e+10 NA 7.50e+11 1.58e+11 1.80e+10 ...\n $ Gov_Exp_Subsidies           : num  2.09e+11 NA 3.05e+12 5.44e+11 3.41e+11 ...\n $ Gov_Exp_Military            : num  2.17e+10 NA 1.51e+11 3.24e+11 3.73e+10 ...\n $ Wage_Workers                : num  45.7 NA 73.5 66 83.4 ...\n $ Vulnerable_Employment       : num  51.2 NA 22.7 33.1 10.6 ...\n $ WGI_Control_Corruption      : num  -0.5434 1.231 -0.0837 -0.2038 1.8221 ...\n $ WGI_Government_Effectiveness: num  -0.0333 1.901 -0.0965 -0.1975 1.5649 ...\n $ WGI_Political_Stability     : num  0.1112 1.6022 -0.0914 -0.4134 0.9117 ...\n $ WGI_Regulatory_Quality      : num  0.286 1.227 -0.437 0.256 1.872 ...\n $ WGI_Rule_of_Law             : num  -0.403 1.572 -0.408 -0.157 1.726 ...\n $ WGI_Voice_Accountability    : num  0.1427 1.1101 0.5724 0.0555 1.2674 ...\n $ Gov_Confidence              : num  0.148 0.491 0.314 0.308 0.313 ...\n $ Gov_Confidence_Mean         : num  3.39 2.56 2.94 2.97 2.82 ...\n\n\nCode\nsummary(data_final)\n\n\n       X        Power_Distance   Individualism    Masculinity    \n Min.   : 1.0   Min.   : 11.00   Min.   : 6.00   Min.   :  5.00  \n 1st Qu.:23.5   1st Qu.: 41.00   1st Qu.:24.00   1st Qu.: 39.50  \n Median :46.0   Median : 63.00   Median :41.00   Median : 49.00  \n Mean   :46.0   Mean   : 59.61   Mean   :45.32   Mean   : 49.31  \n 3rd Qu.:68.5   3rd Qu.: 72.00   3rd Qu.:68.50   3rd Qu.: 63.50  \n Max.   :91.0   Max.   :104.00   Max.   :91.00   Max.   :110.00  \n                NA's   :32       NA's   :32      NA's   :32      \n Uncertainty_Avoidance Time_Perspective   Indulgence     Country_Final     \n Min.   :  8.00        Min.   :  0.00   Min.   :  0.00   Length:91         \n 1st Qu.: 51.00        1st Qu.: 31.25   1st Qu.: 28.00   Class :character  \n Median : 68.00        Median : 51.50   Median : 42.50   Mode  :character  \n Mean   : 66.46        Mean   : 50.46   Mean   : 44.42                     \n 3rd Qu.: 85.00        3rd Qu.: 69.00   3rd Qu.: 63.50                     \n Max.   :112.00        Max.   :100.00   Max.   :100.00                     \n NA's   :32            NA's   :21       NA's   :19                         \n CPI.Score.2018  GDP_per_Capita   Homicides_per_100K Gov_Exp_Employees  \n Min.   :17.00   Min.   :  2221   Min.   : 0.2067    Min.   :1.635e+09  \n 1st Qu.:33.00   1st Qu.: 12845   1st Qu.: 0.7453    1st Qu.:1.830e+10  \n Median :44.50   Median : 25641   Median : 1.3927    Median :7.092e+10  \n Mean   :50.14   Mean   : 30218   Mean   : 3.8824    Mean   :7.315e+12  \n 3rd Qu.:71.25   3rd Qu.: 42847   3rd Qu.: 3.7136    3rd Qu.:4.237e+11  \n Max.   :88.00   Max.   :127273   Max.   :28.7367    Max.   :3.726e+14  \n NA's   :3       NA's   :3        NA's   :20         NA's   :17         \n Gov_Exp_GoodsAndServices Gov_Exp_Total       Gov_Exp_Interest    \n Min.   :7.335e+08        Min.   :8.294e+09   Min.   :-1.270e+09  \n 1st Qu.:8.305e+09        1st Qu.:8.688e+10   1st Qu.: 3.377e+09  \n Median :3.493e+10        Median :4.187e+11   Median : 1.754e+10  \n Mean   :5.153e+12        Mean   :4.773e+13   Mean   : 4.768e+12  \n 3rd Qu.:2.275e+11        3rd Qu.:2.338e+12   3rd Qu.: 2.648e+11  \n Max.   :2.510e+14        Max.   :2.295e+15   Max.   : 2.751e+14  \n NA's   :17               NA's   :17          NA's   :16          \n Gov_Exp_Subsidies   Gov_Exp_Military     Wage_Workers   Vulnerable_Employment\n Min.   :2.170e+09   Min.   :0.000e+00   Min.   :15.85   Min.   : 3.30        \n 1st Qu.:4.227e+10   1st Qu.:3.952e+09   1st Qu.:57.30   1st Qu.: 9.09        \n Median :2.930e+11   Median :2.669e+10   Median :77.26   Median :18.87        \n Mean   :2.540e+13   Mean   :9.487e+12   Mean   :71.64   Mean   :24.75        \n 3rd Qu.:9.952e+11   3rd Qu.:1.584e+11   3rd Qu.:86.35   3rd Qu.:36.96        \n Max.   :1.138e+15   Max.   :5.314e+14   Max.   :95.73   Max.   :83.70        \n NA's   :17          NA's   :11          NA's   :2       NA's   :2            \n WGI_Control_Corruption WGI_Government_Effectiveness WGI_Political_Stability\n Min.   :-1.560314      Min.   :-1.7516              Min.   :-2.603781      \n 1st Qu.:-0.533848      1st Qu.:-0.1993              1st Qu.:-0.565488      \n Median : 0.009211      Median : 0.2023              Median : 0.111169      \n Mean   : 0.272002      Mean   : 0.4396              Mean   : 0.007762      \n 3rd Qu.: 1.221506      3rd Qu.: 1.3999              3rd Qu.: 0.775985      \n Max.   : 2.167130      Max.   : 2.2127              Max.   : 1.639301      \n                                                                            \n WGI_Regulatory_Quality WGI_Rule_of_Law   WGI_Voice_Accountability\n Min.   :-2.3622        Min.   :-2.2536   Min.   :-1.7968         \n 1st Qu.:-0.3226        1st Qu.:-0.4966   1st Qu.:-0.4587         \n Median : 0.5280        Median : 0.1570   Median : 0.2624         \n Mean   : 0.4615        Mean   : 0.3214   Mean   : 0.2182         \n 3rd Qu.: 1.3576        3rd Qu.: 1.3373   3rd Qu.: 1.0170         \n Max.   : 2.1601        Max.   : 2.0488   Max.   : 1.6552         \n                                                                  \n Gov_Confidence    Gov_Confidence_Mean\n Min.   :0.08744   Min.   :1.561      \n 1st Qu.:0.24726   1st Qu.:2.502      \n Median :0.39372   Median :2.711      \n Mean   :0.41976   Mean   :2.708      \n 3rd Qu.:0.53634   3rd Qu.:3.020      \n Max.   :0.95441   Max.   :3.448      \n                                      \n\n\nA lot going on there, but we can see that the data was 91 observations fo 27 variables. Each observation in the data is a country and there are observations for a number of potentially useful variables. Choosing the best variables and assessing the power of our test will be important due to the noticeable number of NA values for some of the variables."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "href": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "title": "DACSS 603: Final Part 1",
    "section": "Transparency International",
    "text": "Transparency International\nThe Corruption Perceptions Index (CPI) is created by Transparency International by taking a combination of 13 different data sources including assessments and surveys. These sources are largely comprised of experts and business interests so are not a direct reflection of the general public. The scores from each of the sources are standardized, averaged, and then scaled to provide a score for each of the countries in the data sources. What we end up with is Corruption Perception score between 1-100 for each of the countries.\n\n\nCode\ndata_final %>% ggplot(aes(x=CPI.Score.2018)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(CPI.Score.2018,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(CPI.Score.2018,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(CPI.Score.2018,na.rm=TRUE)+\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(CPI.Score.2018,na.rm=TRUE)-\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Corruption Perceptions Index\",\n       subtitle=\"Distribution of CPI 2018\",\n       x=\"CPI Score\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "title": "DACSS 603: Final Part 1",
    "section": "World Bank Development and Governance Indicators",
    "text": "World Bank Development and Governance Indicators\nThe World Bank collects data from many different sources to obtain indicators for world development as well as the World Governance Indicators project.\nThe Development Indicators are taken from a wide variety of sources. We will be using two primary indicators: GDP per Capita and Intentional Homicides per 100K people. GDP per capita is derived from the World Bank and OECD National Accounts data while Intentional Homicides are taken from the UN Office on Drugs and Crime’s International Homicide Statistics database.\n\n\nCode\ndata_final %>% ggplot(aes(x=GDP_per_Capita)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(GDP_per_Capita,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(GDP_per_Capita,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(GDP_per_Capita,na.rm=TRUE)+\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(GDP_per_Capita,na.rm=TRUE)-\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Gross Domestic Product per Capita\",\n       subtitle=\"Distribution of GDP per capita 2019\",\n       x=\"GDP per Capita\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\ndata_final %>% ggplot(aes(x=Homicides_per_100K)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Homicides_per_100K,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Homicides_per_100K,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Homicides_per_100K,na.rm=TRUE)+\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Homicides_per_100K,na.rm=TRUE)-\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Intentional Homicides per 100K Residents\",\n       subtitle=\"Distribution of homicides per 100K 2019\",\n       x=\"Homicides per 100K\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\nThe World Governance Indicators are a combination of enterprise, citizen, and expert survey respondents from around the world. They use more than 30 surveys to create their six indicators with each indicator using different surveys and different data from each survey to aggregate to the final indicator. The one I am most interested in at this juncture is Voice and Accountability which attempts to measure the level of political freedom (i.e. freedom of speech, press, etc.) and access to participation in goverance by the public (i.e. free and fair elections).\n\n\nCode\ndata_final %>% ggplot(aes(x=WGI_Voice_Accountability)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(WGI_Voice_Accountability,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(WGI_Voice_Accountability,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(WGI_Voice_Accountability,na.rm=TRUE)+\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(WGI_Voice_Accountability,na.rm=TRUE)-\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"World Governance Indicators - Voice and Accountability\",\n       subtitle=\"Distribution of Voice and Accountability 2019\",\n       x=\"Voice and Accountability\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "title": "DACSS 603: Final Part 1",
    "section": "World Values Survey and European Values Survey",
    "text": "World Values Survey and European Values Survey\nThe World Values Survey and European Values Survey collect data by conducting representative surveys in around 100 countries every five years. Their surveys are specifically designed to gather opinions on values ranging from political to religious to social. One of the questions they consistently ask is for respondents to indicate what level of confidence they have in their government. This will be our dependent variable of interest, Confidence in Government. In the plot below it is displayed as a proportion of respondents that said they had at least some confidence in their government.\n\n\nCode\ndata_final %>% ggplot(aes(x=Gov_Confidence)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Gov_Confidence,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Gov_Confidence,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Gov_Confidence,na.rm=TRUE)+\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Gov_Confidence,na.rm=TRUE)-\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Proportion of Population that has Confidence in Government\",\n       subtitle=\"Distribution of Confidence in Government 2017-2020\",\n       x=\"Confidence in Government\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/HW2answers-DonnySnyder.html",
    "href": "posts/HW2answers-DonnySnyder.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\n\n\nCode\nbyPassConfInt <- NA\nbyPassConfInt[1] <- 19 + .9*(10/(sqrt(539)))\nbyPassConfInt[2] <- 19 - .9*(10/(sqrt(539)))\n\nangConfInt <- NA\nangConfInt[1] <- 18 + .9*(9/(sqrt(847)))\nangConfInt[2] <- 18 - .9*(9/(sqrt(847)))\n\nprint(byPassConfInt)\n\n\n[1] 19.38766 18.61234\n\n\nCode\nprint(angConfInt)\n\n\n[1] 18.27832 17.72168\n\n\nThe confidence interval is narrower than for angiography than for bypass surgery.\n\n\nQuestion 2\n\n\nCode\npointEstData <- NA\npointEstData[1:567] <- 1\npointEstData[568:1031] <- 0\npointSD <- sd(pointEstData)\npointEst <- 567/1031\npointConfInt <- NA\npointConfInt[1] <- pointEst + .95*(pointSD/(sqrt(1031)))\npointConfInt[2] <- pointEst - .95*(pointSD/(sqrt(1031)))\nprint(pointConfInt)\n\n\n[1] 0.5646779 0.5352251\n\n\nThe confidence interval here suggests that we can assume with 95% confidence that between 56.5% of adult Americans and 53.5% believe that college education is essential for success.\n\n\nQuestion 3\n\n\nCode\npopSD <- (200 - 30)/4\ncriticalVal <- 1.96\nsampSize <- ((popSD * criticalVal)/5)^2\nprint(sampSize)\n\n\n[1] 277.5556\n\n\nThe size of the sample should be 278.\n\n\nQuestion 4a\nNull hypothesis: Womens income does not deviate from the mean income of senior-level workers.\nAlternative hypothesis: Womens income does deviate from the mean income of senior-level workers.\n\n\nCode\ntStat <- (410 - 500)/(90/(sqrt(9)))\ndegreeFree <- 9-1\n2*pt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.01707168\n\n\nCode\npt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.008535841\n\n\nCode\npt(tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\nThe p value of this test statistic and degrees of freedom is 0.017.\n#4b The p-value for the one-tailed test h0 < 500 is 0.0085. This is half because it is only measuring half of the distribution.\n#4c The p-value for the one-tailed test h0 > 500 is 0.9915. This is because this is measuring in the opposite direction of the actual mean.\n#Question 5\n\n\nCode\ntStatJones <- (519.5 - 500)/(10)\ntStatSmith <- (519.7 - 500)/(10)\ndegreeFree <- 1000-1\n\n2*pt(tStatJones,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.05145555\n\n\nCode\n2*pt(tStatSmith,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.04911426\n\n\n#Question 5b As you can see from the printed values, the Smith study is statistically significant while the Jones study is not.\n#Question 5c If you do not report the p-value, you cannot tell how close the p-value is to being significant, so it can get rid of the value of running the study to not report it.\n#Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ntStatGas <- (mean(gas_taxes) - 45)/(sqrt(sd(gas_taxes)/length(gas_taxes)))\ndegreeFree <- length(gas_taxes) - 1\n\n2*pt(-tStatGas,degreeFree, lower.tail = FALSE)\n\n\n[1] 2.341428e-05\n\n\nYes there is enough evidence. The p-value is far below p = 0.05, at 0.0000234."
  },
  {
    "objectID": "posts/FinalProjectPart2.html",
    "href": "posts/FinalProjectPart2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nstudentsurvey <- read.csv(\"student_prediction.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'student_prediction.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nsummary(studentsurvey)\n\n\nError in summary(studentsurvey): object 'studentsurvey' not found\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nQuestion: Does classroom engagement (i.e., taking notes, attending class, listening) result in a higher GPA in university students?\nHypothesis: Classroom engagement factors (i.e., taking notes, attending class, listening) will have a positive correlation with higher GPA.\nExplanatory Variable: Classroom engagement factors (1) taking notes, 2) attending class, 3) listening)\nResponse Variable: University students’ GPA\n\n\nNotes & GPA\n\n\nCode\nnfit <- lm(NOTES ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(nfit)\n\n\nError in summary(nfit): object 'nfit' not found\n\n\nCode\ncor.test(studentsurvey$NOTES, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$NOTES, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nAttendance & GPA\n\n\nCode\nafit <- lm(ATTEND ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(afit)\n\n\nError in summary(afit): object 'afit' not found\n\n\nCode\ncor.test(studentsurvey$ATTEND, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$ATTEND, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nListening & GPA\n\n\nCode\nlfit <- lm(LISTENS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(lfit)\n\n\nError in summary(lfit): object 'lfit' not found\n\n\nCode\ncor.test(studentsurvey$LISTENS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$LISTENS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nsummary(lm(CUML_GPA ~ NOTES + ATTEND + LISTENS, data = studentsurvey))\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\n\n\n\nImpact of Taking Notes on University Student GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = NOTES, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = NOTES, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nImpact of Class Attendance on GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = ATTEND, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = ATTEND, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nImpact of Listening in Class on GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = LISTENS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = LISTENS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nTo analyze the influence of classroom engagement on student GPA, I chose to run a simple linear regression and a correlation test. I did also conduct a multiple regression analysis, but I preferred to separate the three variables within my definition of “classroom engagement” so I could analyze them individually.\nTaking notes did not appear to have a significant impact on cumulative GPA. The p-value (0.08499) was greater than 0.05, indicating the result was not statistically significant. Additionally, the correlation coefficient was positive, but only slightly (0.1435413). The adjusted r squared also indicated a low correlation (0.01376).\nClass attendance was found to be a statistically significant, as the p-value was less than 0.05 (0.0319). Interestingly, classroom attendance actually had a negative correlation with GPA, indicating that students who attended class less frequently obtained higher GPAs. This correlation is also slight, as indicated by the correlation coefficient (-0.1783047) and the adjusted r squared (0.02502).\nStudents’ reported listening during class was not statistically significant on GPA, with a p-value higher than 0.05 (0.5079). The correlation was also extremely slight, with a positive correlation coefficient of 0.05542742 and an adjusted r squared value of -0.003899.\nThus, my hypothesis that classroom engagement would have a positive influence on GPA would be rejected.\n\n\n\n\nQuestion: Does reported studying (i.e., weekly study hours) result in a higher GPA in university students?\nHypothesis: More hours studied will have a positive impact on student cumulative GPA.\nExplanatory Variable: Hours reported studying a week\nResponse Variable: Cumulative GPA\n\n\n\n\nCode\nshfit <- lm(STUDY_HRS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(shfit)\n\n\nError in summary(shfit): object 'shfit' not found\n\n\nCode\ncor.test(studentsurvey$STUDY_HRS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$STUDY_HRS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nggplot(data = studentsurvey, aes(x = STUDY_HRS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = STUDY_HRS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nLike with my previous research question, I chose to run a simple linear regression and a correlation test to analyze the data. The results indicated that hours spent studying had very little impact on cumulative GPA. The p-value was greater than 0.05 (0.9225), and both the correlation coefficient, although positive, and the adjusted r r squared values were extremely small (0.008144991 and -0.006926). Thus, my hypothesis would be refuted.\n\n\n\n\nQuestion: Does collaboration between students (i.e., studying together, positive class discussions) result in a higher GPA in university students?\nHypothesis: Collaboration between students would have a positive impact on cumulative GPA.\nExplanatory Variable: Collaboration (Studying with peers, perceiving classroom discussions as positive)\nResponse Variable: Cumulative GPA\n\n\nStudying with peers & GPA\n\n\nCode\nstudentsurvey$PREP_STUDY <- ifelse(studentsurvey$PREP_STUDY==2, 2, 1)\n\n\nError in ifelse(studentsurvey$PREP_STUDY == 2, 2, 1): object 'studentsurvey' not found\n\n\nCode\nspfit <- lm(PREP_STUDY ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(spfit)\n\n\nError in summary(spfit): object 'spfit' not found\n\n\nCode\ncor.test(studentsurvey$PREP_STUDY, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$PREP_STUDY, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nPositive discussions & GPA\n\n\nCode\nstudentsurvey$PREP_STUDY <- ifelse(studentsurvey$LIKES_DISCUSS==1, 1, 2)\n\n\nError in ifelse(studentsurvey$LIKES_DISCUSS == 1, 1, 2): object 'studentsurvey' not found\n\n\nCode\nldfit <- lm(LIKES_DISCUSS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(ldfit)\n\n\nError in summary(ldfit): object 'ldfit' not found\n\n\nCode\ncor.test(studentsurvey$LIKES_DISCUSS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$LIKES_DISCUSS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nsummary(lm(CUML_GPA ~ PREP_STUDY + LIKES_DISCUSS, data = studentsurvey))\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\n\n\n\nStudying with peers & GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = PREP_STUDY, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = PREP_STUDY, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nPositive discussions & GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = LIKES_DISCUSS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = LIKES_DISCUSS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nStudents who study with their peers are more likely to have higher GPAs, according to the simple linear regression and correlation test. The p-value was less than 0.05 (0.01535). However, the correlation was not extremely high (0.2009882) and neither was the adjusted r-squared value (0.03369). That being said, the results were statistically significant.\nAdditionally, students who found class discussions to be helpful (always or some of the time, compared to those who did not find class discussions to be a positive experience) to their education and learning were significantly more likely to have higher GPAs. The p-value was less than 0.01 (0.007804). Again the correlation was not extreme (0.2201251) as well as the adjusted r-squared (0.0418).\nThe multiple regression analysis also found the combined two variables to be statistically significant (0.01666). Thus, it could be concluded that collaboration has a positive impact on GPA, supporting my hypothesis."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html",
    "href": "posts/HW1_Saaradhaa.html",
    "title": "Homework 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a",
    "href": "posts/HW1_Saaradhaa.html#a",
    "title": "Homework 1",
    "section": "1 (a)",
    "text": "1 (a)\nReading in the data:\n\n# load packages.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(lsr)\n\n# read in data.\ndf <- read_excel(\"_data/LungCapData.xls\")\n\nDistribution of LungCap:\n\nhist(df$LungCap)\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b",
    "href": "posts/HW1_Saaradhaa.html#b",
    "title": "Homework 1",
    "section": "1 (b)",
    "text": "1 (b)\nThe boxplots below show the probability distributions grouped by gender.\n\nboxplot(LungCap~Gender, data = df)\n\n\n\n\nMales appear to have a slightly greater lung capacity than females."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c",
    "href": "posts/HW1_Saaradhaa.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n# check class of Smoke.\nclass(df$Smoke)\n\n[1] \"character\"\n\n# convert Smoke to factor type.\ndf$Smoke <- as.factor(df$Smoke)\n\n# mean lung capacity for smokers.\ndf %>% select(Smoke, LungCap) %>% group_by(Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nIt does not make sense, as I did not expect smokers to have greater mean lung capacities than non-smokers."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d",
    "href": "posts/HW1_Saaradhaa.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n# check class of Age.\nclass(df$Age)\n\n[1] \"numeric\"\n\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n\n\nMost people seem to be non-smokers, and non-smokers seem to have greater lung capacity."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e",
    "href": "posts/HW1_Saaradhaa.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n# check class of AgeGroup.\nclass(df$AgeGroup)\n\n[1] \"character\"\n\n# convert AgeGroup to factor.\ndf$AgeGroup <- as.factor(df$AgeGroup)\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nNon-smokers have greater mean lung capacity for ages 14-15, 16-17 and 18 and above. Smokers have greater mean lung capacity for age 13 and below, which is different from 1(d). There might be some extreme outliers affecting the results for those age 13 and below."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f",
    "href": "posts/HW1_Saaradhaa.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n# correlation.\ncor(df$LungCap,df$Age)\n\n[1] 0.8196749\n\n# covariance.\ncov(df$LungCap,df$Age)\n\n[1] 8.738289\n\n\nThe value of 0.82 for correlation indicates a strong positive relationship between lung capacity and age - as age increases, lung capacity increases. The covariance is a little harder to interpret - the positive value reflects a positive relationship between lung capacity and age, but it is hard to assess the strength of the relationship, given that covariance ranges from negative infinity to infinity. I would prefer to use correlation in most cases."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a-1",
    "href": "posts/HW1_Saaradhaa.html#a-1",
    "title": "Homework 1",
    "section": "2 (a)",
    "text": "2 (a)\n\na <- 160/810\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b-1",
    "href": "posts/HW1_Saaradhaa.html#b-1",
    "title": "Homework 1",
    "section": "2 (b)",
    "text": "2 (b)\n\nb <- (128+434)/810\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c-1",
    "href": "posts/HW1_Saaradhaa.html#c-1",
    "title": "Homework 1",
    "section": "2 (c)",
    "text": "2 (c)\n\nc <- (128+434+160)/810\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d-1",
    "href": "posts/HW1_Saaradhaa.html#d-1",
    "title": "Homework 1",
    "section": "2 (d)",
    "text": "2 (d)\n\nd <- (64+24)/810\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e-1",
    "href": "posts/HW1_Saaradhaa.html#e-1",
    "title": "Homework 1",
    "section": "2 (e)",
    "text": "2 (e)\n\n# multiply each value of X by its probability and add the products.\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\nThe expected value for the number of prior convictions is 1.2864198. To be more precise, since number of prior convictions should not have decimal places, we can round this down to 1, which is what the line graph showed us as well."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f-1",
    "href": "posts/HW1_Saaradhaa.html#f-1",
    "title": "Homework 1",
    "section": "2 (f)",
    "text": "2 (f)\n\n# calculate required formula for each value of X.\nf1_0 <- ((0-e)^2) * (128/810)\nf1_1 <- ((1-e)^2) * (434/810)\nf1_2 <- ((2-e)^2) * (160/810)\nf1_3 <- ((3-e)^2) * (64/810)\nf1_4 <- ((4-e)^2) * (24/810)\n\n# sum up the above for variance.\nf1 <- f1_0 + f1_1 + f1_2 + f1_3 + f1_4\n\n# square root for SD.\nf2 <- sqrt(f1)\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298. In general, I think it might be more meaningful to calculate mode and proportions when generating descriptive statistics for number of prior convictions."
  },
  {
    "objectID": "posts/Final pt 1.html",
    "href": "posts/Final pt 1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/Final pt 1.html#research-question",
    "href": "posts/Final pt 1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nIn the United States, wage stagnation has become a hot-button issue for many people in various fields of employment. Graduate students have been at the center of this issue in recent years- strikes for wage increases and cost-of-living adjustments have taken place at multiple universities throughout the country. Because PhD students often do not have the time to earn extra income (and their contracts often prohibit them from pursuing work elsewhere), how much they will earn from their stipend is a huge factor in considering where to pursue their research (Powell, 2004; Soar et al., 2022). Knowing how much My research question is: What is the strongest predictor of the value of a PhD stipend?"
  },
  {
    "objectID": "posts/Final pt 1.html#hypothesis",
    "href": "posts/Final pt 1.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nH₀: Cost of living is not the strongest predictor of the value of a PhD stipend.\nH₁: Cost of living is the strongest predictor of the value of a PhD stipend."
  },
  {
    "objectID": "posts/Final pt 1.html#dataset",
    "href": "posts/Final pt 1.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThis dataset is comprised of self-reported survey data collected by PhDStipends.com. Respondents are asked their university, department, academic year, and year in the program. They are also asked whether they receive a 12-month or 9-month salary, gross pay, and required fees. PhDStipends automatically calculators the LW Ratio (living wage ratio), which is the stipend divided by the living wage of the country the university is located in. I will likely need to add additional information for my own analysis.\nThe variables of interest for me are the university, department, and program year.\n\n\nCode\nlibrary(readr)\ncsv <- read_csv(\"~/School/UMASS/DACSS 603/Final Project/csv.csv\")\n\n\nError: '~/School/UMASS/DACSS 603/Final Project/csv.csv' does not exist.\n\n\nCode\nsummary(csv)\n\n\nError in summary(csv): object 'csv' not found\n\n\n\n\nCode\nprint(summarytools::dfSummary(csv,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nError in summarytools::dfSummary(csv, varnumbers = FALSE, plain.ascii = FALSE, : object 'csv' not found\n\n\nBased on this summary, there are some extreme outliers in need of removal, particularly in the Overall Pay column. Interesting, the mean Overall Pay of $27549.4 does not seem unreasonable,."
  },
  {
    "objectID": "posts/Final pt 1.html#references",
    "href": "posts/Final pt 1.html#references",
    "title": "Final Project Proposal",
    "section": "References",
    "text": "References\nLiving Wage Calculator. (n.d.). Retrieved October 10, 2022, from https://livingwage.mit.edu/\nPowell, K. Stipend survival. Nature 428, 102–103 (2004). https://doi.org/10.1038/nj6978-102a\nEmily Roberts & Kyle Roberts. (2022, October 10). PhD stipends Dataset. http://www.phdstipends.com/csv\nSoar, M., Stewart, L., Nissen, S. et al. Sweat Equity: Student Scholarships in Aotearoa New Zealand’s Universities. NZ J Educ Stud (2022). https://doi.org/10.1007/s40841-022-00244-5"
  },
  {
    "objectID": "posts/Homework 2 LJones.html",
    "href": "posts/Homework 2 LJones.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(readr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAngiography\n\n\nCode\na_mean <- 18\na_sd <- 9\na_ss <- 847\n\na_se <- a_sd/sqrt(a_ss)\n\na_cl <- 0.90  \na_tail <- (1-a_cl)/2\na_tscore <- qt(p = 1-a_tail, df = a_ss-1)\n\na_ci <- c(a_mean - a_tscore * a_se,\n        a_mean + a_tscore * a_se)\nprint(a_ci)\n\n\n[1] 17.49078 18.50922\n\n\nBypass\n\n\nCode\nb_mean <- 19\nb_sd <- 10\nb_ss <- 539\n\nb_se <- b_sd/sqrt(b_ss)\n\nb_cl <- 0.90  \nb_tail <- (1-b_cl)/2\nb_tscore <- qt(p = 1-b_tail, df = b_ss-1)\n\nb_ci <- c(b_mean - b_tscore * b_se,\n        b_mean + b_tscore * b_se)\nprint(b_ci)\n\n\n[1] 18.29029 19.70971\n\n\n\n\nCode\n#assessing which confidence interval is larger\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nThe confidence interval is more narrow for angiographies.\n\n\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = American adults (population), x = sample (surveyed)\nn = 1031\nx = 567\n\n#use prop.test to find p (confidence interval is 95% by default)\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\np (the proportion of adult Americans who believe that college education is essential for success) is 0.5499515. The 95% confidence interval is [0.5189682, 0.5805580], meaning we can be 95% certain this interval captured the true proportion.\n\n\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculating standard deviation using the given\n(200-30)/4\n\n\n[1] 42.5\n\n\nSince our significance level is 5%, our confidence level is 95%. A 95% confidence level corresponds to a z-score of 1.96. From here we can calculate the ideal sample size.\n\n\nCode\n#solve 5 = 1.96((200-30)*.25)/sqrt(x)\n\n(((170*.25)/5)*1.96)^2\n\n\n[1] 277.5556\n\n\nUMass needs a sample of approximately 278 students to estimate the mean cost of textbooks.\n\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\n\nAssumptions: normal distribution, significance level .05 H₀: μ=500 H₁: μ≠500 Test statistic: 410\n\n\nCode\n#given\np_mean = 500\ns_mean = 410\ns_size = 9\ns = 90\n\n#find standard error\nsem = s/sqrt(s_size)\n\n#find t-score\nt_score <- (s_mean - p_mean)/sem\nt_score\n\n\n[1] -3\n\n\nCode\n#find p-value\npvalue = 2 * pt(t_score, df=(s_size - 1))\npvalue\n\n\n[1] 0.01707168\n\n\nBecause the p-value is less than the significance level (.05), we reject the null hypothesis that μ=500.\n\n\n\n\n\nCode\n#calculate the p-value for lower tail only\nltail <- pt(t_score, df=(s_size - 1), lower.tail = TRUE)\nltail\n\n\n[1] 0.008535841\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we have evidence that μ < 50.\n\n\n\n\n\nCode\n#calculate the p-value for upper tail only\nutail <- pt(t_score, df=(s_size - 1), lower.tail = FALSE)\nutail\n\n\n[1] 0.9914642\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we do not have evidence that μ > 500.\nChecking my work:\n\n\nCode\nltail + utail\n\n\n[1] 1\n\n\n\n\n\n\nJones and Smith separately conduct studies to test H₀: μ = 500 against H₁ : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n\nJones\n\n\nCode\n#t-score\nJt_score = (519.5 - 500)/10\nJt_score\n\n\n[1] 1.95\n\n\nCode\n#p-value\nJp = 2 * pt(Jt_score, df=(1000 - 1), lower.tail = FALSE)\nJp\n\n\n[1] 0.05145555\n\n\nSmith\n\n\nCode\n#t-score\nSt_score = (519.7 - 500)/10\nSt_score\n\n\n[1] 1.97\n\n\nCode\n#p-value\nSp = 2 * pt(St_score, df=(1000 - 1), lower.tail = FALSE)\nSp\n\n\n[1] 0.04911426\n\n\n\n\n\nAt α = 0.05, Jones’ result is statistically significant (because the p-value is greater than α) but Smith’s result is not.\n\n\n\nJones’ p-value is only just barely greater than 0.05, and Smith’s p-value is only just barely less than 0.05. It is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected, leading to very different conclusions based on data that is extremely similar.\n\n\n\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\nt.test(gas_taxes, mu = 45.0, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nSince the p-value is 0.03827, we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. However, we do not know from what year(s) the data was collected. Therefore we can conclude with certainty that the average tax per gallon in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/HW1answers_DonnySnyder.html",
    "href": "posts/HW1answers_DonnySnyder.html",
    "title": "Homework 1 - Donny Snyder",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = Gender, y = LungCap)) + geom_boxplot()\n\n\n\n\n\nThe probability distribution suggests that the lung capacity of males tends to be higher.\n##c\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity of smokers vs nonsmokers appears to be higher for smokers. This doesn’t really make sense because I’ve been taught to think smokers tend to have reduced lung capacity.\n##d and e\n\n\nCode\nx=1\ndf$AgeGroup <- rep(c(\"NA\"),times=725)\nwhile(x <= 725){\n  if(df$Age[x] <= 13){\n    df$AgeGroup[x] = \"less than or equal to 13\"\n  }\n  else if((df$Age[x] >= 14)&&(df$Age[x] <= 15)){\n    df$AgeGroup[x] = \"14 to 15\"\n  }\n  else if((df$Age[x] >= 16)&&(df$Age[x] <= 17)){\n    df$AgeGroup[x] = \"16 to 17\"\n  }\n  else if(df$Age[x] >= 18){\n    df$AgeGroup[x] = \"greater than 18\"\n  }\nx = x + 1\n}\naggregate(data = df, LungCap~AgeGroup+Smoke, mean)\n\n\n                  AgeGroup Smoke   LungCap\n1                 14 to 15    no  9.138810\n2                 16 to 17    no 10.469805\n3          greater than 18    no 11.068846\n4 less than or equal to 13    no  6.358746\n5                 14 to 15   yes  8.391667\n6                 16 to 17   yes  9.383750\n7          greater than 18   yes 10.513333\n8 less than or equal to 13   yes  7.201852\n\n\nCode\naggregate(data = df,LungCap~AgeGroup+Smoke,length)\n\n\n                  AgeGroup Smoke LungCap\n1                 14 to 15    no     105\n2                 16 to 17    no      77\n3          greater than 18    no      65\n4 less than or equal to 13    no     401\n5                 14 to 15   yes      15\n6                 16 to 17   yes      20\n7          greater than 18   yes      15\n8 less than or equal to 13   yes      27\n\n\nCode\naggregate(data = df,Age~Smoke,mean)\n\n\n  Smoke      Age\n1    no 12.03549\n2   yes 14.77922\n\n\nIt seems like people tend to have a lung capacity that increases with age. However, nonsmokers have a higher lung capacity for each age break down besides less than or equal to 13. It seems like smokers just might tend to be older. I confirmed this by looking at the length and mean ages per group, where you can see a majority of smokers are older, whereas non smokers tend to be younger. The mean age for smokers also tends to be older.\n##f\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity appears to be quite correlated with age. This means that Lung capacity tends to go up as age goes up, and vice versa. This is confirmed also by the covariance.\n#Question 2\n##a\n\n\nCode\nprint((160/810) * 100)\n\n\n[1] 19.75309\n\n\nThe probability is 19.75309% that a randomly selected inmate has exactly 2 prior convictions.\n##b\n\n\nCode\nprint(((434+128)/810) * 100)\n\n\n[1] 69.38272\n\n\nThe probability is 69.38272% that a randomly selected inmate has fewer than 2 prior convictions.\n##c\n\n\nCode\nprint(((160+434+128)/810) * 100)\n\n\n[1] 89.1358\n\n\nThe probability is 89.1358% that a randomly selected inmate has 2 or fewer prior convictions.\n##d\n\n\nCode\nprint(((64+24)/810) * 100)\n\n\n[1] 10.8642\n\n\nThe probability is 10.8642% that a randomly selected inmate has more than 2 prior convictions.\n##e\n\n\nCode\nnewDf <- NA\nnewDf[1:128] <- 0\nnewDf[129:562] <- 1\nnewDf[563:722] <- 2\nnewDf[723:786] <- 3\nnewDf[787:810] <- 4\nnewDf <- as.data.frame(newDf)\nmean(newDf$newDf)\n\n\n[1] 1.28642\n\n\nThe expected value, known as the “mean” when it deals in data that are not probability distributions, is 1.28642. Because I created a vector here, I took the mean, though I also could have calculated the expected value by multiplying the probabilities by the numbers. They are both the same value in this case.\n##f\n\n\nCode\nsd(newDf$newDf)\n\n\n[1] 0.9259016\n\n\nCode\nvar(newDf$newDf)\n\n\n[1] 0.8572937\n\n\nThe variance of prior convictions is 0.8572937, the standard deviation of prior convictions is 0.9259016."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html",
    "href": "posts/MEGHA JOSEPH_BLOG1.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "title": "Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nAccording to statistics,Cardiovascular diseases (CVDs) kill approximately 17 million people globally every year.Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies. People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management. Research question is: Which clinical feature will lead to high cardiovascular risk?"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "title": "Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\n1:Behavioral risk factors will not underline significant predictors of predicting Heart Failure.\n2:Behavioral risk factors will underline significant predictors of predicting Heart Failure ."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "title": "Project Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI am going to analyze a dataset of 299 patients with heart failure collected in 2015.This dataset is comprised of self-reported survey, with 13 clinical features. data.https://www.kaggle.com/datasets/whenamancodes/heart-failure-clinical-records. The important variables of research are ejection fraction, serum creatinine, and smoking.\n.\n\n\nCode\nlibrary(readr)\nmf <- read_csv(\"C:/Users/user/Downloads/Heart Failure Clinical Records.csv\")\nsummary(mf)\n``\n\n\nError: attempt to use zero-length variable name"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "title": "Project Proposal",
    "section": "References",
    "text": "References\n\nSurvival prediction of heart failure patients using machine learning techniques *. (n.d.). Retrieved October 11, 2022, from https://www.sciencedirect.com/science/article/pii/S2352914821002458\n\nMachine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5"
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html",
    "href": "posts/HW1_ToryBartelloni.html",
    "title": "DACSS 603: Homework 1",
    "section": "",
    "text": "First, let’s load our packages and read in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\nlcdata <- read_xls(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1a",
    "href": "posts/HW1_ToryBartelloni.html#q1a",
    "title": "DACSS 603: Homework 1",
    "section": "Q1a",
    "text": "Q1a\nWhat does the distribution of LungCap look like?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_histogram(bins=45) +\n  theme_bw() +\n  labs(x=\"Lung Capacity\", y=\"Frequency\", \n       title = \"Lung Capacity Distribution\")\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1b",
    "href": "posts/HW1_ToryBartelloni.html#q1b",
    "title": "DACSS 603: Homework 1",
    "section": "Q1b",
    "text": "Q1b\nCompare the probability density of the LungCap with respect to Males and Females.\n\n\nCode\nlcdata %>%\n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(group=Gender, fill=Gender)) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(x=\"Lung Capacity\", title = \"Lung Capacity Distribution\",\n       subtitle = \"Comparing lung capacity between genders\")\n\n\n\n\n\nThe boxplot comparison indicates that on average males have larger lung capacity, but it also shows that the range and IQR for each gender are similar and have a significant amount of overlap."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1c",
    "href": "posts/HW1_ToryBartelloni.html#q1c",
    "title": "DACSS 603: Homework 1",
    "section": "Q1c",
    "text": "Q1c\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers\")\n\n\n\n\n\nComparing the distributions shows that Smokers have a higher mean lung capacity and a significantly smaller range and IQR. This does not make sense intuitively so I would want to investigate the data a bit more to understand the possible reasons."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1d",
    "href": "posts/HW1_ToryBartelloni.html#q1d",
    "title": "DACSS 603: Homework 1",
    "section": "Q1d",
    "text": "Q1d\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nlc_with_age_groups <- lcdata %>%\n  mutate(Age_Group = factor(case_when(\n    Age <= 13 ~ \"<=13\",\n    Age %in% c(14,15) ~ \"14-15\",\n    Age %in% c(16,17) ~ \"16-17\",\n    Age >= 18 ~ \">=18\"\n      ),\n    levels = c(\"<=13\",\"14-15\",\"16-17\",\">=18\")\n    )\n  )\n\nlc_with_age_groups %>% \n  ggplot(aes(x=Age_Group,y=LungCap)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nComparing age groups shows a consistent and clear increase in lung capacity as ages increase up to and over 18 years old."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1e",
    "href": "posts/HW1_ToryBartelloni.html#q1e",
    "title": "DACSS 603: Homework 1",
    "section": "Q1e",
    "text": "Q1e\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\n\n\nCode\nlc_with_age_groups %>%\n  ggplot(aes(x=Smoke, y=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  facet_wrap(~Age_Group) +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers within age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nOutside of ages 13 and under all ages groups show higher average, range, and IRQ for non-smokers. It seems likely that the youngest age group, 13 and under, has the largest number of observations of non-smokers which is bringing down the overall average and lower end of the range. This effect is what is causing us to see the higher lung capacity in smokers overall, but we can infer that the causal factor is more likely age than smoking."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1f",
    "href": "posts/HW1_ToryBartelloni.html#q1f",
    "title": "DACSS 603: Homework 1",
    "section": "Q1f",
    "text": "Q1f\nCalculate the correlation and covariance between Lung Capacity and Age (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nknitr::kable(\n  lcdata %>% summarise(Covariance = cov(LungCap, Age),\n                     Correlation = cor(LungCap, Age)),\n  caption = \"Relationship between lung capacity and age.\"\n)\n\n\n\nRelationship between lung capacity and age.\n\n\nCovariance\nCorrelation\n\n\n\n\n8.738289\n0.8196749\n\n\n\n\n\nThe covariance shows us that the relationship is positive and the correlation coefficient shows us that the relationship is a strong, positive relationship. So the older the people in the data the larger the lung capacity was observed, on average."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2a",
    "href": "posts/HW1_ToryBartelloni.html#q2a",
    "title": "DACSS 603: Homework 1",
    "section": "Q2a",
    "text": "Q2a\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nprison_props <-  prison_data %>% group_by(X) %>%\n    summarise(Frequency = Frequency,\n      Proportion = Frequency / sum(prison_data$Frequency))\nknitr::kable(prison_props,\n  caption=\"Proportion of Inmates\"\n\n)\n\n\n\nProportion of Inmates\n\n\nX\nFrequency\nProportion\n\n\n\n\n0\n128\n0.1580247\n\n\n1\n434\n0.5358025\n\n\n2\n160\n0.1975309\n\n\n3\n64\n0.0790123\n\n\n4\n24\n0.0296296\n\n\n\n\n\nBy calculating the proportion of inmates with each number of prior convictions we can see that the probability of randomly selecting an inmate with 2 prior convictions is 0.1975 or about 19.8%."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2b",
    "href": "posts/HW1_ToryBartelloni.html#q2b",
    "title": "DACSS 603: Homework 1",
    "section": "Q2b",
    "text": "Q2b\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nprint(paste(\"Probability of fewer than 2 prior convictions:\",\nsum(filter(prison_props, X < 2)$Proportion)))\n\n\n[1] \"Probability of fewer than 2 prior convictions: 0.693827160493827\"\n\n\nSumming prisoners with zero and one prior conviction provides us a probability that 0.6938 or about 69.4% chance that a randomly selected inmate would have less than 2 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2c",
    "href": "posts/HW1_ToryBartelloni.html#q2c",
    "title": "DACSS 603: Homework 1",
    "section": "Q2c",
    "text": "Q2c\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nprint(paste(\"Probability of 2 or fewer prior convictions:\",\n            sum(filter(prison_props, X <=2)$Proportion)))\n\n\n[1] \"Probability of 2 or fewer prior convictions: 0.891358024691358\"\n\n\nSumming the prisoners with two or fewer prior convictions gives us the probability that 0.89 or about 89% probability that a randomly selected inmate would have two prior convictions or fewer."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2d",
    "href": "posts/HW1_ToryBartelloni.html#q2d",
    "title": "DACSS 603: Homework 1",
    "section": "Q2d",
    "text": "Q2d\nWhat is the probability that a randomly selected inmate has more than two prior convictions?\n\n\nCode\nprint(paste(\"Probability of more than 2 prior convictions:\",\nsum(filter(prison_props, X > 2)$Proportion)))\n\n\n[1] \"Probability of more than 2 prior convictions: 0.108641975308642\"\n\n\nThe probability found for either 3 or 4 prior convictions (there is no inmate with more than 4 prior convictions) is 0.1084 or about 10.8% probability."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2e",
    "href": "posts/HW1_ToryBartelloni.html#q2e",
    "title": "DACSS 603: Homework 1",
    "section": "Q2e",
    "text": "Q2e\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nprint(paste(\"The expected value for prior convictions:\", mean(prison_indi_data$X)))\n\n\n[1] \"The expected value for prior convictions: 1.28641975308642\"\n\n\nBy taking a weighted average or an average of all possible observations to select from the expected value is 1.28642 or about 1.3 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2f",
    "href": "posts/HW1_ToryBartelloni.html#q2f",
    "title": "DACSS 603: Homework 1",
    "section": "Q2f",
    "text": "Q2f\nCalculate the variance and standard deviation for Prior Convictions.\n\n\nCode\nknitr::kable(\n  prison_indi_data %>% summarise(Variance = var(X),\n                     \"Standard Deviation\" = sd(X)),\n  caption = \"Spread of Inmate Prior Convictions\"\n)\n\n\n\nSpread of Inmate Prior Convictions\n\n\nVariance\nStandard Deviation\n\n\n\n\n0.8572937\n0.9259016"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html",
    "href": "posts/FinalProject2_EthanCampbell.html",
    "title": "Final Project",
    "section": "",
    "text": "Climate has always been a topic that sparks debate and there is continuous research being done on it every day. I wanted to contribute to this research and analyze the impacts climate factors like temperature and humidity have on bike users. There has been study related to weather conditions and biking and whether or not it results in more accidents which concluded in an increase in accidents. “It suggests that weather conditions should be considered in every analysis where bicycle volume data is needed” (Pazdan, 2020). The paper describes the importance of weather condition and how they should be used in any analysis regarding biking data. Here, I thought about if we know that these factors are important, how important are they? I can find this data online somewhere however, I would like to conduct my own study and determine the results from that and then compare it to results online.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. How has temperature and humidity impacted bike users?\n\n\nMy motivation is driven by my own interest in biking and climate factors and the study of how the climate is impacting human movement. I think it is interesting how significant climate factors can impact certain human activities and I want to learn more about which ones are presenting the largest impact on human activity. The reason this study is different is that it is based on normalized data and is focusing how bikers are impacted by weather condition."
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#reading-in-the-data",
    "href": "posts/FinalProject2_EthanCampbell.html#reading-in-the-data",
    "title": "Final Project",
    "section": "Reading in the data",
    "text": "Reading in the data\n\n\nCode\nbike <- read.csv(\"_data/hour.csv\")\nbike2 <- read.csv(\"_data/day.csv\")\n\ndim(bike)\n\n\n[1] 17379    17\n\n\nCode\ndim(bike2)\n\n\n[1] 731  16\n\n\nCode\nsummary(bike)\n\n\n    instant         dteday              season            yr        \n Min.   :    1   Length:17379       Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 4346   Class :character   1st Qu.:2.000   1st Qu.:0.0000  \n Median : 8690   Mode  :character   Median :3.000   Median :1.0000  \n Mean   : 8690                      Mean   :2.502   Mean   :0.5026  \n 3rd Qu.:13034                      3rd Qu.:3.000   3rd Qu.:1.0000  \n Max.   :17379                      Max.   :4.000   Max.   :1.0000  \n      mnth              hr           holiday           weekday     \n Min.   : 1.000   Min.   : 0.00   Min.   :0.00000   Min.   :0.000  \n 1st Qu.: 4.000   1st Qu.: 6.00   1st Qu.:0.00000   1st Qu.:1.000  \n Median : 7.000   Median :12.00   Median :0.00000   Median :3.000  \n Mean   : 6.538   Mean   :11.55   Mean   :0.02877   Mean   :3.004  \n 3rd Qu.:10.000   3rd Qu.:18.00   3rd Qu.:0.00000   3rd Qu.:5.000  \n Max.   :12.000   Max.   :23.00   Max.   :1.00000   Max.   :6.000  \n   workingday       weathersit         temp           atemp       \n Min.   :0.0000   Min.   :1.000   Min.   :0.020   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.340   1st Qu.:0.3333  \n Median :1.0000   Median :1.000   Median :0.500   Median :0.4848  \n Mean   :0.6827   Mean   :1.425   Mean   :0.497   Mean   :0.4758  \n 3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.660   3rd Qu.:0.6212  \n Max.   :1.0000   Max.   :4.000   Max.   :1.000   Max.   :1.0000  \n      hum           windspeed          casual         registered   \n Min.   :0.0000   Min.   :0.0000   Min.   :  0.00   Min.   :  0.0  \n 1st Qu.:0.4800   1st Qu.:0.1045   1st Qu.:  4.00   1st Qu.: 34.0  \n Median :0.6300   Median :0.1940   Median : 17.00   Median :115.0  \n Mean   :0.6272   Mean   :0.1901   Mean   : 35.68   Mean   :153.8  \n 3rd Qu.:0.7800   3rd Qu.:0.2537   3rd Qu.: 48.00   3rd Qu.:220.0  \n Max.   :1.0000   Max.   :0.8507   Max.   :367.00   Max.   :886.0  \n      cnt       \n Min.   :  1.0  \n 1st Qu.: 40.0  \n Median :142.0  \n Mean   :189.5  \n 3rd Qu.:281.0  \n Max.   :977.0"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#data",
    "href": "posts/FinalProject2_EthanCampbell.html#data",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nThe data was collected from UCI machine learning repository. Where I collected for both daily and hourly information for 17 variables. Each variable is described below. Important variables will be in bold. Their relationship towards the analysis will be described underneath this section.\n\ninstant - This is the record index. This is the count of how many rows there are. This object will not be utilized in this study\ndteday - This is the date. The date is currently in year-day-month format. This will be used to observe change over time\nseason - This is the 4 seasons. This is expressed as: 1-Winter, 2-Spring, 3-Summer, 4- Fall. This will be used as a control variable since this could impact the sales of bikes and the independent variables\nyr - This is the year ranging from 2011-2012. This will be used analyze change over each year.\nmnth - This is the month 1-12. This will be used to analyze the change over months.\nhr - This is hour 0-23. This will be used to analyze the change by the hour.\nholiday - This is the holidays so whether or not it is a holiday. This will be used as an independent variable in conjunction with another\nweekday - Day of the week.\nworkingday - if day is neither weekend nor holiday is 1, otherwise is 0.\nweathersit -\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\natemp - Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\nhum - Normalized humidity. The values are divided to 100 (max)\nwindspeed - Normalized wind speed. The values are divided to 67 (max)\ncasual - count of casual users\nregistered - count of registered users\ncnt - count of total rental bikes including both casual and registered\n\nRegression 1\n\nExplanatory - normalized temperature feeling\nOutcome - cnt\nControl - Season, holiday, weekday, weathersit\n\nRegression 2\n\nExplanatory - Normalized humidity\nOutcome - cnt\nControl - temperature(normalized and normalized feeling), wind speed, weekday, holiday, weathersit\n\nRegression Model\nInteraction Terms\nI do not believe I need to use a quardratic or anything like that however, there is heteroskedascity so using a log may help with the funneling?\n\\[\n\\hat{Y} = b0 + b1X1+b2X2\n\\]\nRegression Model 1\n\\[\nCnt = FeelingTemperature + Humidity + Season + Week Day + Hour + Year + Holiday + Windspeed\n\\] Regression Model 2\n\\[\nCnt = FeelingTemperature + Humidity + Season + Week Day + Weather Type + Year + Holiday + Windspeed\n\\]"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#cleaning-the-data",
    "href": "posts/FinalProject2_EthanCampbell.html#cleaning-the-data",
    "title": "Final Project",
    "section": "Cleaning the data",
    "text": "Cleaning the data\n\n\nCode\nbike <- bike %>%\n  dplyr::rename('Date' = dteday) %>%\n  dplyr::rename('Year' = yr) %>%\n  dplyr::rename('Month' = mnth) %>%\n  dplyr::rename('Hour' = hr) %>%\n  dplyr::rename('Normalized_temperature_C' = temp) %>%\n  dplyr::rename('Normalized_feeling_temperature_C' = atemp) %>%\n  dplyr::rename('Normalized_Humidity' = hum) %>%\n  dplyr::rename(\"Total_bike_users\" = cnt)\n\nbike2 <- bike2 %>%\n  dplyr::rename('Date' = dteday) %>%\n  dplyr::rename('Year' = yr) %>%\n  dplyr::rename('Month' = mnth) %>%\n  dplyr::rename('Normalized_temperature_C' = temp) %>%\n  dplyr::rename('Normalized_feeling_temperature_C' = atemp) %>%\n  dplyr::rename('Normalized_Humidity' = hum) %>%\n  dplyr::rename(\"Total_bike_users\" = cnt)\n\nbike2$Date <- ymd(bike2$Date)\n\n# Checking for multicollinearity (We notice that temp and feeling temp are almost identical) so we removed the normalized temperature form the study since I want to focus on feeling temperature. Also removing instant since it is just the count of rows.\n\nbike <- bike[,-11]\nbike2 <- bike2[,-10]\n\ncor(bike[3:16])\n\n\n                                       season         Year        Month\nseason                            1.000000000 -0.010742486  0.830385892\nYear                             -0.010742486  1.000000000 -0.010472929\nMonth                             0.830385892 -0.010472929  1.000000000\nHour                             -0.006116901 -0.003867005 -0.005771909\nholiday                          -0.009584526  0.006691617  0.018430325\nweekday                          -0.002335350 -0.004484851  0.010400061\nworkingday                        0.013743102 -0.002196005 -0.003476922\nweathersit                       -0.014523552 -0.019156853  0.005399522\nNormalized_feeling_temperature_C  0.319379811  0.039221595  0.208096131\nNormalized_Humidity               0.150624745 -0.083546421  0.164411443\nwindspeed                        -0.149772751 -0.008739533 -0.135386323\ncasual                            0.120206447  0.142778528  0.068457301\nregistered                        0.174225633  0.253684310  0.122272967\nTotal_bike_users                  0.178055731  0.250494899  0.120637760\n                                         Hour      holiday      weekday\nseason                           -0.006116901 -0.009584526 -0.002335350\nYear                             -0.003867005  0.006691617 -0.004484851\nMonth                            -0.005771909  0.018430325  0.010400061\nHour                              1.000000000  0.000479136 -0.003497739\nholiday                           0.000479136  1.000000000 -0.102087791\nweekday                          -0.003497739 -0.102087791  1.000000000\nworkingday                        0.002284998 -0.252471370  0.035955071\nweathersit                       -0.020202528 -0.017036113  0.003310740\nNormalized_feeling_temperature_C  0.133749965 -0.030972737 -0.008820945\nNormalized_Humidity              -0.276497828 -0.010588465 -0.037158268\nwindspeed                         0.137251568  0.003987632  0.011501545\ncasual                            0.301201730  0.031563628  0.032721415\nregistered                        0.374140710 -0.047345424  0.021577888\nTotal_bike_users                  0.394071498 -0.030927303  0.026899860\n                                   workingday   weathersit\nseason                            0.013743102 -0.014523552\nYear                             -0.002196005 -0.019156853\nMonth                            -0.003476922  0.005399522\nHour                              0.002284998 -0.020202528\nholiday                          -0.252471370 -0.017036113\nweekday                           0.035955071  0.003310740\nworkingday                        1.000000000  0.044672224\nweathersit                        0.044672224  1.000000000\nNormalized_feeling_temperature_C  0.054667235 -0.105563108\nNormalized_Humidity               0.015687512  0.418130329\nwindspeed                        -0.011829789  0.026225652\ncasual                           -0.300942486 -0.152627885\nregistered                        0.134325791 -0.120965520\nTotal_bike_users                  0.030284368 -0.142426138\n                                 Normalized_feeling_temperature_C\nseason                                                0.319379811\nYear                                                  0.039221595\nMonth                                                 0.208096131\nHour                                                  0.133749965\nholiday                                              -0.030972737\nweekday                                              -0.008820945\nworkingday                                            0.054667235\nweathersit                                           -0.105563108\nNormalized_feeling_temperature_C                      1.000000000\nNormalized_Humidity                                  -0.051917696\nwindspeed                                            -0.062336043\ncasual                                                0.454080065\nregistered                                            0.332558635\nTotal_bike_users                                      0.400929304\n                                 Normalized_Humidity    windspeed      casual\nseason                                    0.15062475 -0.149772751  0.12020645\nYear                                     -0.08354642 -0.008739533  0.14277853\nMonth                                     0.16441144 -0.135386323  0.06845730\nHour                                     -0.27649783  0.137251568  0.30120173\nholiday                                  -0.01058846  0.003987632  0.03156363\nweekday                                  -0.03715827  0.011501545  0.03272142\nworkingday                                0.01568751 -0.011829789 -0.30094249\nweathersit                                0.41813033  0.026225652 -0.15262788\nNormalized_feeling_temperature_C         -0.05191770 -0.062336043  0.45408007\nNormalized_Humidity                       1.00000000 -0.290104895 -0.34702809\nwindspeed                                -0.29010490  1.000000000  0.09028678\ncasual                                   -0.34702809  0.090286775  1.00000000\nregistered                               -0.27393312  0.082320847  0.50661770\nTotal_bike_users                         -0.32291074  0.093233784  0.69456408\n                                  registered Total_bike_users\nseason                            0.17422563       0.17805573\nYear                              0.25368431       0.25049490\nMonth                             0.12227297       0.12063776\nHour                              0.37414071       0.39407150\nholiday                          -0.04734542      -0.03092730\nweekday                           0.02157789       0.02689986\nworkingday                        0.13432579       0.03028437\nweathersit                       -0.12096552      -0.14242614\nNormalized_feeling_temperature_C  0.33255864       0.40092930\nNormalized_Humidity              -0.27393312      -0.32291074\nwindspeed                         0.08232085       0.09323378\ncasual                            0.50661770       0.69456408\nregistered                        1.00000000       0.97215073\nTotal_bike_users                  0.97215073       1.00000000"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#residuals-vs-fittedwill-need-to-be-adjusted",
    "href": "posts/FinalProject2_EthanCampbell.html#residuals-vs-fittedwill-need-to-be-adjusted",
    "title": "Final Project",
    "section": "Residuals vs Fitted(Will need to be adjusted)",
    "text": "Residuals vs Fitted(Will need to be adjusted)\nThis shows signs of heteroskedasticity and this is when standard deviations of a predicated variable being monitored over different values of an independent variable are non-constant. The problems that arise from this issue is, the standard error is wrong and thus the confidence intervals and hypothesis tests can not be relied on. This issues needs to be resolved before declaring the conclusion.\n\n\nCode\nmod <- lm(Total_bike_users ~ .-Date -casual -registered, data = bike)\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = Total_bike_users ~ . - Date - casual - registered, \n    data = bike)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-392.56  -93.49  -27.65   60.96  641.27 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -3.109e+01  7.337e+00  -4.237 2.27e-05 ***\ninstant                          -4.527e-03  5.126e-03  -0.883  0.37719    \nseason                            1.993e+01  1.819e+00  10.959  < 2e-16 ***\nYear                              1.207e+02  4.487e+01   2.690  0.00714 ** \nMonth                             3.283e+00  3.775e+00   0.870  0.38457    \nHour                              7.671e+00  1.650e-01  46.477  < 2e-16 ***\nholiday                          -2.151e+01  6.692e+00  -3.214  0.00131 ** \nweekday                           1.928e+00  5.403e-01   3.569  0.00036 ***\nworkingday                        4.031e+00  2.396e+00   1.683  0.09245 .  \nweathersit                       -3.334e+00  1.904e+00  -1.751  0.07994 .  \nNormalized_feeling_temperature_C  3.197e+02  6.777e+00  47.176  < 2e-16 ***\nNormalized_Humidity              -1.989e+02  6.880e+00 -28.906  < 2e-16 ***\nwindspeed                         4.612e+01  9.401e+00   4.906 9.39e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 141.9 on 17366 degrees of freedom\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3883 \nF-statistic: 920.3 on 12 and 17366 DF,  p-value: < 2.2e-16\n\n\n\nResolving Heteroskedasticity\n\n\n\n\n\n\nH0A\n\n\n\nThere is no Heteroskedasticity\n\n\n\n\n\n\n\n\nH1A\n\n\n\nThere is Heteroskedasticity\n\n\nHere we will conduct the Breusch-Pagan test using the lmtest package and bptest() function. This will let us know if there is heteroskedascity if the P < .05. Here we see that both meet this standard and thus we have evidence to reject the null hypothesis\n\n\nCode\n# Breusch-Pagan test to determine if Heteroskedasticity exist\nbptest(Regression1)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  Regression1\nBP = 1450.9, df = 8, p-value < 2.2e-16\n\n\nCode\nbptest(Regression2)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  Regression2\nBP = 47.248, df = 8, p-value = 1.375e-07\n\n\nCode\n# both of these are p<.05 meaning that we reject the null hypothesis and say yes there is heteroskedacity"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#visualizations",
    "href": "posts/FinalProject2_EthanCampbell.html#visualizations",
    "title": "Final Project",
    "section": "Visualizations",
    "text": "Visualizations\nHere we see as temperature increases we can expect bike users to increase while as humidity increases we expect the opposite.\n\n\nCode\nplot(Regression1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(regression2)\n\n\nError in plot(regression2): object 'regression2' not found\n\n\nCode\nBike_users_plot <- bike2 %>%\n  ggplot(aes(x=Date, y=Total_bike_users)) +\n    geom_area(fill=\"#69b3a2\", alpha=0.5) +\n    geom_line(color=\"#69b3a2\") +\n    ylab(\"total bike user\") +\n    theme_ipsum()\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nCode\n# Making it interactive\nBike_users_plot <- ggplotly(Bike_users_plot)\n\n\nError in ggplotly(Bike_users_plot): object 'Bike_users_plot' not found\n\n\nCode\nBike_users_plot\n\n\nError in eval(expr, envir, enclos): object 'Bike_users_plot' not found\n\n\nCode\n# Value used to transform the data\ncoeff <- 10000\n\n# A few constants\ntemperatureColor <- \"#69b3a2\"\npriceColor <- rgb(0.2, 0.6, 0.9, 1)\n\nggplot(bike2, aes(x=Date)) +\n  \n  geom_line( aes(y=Normalized_feeling_temperature_C), size=2, color=temperatureColor) + \n  geom_line( aes(y=Total_bike_users / coeff), size=2, color=priceColor) +\n  \n  scale_y_continuous(\n    \n    # Features of the first axis\n    name = \"Temperature (Normalized)\",\n    \n    # Add a second axis and specify its features\n    sec.axis = sec_axis(~.*coeff, name=\"Bike users\")\n  ) + \n  \n  theme_ipsum() +\n\n  theme(\n    axis.title.y = element_text(color = temperatureColor, size=13),\n    axis.title.y.right = element_text(color = priceColor, size=13)\n  ) +\n\n  ggtitle(\"Temperature correlation with bikes users\")\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nCode\n# Value used to transform the data\ncoeff <- 10000\n\n# A few constants\nhumidityColor <- \"#69b3a2\"\nbikeColor <- rgb(0.2, 0.6, 0.9, 1)\n\nggplot(bike2, aes(x=Date)) +\n  \n  geom_line( aes(y=Normalized_Humidity), size=2, color=temperatureColor) + \n  geom_line( aes(y=Total_bike_users / coeff), size=2, color=priceColor) +\n  \n  scale_y_continuous(\n    \n    # Features of the first axis\n    name = \"Humidity (Normalized)\",\n    \n    # Add a second axis and specify its features\n    sec.axis = sec_axis(~.*coeff, name=\"Bike users\")\n  ) + \n  \n  theme_ipsum() +\n\n  theme(\n    axis.title.y = element_text(color = temperatureColor, size=13),\n    axis.title.y.right = element_text(color = priceColor, size=13)\n  ) +\n\n  ggtitle(\"Humidity correlation with bikes users\")\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nCode\n# plotting the data to visualize \nggplot(data = bike2, aes(x=Normalized_feeling_temperature_C, y = Total_bike_users)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_fivethirtyeight(base_size = 10, base_family = 'serif') +\n  theme(axis.title = element_text(family = 'serif', size = 15)) + ylab('Total Bike Users') + xlab('Normalized Feeling Temperature') +\n  labs(title = \"Relationship between Temerpature and Bike users\", caption = \"\")\n\n\nError in theme_fivethirtyeight(base_size = 10, base_family = \"serif\"): could not find function \"theme_fivethirtyeight\"\n\n\nCode\nggplot(data = bike2, aes(x=Normalized_Humidity, y = Total_bike_users)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_fivethirtyeight(base_size = 10, base_family = 'serif') +\n  theme(axis.title = element_text(family = 'serif', size = 15)) + ylab('Total Bike Users') + xlab('Normalized Humidity') +\n  labs(title = \"Relationship between Humidity and Bike users\", caption = \"\")\n\n\nError in theme_fivethirtyeight(base_size = 10, base_family = \"serif\"): could not find function \"theme_fivethirtyeight\"\n\n\nCode\nggpairs(bike2, columns = c(10, 11, 15), ggplot2::aes(colour='red'))\n\n\nError in ggpairs(bike2, columns = c(10, 11, 15), ggplot2::aes(colour = \"red\")): could not find function \"ggpairs\""
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#conclusions",
    "href": "posts/FinalProject2_EthanCampbell.html#conclusions",
    "title": "Final Project",
    "section": "Conclusions",
    "text": "Conclusions\nIn conclusion, for hypothesis one we reject the null hypothesis with evidence of the extremely significant p-value of 2e-16. This give us evidence that we can accept the alternative and say yes temperature has an impact on bike sales. There were two different tests done here, we did the daily data compared to the hourly data. Both were significant and we controlled for 7 variables which are specified at the top. The reason we controlled these variables is that they could impact the outcome variable and thus we controlled them to make sure that they were not impacting the results. So in conclusion, normalized feeling temperature Celsius has an impact on bike users. Looking at our correlation graph we can see that it has a positive correlation with bike users at .631.\nFor the second questions we can also reject the null hypothesis as humidity is significant with a p-value of < 2e-16 in data set one and 0.000323 in data set two. This is further evidence that we can reject the null as in both scales is what significant. This data held the same control variables as temperature and thus we can yes humidity has an impact on bike users and looking at our correlation we see a negative correlation of -.101.\nOnce I correct the heteroskedascity then I am curious as to how the information is altered as of right now I am unable to fully accept my claims until these are removed."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html",
    "href": "posts/HW3_EmmaRasmussen.html",
    "title": "HW3_EmmaRasmussen",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(alr4)\nlibrary(smss)\nlibrary(stargazer)\n\nError in library(stargazer): there is no package called 'stargazer'"
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section",
    "href": "posts/HW3_EmmaRasmussen.html#section",
    "title": "HW3_EmmaRasmussen",
    "section": "1.1.1",
    "text": "1.1.1\nThe predictor is ppgdp, and the response variable is fertility. ## 1.1.2\n\ndata(list=\"UN11\")\n\nplot(x=UN11$ppgdp, y=UN11$fertility)\n\n\n\n\nA straight line function would not be a good model for this graph. It appears that ppgdp has the biggest impact on fertility towards the left side of the graph (closer to x=0). In other words, ppgd has the biggest impact on fertility in lower ppgdp values and then does not change as drastically as ppgdp gets even larger (right side of the graph). ## 1.1.3\n\nplot(x=log(UN11$ppgdp), y=log(UN11$fertility))\n\n\n\n\nA logarithmic function makes a lot more sense for this data frame in order to apply a linear regression model. When both variables are logged, the data appears more linear and has a negative trend."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#a",
    "href": "posts/HW3_EmmaRasmussen.html#a",
    "title": "HW3_EmmaRasmussen",
    "section": "2a",
    "text": "2a\nI created an example data frame to explore this question. According to the output of the lm() function, the slopes are different (the one multiplied by 1.33 has a greater slope). The plots appear to have the same slope, however the y scale is different which likely explains why the lm() function gives different slopes.\n\ndfexample<-data.frame(col1=c(2004, 2005, 2006, 2007, 2008, 2009, 2010),\ncol2=c(50000, 56000, 70000, 68000, 58000, 72000, 80000),\ncol3=col2*1.33)\n\nError in data.frame(col1 = c(2004, 2005, 2006, 2007, 2008, 2009, 2010), : object 'col2' not found\n\ndfexample\n\nError in eval(expr, envir, enclos): object 'dfexample' not found\n\nlm(col2~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nlm(col3~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nfit2<-lm(col2~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nfit3<-lm(col3~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nsummary(fit2)\n\nError in summary(fit2): object 'fit2' not found\n\nsummary(fit3)\n\nError in summary(fit3): object 'fit3' not found\n\nplot(x=dfexample$col1, y=dfexample$col2)\n\nError in plot(x = dfexample$col1, y = dfexample$col2): object 'dfexample' not found\n\nplot(x=dfexample$col1, y=dfexample$col3)\n\nError in plot(x = dfexample$col1, y = dfexample$col3): object 'dfexample' not found"
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#b",
    "href": "posts/HW3_EmmaRasmussen.html#b",
    "title": "HW3_EmmaRasmussen",
    "section": "2b",
    "text": "2b\nThe correlation (adjusted R squared) is the same for both models. See above."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section-1",
    "href": "posts/HW3_EmmaRasmussen.html#section-1",
    "title": "HW3_EmmaRasmussen",
    "section": "3",
    "text": "3\n\ndata(list=\"water\")\npairs(water[2:8])\n\n\n\n\nI don’t know if I am interpreting this correctly but using this matrix we can see which site correlates most closely with stream runoff (BSAAM). Using this matrix, we see there is a strong correlation between OPSLAKE, OPRC and OPBPC site precipitation and runoff. Perhaps precipitation measured at these sites could predict runoff. Moving forward, I might fit a models using those three sites to predict runoff at the site near bishop and figure out which model creates the best prediction (has the highest F statistic)."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section-2",
    "href": "posts/HW3_EmmaRasmussen.html#section-2",
    "title": "HW3_EmmaRasmussen",
    "section": "4",
    "text": "4\n\ndata(list=\"Rateprof\")\npairs(Rateprof[8:12])\n\n\n\n\nThere is a strong positive correlation between quality and helpfulness, quality and clarity, and clarity and helpfulness. In other words, professors that rate high in one of these areas are likely to rate high in the others. easiness is less strongy correlated with quality, helpfulness and clarity, but there is still a positive relationship (i.e. professors with “easy” courses are more likely to rate higher in other categories but this trend is less strong). Finally, raterInterest does not predict the other ratings very well. Easiness does not appears to have much of a correlation with rater interest. There is a positive relationship between rater interest and quality, helpfulness, and clarity, but again it is not a strong relationship.\n##5a\n\ndata(list=\"student.survey\")\nggplot(student.survey, aes(x=re, y=pi)) +geom_point()\n\n\n\nggplot(student.survey, aes(x= tv, y=hi))+geom_point()+geom_smooth(method=\"lm\")\n\n\n\n\nPolitical Affiliation and Religiosity: This graph is not super useful given there are multiple observations contained in each point on the graph but even so, it appears that more frequently attending religious services correlates with more conservative political ideology.\nTV and GPA: There appears to be a negative correlation between time spent watching tv and high school gpa.\n##5b\n\nstudent.survey\n\n   subj ge ag  hi  co   dh    dr   tv sp ne ah    ve pa                    pi\n1     1  m 32 2.2 3.5    0  5.00  3.0  5  0  0 FALSE  r          conservative\n2     2  f 23 2.1 3.5 1200  0.30 15.0  7  5  6 FALSE  d               liberal\n3     3  f 27 3.3 3.0 1300  1.50  0.0  4  3  0 FALSE  d               liberal\n4     4  f 35 3.5 3.2 1500  8.00  5.0  5  6  3 FALSE  i              moderate\n5     5  m 23 3.1 3.5 1600 10.00  6.0  6  3  0 FALSE  i          very liberal\n6     6  m 39 3.5 3.5  350  3.00  4.0  5  7  0 FALSE  d               liberal\n7     7  m 24 3.6 3.7    0  0.20  5.0 12  4  2 FALSE  i               liberal\n8     8  f 31 3.0 3.0 5000  1.50  5.0  3  3  1 FALSE  i               liberal\n9     9  m 34 3.0 3.0 5000  2.00  7.0  5  3  0 FALSE  i          very liberal\n10   10  m 28 4.0 3.1  900  2.00  1.0  1  2  1 FALSE  i      slightly liberal\n11   11  m 23 2.3 2.6  253  1.50 10.0 15  1  1 FALSE  r slightly conservative\n12   12  f 27 3.5 3.6  190  3.00 14.0  3  7  0 FALSE  d               liberal\n13   13  m 36 3.3 3.5  245  1.50  6.0 15 12  5 FALSE  d          very liberal\n14   14  m 28 3.2 3.2  500  6.00  3.0 10  1  2 FALSE  i              moderate\n15   15  f 28 3.0 3.5 3500  1.00  4.0  3  1  0 FALSE  d          very liberal\n16   16  f 25 3.8 3.3  210 10.00  7.0  6  1  0 FALSE  i               liberal\n17   17  f 41 4.0 3.0 1000 15.00  6.0  7  3 10 FALSE  i      slightly liberal\n18   18  m 50 3.8 3.8    0  3.00  5.0  9  6 10 FALSE  d               liberal\n19   19  m 71 4.0 3.5 5000  3.00  6.0 12  2  2 FALSE  i               liberal\n20   20  f 28 3.0 3.8  120  1.00 25.0  0  0  2 FALSE  d          very liberal\n21   21  f 26 3.7 3.7 8000  8.00  4.0  4  4  1 FALSE  i              moderate\n22   22  f 27 4.0 3.7    2  2.50  4.0  2  7  0 FALSE  i               liberal\n23   23  m 31 2.7 3.5 1700  5.00  7.0  7  2  0 FALSE  r     very conservative\n24   24  f 23 3.7 3.7    2  2.00  7.0  4  2  0 FALSE  i              moderate\n25   25  m 23 3.2 3.8  450  4.00  0.0  7  7  3 FALSE  i          very liberal\n26   26  f 44 3.0 3.0    0  2.00  2.0  3  2  3 FALSE  i      slightly liberal\n27   27  m 26 3.7 3.0 1000  3.00  8.0  2  7  0 FALSE  d               liberal\n28   28  f 31 3.7 3.8  850 10.00 10.0  3  7  0 FALSE  r slightly conservative\n29   29  m 24 3.3 3.1  420  2.00 10.0  6  5  0 FALSE  d              moderate\n30   30  f 26 3.3 3.3 1200  0.75 10.0  0  3  0 FALSE  r               liberal\n31   31  m 26 3.3 3.5 1000  1.50  0.0  3  3  3 FALSE  d               liberal\n32   32  f 32 3.5 3.9  150 12.00 10.0  2  0  0 FALSE  d               liberal\n33   33  m 26 3.4 3.4 2000  1.50  2.0  7 14  0 FALSE  d               liberal\n34   34  f 22 3.2 2.8  316  2.00 10.0  3  5  2 FALSE  i               liberal\n35   35  f 24 3.5 3.9  900  1.75  8.0  0  0  1 FALSE  d          very liberal\n36   36  m 24 3.6 3.3  250  2.00  4.0  6  3  1 FALSE  r slightly conservative\n37   37  m 23 3.8 3.7  180  0.50 10.0  5  7  0 FALSE  i               liberal\n38   38  m 33 3.4 3.4 6000  1.50  8.0  5  6  2 FALSE  i               liberal\n39   39  m 23 2.8 3.2  950  2.00 37.0 10  5  0 FALSE  r slightly conservative\n40   40  m 31 3.8 3.5 1100  0.75  0.5  3  5  2 FALSE  r          conservative\n41   41  m 26 3.4 3.4 1300  1.20  0.0  8  2  0 FALSE  i               liberal\n42   42  m 28 2.0 3.0  360  0.25 10.0  8  3  0 FALSE  d      slightly liberal\n43   43  f 24 3.8 3.9 1800  2.00  2.0  5  4  1 FALSE  r          conservative\n44   44  m 23 3.0 3.6  900 15.00 12.0  0  5  0 FALSE  r slightly conservative\n45   45  f 25 3.0 4.0 5000  5.00  1.5  0  4  0 FALSE  i              moderate\n46   46  f 24 3.0 3.5  300  1.00 10.0  5  5  0 FALSE  d               liberal\n47   47  f 27 3.0 3.8 2000 20.00 28.0  7 14  2 FALSE  r      slightly liberal\n48   48  m 24 3.3 3.8  630  1.30  2.0  3  5  0 FALSE  r     very conservative\n49   49  f 26 3.8 4.0 1200  1.00  0.0  4  3  1 FALSE  d               liberal\n50   50  f 27 3.0 4.0  580  2.00  5.0 15  1  2 FALSE  d          very liberal\n51   51  m 32 3.0 3.0 2000  5.00  5.0  5  2  1 FALSE  r slightly conservative\n52   52  f 41 4.0 4.0    0  8.00  8.0  4  2  2 FALSE  r              moderate\n53   53  f 29 3.0 3.9  300  3.70  2.0  5  1 11 FALSE  d               liberal\n54   54  f 50 3.5 3.8    6  6.00  7.0  3  7  0 FALSE  d               liberal\n55   55  f 22 3.4 3.7   80  7.00 10.0  1  2  2 FALSE  i               liberal\n56   56  f 23 3.6 3.2  375  1.50  5.0 10  5  0 FALSE  r          conservative\n57   57  m 26 3.5 3.6 2000  0.30 16.0  8  3  0 FALSE  d              moderate\n58   58  m 30 3.0 3.0    1  1.10  1.0  4  3  0 FALSE  i      slightly liberal\n59   59  f 23 3.0 3.0  112  0.50 15.0  3  3  0 FALSE  i              moderate\n60   60  f 22 3.4 3.0  650  4.00  8.0 16  7  1 FALSE  i              moderate\n             re    ab    aa    ld\n1    most weeks FALSE FALSE FALSE\n2  occasionally FALSE FALSE    NA\n3    most weeks FALSE FALSE    NA\n4  occasionally FALSE FALSE FALSE\n5         never FALSE FALSE FALSE\n6  occasionally FALSE FALSE    NA\n7  occasionally FALSE FALSE FALSE\n8  occasionally FALSE FALSE FALSE\n9  occasionally FALSE FALSE    NA\n10        never FALSE FALSE FALSE\n11 occasionally FALSE FALSE FALSE\n12 occasionally FALSE FALSE    NA\n13 occasionally FALSE FALSE FALSE\n14 occasionally FALSE FALSE FALSE\n15        never FALSE FALSE FALSE\n16   every week FALSE FALSE FALSE\n17   every week FALSE    NA FALSE\n18        never FALSE FALSE FALSE\n19        never FALSE FALSE FALSE\n20 occasionally FALSE FALSE FALSE\n21 occasionally FALSE FALSE FALSE\n22 occasionally FALSE FALSE FALSE\n23   every week FALSE FALSE FALSE\n24        never FALSE FALSE FALSE\n25        never FALSE FALSE FALSE\n26   most weeks FALSE FALSE FALSE\n27 occasionally FALSE FALSE    NA\n28   most weeks FALSE FALSE FALSE\n29 occasionally FALSE FALSE    NA\n30 occasionally FALSE FALSE    NA\n31 occasionally FALSE FALSE FALSE\n32 occasionally FALSE FALSE FALSE\n33        never FALSE FALSE FALSE\n34 occasionally FALSE FALSE    NA\n35 occasionally FALSE FALSE    NA\n36   every week FALSE FALSE FALSE\n37        never FALSE FALSE    NA\n38        never FALSE FALSE FALSE\n39   most weeks FALSE FALSE FALSE\n40   most weeks FALSE FALSE    NA\n41 occasionally FALSE FALSE FALSE\n42        never FALSE FALSE    NA\n43   every week FALSE FALSE FALSE\n44        never FALSE FALSE FALSE\n45 occasionally FALSE FALSE FALSE\n46        never FALSE FALSE FALSE\n47 occasionally FALSE FALSE FALSE\n48   every week FALSE FALSE FALSE\n49        never FALSE FALSE FALSE\n50 occasionally FALSE FALSE FALSE\n51   every week FALSE FALSE FALSE\n52 occasionally FALSE FALSE FALSE\n53 occasionally FALSE FALSE FALSE\n54 occasionally FALSE FALSE    NA\n55        never FALSE FALSE    NA\n56   every week FALSE FALSE FALSE\n57 occasionally FALSE FALSE    NA\n58   every week FALSE FALSE FALSE\n59   most weeks FALSE FALSE FALSE\n60 occasionally FALSE FALSE FALSE\n\nlm(pi ~ re, data=student.survey)\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nCoefficients:\n(Intercept)         re.L         re.Q         re.C  \n     3.5253       2.1864       0.1049      -0.6958  \n\nlm(formula= hi ~ tv, data=student.survey)\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nCoefficients:\n(Intercept)           tv  \n    3.44135     -0.01831  \n\n\nPolitical Affiliation and Religiosity: I have no idea what re.Q re.L and re.C is. Both categorical variables take on more than three possible values so I am guessing I have the wrong code.\nTV and GPA: For every +1 hr spend watching tv per week, gpa decreases by 0.018. For a student that watches no tv in the week, their predicted gpa is 3.44."
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html",
    "href": "posts/FinalProjectProposal_Saaradhaa.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\n# load libraries.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nPrior research literature in the social sciences has continually stressed the need for more research on the Global South. However, few papers actually focus on it. Hence, I am interested to learn more about this region. A data source that lends itself useful for this is the World Values Survey, a global survey with an easily accessible database.\nI am specifically interested in understanding what drives subjective well-being, which can be interpreted via happiness and life satisfaction (Addai et al., 2013).\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. What predicts happiness and life satisfaction in the Global South?\nB. Do predictors of happiness and life satisfaction differ between the Global North and South?\n\n\nThis project will be useful to better understand motivations and desires in the Global South, reduce inter-cultural tensions and enhance cross-cultural cohesion. Governments can also benefit from this research in terms of policy prioritization to maximize citizens’ well-being."
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nPast researchers have studied happiness and life satisfaction in the Global South via the World Values Survey (Addai et al., 2013; Ngamaba, 2016). The studies focused on Ghana and Rwanda respectively. The common predictors of happiness and life satisfaction across both countries were satisfaction with health and income.\nTo the best of my knowledge, few studies comparing well-being in the Global North and South exist. Alba (2019) found that happiness was generally greater in the Global North than the Global South, and indicated that future research should attempt to cover the factors behind this. I think happiness and well-being in the Global North may depend on more subjective measures, given that health and income-related issues should be relatively more accounted for.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nHealth and financial satisfaction will not be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nHealth and financial satisfaction will be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nPredictors of happiness and life satisfaction will not differ between the Global North and South.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nPredictors of happiness and life satisfaction will differ between the Global North and South."
  },
  {
    "objectID": "posts/Final Project Proposal.html",
    "href": "posts/Final Project Proposal.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "The research questions that I am looking to investigate involve the factors that increase university students’ GPA. These include the following:\n1) Does classroom engagement (i.e., taking notes, attending class, listening) result in a higher GPA in university students?\n2) Does reported studying (i.e., weekly study hours) result in a higher GPA in university students?\n3) Does collaboration between students (i.e., studying together, positive class discussions) result in a higher GPA in university students?\n\n\n\n\n\nFor the first research question, it is reasonable to hypothesize that classroom engagement will have a positive effect on students’ academic achievement. Previous research supports this hypothesis. For example, one study found that classroom engagement, as well as other related factors such as time management and autonomous motivation, are predictors of academic achievement (Fokkens-Bruinsma, et al., 2021). Another study found that attendance in higher education is a small, but still statistically significant, predictor of academic performance (Büchele, 2021). In this study, classroom engagement will be defined as “taking notes, attendance, and frequency of listening.” These measures will be reported by university students via survey.\n\n\n\nIn regards to the second research question, it is hypothesized that students who study more will have a higher GPA. There are many previous studies that support this claim. For instance, one study found that university freshmen who studied more than eight hours a week saw an average increase in GPA of 0.580 (Nelson, 2003). Research has also found that increasing study time leads to an increased GPA (Thibodeaux, et al., 2017). In this study, hours spent studying will be measured through students’ estimated range of hours studied, reported via survey.\n\n\n\nIn response to the third research question, it is hypothesized that student collaboration will have a positive effect on student GPA. There is some research literature that supports this statement. One study found that students who study with their peers achieve significantly higher homework scores (Vargas, et al., 2018). Another study found that university students who had a strong social network and exhibited collaborative behaviors tended to achieve higher grades (Ellis & Han, 2021). Effective student collaboration can also occur during class time, such as through small group discussions. Research has found that students who participate in small group discussions demonstrate an increase in resilience, which has shown to improve academic performance (Torrento-estimo, et al, 2012). In this study, student collaboration will be measured through students’ reported time spent studying with peers, and impact that their class discussions have.\n\n\n\n\nThe dataset used is one retrieved from Kaggle using the link here. The dataset is named, “Higher Education Student Performance Evaluation.” This dataset was used in a self-report survey study conducted by Yılmaz and Sekeroglu (2019).\n\n\nCode\nstudentsurvey <- read.csv(\"student_prediction.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'student_prediction.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nsummary(studentsurvey)\n\n\nError in summary(studentsurvey): object 'studentsurvey' not found\n\n\nCode\nlibrary(ggplot2)\n\n\nTo begin, it is important to examine the demographic variables through descriptive statistics to observe the sample.\n\n\nTo start, students’ reported gender (1 = female and 2 = male) is plotted in the bar graph below.\n\n\nCode\nggplot(studentsurvey, aes(x = GENDER)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = GENDER)): object 'studentsurvey' not found\n\n\nIn this sample, there are more males than females.\n\n\n\nThe bar graph below plots the students’ reported ages at the time of the survey (1 = 18-21, 2 = 22-25, 3 = 26 or above).\n\n\nCode\nggplot(studentsurvey, aes(x = AGE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = AGE)): object 'studentsurvey' not found\n\n\nThe majority of students are between the ages 18-25, with very few above the age of 26.\n\n\n\nThe bar graph below depicts what type of high school the university students graduated from (1= private, 2 = state, 3 = other).\n\n\nCode\nggplot(studentsurvey, aes(x = HS_TYPE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = HS_TYPE)): object 'studentsurvey' not found\n\n\nAccording to the graph, most students attended a state (public) high school.\n\n\n\nThe bar graph below demonstrates what percentage of their tuition was paid for by scholarship (1 = None, 2 = 25%, 3 = 50%, 4 = 75%, 5 = Full)\n\n\nCode\nggplot(studentsurvey, aes(x = SCHOLARSHIP)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = SCHOLARSHIP)): object 'studentsurvey' not found\n\n\nMost students have received at least 50% scholarship at this university.\n\n\n\nThe bar graph below depicts how many students work a job outside of their classes (1 = Yes, 2 = No)\n\n\nCode\nggplot(studentsurvey, aes(x = WORK)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = WORK)): object 'studentsurvey' not found\n\n\nMost students do not have a job while they are studying at university in this sample.\n\n\n\nThis sample may not be representative of the U.S. student population. There are more male than female students, which is not the case at most schools: there is about a 1:2 male to female ratio at U.S. colleges (Leukhina & Smaldone, 2022). The ages of students, however, do align with the ages of current university students: about a third of students in university are ages 24 and under (Hanson, 2022). Additionally, like in the sample, the vast majority of students attended public schools (Riser-Kositsky, 2022). In regards to scholarships, the students at this particular university receive scholarships at significantly higher rates than the rest of the U.S. Only about one in eight students receive a scholarship, and only 5% receive a full scholarship (Scholarship Statistics, 2021). While the enrollment statuses of the students were not given, if all students were full-time students, it would align with research that shows that less than half of full-time students (40%) in U.S. universities work while in school. While this sample may not be entirely representative of the U.S. college student population, analyses of this dataset conducted may provide some insight on factors that improve university students GPA.\n\n\n\n\nBüchele, S. (2021). Evaluating the link between attendance and performance in higher education: the role of classroom engagement dimensions. Assessment & Evaluation in Higher Education, 46(1), 132-150.\nEllis, R., & Han, F. (2021). Assessing university student collaboration in new ways. Assessment & Evaluation in Higher Education, 46(4), 509-524.\nFokkens-Bruinsma, M., Vermue, C., Deinumdataset, J. F., & van Rooij, E. (2021). First-year academic achievement: the role of academic self-efficacy, self-regulated learning and beyond classroom engagement. Assessment & Evaluation in Higher Education, 46(7), 1115-1126.\nHanson, M. (2022, July 26). College Enrollment & Student Demographic Statistics. EducationData.org. Retrieved from https://educationdata.org/college-enrollment-statistics.\nLeukhina, O., & Smaldone, A. (2022, March 14). Why do women outnumber men in college enrollment? Saint Louis Fed Eagle. Retrieved from https://www.stlouisfed.org/on-the-economy/2022/mar/why-women-outnumber-men-college-enrollment#:~:text=When%20the%20fall%20college%20enrollment,seen%20in%20U.S.%20college%20enrollment.\nNational Center for Education Statistics. (2022, May). College Student Employment. Coe - college student employment. Retrieved from https://nces.ed.gov/programs/coe/indicator/ssa/college-student-employment\nNelson, R. (2003). Student Efficiency: A study on the behavior and productive efficiency of college students and the determinants of GPA. Issues in Political Economy, 12, 32-43.\nRiser-Kositsky, M. (2022, August 2). Education statistics: Facts about American Schools. Education Week. Retrieved from https://www.edweek.org/leadership/education-statistics-facts-about-american-schools/2019/01.\nScholarship statistics. ThinkImpact.com. (2021, November 10). Retrieved from https://www.thinkimpact.com/scholarship-statistics/.\nThibodeaux, J., Deutsch, A., Kitsantas, A., & Winsler, A. (2017). First-year college students' time use: Relations with self-regulation and GPA. Journal of Advanced Academics, 28(1), 5-27.\nTorrento-estimo, E., Lourdes, C., & Evidente, L. G. (2012). Collaborative Learning in Small Group Discussions and Its Impact on Resilience Quotient and Academic Performance. JPAIR Multidisciplinary Research Journal, 7(1), 1-1.\nVargas, D. L., Bridgeman, A. M., Schmidt, D. R., Kohl, P. B., Wilcox, B. R., & Carr, L. D. (2018). Correlation between student collaboration network centrality and academic performance. Physical Review Physics Education Research, 14(2), 020112.\nYılmaz, N., & Sekeroglu, B. (2019, August). Student Performance Classification Using Artificial Intelligence Techniques. In International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions (pp. 596-603). Springer, Cham."
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html",
    "href": "posts/Final Project 1_Kaushika Potluri.html",
    "title": "Final Project Submission 1",
    "section": "",
    "text": "the research question that I have been interested in is the impact of education about sex and fertility for women and how that changes the fetility rate. Women’s education raises the value of time spent working in the market and, as a result, the opportunity cost of spending time to take care of their child seems less. Across time and places, there is a clear negative link between women’s education and fertility, although its meaning is ambiguous. Women’s level of education may impact fertility through its effects on children’s health, the number of children desired, and women’s ability to give birth and understanding of various birth control options. Each of these are influenced by local, institutional, and national circumstances. Their relative importance may fluctuate as a society develops economically. Since having children affects how much mothers must pay for childcare, women’s education may also be correlated with fertility. The data was acquired from various years of the National Opinion Resource Center’s General Social Survey. Compared to other women, mothers who stay at home with their kids are less likely to invest more money in their education. The correlation between women’s education and unobservable qualities that are jointly linked with fertility may be even more significant.\n###Hypothesis It can be thought of as the total number of unplanned and intended children. The number of kids a family can have, the number of kids the family desires, and the capability to regulate birth through the availability of modern contraceptives and the knowledge of how to use them are all impacted by advancements in women’s education. The number of children a woman has is halfway between the amount she wants and her level of natural fertility. Age and fertility control are the determining variables.If there was a variation by region in birth control availability, such information might be valuable. However, our data set does not contain geographical information (parameters). My assumption would be that if the level of education increases, the number of children would decrease.\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "title": "Final Project Submission 1",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "title": "Final Project Submission 1",
    "section": "Reading in Data:",
    "text": "Reading in Data:\nThe data was acquired from Professor Sander’s article that he used.\n\n\nCode\nWomendata <-  read.csv(\"_data/data.csv\")"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "title": "Final Project Submission 1",
    "section": "Summary of the data",
    "text": "Summary of the data\n\n\nCode\nsummary(Womendata)\n\n\n       X           mnthborn         yearborn          age       \n Min.   :   1   Min.   : 1.000   Min.   :38.00   Min.   :15.00  \n 1st Qu.:1091   1st Qu.: 3.000   1st Qu.:55.00   1st Qu.:20.00  \n Median :2181   Median : 6.000   Median :62.00   Median :26.00  \n Mean   :2181   Mean   : 6.331   Mean   :60.43   Mean   :27.41  \n 3rd Qu.:3271   3rd Qu.: 9.000   3rd Qu.:68.00   3rd Qu.:33.00  \n Max.   :4361   Max.   :12.000   Max.   :73.00   Max.   :49.00  \n                                                                \n    electric          radio              tv             bicycle      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1402   Mean   :0.7018   Mean   :0.09291   Mean   :0.2758  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n NA's   :3        NA's   :2        NA's   :2         NA's   :3       \n      educ             ceb            agefbrth        children     \n Min.   : 0.000   Min.   : 0.000   Min.   :10.00   Min.   : 0.000  \n 1st Qu.: 3.000   1st Qu.: 1.000   1st Qu.:17.00   1st Qu.: 0.000  \n Median : 7.000   Median : 2.000   Median :19.00   Median : 2.000  \n Mean   : 5.856   Mean   : 2.442   Mean   :19.01   Mean   : 2.268  \n 3rd Qu.: 8.000   3rd Qu.: 4.000   3rd Qu.:20.00   3rd Qu.: 4.000  \n Max.   :20.000   Max.   :13.000   Max.   :38.00   Max.   :13.000  \n                                   NA's   :1088                    \n    knowmeth         usemeth          monthfm          yearfm     \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.00   Min.   :50.00  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.: 3.00   1st Qu.:72.00  \n Median :1.0000   Median :1.0000   Median : 6.00   Median :78.00  \n Mean   :0.9633   Mean   :0.5776   Mean   : 6.27   Mean   :76.91  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 9.00   3rd Qu.:83.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :12.00   Max.   :88.00  \n NA's   :7        NA's   :71       NA's   :2282    NA's   :2282   \n     agefm          idlnchld          heduc            agesq       \n Min.   :10.00   Min.   : 0.000   Min.   : 0.000   Min.   : 225.0  \n 1st Qu.:17.00   1st Qu.: 3.000   1st Qu.: 0.000   1st Qu.: 400.0  \n Median :20.00   Median : 4.000   Median : 6.000   Median : 676.0  \n Mean   :20.69   Mean   : 4.616   Mean   : 5.145   Mean   : 826.5  \n 3rd Qu.:23.00   3rd Qu.: 6.000   3rd Qu.: 8.000   3rd Qu.:1089.0  \n Max.   :46.00   Max.   :20.000   Max.   :20.000   Max.   :2401.0  \n NA's   :2282    NA's   :120      NA's   :2405                     \n     urban           urb_educ          spirit          protest      \n Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median : 0.000   Median :0.0000   Median :0.0000  \n Mean   :0.5166   Mean   : 3.469   Mean   :0.4222   Mean   :0.2277  \n 3rd Qu.:1.0000   3rd Qu.: 7.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :20.000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n    catholic         frsthalf          educ0           evermarr     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.1025   Mean   :0.5405   Mean   :0.2078   Mean   :0.4767  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n\n\n\n\nCode\nglimpse(Womendata)\n\n\nRows: 4,361\nColumns: 28\n$ X        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ mnthborn <int> 5, 1, 7, 11, 5, 8, 7, 9, 12, 9, 6, 10, 12, 2, 1, 6, 1, 8, 4, …\n$ yearborn <int> 64, 56, 58, 45, 45, 52, 51, 70, 53, 39, 46, 59, 42, 40, 53, 6…\n$ age      <int> 24, 32, 30, 42, 43, 36, 37, 18, 34, 49, 42, 29, 45, 48, 35, 2…\n$ electric <int> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ radio    <int> 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tv       <int> 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ bicycle  <int> 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…\n$ educ     <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ ceb      <int> 0, 3, 1, 3, 2, 1, 4, 0, 1, 0, 3, 3, 4, 10, 3, 0, 4, 2, 0, 1, …\n$ agefbrth <int> NA, 25, 27, 17, 24, 26, 20, NA, 19, NA, 25, 23, 18, 19, 23, N…\n$ children <int> 0, 3, 1, 2, 2, 1, 4, 0, 1, 0, 3, 3, 2, 8, 3, 0, 4, 2, 0, 1, 0…\n$ knowmeth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ usemeth  <int> 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1…\n$ monthfm  <int> NA, 11, 6, 1, 3, 11, 5, NA, 7, 11, 6, 1, 1, 10, 1, NA, NA, NA…\n$ yearfm   <int> NA, 80, 83, 61, 66, 76, 78, NA, 72, 61, 70, 84, 66, 66, 74, N…\n$ agefm    <int> NA, 24, 24, 15, 20, 24, 26, NA, 18, 22, 24, 24, 23, 26, 21, N…\n$ idlnchld <int> 2, 3, 5, 3, 2, 4, 4, 4, 4, 4, 3, 6, 6, 4, 3, 4, 5, 1, 2, 3, 2…\n$ heduc    <int> NA, 12, 7, 11, 14, 9, 17, NA, 3, 1, 16, 7, NA, 3, 16, NA, NA,…\n$ agesq    <int> 576, 1024, 900, 1764, 1849, 1296, 1369, 324, 1156, 2401, 1764…\n$ urban    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ urb_educ <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ spirit   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0…\n$ protest  <int> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1…\n$ catholic <int> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ frsthalf <int> 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0…\n$ educ0    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ evermarr <int> 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1…\n\n\nWe can see that we have 28 variables and 4361 observations in this dataset. The dependent variable of interest - number of living children Then I will perform data manipulation to tidy the data. The variables of interest are age, yearborn, month born, urban education and many more variables that seem intriguing. Variables like radio, bicycle, electric can be ignored in this.\n###References [1] The effect of women’s schooling on fertility by W Sander · 1992 [2] The Impact of Women’s Schooling on Fertility and Contraceptive Use by M Ainsworth · 1996"
  },
  {
    "objectID": "posts/HW_3_603.html",
    "href": "posts/HW_3_603.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_3_603.html#question-1",
    "href": "posts/HW_3_603.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/HW_3_603.html#a",
    "href": "posts/HW_3_603.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe Predicted variable here is ppgdp."
  },
  {
    "objectID": "posts/HW_3_603.html#b",
    "href": "posts/HW_3_603.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe graph show negative realtionship between ppgdp and fertility and here straight line mean function does not seem an appropriate measure for a summary of this graph."
  },
  {
    "objectID": "posts/HW_3_603.html#c",
    "href": "posts/HW_3_603.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/HW_3_603.html#question-2",
    "href": "posts/HW_3_603.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW_3_603.html#a-1",
    "href": "posts/HW_3_603.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11"
  },
  {
    "objectID": "posts/HW_3_603.html#question-3",
    "href": "posts/HW_3_603.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/HW_3_603.html#question-4",
    "href": "posts/HW_3_603.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/HW_3_603.html#question-5",
    "href": "posts/HW_3_603.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/HW_3_603.html#a-2",
    "href": "posts/HW_3_603.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/HW_3_603.html#b-2",
    "href": "posts/HW_3_603.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW1_EmmaRasmussen.html",
    "href": "posts/HW1_EmmaRasmussen.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlungcap<-read_excel(\"_data/LungCapData.xls\")\nhead(lungcap)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\n#saving a copy of original dataset\nlungcap_orig<-lungcap\n\n#checking for missing values in LungCap\nwhich(is.na(lungcap$LungCap))\n\n\ninteger(0)\n\n\n\n1a.\nThe distribution of LungCapData is plotted as a histogram below.\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_histogram()\n\n\n\n\n\nThe histogram looks approximately normally distributed\n\n\n1b.\nThe probability distribution of LungCap data for males and females is compared using the boxplots below:\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Gender))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity of males appears slightly higher than that of females. The IQR and range for males and females appears similarly spread with a higher average for males.\n\n\n1c.\nBelow the mean and standard deviation of smokers and non-smokers is compared. They are also plotted as a boxplot to help visualize the distribution.\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity for smokers (8.645) in this sample is higher than that of non-smokers (7.770). This does not make sense. However, the standard deviation of non-smokers (2.726) is much higher than smokers (1.883) so there might be something else going on (see boxplot).\n\n\n1d.\nBelow, means are taken by age groups of smokers/non-smokers. I also created a new age category variable (“AgeCat”) to plot the data by smoking status and age category.\n\n\nCode\n#Mean under 13 and nonsmoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n#Mean under 13 and smoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 7.201852\n\n\nCode\n#Mean 14-15 and nonsmoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.068018\n\n\nCode\n#Mean 14-15 and smoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.689231\n\n\nCode\n#Mean 16-17 and nonsmoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.30523\n\n\nCode\n#Mean 16-17 and smoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.850385\n\n\nCode\n#Mean over 18 and nonsmoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n#Mean over 18 and smoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\n#creating new variable AgeCat to create boxplots\nlungcap<-lungcap %>% \n  mutate(AgeCat= as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\n\n#new Category AgeCat is the last column\nlungcap\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeCat      \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_boxplot()+facet_grid(Smoke ~ AgeCat)\n\n\n\n\n\n\n\n1e.\nComparing the lung capacities for smokers and non-smokers in different age categories:\nNow we can see that the mean lung capacity for smokers by age group is generally lower than that of nonsmokers. This is true in all categories except for Under 13, which is likely because smokers in that category are going to be older than nonsmokers in that category (i.e. it is more likely that a 12 year old smokes than a 6 year old, and a 12 year old has a larger lung capacity than a 6 year old regardless of smoking status)\nThis explains the first calculation of mean by smoking status (before finding the mean by age categories). Smokers are generally going to be older than non-smokers for this sample (the oldest participant in the sample is 19- see code below), which explains why the mean for smokers versus non-smokers (not separated by age categories) makes it look like smokers have a higher average lung capacity.\n\n\nCode\n#checking how old participants in the sample are\nlungcap %>% \n  summarize(range(Age))\n\n\n# A tibble: 2 × 1\n  `range(Age)`\n         <dbl>\n1            3\n2           19\n\n\n\n\n1f.\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\n#Creating vectors of Age and Lung Capacity from df (lungcap) to apply cov() and cor() functions to\nx<-c(lungcap$Age)\ny<-c(lungcap$LungCap)\n\n\n#Calculating covariance\ncov(x, y)\n\n\n[1] 8.738289\n\n\nCode\n#calculating correlation\ncor(x, y)\n\n\n[1] 0.8196749\n\n\nThe covariance, 8.738 is fairly high and positive, meaning as age increases, so does lung capacity (i.e. age and lung capacity co-vary). The correlation (0.82) is fairly close to one and positive, indicating they correlate fairly closely.\n\n\n2a-f.\nPrior Conviction Data\n\n\nCode\n#creating a data frame\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nprison<- data.frame(X, Frequency)\nprison\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nprison<-rename(prison, PriorConvictions=X)\nprison\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(prison, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\nAnswering the Questions\n\n\nCode\n#creating a vector of probabilities\nprobs<-Frequency/810\nprobs\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, probs)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions? 19.75% probability (or 0.1975)\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69.38% probability\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89.14% probability\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions? 10.86% probability\nWhat is the expected value for the number of prior convictions? 1.28642 prior convictions\nCalculate the variance and the standard deviation for the Prior Convictions. variance: 0.8572937 standard deviation: 0.9259016 prior convictions"
  },
  {
    "objectID": "posts/Homework 3 LJones.html",
    "href": "posts/Homework 3 LJones.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(ggplot2)\n\n\n\n\n\n\n\nCode\ndata(UN11)\n\n\n\n\nIdentify the predictor and the response.\nSince we’re studying the dependence of fertility on ppgdp, the predictor is ppgdp and the response is fertility.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nscatterplot(fertility ~ ppgdp, UN11)\n\n\n\n\n\nThe data appears curvilinear, so a straight-line function would be inaccurate.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nscatterplot (log(fertility) ~ log(ppgdp), UN11)\n\n\n\n\n\nThe logarithm helps adjust the plots on the graph, so this model is much more plausible.\n\n\n\n\n\n\nHow, if at all, does the slope of the prediction equation change?\nThe slope of the equation increases by 1.33.\n\n\n\nHow, if at all, does the correlation change?\nThe correlation should not change because the ratio of the values is constant.\n\n\n\n\nDraw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nThere appears to be a strong positive correlation between stream runoff and precipitation at OPBPC, OPRC, and OPSLAKE, so you could potentially predict water supply near those sites. Correlation between the two at the other sites seems loosely positively correlated, if at all.\n\n\n\nCreate a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\ndata(\"Rateprof\")\n\nrp <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\n\n\nError in select(., quality, helpfulness, clarity, easiness, raterInterest): could not find function \"select\"\n\n\nCode\npairs(rp)\n\n\nError in pairs(rp): object 'rp' not found\n\n\n\nRater interest appears to have no correlation (or possibly a very weak positive correlation) with any other variable.\nQuality has a strong positive correlation with helpfulness and clarity, a weak positive correlation with easiness.\nHelpfulness has a strong positive correlation with clarity and a week positive correlation with easiness.\nClarity has a weak positive correlation with easiness (easiness has a weak positive correlation with every variable).\n\n\n\n\n\n\nCode\ndata(\"student.survey\")\n\nss <- student.survey\n\n\n\n\n\n\n\n\nCode\nlm(pi ~ re, data = ss)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\n\nCall:\nlm(formula = pi ~ re, data = ss)\n\nCoefficients:\n(Intercept)         re.L         re.Q         re.C  \n     3.5253       2.1864       0.1049      -0.6958  \n\n\n\n\n\nI could not make my code work for the categorical variables in this particular regression.\n\n\n\n\n\n\n\n\nCode\nfit2 <- lm(hi ~ tv, data = ss)\n\nplot(hi ~ tv, data = ss)\nabline(fit2)\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(lm(hi ~ tv, data = ss))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = ss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThe p-value and the plot both suggest that the negative correlation between hours spent watching TV and high school GPA is not strong. R-squared is not very close to 1, which also demonstrates the weakness of this relationship."
  },
  {
    "objectID": "posts/shelton_final1.html",
    "href": "posts/shelton_final1.html",
    "title": "Final Project: Part 1",
    "section": "",
    "text": "Homelessness is a complex living situation with several qualifying conditions; at its most simple state, the U.S Dept. of Housing and Urban Development defines it as lacking a fixed, regular nighttime residence (not a shelter) or having a nighttime residence not designed for human accommodation1.\nOn a single night in 2020, over 500,0002 people experienced homelessness in the United States. Florida, with the third largest state population , had the fourth largest homeless population of 2020 with 27,4872.\nFlorida counties represent a large age range and varying demographic profiles; the state is a hub to a variety of industries including tourism, defense, agriculture, and information technology. Investigating homelessness in Florida counties with robust data can lead to several conclusions about who is being impacted where, and how state policy is failing groups of a diverse population.\n\nResearch QuestionHypothesisIntroduction to DataImprovementsCodebookReferences\n\n\nCarole Zugazaga’s 2004 study of 54 single homeless men, 54 single homeless women, and 54 homeless women with children in the Central Florida area investigated stressful life events common among homeless people. The interviews revealed that women were more likely to have been sexually or physically assaulted, while men were more likely to have been incarcerated or abuse drugs/alcohol. Homeless women with children were more likely to be in foster care as a youth.\nNearly a decade later,county-level data can be used to investigate the relationship between Zugazaga’s reported stressful life events (incarceration, drug arrests, poverty, forcible sex…)3 and homelessness counts.\n\n\n\n\n\n\nResearch Question\n\n\n\nDo particular life stressors increase a population’s vulnerability to homelessness?\n\n\n\n\nHomelessness is not a new issue in the United States, yet homeless policy targets elimination via criminalization rather than prevention. Despite state and federal governments being aware of the circumstances that increase vulnerability to homelessness for decades, I anticipate all of the variables to remain significant in a model relating stressors to Florida homelessness counts 2018-2020.\n\n\n\n\n\n\nResearch Hypothesis\n\n\n\nH0: All stressors are insignificant in predicting homelessness counts ( Bi = 0 for i=0,1,2,…n )\nHA: At least one stressor Bi is significant in predicting homelessness counts\n\n\n\n\nThe data florida_1820.csv4 describes population, homelessness counts, poverty counts and several other demographic indicators3 at the county level for 2018-2020. All 67 Florida counties have observations for the 3 years giving us 201 observations of 15 variables. Each observation provides a count of each variables from a single county for a year within 2018-2020.\nThe data were collected from the Florida Department of Health. Variable names3 were used as search indicators to produce counts for Florida counties. Unfortunately, we cannot accurately analyze the effect of COVID-19 as data is incomplete for the majority of counties in 2021.\n\n\n\n\n\n\nIntro to Data\n\n\n\n\n\n\n\n\n\n  \n\n\n\n    County               Year      Homeless (Count)   Population     \n Length:201         Min.   :2018   Min.   :   0.0   Min.   :   8367  \n Class :character   1st Qu.:2018   1st Qu.:  11.0   1st Qu.:  28089  \n Mode  :character   Median :2019   Median : 151.0   Median : 130642  \n                    Mean   :2019   Mean   : 427.8   Mean   : 317746  \n                    3rd Qu.:2020   3rd Qu.: 563.0   3rd Qu.: 367471  \n                    Max.   :2020   Max.   :3516.0   Max.   :2864600  \n                                                                     \n Unemployment Rate   Median Inc    Incarceration (Rateper1000) Poverty (Count) \n Min.   : 2.100    Min.   :34583   Min.   : 0.60               Min.   :   906  \n 1st Qu.: 3.400    1st Qu.:41401   1st Qu.: 2.50               1st Qu.:  4901  \n Median : 4.000    Median :50640   Median : 3.40               Median : 16210  \n Mean   : 4.697    Mean   :51116   Mean   : 3.84               Mean   : 42922  \n 3rd Qu.: 5.600    3rd Qu.:58093   3rd Qu.: 4.50               3rd Qu.: 46034  \n Max.   :13.500    Max.   :83803   Max.   :18.60               Max.   :482656  \n                                                                               \n Drug Arrests (Count) Relocated (Rate) Sub Abuse Enrollment (Count)\n Min.   :   13        Min.   : 4.689   Min.   :   5.0              \n 1st Qu.:  225        1st Qu.:11.244   1st Qu.:  76.0              \n Median :  729        Median :12.700   Median : 250.0              \n Mean   : 1558        Mean   :13.288   Mean   : 877.6              \n 3rd Qu.: 1903        3rd Qu.:14.544   3rd Qu.:1030.0              \n Max.   :13038        Max.   :22.553   Max.   :6272.0              \n                                                                   \n Adult Pysch Beds (Count) Severe Housing Problems (Rate) Forcible Sex (Count)\n Min.   :  0.00           Min.   : 9.6                   Min.   :   0.0      \n 1st Qu.:  0.00           1st Qu.:13.3                   1st Qu.:  14.0      \n Median :  0.00           Median :15.4                   Median :  45.0      \n Mean   : 66.26           Mean   :15.8                   Mean   : 170.5      \n 3rd Qu.: 84.00           3rd Qu.:17.3                   3rd Qu.: 225.0      \n Max.   :778.00           Max.   :29.8                   Max.   :1408.0      \n                          NA's   :134                                        \n Foster Care (Count)\n Min.   :   3.0     \n 1st Qu.:  33.0     \n Median : 153.0     \n Mean   : 326.1     \n 3rd Qu.: 353.0     \n Max.   :2289.0     \n                    \n\n\n\n\n  \n\n\n\n\n\n\nExpanding Intro to Data exposes summary statistics including mean, range, quantiles, and standard deviation for all 15 variables. The table below the summaries provides arranged figures for basic parameters of interest grouped by county.\nLATER: Plots, Isolate more variables of interest with grouping, group by year?\n\n\nWhile the data is great illustration of homelessness in Florida by county, there are improvements that could be made to both data collection and the research question itself to further the study.\nData:\n\nUnfortunately, FL Health Charts did not provide demographic breakdown for the homeless population (Age, Sex, Race), which would drastically widen the scope of the analysis, leading to far more interesting conclusions.\nThere is only have data for a three year period; this is too small of a range to make a strong statement about the impact of homeless policy on Florida counties or how the relevance of certain stressors has changed over time. For a more in depth study I would begin with a 10 year range.\n\nResearch Question:\n\nDemographic breakdown of stressors’ impact (Age, Sex, Race)\nExtend the question to the entire country, providing a breakdown by state\nCompare to foreign countries to contrast governments’ approaches to homelessness and leading causes of homelessness around the world.\n\n\n\nLATER: Variable Definitions and Collection Methods here\n\n\nLater: Carol Zugazaga\n\n\n\n\n\n1.) Homeless Definition\n2.) US Interagency Council on Homelessness\n3.) Explanation of variables and collection method in Codebook tab\n4.) This data was cleaned and put in a tidy format in another script; manipulations were messy and inefficient (brute force) so I did not include the cleaning file."
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html",
    "href": "posts/HW1_EthanCampbell.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n(The distribution of LungCap looks as follows:)\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n\n\n\n(Comparing lung cap by gender)\nHere we notice that males tend to have a higher lung cap compared to females. Females average tends to sit around 8 while males seems to sit closer to 9\n\n\nCode\nboxplot(df$LungCap~df$Gender)\n\n\n\n\n\n\n\n\n(smoker vs non-smoker lung cap)\nInterestingly, none smokers tend to have a lower lung capacity however, I believe this might be due to age. No this does not make sense at first glance and does betray my expectation.\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\n\n\n\n(relation between smoking and lung cap at different age groups)\nThe lung cap starts off higher but takes and dip then rises as the age continues to grow. I believe the trend is the higher age grows the higher the lung cap until it reaches a certain point.\n\n\nCode\n# lung cap is 9.62\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 13) %>%\n  colMeans()\n\n\n      Age   LungCap \n15.609290  9.628757 \n\n\nCode\n# lung cap is 9.04\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  colMeans()\n\n\n      Age   LungCap \n14.533333  9.045417 \n\n\nCode\n# lung cap is 10.24\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  colMeans()\n\n\n     Age  LungCap \n16.44330 10.24588 \n\n\nCode\n# lung cap is 11.26\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age > 18) %>%\n  colMeans()\n\n\n     Age  LungCap \n19.00000 11.26149 \n\n\n\n\n\n(lung cap for smokers and non smokers broken into age groups)\nWe notice a clear trend that smokers have a lower lung capacity compared to non-smokers\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 13) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.71\n2 yes    9.21\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age > 18) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     11.3\n2 yes    11.3\n\n\n\n\n\n(correlation and covariance between lung capacity and age)\ncorrelation is at .819 meaning they have a positive correlation of about 82%. This means that there is a connection between the two and when one goes up so does the other.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#a-1",
    "href": "posts/HW1_EthanCampbell.html#a-1",
    "title": "Homework 1",
    "section": "2.a",
    "text": "2.a\nprobability of exactly 2 convictions probability = 19.7%\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  filter(X == 2)\n\n\n# A tibble: 1 × 3\n      X  Freq Probability\n  <dbl> <dbl>       <dbl>\n1     2   160       0.197"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#b-1",
    "href": "posts/HW1_EthanCampbell.html#b-1",
    "title": "Homework 1",
    "section": "2.b",
    "text": "2.b\nprobability of fewer than 2 convictions probability = 69.2%\n\n\nCode\nsum(df1$Probability[1:2])\n\n\n[1] 0.6921182"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#c-1",
    "href": "posts/HW1_EthanCampbell.html#c-1",
    "title": "Homework 1",
    "section": "2.c",
    "text": "2.c\nProbability of having 2 or fewer convictions probability = 88.9%\n\n\nCode\nsum(df1$Probability[1:3])\n\n\n[1] 0.8891626"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#d-1",
    "href": "posts/HW1_EthanCampbell.html#d-1",
    "title": "Homework 1",
    "section": "2.d",
    "text": "2.d\nprobability of having more than 2 convictions probability = 11.08%\n\n\nCode\nsum(df1$Probability[4:5])\n\n\n[1] 0.1108374"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#e-1",
    "href": "posts/HW1_EthanCampbell.html#e-1",
    "title": "Homework 1",
    "section": "2.e",
    "text": "2.e\nWhat is the expected value expected value is 1.29 convictions\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  mutate(expected_value = (0*0.15763547)+(1*0.53448276)+(2*0.19704433)+(3*0.07881773)+(4*0.03201970))\n\n\n# A tibble: 5 × 4\n      X  Freq Probability expected_value\n  <dbl> <dbl>       <dbl>          <dbl>\n1     0   128      0.158            1.29\n2     1   434      0.534            1.29\n3     2   160      0.197            1.29\n4     3    64      0.0788           1.29\n5     4    26      0.0320           1.29"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#f-1",
    "href": "posts/HW1_EthanCampbell.html#f-1",
    "title": "Homework 1",
    "section": "2.f",
    "text": "2.f\nWhat is the variance and standard deviation of the prior convictions Variance = 25810.8 standard deviation = 160.6574\n\n\nCode\nvar(df$Freq)\n\n\n[1] 25810.8\n\n\nCode\nsd(df$Freq)\n\n\n[1] 160.6574"
  },
  {
    "objectID": "posts/Final_Project_1.html",
    "href": "posts/Final_Project_1.html",
    "title": "Final_Project_1",
    "section": "",
    "text": "Research Question : examining the relationship between the maximum heart rate one can achieve during exercise and the likelihood of developing heart disease. Using multiple logistic regression, examining handle the confounding effects of age and gender.\nHypothesis Testing : Is there any statistical difference between the gender and age in terms of heart attack prediction.\n#Loading Dataset\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nheart_cleveland_upload <- read_csv(\"heart_cleveland_upload.csv\")\n\n\nRows: 297 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(heart_cleveland_upload)\n\n\n\n\n  \n\n\n\n\n\nCode\ndim(heart_cleveland_upload)\n\n\n[1] 297  14\n\n\nData set contains 297 Columns and 14 columns\n\n\nCode\ncolnames(heart_cleveland_upload)\n\n\n [1] \"age\"       \"sex\"       \"cp\"        \"trestbps\"  \"chol\"      \"fbs\"      \n [7] \"restecg\"   \"thalach\"   \"exang\"     \"oldpeak\"   \"slope\"     \"ca\"       \n[13] \"thal\"      \"condition\"\n\n\nhere are 13 attributes\nage: age in years sex: sex (1 = male; 0 = female) cp: chest pain type – Value 0: typical angina – Value 1: atypical angina – Value 2: non-anginal pain – Value 3: asymptomatic trestbps: resting blood pressure (in mm Hg on admission to the hospital) chol: serum cholestoral in mg/dl fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) restecg: resting electrocardiographic results – Value 0: normal – Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) – Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria thalach: maximum heart rate achieved exang: exercise induced angina (1 = yes; 0 = no) oldpeak = ST depression induced by exercise relative to rest slope: the slope of the peak exercise ST segment – Value 0: upsloping – Value 1: flat – Value 2: downsloping ca: number of major vessels (0-3) colored by flourosopy thal: 0 = normal; 1 = fixed defect; 2 = reversable defect and the label condition: 0 = no disease, 1 = disease\n\nDescriptive statistics\n\n\nCode\nsummary(heart_cleveland_upload)\n\n\n      age             sex               cp           trestbps    \n Min.   :29.00   Min.   :0.0000   Min.   :0.000   Min.   : 94.0  \n 1st Qu.:48.00   1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:120.0  \n Median :56.00   Median :1.0000   Median :2.000   Median :130.0  \n Mean   :54.54   Mean   :0.6768   Mean   :2.158   Mean   :131.7  \n 3rd Qu.:61.00   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:140.0  \n Max.   :77.00   Max.   :1.0000   Max.   :3.000   Max.   :200.0  \n      chol            fbs            restecg          thalach     \n Min.   :126.0   Min.   :0.0000   Min.   :0.0000   Min.   : 71.0  \n 1st Qu.:211.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:133.0  \n Median :243.0   Median :0.0000   Median :1.0000   Median :153.0  \n Mean   :247.4   Mean   :0.1448   Mean   :0.9966   Mean   :149.6  \n 3rd Qu.:276.0   3rd Qu.:0.0000   3rd Qu.:2.0000   3rd Qu.:166.0  \n Max.   :564.0   Max.   :1.0000   Max.   :2.0000   Max.   :202.0  \n     exang           oldpeak          slope              ca        \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.800   Median :1.0000   Median :0.0000  \n Mean   :0.3266   Mean   :1.056   Mean   :0.6027   Mean   :0.6768  \n 3rd Qu.:1.0000   3rd Qu.:1.600   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :6.200   Max.   :2.0000   Max.   :3.0000  \n      thal         condition     \n Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000  \n Mean   :0.835   Mean   :0.4613  \n 3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :2.000   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(heart_cleveland_upload)\n\n\nRows: 297\nColumns: 14\n$ age       <dbl> 69, 69, 66, 65, 64, 64, 63, 61, 60, 59, 59, 59, 59, 58, 56, …\n$ sex       <dbl> 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n$ cp        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ trestbps  <dbl> 160, 140, 150, 138, 110, 170, 145, 134, 150, 178, 170, 160, …\n$ chol      <dbl> 234, 239, 226, 282, 211, 227, 233, 234, 240, 270, 288, 273, …\n$ fbs       <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ restecg   <dbl> 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, …\n$ thalach   <dbl> 131, 151, 114, 174, 144, 155, 150, 145, 171, 145, 159, 125, …\n$ exang     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ oldpeak   <dbl> 0.1, 1.8, 2.6, 1.4, 1.8, 0.6, 2.3, 2.6, 0.9, 4.2, 0.2, 0.0, …\n$ slope     <dbl> 1, 0, 2, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, …\n$ ca        <dbl> 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, …\n$ thal      <dbl> 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, …\n$ condition <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html",
    "href": "posts/Blog Post 2_Kaushika Potluri.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "title": "Homework 1",
    "section": "1(a) Distribution of LungCap:",
    "text": "1(a) Distribution of LungCap:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe distribution appears to be very similar to a normal distribution, according to the histogram."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "title": "Homework 1",
    "section": "1(b)",
    "text": "1(b)\nThe boxplots below show the probability distributions grouped by Gender.\n\n\nCode\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nLooks like males have a slightly higher lung capacity than females."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nSurprisingly, the mean lung capacity is higher for smokers than it is for non-smokers."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n\nCode\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and lower\", Age == 14 | Age == 15 ~ \"14-15\", Age == 16 | Age == 17 ~ \"16-17\", Age >= 18 ~ \"18 and above\"))\narrange(df, Age)\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeGroup    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <chr>       \n 1   5.88      3   55.9 no    male   no        13 and lower\n 2   0.507     3   51.6 no    female yes       13 and lower\n 3   1.18      3   51.9 no    male   no        13 and lower\n 4   4.7       3   52.7 no    male   no        13 and lower\n 5   5.48      3   52.9 no    male   no        13 and lower\n 6   1.02      3   47   no    female no        13 and lower\n 7   2         3   51   no    female no        13 and lower\n 8   1.68      3   51.9 no    male   no        13 and lower\n 9   4.08      3   53.6 no    male   yes       13 and lower\n10   1.45      3   45.3 no    female no        13 and lower\n# … with 715 more rows\n\n\nCode\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMajority seem to be non-smokers, and looks like non-smokers seem to have higher lung capacity."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n\nCode\nclass(df$AgeGroup)\n\n\n[1] \"character\"\n\n\n\n\nCode\ndf$AgeGroup <- as.factor(df$AgeGroup) #converting to factor\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke `mean(LungCap)`\n  <fct>        <chr>           <dbl>\n1 13 and lower no               6.36\n2 13 and lower yes              7.20\n3 14-15        no               9.14\n4 14-15        yes              8.39\n5 16-17        no              10.5 \n6 16-17        yes              9.38\n7 18 and above no              11.1 \n8 18 and above yes             10.5 \n\n\nThe mean lung capacity for smokers aged 13 and under is greater than that of non-smokers in the same age group which is different from expectation. Non-smokers have higher mean lung capacity for ages 14-15, 16-17 and 18 and above. Either there may be an error or extreme outlier in the data for smokers aged 13 and under."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n\nCode\ncor(df$LungCap,df$Age)\n\n\n[1] 0.8196749\n\n\n\n\nCode\ncov(df$LungCap,df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity and age have a high positive correlation of 0.82, meaning that as age increases, lung capacity also does. The covariance is a little more challenging to interpret; the positive number indicates a positive association between lung capacity and age, but because covariance varies from negative infinity to infinity, it is difficult to judge the strength of the relationship. In most situations, I would choose to employ correlation."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "title": "Homework 1",
    "section": "2",
    "text": "2\n\n\nCode\ndf1 <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(df1, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "title": "Homework 1",
    "section": "2(a)",
    "text": "2(a)\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n# A tibble: 5 × 3\n    df1 Inmate_count Probability\n  <int>        <dbl>       <dbl>\n1     0          128      0.158 \n2     1          434      0.536 \n3     2          160      0.198 \n4     3           64      0.0790\n5     4           24      0.0296\n\n\n\n\nCode\nIP %>%\n  filter(df1 == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability is about 19.75%."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "title": "Homework 1",
    "section": "(b)",
    "text": "(b)\n\n\nCode\ndf2 <- IP %>%\n  filter(df1 < 2)\nsum(df2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "title": "Homework 1",
    "section": "2(c)",
    "text": "2(c)\n\n\nCode\ndf3 <- IP %>%\n  filter(df1 <= 2)\nsum(df3$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "title": "Homework 1",
    "section": "2(d)",
    "text": "2(d)\n\n\nCode\ndf4 <- IP %>%\n  filter(df1 > 2)\nsum(df4$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "title": "Homework 1",
    "section": "2(e)",
    "text": "2(e)\n\n\nCode\nIP <- mutate(IP, X = df1*Probability)\nexpectedvalue<- sum(IP$X)\nexpectedvalue\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.2864198. We can round this to 1."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "title": "Homework 1",
    "section": "2(f)",
    "text": "2(f)\n\n\nCode\nvar1 <-sum(((IP$df1-expectedvalue)^2)*IP$Probability)\nvar1\n\n\n[1] 0.8562353\n\n\n\n\nCode\nsqrt(var1)\n\n\n[1] 0.9253298\n\n\nThe variance and the standard deviation for prior convictions are 0.8562353 and 0.9253298 respectively."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html",
    "href": "posts/KarenDetter_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.1",
    "href": "posts/KarenDetter_HW3.html#q.1",
    "title": "HW3",
    "section": "Q.1",
    "text": "Q.1\n\n\nCode\n##load data\ndata(UN11)\n\n\n\n1.1.1\nIn this model, the predictor variable is ‘ppgdp’ ($ gross national product per person) and the response variable is ‘fertility’.\n\n\n1.1.2\n\n\nCode\n##draw scatterplot\nplot(x = UN11$ppgdp, y = UN11$fertility)\n\n\n\n\n\nThe relationship between fertility and ppgdp is not exactly linear because increasing gross national product only decreases birth rate until it nears about the 10,000 point; after that, the effect seems to disappear.\n\n\n1.1.3\n\n\nCode\n##draw scatterplot with logs of both variables\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility))\n\n\n\n\n\nThe log transformation scatterplot shows a relationship that looks much closer to that of the linear regression model."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.2",
    "href": "posts/KarenDetter_HW3.html#q.2",
    "title": "HW3",
    "section": "Q.2",
    "text": "Q.2\n\n(a)\nConverting the currency from American dollars to British pounds causes the mean of the x-axis (explanatory variable) to increase while the mean of the y-axis (response variable) remains the same. As a result, the prediction equation line becomes less steep, as each value of x is increased for the identical corresponding y-value.\n\n\n(b)\nThe currency conversion would not change the correlation, as the relative values of the variables remain unchanged."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.3",
    "href": "posts/KarenDetter_HW3.html#q.3",
    "title": "HW3",
    "section": "Q.3",
    "text": "Q.3\n\n\nCode\n##load data\ndata(water)\n##draw scatterplot matrix\npairs(water)\n\n\n\n\n\nThese scatterplots show that when precipitation at OPBPC, OPRC, and OPSLAKE increases, the runoff volume at BSAAM goes up. Precipitation at the other three locations does not seem to have a strong linear relationship with stream runoff volume.\nAlso, precipitation rates at the first three sites seem to be somewhat intercorrelated, as do the rates at the last three sites, indicating that the sites in each set may be closer to each other or share similar geographic features."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.4",
    "href": "posts/KarenDetter_HW3.html#q.4",
    "title": "HW3",
    "section": "Q.4",
    "text": "Q.4\n\n\nCode\n##load data\ndata(\"Rateprof\")\n##draw scatterplot matrix of selected variables\npairs(~Rateprof$quality+Rateprof$helpfulness+Rateprof$clarity+Rateprof$easiness+Rateprof$raterInterest, lwd=2, labels = c(\"QUALITY\", \"HELPFULNESS\", \"CLARITY\", \"EASINESS\", \"Rater INTEREST\"), pch=19, cex = 0.75, col = \"blue\")\n\n\n\n\n\nSurprisingly, it doesn’t seem that reviewers’ ratings of their interest in the subject or the easiness of the course correlate with ratings of the professor’s quality, helpfulness, or clarity. Ratings for those three traits, however, all seem to have linear relationships with each other."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.5",
    "href": "posts/KarenDetter_HW3.html#q.5",
    "title": "HW3",
    "section": "Q.5",
    "text": "Q.5\n\n\nCode\n##load data\ndata(student.survey)\n\n\n\n(i)\n\n\nCode\n##convert factor variables to numeric\npi_conv <- as.numeric(student.survey$pi)\nre_conv <- as.numeric(student.survey$re)\n##run regression analysis\nmodel1 <- lm(pi_conv ~ re_conv, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_conv ~ re_conv, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre_conv       0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\n(ii)\n\n\nCode\n##run regression analysis\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\n(a) & (b)\n\n\nCode\n##visualize relationships in the two models with scatterplots\n##include regression lines of coefficients\n##use jitter plots due to small sample size of 60\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_jitter(color = \"blue\") +\n    geom_abline(intercept = .9308, slope = .9704) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe first regression model shows evidence of a strong, statistically significant effect of religiousness on political ideology, with the p-value of .00000122 being well below the significance threshold of .05. As the level of religiousness increases, political ideology becomes more conservative, with religiousness explaining 34% of the variance in ideology. Because of the small number of observations (n=60), scatterplot points do not appear tightly aligned to the regression line, but there is a clear upward-moving trend.\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_jitter(color = \"blue\") +\n  geom_abline(intercept = 3.441353, slope = -0.018305) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe effect of hours of tv watched on grade point average is not very strong, with the p-value of .0388 being just below the significance threshold. The relationship between the variables is inverse - mean gpa decreases by .02 for every increase in hours of tv watched. Hours of tv watched per week explain 7% of the variance in grade point averages. The scatterplot, again affected by small sample size, does show a slight trend of gpa decreasing as tv level increases."
  },
  {
    "objectID": "posts/HW 1 DACSS.html",
    "href": "posts/HW 1 DACSS.html",
    "title": "Graphs and Probz",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ readr   2.1.2     ✓ stringr 1.4.0\n✓ purrr   0.3.4     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\n\nlibrary(readxl)\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)                                                              \n\n\nm_lung<-mean(LungCapData$LungCap)\nsd_lung<-sd(LungCapData$LungCap)\n\nhist(LungCapData$LungCap, prob= TRUE, xlim = c(0, 20))\ncurve(dnorm(x, m_lung, sd_lung), add= TRUE,lwd= 2,col= \"blue\")"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-5",
    "href": "posts/HW 1 DACSS.html#question-5",
    "title": "Graphs and Probz",
    "section": "Question 5",
    "text": "Question 5\nDoesnt look like its good being a smoker under the age of 18, or any age. Lung capacity is smaller for these groups\n\nggplot(LungCapData, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-6",
    "href": "posts/HW 1 DACSS.html#question-6",
    "title": "Graphs and Probz",
    "section": "Question 6",
    "text": "Question 6\n\ncovar<-cov(LungCapData$LungCap, LungCapData$Age)\nprint(covar)\n\n[1] 8.738289\n\ncorre<-cor(LungCapData$LungCap, LungCapData$Age, method = \"pearson\")\nprint(corre)\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW2_StephRoberts.html",
    "href": "posts/HW2_StephRoberts.html",
    "title": "HW2",
    "section": "",
    "text": "###Homework 2\n##Question 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\nSurgical Procedure Sample Size Mean Wait Time Standard Deviation Bypass 539 19 10 Angiography 847 18 9\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#Assign values\nbypass_ss <- 539\nbypass_mean <- 19\nbypass_sd <- 10\nangio_ss <- 847\nangio_mean <- 18\nangio_sd <- 9\n\n#calculate bypass t-score\nalpha <- 0.1\nt_score_b <- qt(p = 1-alpha/2, df = bypass_ss-1)\nprint(t_score_b)\n\n\n[1] 1.647691\n\n\nThe t-score for bypass wait time is 1.65.\n\n\nCode\n#Calculate 90% confidence interval of bypass wait time\nCI_bypass <- c(bypass_mean - t_score_b * bypass_sd ,\n               bypass_mean + t_score_b * bypass_sd)\nprint(CI_bypass)\n\n\n[1]  2.523092 35.476908\n\n\nThe 90% confidence interval for bypass wait time is 2.53 to 35.48 days.\n\n\nCode\n#calculate angiography t-score\nalpha <- 0.1\nt_score_a <- qt(p = 1-alpha/2, df = angio_ss-1)\nprint(t_score_a)\n\n\n[1] 1.646657\n\n\nThe t-score for bypass wait time is 1.65.\n\n\nCode\n#Calculate 90% confidence interval of bypass wait time\nCI_angio <- c(angio_mean - t_score_a * angio_sd ,\n               angio_mean + t_score_a * angio_sd)\nprint(CI_angio)\n\n\n[1]  3.180089 32.819911\n\n\nThe 90% confidence interval for angiography wait time is 3.18 to 32.82 days.\n##Question 2 A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#Sample size\nn <- 1031\n\n#Number of those who believed that college education is essential for success\nk <- 567\n\n#find sample proportion\np <- k/n\np\n\n\n[1] 0.5499515\n\n\nCode\nprop.test(x=k, n=n, p=p, alternative=\"two.sided\")\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  k out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThe proportion of all adult Americans who believe that a college education is essential for success is 0.55.The confidence interval for the proportion is 0.52 to 0.58.\n##Question 3 Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\nThis information gives us a confidence interval of 10, a confidence level of 95%, and an unknown population size.\n\n\nCode\nz <- 1.96\nbook_sd <- (200-30)/4\nmargin <- 5\n\nbook_ss <- ((z*book_sd)/margin)^2\nbook_ss\n\n\n[1] 277.5556\n\n\nThe sample size should be at least 278 students.\n##Question 4 According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. Hint: The P-values for the two possible one-sided tests must sum to 1.\nThe null hypothesis is that female employees income is $500, H0 : μ = 500. The alternative hypothesis is that female employees income is not, H1 : μ ≠ 500. A second alternative is that income is under $500, H2 : μ < 500. A third alternative is that income is greater than $500, H3 : μ > 500. We assume that the sample is random and that the population has a normal distribution. We will use a p-value of 0.05 and reject the null if it is any less.\n\n\nCode\nsample <- 9\nmean_income <- 410\ns <- 90\nμ <- 500\n\n#Calculate t-score\nt_score_income <- (mean_income-μ)/(s/sqrt(sample))\nt_score_income\n\n\n[1] -3\n\n\n\n\nCode\n#Calculate p-value\nincome_p <- (pt(t_score_income, sample-1))*2\nincome_p\n\n\n[1] 0.01707168\n\n\nWe find a p-value of 0.017. With a p-value of less than 0.05, we reject the null hypothesis. The female employee mean income is not equal to that of all senior-level workers.\n\n\nCode\n#Calculate p-value <500\nincome_p_lower <- pt(t_score_income, sample-1, lower.tail = TRUE)\nincome_p_lower\n\n\n[1] 0.008535841\n\n\nWe find a p-value of 0.009 when assessing the lower limits of the distribution. The smaller number suggests a stronger correlation. Again, we reject the null hypothesis. The mean female income is likely less than $500.\n\n\nCode\n#Calculate p-value >500\nincome_p_upper <- pt(t_score_income, sample-1, lower.tail = FALSE)\nincome_p_upper\n\n\n[1] 0.9914642\n\n\nWe find a p-value of 0.99. Here the p-value is much greater than 0.05, so we fail to reject the null hypothesis.\n##Question 5 Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nNull hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05 We assume that the sample is random and that the population has a normal distribution.\n\n\nCode\njmean <- 519.5\njse <- 10\nss <- 1000\nμ <- 500\nsmean <- 519.7\nsse <- 10\n\n#Calculate t-score for Jones study\nt_score_j <- (jmean-μ)/jse\nt_score_j\n\n\n[1] 1.95\n\n\nCode\n#Calculate p-value for Jones study\njp <- 2*pt(t_score_j, ss-1, lower.tail = FALSE)\njp\n\n\n[1] 0.05145555\n\n\nThe p-value for Jones’s study is 0.051, which exceeds our threshold of 0.05. Therefore, we reject the null.\n\n\nCode\n#Calculate t-score for Smith study\nt_score_s <- (smean-μ)/sse\nt_score_s\n\n\n[1] 1.97\n\n\nCode\n#Calculate p-value for Smith study\nsp <- 2*pt(t_score_s, ss-1, lower.tail = FALSE)\nsp\n\n\n[1] 0.04911426\n\n\nThe p-value for Smith’s study is 0.049, which falls under our threshold of 0.05. Therefore, we fail to reject the null.\nThese studies have nearly identical means and p-values. It illustrates the importance of reporting the actually calculated p-value along with its interpretation. Without the number, we might conclude Jones’s study indicates strong statistical significance while Smith’s did not, when in fact they were almost identical.\n##Question 6 Are the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence level of state and local taxes per gallon is 36.23 to 45.49. We can interpret this by saying that if we took another 100 samples, 95 of them would have a mean that falls within our confidence interval. We cannot conclude, however, that the mean is less than 45 cents per gallon, because that number falls within our 95% confidence interval range."
  },
  {
    "objectID": "posts/HW2_CalebHill.html",
    "href": "posts/HW2_CalebHill.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s load the relevant libraries.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nFor question 1, we need to construct the 90% confident interval to estimate the actual mean wait time for eahc of the two procedures.\n\n\nCode\ns_mean_b <- 19\ns_sd_b <- 10\ns_size_b <- 539\nstandard_error_b <- s_sd_b / sqrt(s_size_b)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_b <- qt(p = 1 - tail_area, df = s_size_b - 1)\nCI_b <- c(s_mean_b - t_score_b * standard_error_b,\n        s_mean_b + t_score_b * standard_error_b)\nprint(CI_b)\n\n\n[1] 18.29029 19.70971\n\n\nThis is the CI for bypass. The following code chunk is for angiography.\n\n\nCode\ns_mean_a <- 18\ns_sd_a <- 9\ns_size_a <- 847\nstandard_error_a <- s_sd_a / sqrt(s_size_a)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_a <- qt(p = 1 - tail_area, df = s_size_a - 1)\nCI_a <- c(s_mean_a - t_score_a * standard_error_a,\n        s_mean_a + t_score_a * standard_error_a)\nprint(CI_a)\n\n\n[1] 17.49078 18.50922\n\n\nIs the confidence interval narrower for angiograpy or bypass survey? Answer = angiography."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a",
    "href": "posts/HW2_CalebHill.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\n\n\nCode\ns_mean_4a <- 410\ns_sd_4a <- 90\ns_size_4a <- 9\nstandard_error_4a <- s_sd_4a / sqrt(s_size_4a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\nCI_4a <- c(s_mean_4a - t_score_4a * standard_error_4a,\n        s_mean_4a + t_score_4a * standard_error_4a)\nprint(CI_4a)\n\n\n[1] 340.8199 479.1801\n\n\nBased upon the data provided, we can be within a 95% CI that mean income for female employees is less than $500 per week. If Ha : μ < 500, then we can accept the hypothesis, based upon the CI. However, for section B, we’ll report the P-value via the t-score."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b",
    "href": "posts/HW2_CalebHill.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value=pt(q = t_score_4a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.025\n\n\nWith a P-value of 0.025, we can accept the Ha : μ < 500. However, let’s change the lower.tail value to TRUE to see about Ha : μ > 500."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c",
    "href": "posts/HW2_CalebHill.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value = pt(q = t_score_4a, df = 8, lower.tail = TRUE)\nprint(p_value)\n\n\n[1] 0.975\n\n\nJust as I thought. We have to reject the second hypothesis, that Ha : μ > 500, as the P-value is 0.975, outside of statistical significance minimum of 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a-1",
    "href": "posts/HW2_CalebHill.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nFor Jones:\n\n\nCode\ns_mean_5a <- 519.5\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 499.8766 539.1234\n\n\nFor Smith:\n\n\nCode\ns_mean_5a <- 519.7\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 500.0766 539.3234"
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b-1",
    "href": "posts/HW2_CalebHill.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nCode for Section B are the P-values shown for each code chunk. Are they statistically significant? At 0.043 for both, yes as they are below 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c-1",
    "href": "posts/HW2_CalebHill.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nThe P-value is the likelihood of finding the particular set of observations if the null hypothesis were true. As the P-value is traditionally use in frequentist statistics, we are only able to ascribe probability to this specific set of observations – which are themselves a set amount of observations.\nTherefore, it can sometimes be misleading to report a P-value as 0.05. CI levels allow a range within the set of observations. We can see this problem best with the above results via Jones and Smith. They do not get the same sample mean, even with similar observations."
  },
  {
    "objectID": "posts/ShoshanaBuck- HW1.html",
    "href": "posts/ShoshanaBuck- HW1.html",
    "title": "ShoshanaBuck-HW1",
    "section": "",
    "text": "Question 1\nUse the LungCapData to answer the following questions.\n\n\nCode\n# read in data\nlung_cap<- read_xls(\"_data/LungCapData.xls\")\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\na. What does the distribution of LungCap look like?\n\n\nCode\n#plot histogram\nhist(lung_cap$LungCap)\n\n\n\n\n\nThe histogram shows that the distribution is pretty close to a normal distribution, with an almost a bell shaped curve. Meaning that the data near the mean are more of a frequent occurrence which is true because there are fewer observations near the margins.\n\n\nb. compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\n#Box plot\nggplot(lung_cap, aes (Gender,LungCap)) + geom_boxplot()\n\n\n\n\n\nThe box plot shows that male’s have a slightly higher lung capacity than females. Female’s have more values in the first quartile range and a lower minimum value than male’s. On the other hand male’s have a higher max value and more values in the 3rd quartile range.\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlung_cap %>% \n  group_by(Smoke) %>% \n  summarise(avg_lung_cap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke avg_lung_cap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nThis does not make sense. Smokers have a higher sample mean than non-smokers which intuitively does not make sense because we would assume non-smokers would have a higher lung capacity.\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#categorical variable of age_groups \ndf<- lung_cap %>% \ngroup_by(Smoke,LungCap) %>% \n  summarise(age_group = case_when(Age<=13 ~ \"13 & under\",Age ==14 | Age == 15 ~ \"14 to 15\",Age== 16 | Age == 17 ~ \"16 to 17\", Age>=18 ~ \"18 & older\")) \n\n\n`summarise()` has grouped output by 'Smoke', 'LungCap'. You can override using\nthe `.groups` argument.\n\n\nCode\n#mean of lung capacity with new variable\ndf %>% \n  group_by(Smoke, age_group) %>% \n  summarise(avg_lung_cap = mean(LungCap)) %>% \n  arrange(desc(avg_lung_cap))\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke age_group  avg_lung_cap\n  <chr> <chr>             <dbl>\n1 no    18 & older        11.1 \n2 yes   18 & older        10.5 \n3 no    16 to 17          10.5 \n4 yes   16 to 17           9.38\n5 no    14 to 15           9.14\n6 yes   14 to 15           8.39\n7 yes   13 & under         7.20\n8 no    13 & under         6.36\n\n\nCode\n#histogram\nggplot(df, aes(x = LungCap)) + facet_grid(Smoke ~age_group) +geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nUsing the package ggplot I used the function facet_grids to show a good visualization of the lung capacity between non-smokers and smokers within each age group. Looking at the histograms all age_groups that are non-smokers have a higher sample mean proving that non-smokers have\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\n# Mean of non-smokers 13 & younger\ndf %>% \n  filter(Smoke == 'no' & age_group == '13 & under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n# Mean of smokers 13 & younger\ndf %>% \n  filter(Smoke == 'yes' & age_group == '13 &under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] NaN\n\n\nCode\n#Mean of non-smokers 14 to 15\ndf %>% \n  filter(Smoke == 'no' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.13881\n\n\nCode\n#Mean of smokers 14 to 15\ndf %>% \n  filter(Smoke == 'yes' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.391667\n\n\nCode\n#Mean of non-smokers 16 to 17\ndf %>% \n  filter(Smoke == 'no' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.46981\n\n\nCode\n#Mean of smokers 16 to 17\ndf %>% \n  filter(Smoke == 'yes' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.38375\n\n\nCode\n#Mean of non-smokers 18 & older\ndf %>% \n  filter(Smoke == 'no' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n# Mean of smokers 18 & older\ndf %>% \n  filter(Smoke == 'yes' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\n\nf. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\n#Correlation\ncor(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#Covariance\ncov(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is 0.81 which is pretty close to +1, meaning that the there is a positive relationship between lung capacity and age. The covariance is also high which shows that the two variables of lung capacity and age have a positive relationship. #2 Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n# x= prior convictions \nx<-c(0, 1, 2, 3, 4)\nfrequency<-c(128, 434, 160, 64, 24)\nstate_prison<- data_frame(x,frequency)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nstate_prison\n\n\n# A tibble: 5 × 2\n      x frequency\n  <dbl>     <dbl>\n1     0       128\n2     1       434\n3     2       160\n4     3        64\n5     4        24\n\n\n\n\na. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n# 2 prior convictions.P(2)/total\n160/810\n\n\n[1] 0.1975309\n\n\nThere is a 1.9% probability that a randomly selected inmate has exactly 2 prior convictions.\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#  less than 2 prior convictions. (P(0) + P(1))/total \n(128+434)/810\n\n\n[1] 0.6938272\n\n\nThere is a 6.9% probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n# 2 or fewer prior convictions. (P(0) + P(1) + P(2)) +total\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nThere is a 8.9% probability that a randomly selected inmate has 2 or fewer prior convictions.\n\n\nd. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n# More than 2 prior convictions. (P(3) +P(4)) + total\n(64+24)/810\n\n\n[1] 0.108642\n\n\nThere is a 10.8% probability that a randomly selected inmate has more than 2 prior convictions.\n\n\ne. What is the expected value for the number of prior convictions?\n\n\nCode\n#Prior convictions. ((0*P(0)) +(1*(P(1)) + (2*P(2)) + (3*P(3)) + (4*P(4)))\ndf<-((128*0/810) +(434*1/810) +(160*2/810) +(64*3/810) +(24*4/810)) \nmean(df)\n\n\n[1] 1.28642\n\n\nThe expected value for number of prior convictions is 1.28\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n# variance\nv<- ((0-1.28)^2) *(128/810) +((1-1.28)^2) * (434/810)+((2-1.28)^2) * (160/810)+((3-1.28)^2) *(64/810) +((4-1.28)^2) * (24/810)\nv\n\n\n[1] 0.8562765\n\n\nCode\n# standard deviation\nsd<- sqrt(v)\nsd\n\n\n[1] 0.9253521\n\n\nThe variance is 0.856 and the standard deviation is 0.925."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html",
    "href": "posts/HW3_PrahithaMovva.html",
    "title": "Homework 3 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(stats)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-1",
    "href": "posts/HW3_PrahithaMovva.html#question-1",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 1",
    "text": "Question 1\n\n1.1.1\nThe predictor is ppgdp and the response is fertility because we are studying the dependence of fertility on ppgdp.\n\n\n1.1.2\n\n\nCode\nggplot(data=UN11, aes(x=ppgdp, y=fertility))+geom_point()\n\n\n\n\n\nThere are very few observations with gross national product per person between 20,000 and 100,000 when compared to those below 20,000 (in addition to the huge dip in fertility). Since the above graph does not seem to exhibit a linear relationship, it is not plausible for it to have a straight-line mean function.\n\n\n1.1.3\n\n\nCode\nggplot(data=UN11, aes(x=log(ppgdp), y=log(fertility)))+geom_point()\n\n\n\n\n\nWe can see that as log(ppgdp) increases, log(fertility) decreases linearly. So, we can say that the relationship between these two variables is linear and a simple linear regression model would be plausible."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-2",
    "href": "posts/HW3_PrahithaMovva.html#question-2",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 2",
    "text": "Question 2\n\na\nThe slope gets divided by 1.33 due to the conversion from US Dollar to British Pound. So the slope of the British prediction equation will be ~25% less than the US prediction equation. We can also observe this from the below graph, where the British equation is represented in red and the US in blue.\n\n\nCode\nUN11$british.ppgdp <- UN11$ppgdp/1.33\n\nggplot() +\n  geom_smooth(data=UN11, aes(x = british.ppgdp, y = fertility), \n              method = \"lm\", se = FALSE, color = \"red\") + \n  geom_smooth(data=UN11, aes(x = ppgdp, y = fertility), \n              method = \"lm\", se = FALSE, color = \"blue\") + \n  geom_point(data=UN11, aes(x = british.ppgdp, y = fertility), color = \"red\") + \n  geom_point(data=UN11, aes(x = ppgdp, y = fertility), color = \"blue\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nb\nThe correlation between the explanatory variable and the response will not change as both will increase 1.33x\n\n\nCode\nUN11$british.ppgdp <- UN11$ppgdp/1.33\n\ncor(UN11$british.ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-3",
    "href": "posts/HW3_PrahithaMovva.html#question-3",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the matrix, we can see that the first three locations - APMAM, APSAB and APSLAKE - are strongly correlated with each other. Similarly, the next three location - OPBPC, OPRC and OPSLAKE - are also correlated with each other. However, the first three locations do not share a strong correlation with the response - BSAAM, whereas the next three locations do. This implies that using one of the last three locations will give a better fit/ predictions for the stream runoff volume. On close inspection, we can further say that OPSLAKE might be a better choice among the last three locations to predict BSAAM.\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPBPC, data=water))$adj.r.squared\n\n\n[1] 0.7792942\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPRC, data=water))$adj.r.squared\n\n\n[1] 0.8419507\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPSLAKE, data=water))$adj.r.squared\n\n\n[1] 0.8777515"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-4",
    "href": "posts/HW3_PrahithaMovva.html#question-4",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\nrates <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rates)\n\n\n\n\n\nAll of them have a positive correlation but with different magnitudes. Quality, helpfulness and clarity seem to have a linear relationship with one another with a strong positive correlation. Quality, with easiness and rater interest has a positive correlation but is weak (weaker for rater interest). The plot for easiness and rater interest seems flat amongst all, implying almost no (very weak) correlation."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-5",
    "href": "posts/HW3_PrahithaMovva.html#question-5",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\npi.clean <- as.numeric(student.survey$pi)\nre.clean <- as.numeric(student.survey$re)\nmodel.i <- lm(pi.clean ~ re.clean, data=student.survey)\nsummary(model.i)\n\n\n\nCall:\nlm(formula = pi.clean ~ re.clean, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre.clean      0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nmodel.ii <- lm(hi ~ tv, data = student.survey)\nsummary(model.ii)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\na\n\n\nCode\nggplot(data=student.survey, aes(x=re, y=pi)) +\n  geom_jitter(data=student.survey, aes(x=re, y=pi), color=\"blue\") +\n  geom_abline(intercept=0.9308, slope=0.9704) +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the above visualization, we can also say that as religiosity increases, political ideology becomes more conservative.\n\n\nCode\nggplot(data=student.survey, aes(x=tv, y=hi)) +\n  geom_point(data=student.survey, aes(x=tv, y=hi), color=\"blue\") +\n  geom_abline(intercept=3.441353, slope=-0.018305) +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFrom the above visualization, we can say that as the number of hours of TV watching increases, the high school GPA of the student decreases.\n\n\nb\nIn (i), ~33.6% of the variance in political ideology is explained by religiosity and with 1 unit increase in religiosity, political ideology increases approximately by 0.97 units. We can also see that the p-value for (i) is way below the significance threshold of 0.05 and is therefore statistically significant. Similarly, in (ii), only ~7.2% of the variance in high school GPA is explained by the hours of TV watching. As expected, the p-value for (ii) is less than the usual 0.05 significance threshold but is close to it so it does not exhibit strong statistical significance."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html",
    "href": "posts/RahulGundeti_DACSS603_HW1.html",
    "title": "DACSS603_HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "title": "DACSS603_HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "title": "DACSS603_HW1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nlung <- read_excel(\"C:/Users/gunde/Downloads/LungCapData.xls\")\n\n\nError: `path` does not exist: 'C:/Users/gunde/Downloads/LungCapData.xls'\n\n\nCode\nlung\n\n\nError in eval(expr, envir, enclos): object 'lung' not found\n\n\nThe Lung Capacity data contains 725 rows and 6 columns that determine age, height etc., The key classification parameter is based on smoker vs non-smoker."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "title": "DACSS603_HW1",
    "section": "1_A",
    "text": "1_A\nThe distribution of LungCap looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 40, color = \"red\") +\n  geom_density(color = \"green\") +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution\", x = \"Lung Capcity\", y = \"Probability Density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'lung' not found\n\n\nThe observations plotted by histogram are closer to mean which suggests that it is a normal distribution."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "title": "DACSS603_HW1",
    "section": "1_B",
    "text": "1_B\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution based on gender\", y = \"Probability Density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'lung' not found\n\n\nThe box plot shows that the probability density of the male < female."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "title": "DACSS603_HW1",
    "section": "1_C",
    "text": "1_C\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- lung %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'lung' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nThe table contains the mean lung capacity. The observations suggest that the mean value is higher for smokers than non-smokers. This isn’t entirely correct as the individual biological factors plays a main role. So the data is inadequate to form an opinion."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "title": "DACSS603_HW1",
    "section": "1_D",
    "text": "1_D\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nlung <- mutate(lung, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(lung, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'lung' not found\n\n\nCode\nlung %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'lung' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non-smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "title": "DACSS603_HW1",
    "section": "1_E",
    "text": "1_E\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nlung %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'lung' not found\n\n\nComparing 1_D and 1_E we can find similarity which points that only 10 and above age group smoke."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "title": "DACSS603_HW1",
    "section": "1_F",
    "text": "1_F\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(lung$LungCap, lung$Age)\n\n\nError in is.data.frame(y): object 'lung' not found\n\n\nCode\nCorrelation <- cor(lung$LungCap, lung$Age)\n\n\nError in is.data.frame(y): object 'lung' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nThe comparison shows that the covariance is positive, indicating that lung capacity and age have a direct relationship. As a result, they are moving in the same direction due to the positive correlation as well. This means that as age increases, lung capacity increases as well, which means they are directly proportional."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "title": "DACSS603_HW1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "title": "DACSS603_HW1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nprior <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nprior\n\n\n\n\n  \n\n\n\n\n\nCode\nprior <- mutate(prior, Probability = Inmate_count/sum(Inmate_count))\nprior"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "title": "DACSS603_HW1",
    "section": "2_A",
    "text": "2_A\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nprior %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "title": "DACSS603_HW1",
    "section": "2_B",
    "text": "2_B\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions < 2)\nsum(random$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "title": "DACSS603_HW1",
    "section": "2_C",
    "text": "2_C\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions <= 2)\nsum(random$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "title": "DACSS603_HW1",
    "section": "2_D",
    "text": "2_D\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions > 2)\nsum(random$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "title": "DACSS603_HW1",
    "section": "2_E",
    "text": "2_E\nExpected value for the number of prior convictions:\n\n\nCode\nprior <- mutate(prior, Wm = Prior_convitions*Probability)\nev <- sum(prior$Wm)\nev\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "title": "DACSS603_HW1",
    "section": "2_F",
    "text": "2_F\nVariance for the Prior Convictions:\n\n\nCode\nvariance <-sum(((prior$Prior_convitions-ev)^2)*prior$Probability)\nvariance\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW3_EthanCampbell.html",
    "href": "posts/HW3_EthanCampbell.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n(a)\nHow, if at all, does the slope of the prediction equation change?\nYes, the slope will change by 1.33 since this is rate at which it is changing and the conversion between the two values. The US version will increase by 1.33 times compared to the British version.\n\n\n(b)\nHow, if at all, does the correlation change?\nThe correlation should not change as the scale changes in relation to the amount.\n\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\ndata(water)\n\nhead(water)\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n# here is the pairs of each variable and we notice some correlation however, it is really hard to get a closer look here. \npairs(water,\n      bg = 'blue')\n\n\n\n# creating the regression to look closer \nbsaam_water <- lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\n\n# summary\nsummary(bsaam_water)\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\n\nSolution\nHere we see that there are two variables that have a statistically significant relationship with BSAAM. These are OPRC and OPSLAKE each less than .05 however, questions regarding multicollinarity arise with this strong corrleation. The other variables such as the AP variables seem to also be correlated however, it does not appear as strongly as the two before. When looking at the range of residuals we notice a very large difference and this indicates that there may be some large and small outliers. This will effect the bests fitted line and lead to less robust analysis. When we are looking at the R^2 we notice it is very high which means it is fairly well fitted as it is close to 1.00.\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\ndata(Rateprof)\nhead(Rateprof)\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\nRates <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\n\npairs(Rates)\n\n\n\n\n\nSolution\nHere we see a vary of different strengths of correlations these will be discussed one by one. These range from very weak correlation to very strong correlation which is interesting to see. These are all linear and positive.\nQuality ~ Helpfulness - Here we notice a strong correlation as the data is very linear. This data is also showing a very strong correlation in a positive direction.\nQuality ~ Clarity- Here we notice a strong correlation that is positive and linear.\nQuality ~ Easiness- Here we notice a weaker correlation compared to the last too but still a positive linear correlation.\nQuality ~ RaterInterest- Here we notice a weak correlation but still a positive linear correlation.\nEasiness ~ RaterInterst - Shows a very flat line which shows a very weak correlation.\nClarity ~ Easiness- Shows a weak linear correlation.\n\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\ndata(student.survey)\nSS <- student.survey\nhead(SS)\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n# getting the data that we will be working with and checking to see which each variable means\n#?student.survey\n\nSS <- SS %>%\n  select(hi, tv, pi, re)\n\n\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n# plotting the comparison between political ideology and religiosity\nSS_plot <- plot(pi~re, data = SS)\n\n\n\n# plotting the comparison between high school gpa and hours of tv watching\n\nggplot(data = SS, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# making a table comparing these variabels\nxtabs(~pi+re, SS)\n\n                       re\npi                      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nsummary(SS)\n\n       hi              tv                             pi                re    \n Min.   :2.000   Min.   : 0.000   very liberal         : 8   never       :15  \n 1st Qu.:3.000   1st Qu.: 3.000   liberal              :24   occasionally:29  \n Median :3.350   Median : 6.000   slightly liberal     : 6   most weeks  : 7  \n Mean   :3.308   Mean   : 7.267   moderate             :10   every week  : 9  \n 3rd Qu.:3.625   3rd Qu.:10.000   slightly conservative: 6                    \n Max.   :4.000   Max.   :37.000   conservative         : 4                    \n                                  very conservative    : 2                    \n\n\n\n\n(b)\nSummarize and interpret results of inferential analyses.\nHere we notice that the more conservative you are the more likely you are to visit religious service on more occasions. However, this is not a very significant trend and it is hard to say with the graph alone. When looking into the xtabs comparing these two results we notice a similar shift there tends to be a higher number of liberals in the never and occasionally sections and then as it gets into most weeks and every week that number drops off quickly and slowly increases on the conservative side.\nWe are also notice a very clear correlation with the hours of watching tv and the highschool gpa. Here we notice that the more hours spent watching tv the less likelu you are to have a higher gpa. There is a negative slope as it drops off between the 10-15 hour mark."
  },
  {
    "objectID": "posts/FinalPart1_StephRoberts.html",
    "href": "posts/FinalPart1_StephRoberts.html",
    "title": "Final Project: Diabetes Prediction",
    "section": "",
    "text": "####Diabetes risk factors\nAccording to the World Health Organization (WHO), an estimated 537 million people worldwide are living with diabetes. It is a leading cause of health complications and even death. The WHO states close to 1.5 million people died due to diabetes and its complication in 2019 alone. It is a growing problem that requires dedicated research to aim at the slowdown and prevention of future cases.\n###Research Questions 1. What risk factors are most predictive of diabetes?\nResearch on Diabetes is ongoing and in-depth within the medical field. The prevalence and incidence of diabetes mellitus type 2 (DQ2) have increased consistently for decades, giving way to an increase in mortality related to diabetes. Commonly in the medical field, many risk factors are used to measure a patient’s risk of developing DM2, such as obesity, family history, hypertension and changes in fasting blood sugar levels. Moreno et al. (2018) studied risk parameters for diabetes and concluded “risk of being diabetic rises in patients whose father has suffered an acute myocardial infarction, in those whose mother or father is diabetic and in patients with a high waist perimeter.” Their focus on family history leaves room for studies more focused on individual medical factors, such as blood pressure, BMI, number of pregnancies, etc. That is the aim of this project.\nM. L. M. V. J. A. (2018, June 11). Predictive risk model for the diagnosis of diabetes mellitus type 2 in a follow-up study 15 Years on: Prodi2 study. European journal of public health. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/29897477/\n\nCan the use of regression analysis help predict risk of diabetes based on several medical variables?\n\nOther research, such as a Edlitz & Segal (2022) study titled Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards, does focus on using individual medical factors to predict risk of diabetes through regression. This project aims to conduct similar analysis on different data.\nE. Y. S. E. (2022, June 22). Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards. eLife. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/35731045/\n###Hypothesis\n\nBody mass index (BMI), glucose, and age are significant predictors of diabetes mellitus type 2.\nRegression analysis can help predict the risk of developing diabetes mellitus type 2.\n\nBoth hypothesis have been tested in the above mentioned studies. The contribution from this project will be the additional support for or against the hypotheses from the analysis of different data.\n###Descriptive Statistics\nThe data was collected by the “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database (PIDD). A total of 768 cases are available in PIDD. However, 5 patients had a glucose of 0, 11 patients had a body mass index of 0, 28 others had a diastolic blood pressure of 0, 192 others had skin fold thickness readings of 0, and 140 others had serum insulin levels of 0. After cleaning the data by removing the cases with numbers that are incompatible with life, 392 cases remained. All patients belong to the Pima Indian heritage (subgroup of Native Americans), and are females aged 21 years and above.\nThe datasets consists of 9 medical predictor (independent) variables and one target (dependent) variable, outcome.\nPregnancies: Number of times a woman has been pregnant Glucose: Plasma Glucose concentration of 2 hours in an oral glucose tolerance test BloodPressure: Diastollic Blood Pressure (mm hg) SkinThickness: Triceps skin fold thickness(mm) Insulin: 2 hour serum insulin(mu U/ml) BMI: Body Mass Index ((weight in kg/height in m)^2) Age: Age(years) DiabetesPedigreeFunction: scores likelihood of diabetes based on family history) Outcome: 0(doesn’t have diabetes) or 1 (has diabetes)\n\n\nCode\ndf<- read_csv(\"diabetes2.csv\")\n\n\nError: 'diabetes2.csv' does not exist in current working directory ('/home/runner/work/603_Fall_2022/603_Fall_2022/posts').\n\n\nCode\ndim(df)\n\n\nNULL\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\nCode\nhead(df)\n\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\n\n\nCode\n#check for null entries\nis.null(df)\n\n\n[1] FALSE\n\n\n\n\nCode\n#Check number of 0s in each column\ncolSums(df==0)\n\n\nError in df == 0: comparison (==) is possible only for atomic and list types\n\n\nGlucose, blood pressure, skin thickness, insulin, BMI and Age are not variables that should logically have 0s. Those values, if true, are likely incompatible with life. We will remove those cases from analysis.\n\n\nCode\n#Remove rows with 0 in respective columns\ndfc <- df[apply(df[c(2:6, 8)],1,function(z) !any(z==0)),] \n\n\nError in df[c(2:6, 8)]: object of type 'closure' is not subsettable\n\n\nCode\n#Verify 0s are gone in selected rows\ncolSums(dfc==0)\n\n\nError in is.data.frame(x): object 'dfc' not found\n\n\n\n\nCode\n#Check cleaned data frame\nglimpse(dfc)\n\n\nError in glimpse(dfc): object 'dfc' not found\n\n\nThe cleaned data frame includes 392 observations, or cases, along the original 9 variables.\n\n\nCode\n#Summarize df\nsummary(dfc)\n\n\nError in summary(dfc): object 'dfc' not found\n\n\nAt a glance, this summary is more fitting after having cleaned our data. An average of 3 pregnancies, considering our 21+ female population, makes sense. A mean glucose of 122, blood pressure of 70.66, a BMI of 33, and age of 30.86 are reasonable accurate of our population. The data is clean and ready for analysis.\n###Analysis\n###Hypothesis Testing\n###Model Comparisons\n###Diagnostics"
  },
  {
    "objectID": "posts/HW1_SteveONeill.html",
    "href": "posts/HW1_SteveONeill.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 2\nI will make a dataframe from the values provided:\n\n\nCode\nprior_convictions=c(0,1,2,3,4)\nfreq=c(128, 434, 160, 64, 24)\nprisondata <- data.frame(prior_convictions, freq)\nprisondata\n\n\n  prior_convictions freq\n1                 0  128\n2                 1  434\n3                 2  160\n4                 3   64\n5                 4   24\n\n\nAnd add a probability column:\n\n\nCode\nprison_prob <- prisondata %>% mutate(prob = freq/sum(freq))\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\n\n2a.\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nFrom the table above, the probability is 0.19753086, nearly 20 percent.\n\n\n2b.\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nhead(prison_prob,2) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.6938272\n\n\nThe probability a randomly selected inmate has has fewer than 2 prior convictions is ~69%.\n\n\n2c.\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nhead(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1  0.891358\n\n\nThe probability a randomly selected inmate has 2 or fewer convictions is ~89%\n\n\n2d.\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ntail(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.3061728\n\n\nThe probability a randomly selected inmate has more than 2 prior convictions is ~30.6%\n\n\n2e.\nWhat is the expected value of the number of prior convictions?\n\n\nCode\nsum(prison_prob$prior_convictions*prison_prob$prob)\n\n\n[1] 1.28642\n\n\nCode\n#Or another way,\n\nweighted.mean(prison_prob$prior_convictions,prison_prob$prob)\n\n\n[1] 1.28642\n\n\nThe expected value of prior convictions is 1.28642\n\n\n2f\n\n\nCode\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\nCode\nvar(prison_prob$freq)\n\n\n[1] 25948\n\n\nCode\nsd(prison_prob$freq)\n\n\n[1] 161.0838\n\n\nThe variance among all prior convictions is 25948. The standard deviation among all prior convictions is 161.0838."
  },
  {
    "objectID": "posts/KPopiela_FinalProposal.html",
    "href": "posts/KPopiela_FinalProposal.html",
    "title": "KPopiela_final p1",
    "section": "",
    "text": "#I will need to do some more thorough testing to make sure I can actually do this, but I'd like to focus my final project on ethnic violence since I know a lot about it (I wrote my undergrad thesis on ethno-religious violence in the Polish-Ukrainian borderlands). I found some data sets for my DACSS-601 intensive final that were pretty useful - I found a lot of information but now that I will have the statistical background I'd like to see if I can go further with it. Specifically in the sense of finding out statistics related to the likelihood of an eruption of ethnic violence in countries that fit specific criteria on paper. I can come up with a different research question for this topic area if my initial idea isn't feasible though.  \n\n#These criteria are as follows:  \n# - The population doesn't have an overwhelming ethnic majority; there are 2+ groups, each with substantial numbers.  \n# - History of socio-political repression by one group against the other(s) when said group has political power/alternating episodes of targeted political repression depending on what group holds a political majority.  \n# - The country/population is in a state of severe socio-political instability (war, territorial conquest, political power vacuum, etc.)  \n\n#To make this a little less challenging, I'm going to simplify things for myself. First, I will narrow the data down geographically and temporally - I'm going to focus on former Yugoslav states and the former USSR (I might change the location though). Second, since the data sets I'll be using are pretty big, I'm also going to look at certain columns based on the criteria I presented above (ethnic groups involved, group(s) being oppressed and by whom, and presence/absence of political instability). \n\n# Research Question: Based on what the data show, (1) is there actually a higher probability of ethnic armed conflict/war when these conditions, and (2) does one particular condition have a greater effect on political stability than the others?\n\n\nlibrary(readr)\nlibrary(poliscidata)\n\nError in library(poliscidata): there is no package called 'poliscidata'\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(foreign)\n\n##Summary Stats/Visuals\n\n#All data sets I will be using are from the Harvard Dataverse and they are as follows:  \n\n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Power Relations dataset\", https://doi.org/10.7910/DVN/NDJUJM, Harvard Dataverse, V1, UNF:5:k4xxXC2ASI204QZ4jqvUrQ== [fileUNF]  \n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Armed Conflict dataset\", https://doi.org/10.7910/DVN/K3OIJQ, Harvard Dataverse, V1  \n#    UCDP/PRIO Armed Conflict Dataset version 22.1. Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).\n\n#I don't need to look at these in any particular order, so I'm just going to present them in the order they are in above. \n\n\n# Data set 1: \"Ethnic Power Relations dataset\"\n\nethnic_power_relations <- MASTER_EPR_v1_IrgFiR\n\nError in eval(expr, envir, enclos): object 'MASTER_EPR_v1_IrgFiR' not found\n\nethnic_power_relations <- ethnic_power_relations %>%\n  select(statename,from,to,group,status,size) %>%\n  filter(statename == c(\"Albania\",\"Croatia\",\"Bosnia and Herzegovina\",\"Yugoslavia\",\"Macedonia\",\"Poland\",\"Ukraine\",\"Russia\",\"Hungary\",\"Romania\",\"Bulgaria\"), from >= 1980) \n\nError in select(., statename, from, to, group, status, size): object 'ethnic_power_relations' not found\n\nethnic_power_relations\n\nError in eval(expr, envir, enclos): object 'ethnic_power_relations' not found\n\n\n\n#Data set 2: \"Ethnic Armed Conflict dataset\"\n#I'm going to use select() to look at \"country\", \"startyr\", \"endyr\", and \"ETHNOWAR\". Then I will use filter() to meet my geographic requirements.\n\nethnic_armed_conflict <- EAC_edPcfy\n\nError in eval(expr, envir, enclos): object 'EAC_edPcfy' not found\n\nethnic_armed_conflict <- ethnic_armed_conflict %>%\n  select(country, startyr, endyr, ETHNOAIMS, ETHNOWAR) %>%\n  filter(country == c(\"Croatia\",\"Yugoslavia\",\"Bosnia and Herzegovina\",\"USSR\",\"Russia\"),startyr >= 1980)\n\nError in select(., country, startyr, endyr, ETHNOAIMS, ETHNOWAR): object 'ethnic_armed_conflict' not found\n\nethnic_armed_conflict\n\nError in eval(expr, envir, enclos): object 'ethnic_armed_conflict' not found\n\n#This data set is based off of another one (Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).) so I will include that as well\n\n#NOTE: I don't know why only 2 of the 5 countries I listed are showing up\n\n\n#Data set 3: \"UCDP/PRIO Armed Conflict Dataset\" version 22.1\n\nUCDP_Prio_AC <- ucdp_prio_acd_221_wKBkVs\n\nError in eval(expr, envir, enclos): object 'ucdp_prio_acd_221_wKBkVs' not found\n\nUCDP_Prio_AC <- UCDP_Prio_AC %>%\n  select(location, side_a, side_b, start_date) %>%\n  filter(location == c(\"Yugoslavia\",\"Croatia\",\"Serbia\",\"Russia\"))\n\nError in select(., location, side_a, side_b, start_date): object 'UCDP_Prio_AC' not found\n\nUCDP_Prio_AC\n\nError in eval(expr, envir, enclos): object 'UCDP_Prio_AC' not found\n\n#NOTE: I don't know why this one is doing the same thing as the previous one.\n\n\n#I'm obviously going to do more in-depth work with all three data sets, these (below) are just kind of a sample of what I will be doing.\n\n\nethnic_power_relations %>%\n  ggplot(mapping=aes(x=group,y=size,col=status)) + geom_point() + coord_flip()\n\nError in ggplot(., mapping = aes(x = group, y = size, col = status)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  summarise(mean(size))\n\nError in summarise(., mean(size)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  count(status)\n\nError in count(., status): object 'ethnic_power_relations' not found"
  },
  {
    "objectID": "posts/KPopiela_FinalP2.html",
    "href": "posts/KPopiela_FinalP2.html",
    "title": "KPopiela_Finalp2",
    "section": "",
    "text": "library(poliscidata)\n\nError in library(poliscidata): there is no package called 'poliscidata'\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stats)\nlibrary(ggplot2)\n##Intro and Background Info\nI’ve had more time to think about my final, and I’ve decided to simplify my research topic. Rather than look at ethnic conflict, I’m going to look at how internet usage affects a people’s confidence in their country (institutions and governance). In order to focus on this relationship, I will be using the ‘world’ data set in the ‘poliscidata’ package, and I will be using the following variables: ‘country’, ‘unnetuse’ (internet usage per 100 people), confidence (population’s confidence in their country’s institutions; scaled out of 100), and ‘effectiveness’ (government effectiveness scale).\nBelow is a general presentation of the data I will be using, filtered down as I described.\nThe internet and media consumption play a massive role in politics now; at this point, it’s pretty much expected to find out what is happening in the world on the internet (newspapers, social media, etc.). And as we’ve seen since the 2016 election, the internet also plays a substantial role in domestic politics, whether it be through neutral journalism, activism, whistle-blowing, or misinformation. It is both good and bad, and as Stanford professor Evgeny Morozov writes, the internet’s impact “will depend on individual conditions” such as a given country’s political atmosphere (peaceful, volatile, conservative, liberal, etc.), ease of access to the internet, and how united a population is either in favor or against their current government (“The Internet, Politics and the Politics of Internet Debate”). So, what /is/ the relationship between internet use and domestic politics?\nBased on my personal and academic experience, I hypothesize that while internet usage definitely does affect people’s confidence in their government, I do not think that it automatically hinders government effectiveness. If a certain country’s subjects have extremely little confidence in its government, the government would have no legitimacy (and would therefore be ineffective); a government’s power largely comes from its subjects’ willingness to accept it. However, there are instances where a country’s population has little confidence in its government with no effect on its efficacy. But, again, for this project I will be focusing on how internet usage impacts confidence in a given regime.\nNow to transcribe my research question: Does internet usage (social media, news, etc.) have an impact on people’s confidence in their government (‘confidence’)? If so, is said government more or less effective in its duties?\nFirst, lets look at some summary statistics."
  },
  {
    "objectID": "posts/KPopiela_FinalP2.html#hypothesis-testing-analysis",
    "href": "posts/KPopiela_FinalP2.html#hypothesis-testing-analysis",
    "title": "KPopiela_Finalp2",
    "section": "Hypothesis Testing, Analysis",
    "text": "Hypothesis Testing, Analysis\nI will start with some preliminary graphs/visualizations.\n\nggplot(data=world_filter, aes(x=unnetuse,y=confidence, color=regime_type3)) + geom_point() \n\nError in ggplot(data = world_filter, aes(x = unnetuse, y = confidence, : object 'world_filter' not found\n\n\nThis scatterplot, which measures the relationship between internet usage and popular confidence in government, has no obvious relationship or correlation. The points are grouped by color based on regime type, but there is no notable increase or decrease in the points’ location as a variable increases or decreases. The vast majority of values are located in the middle of the graph.\n\nggplot(data=world_filter, aes(x=confidence,y=effectiveness, color=regime_type3)) + geom_point() \n\nError in ggplot(data = world_filter, aes(x = confidence, y = effectiveness, : object 'world_filter' not found\n\n\nWhile there still isn’t a traditional positive or negative relationship between ‘confidence’ and ‘effectiveness’, it is noteworthy that the majority of values are in the center of the graph. They have a wide range, but they are mostly within 25 and 75 on the x-axis, which does make sense giving that the 1st and 3rd quantiles are included in the initial summary statistics.\n\nggplot(world_filter, aes(x=unnetuse,y=confidence,color=regime_type3)) + geom_point() + stat_smooth(method=\"lm\",col=\"blue\")\n\nError in ggplot(world_filter, aes(x = unnetuse, y = confidence, color = regime_type3)): object 'world_filter' not found\n\n\nThis visualization is a recreation of the first scatterplot, but with a multiple regression line. The line, at first glance, is straight, but upon closer inspection it ever so slightly slopes down toward the right. This indicates a slightly negative relationship between internet usage (x) and confidence in government institutions (y).\n\nggplot(world_filter, aes(x=unnetuse,y=effectiveness,color=regime_type3)) + geom_point() + stat_smooth(method=\"lm\",col=\"blue\")\n\nError in ggplot(world_filter, aes(x = unnetuse, y = effectiveness, color = regime_type3)): object 'world_filter' not found\n\n\nThe above scatterplot, however, depicts a very clear positive relationship between variables ‘unnetuse’ (x) and ‘effectiveness’ (y). This is indicative of a strong relationship between internet use and government effectiveness. There is what appears to be a heavier concentration of points below 50 on the x-axis, but the general trend is pretty clear that government effectiveness increases as nationals’ internet use does. Although ‘unnetuse’ comes from a 2008 survey, I would argue that this trend still holds up; especially due to COVID-19, the main way that politicians engage with their constituents is on the internet (social media). There is not really a culture of “visit your politician and talk to them” culture anymore, the new norm is for sucht things to be done over the phone, over email, or in some form, over the internet.\nMy next step is to go into modeling. I’m going to start with a linear regression model. ‘confidence’ is the dependent variable, and ‘unnetuse’, ‘effectiveness’, and ‘regime_type3’ are all independent.\n\nconfidence_lm <- lm(confidence~unnetuse+effectiveness+regime_type3, data=world_filter)\n\nError in is.data.frame(data): object 'world_filter' not found\n\nconfidence_lm\n\nError in eval(expr, envir, enclos): object 'confidence_lm' not found\n\n\nAnd here is a visualization of the above values.\n\nplot(fitted(confidence_lm))\n\nError in fitted(confidence_lm): object 'confidence_lm' not found\n\nabline(h=50, lty=2)\n\nError in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet\n\n\n\nsummary(confidence_lm)\n\nError in summary(confidence_lm): object 'confidence_lm' not found\n\n\nI’m honestly not entirely sure what these values mean, but I input what I had into the function and I got results.\n\nAIC(confidence_lm)\n\nError in AIC(confidence_lm): object 'confidence_lm' not found\n\nBIC(confidence_lm)\n\nError in BIC(confidence_lm): object 'confidence_lm' not found"
  },
  {
    "objectID": "posts/StephRobertsHW1.html",
    "href": "posts/StephRobertsHW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Homework 1\n##1. Use the LungCapData to answer the following questions. (Hint: Using dplyr, especiallygroup_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nCode\ndf<- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nhead(df)\n\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\n#Summarize\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmean(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmedian(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nvar(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nsd(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmin(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\nCode\nmax(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n#a. What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=0.5,col='black',fill='gray')\n\n\nError in `ggplot()`:\n! `data` cannot be a function.\nℹ Have you misspelled the `data` argument in `ggplot()`\n\n\nThe histogram follows a distribution close to normal distibution. In fact, if we change binwidth slightly, it appears even closer to normal distribution.\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')\n\n\nError in `ggplot()`:\n! `data` cannot be a function.\nℹ Have you misspelled the `data` argument in `ggplot()`\n\n\nThis helps illustrate the importance of binwidth and what it can do to our visualization interpretations.\n#b. Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nggplot(df, aes(x = LungCap, y = Gender)) +        \n  geom_boxplot()\n\n\nError in `ggplot()`:\n! `data` cannot be a function.\nℹ Have you misspelled the `data` argument in `ggplot()`\n\n\nThe distribution of male lung capacity is larger and longer than females’.\n#c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean() \n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nCode\ndf %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nIt does not make sense at face value. In this sample, smokers have a higher mean lung capacity than non-smokers. Let’s check how big each subsample is.\n\n\nCode\nlength(which(df$Smoke == 'yes'))\n\n\nError in df$Smoke: object of type 'closure' is not subsettable\n\n\nCode\nlength(which(df$Smoke == 'no'))\n\n\nError in df$Smoke: object of type 'closure' is not subsettable\n\n\nAs suspected, there are far more, almost 10 times as many, non-smokers. If we could gather data from all the smokers, perhaps our means would look a lot different. Maybe our sample was taken from young people whose lungs have not been long affected by the smoking.\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(Age) %>%\n  median() \n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nAgain, as suspected, our sample of smokers is a young age. Therefore, the lack of difference in lung capacity between smokers and non-smokers is not too surprising.\n#d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Create age groups\ndf <- df %>% \n  mutate(agegroup = case_when(\n    Age <= 13  ~ \"less than or equal to 13\",\n    Age >= 14 & Age <= 15 ~ \"14 to 15\",\n    Age >= 16 & Age <= 17 ~ \"16 TO 17\",\n    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in UseMethod(\"mutate\"): no applicable method for 'mutate' applied to an object of class \"function\"\n\n\nCode\ntable(df$agegroup)\n\n\nError in df$agegroup: object of type 'closure' is not subsettable\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  ggplot(aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')+\n  facet_wrap(~agegroup)\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nThese histograms suggest that participants 13 or younger have smaller lung capacity. The Lung capacity seems to generally increase with age as children grow.\n#e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(df, aes(x = LungCap, \n           fill = agegroup)) +\n  geom_density(alpha = 0.4)+\n  facet_wrap(~Smoke)\n\n\nError in `ggplot()`:\n! `data` cannot be a function.\nℹ Have you misspelled the `data` argument in `ggplot()`\n\n\nThis visualization starts to explain furthermore why there is an unexpected result for lung capacity in smokers vs. non-smokers. As we have deducted, lung capacity generally improves with age (in growing years). However, teenagers approaching adulthood are also a group more likely to have access or influence to smoking cigarettes. It is likely that our smokers account for some of the older participants, who happen to be closer to normal smoking age.\n#f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\ncov(df$LungCap, df$Age) #calculate covariance\n\n\nError in df$Age: object of type 'closure' is not subsettable\n\n\nCode\ncor(df$LungCap, df$Age) #calculate correlation\n\n\nError in df$Age: object of type 'closure' is not subsettable\n\n\nA positive coraviance (8.74) indicates lung capacity and age tend to increase together. The positive correlation relatively close to 1 (0.82) indicates there is a fairly strong correlation between the variables.\n##2. Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n#create the sample\nx<-rep(c(0,1,2,3,4),times=c(128, 434, 160, 64, 24))\nsample(x, 10)\n\n\n [1] 1 2 1 1 1 0 2 3 1 1\n\n\nCode\n#Verify n of sample\nsum(128, 434, 160, 64, 24)\n\n\n[1] 810\n\n\n\n\nCode\n#Calculate the mean\nmean(x)\n\n\n[1] 1.28642\n\n\nCode\n#Verify the mean\nsample_mean <- (((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/810)\nprint(sample_mean)\n\n\n[1] 1.28642\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016\n\n\n#a. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n#probability of 2 convictions?\ndnorm.convict <- dnorm(2, mean(x), sd(x))\nprint(dnorm.convict)\n\n\n[1] 0.3201613\n\n\nThe probability of 2 convications in 0.32.\n#b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#probability of <2 convictions\nless.than <- pnorm(2, mean(x), sd(x)) - dnorm.convict\nprint(less.than)\n\n\n[1] 0.4593924\n\n\nThe probability of <2 convictions is 0.46.\n#c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n#probability of =<2 convictions?\npnorm.convict <- pnorm(2, mean(x), sd(x))\nprint(pnorm.convict)\n\n\n[1] 0.7795537\n\n\nThe probability of less than or equal to 2 convictions is 0.78.\n#d. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n#probability of >2 convictions?\ngreater.than <- 1 - pnorm.convict\nprint(greater.than)\n\n\n[1] 0.2204463\n\n\nThe probability of greater than 2 convictions is 0.22.\n\n\nCode\n#Verify all probabilities add to 1\nless.than + dnorm.convict + greater.than\n\n\n[1] 1\n\n\n#e. What is the expected value for the number of prior convictions?\n\n\nCode\n# Expected value of a probability distribution  can be found with μ = Σx * P(x), where x = data value and P(x) = probability of data. \n\n#Calculate probabilities of data\np0 <- dnorm(0, mean(x), sd(x))\np0\n\n\n[1] 0.1641252\n\n\nCode\np1 <- dnorm(1, mean(x), sd(x))\np1\n\n\n[1] 0.410739\n\n\nCode\np2 <- dnorm(2, mean(x), sd(x))\np2\n\n\n[1] 0.3201613\n\n\nCode\np3 <- dnorm(3, mean(x), sd(x))\np3\n\n\n[1] 0.07772916\n\n\nCode\np4 <- dnorm(4, mean(x), sd(x))\np4\n\n\n[1] 0.005877753\n\n\nCode\n#Calculate expected value\nev <- sum((0*p0), (1*p1), (2*p2), (3*p3), (4*p4))\nev\n\n\n[1] 1.30776\n\n\nCode\n#The expected value should be close to the mean in a normal distribution\nmean(x)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.31.\n#f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n#Calculate variance\nvar(x)\n\n\n[1] 0.8572937\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html",
    "href": "posts/KenDocekal_finalproject2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\n#Expanded Data Description\n###Variables and missing data\nSpecifying variables for relevance and data availability\nWe transform co2 emission variable from raw tons to ton per state resident\nCreating data subset with new variables, removing pre-transformation and aggregate population measures\nDue to additional missing observations we will set the new data range as 1980 to 2000.\nNAs are reduced as listwise removal of observations that are missing completely at random analysis will be unbiased\nExcluding District of Columbia, Alaska, Hawaii to further reduce NAs\n###Visual description\nMost explanatory variables like econdev, pldvpag, and urbrenen are dummy variables.\nThe exception is policypriorityscore which is normally distributed.\nControl variables nonwhite, soc_capital_ma, evangelical_pop, pc_inc_ann, gini_coef, hsdiploma, and co2 are normally distributed\nResponse variables are also linear but st_ec and st_soc are only avaliable for the year 2000\nThe relationship between response st_ec and select explanatory and control variables in scatterplots:\nWith log(co2):\nSimilar relationship with boxplots:"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section",
    "href": "posts/KenDocekal_finalproject2.html#section",
    "title": "Final Project Part 2",
    "section": "1",
    "text": "1\nCross-validation with training and test set\n\n\nCode\nset.seed(1)\nsd2 %>% nrow() %>% multiply_by(0.7) %>% round() -> training_set_size\ntrain_indices <- sample(1:nrow(sd2), training_set_size)\ntrain <- sd2[train_indices,]\ntest <- sd2[-train_indices,]\n\n\n\n\nCode\nnrow(train)\n\n\n[1] 706\n\n\n\n\nCode\nnrow(test)\n\n\n[1] 302\n\n\nBase model with all variables\n\n\nCode\nbase <- lm(st_ec ~ . , data = train)\n\n\n\n\nCode\nplot(base)\n\n\nWarning: not plotting observations with leverage one:\n  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nError in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...): y is empty or has only NAs\n\n\n\n\n\nAttempt to remove state\n\n\nCode\nno_region <- lm(st_ec ~ . - state, data = train)\n\n\nWith interaction term\n\n\nCode\nia <- lm(st_ec ~ . + pc_inc_ann*hsdiploma, data = train)\n\n\n\n\nCode\nplot(ia)\n\n\nWarning: not plotting observations with leverage one:\n  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nError in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...): y is empty or has only NAs\n\n\n\n\n\n\n\nCode\nmean((predict(base, test) - test$st_ec)^2) -> MSE_base\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(no_region, test) - test$st_ec)^2) -> MSE_no_region\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(ia, test) - test$st_ec)^2) -> MSE_ia\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(ia, test) - test$st_ec)^2) -> MSE_ia\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section-1",
    "href": "posts/KenDocekal_finalproject2.html#section-1",
    "title": "Final Project Part 2",
    "section": "2",
    "text": "2\nBackward elimination\n\n\nCode\nlm(st_ec ~ ., data = sd2) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ ., data = sd2)\n\nResiduals:\nALL 47 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (14 not defined because of singularities)\n                     Estimate Std. Error t value Pr(>|t|)\n(Intercept)         -0.038403        NaN     NaN      NaN\nstateArizona        -0.012308        NaN     NaN      NaN\nstateArkansas        0.021562        NaN     NaN      NaN\nstateCalifornia      0.031151        NaN     NaN      NaN\nstateColorado       -0.080098        NaN     NaN      NaN\nstateConnecticut     0.131448        NaN     NaN      NaN\nstateDelaware        0.014173        NaN     NaN      NaN\nstateFlorida         0.049612        NaN     NaN      NaN\nstateGeorgia        -0.067636        NaN     NaN      NaN\nstateIdaho          -0.148045        NaN     NaN      NaN\nstateIllinois        0.062778        NaN     NaN      NaN\nstateIndiana        -0.036880        NaN     NaN      NaN\nstateIowa           -0.132971        NaN     NaN      NaN\nstateKansas         -0.071066        NaN     NaN      NaN\nstateKentucky        0.054573        NaN     NaN      NaN\nstateLouisiana       0.021910        NaN     NaN      NaN\nstateMaine           0.147176        NaN     NaN      NaN\nstateMaryland        0.145870        NaN     NaN      NaN\nstateMassachusetts   0.221486        NaN     NaN      NaN\nstateMichigan        0.037647        NaN     NaN      NaN\nstateMinnesota      -0.053475        NaN     NaN      NaN\nstateMississippi     0.021773        NaN     NaN      NaN\nstateMissouri       -0.080424        NaN     NaN      NaN\nstateMontana         0.044836        NaN     NaN      NaN\nstateNevada         -0.018206        NaN     NaN      NaN\nstateNew Hampshire   0.103719        NaN     NaN      NaN\nstateNew Jersey      0.151180        NaN     NaN      NaN\nstateNew Mexico      0.021713        NaN     NaN      NaN\nstateNew York        0.230023        NaN     NaN      NaN\nstateNorth Carolina  0.057222        NaN     NaN      NaN\nstateNorth Dakota   -0.119446        NaN     NaN      NaN\nstateOhio            0.019121        NaN     NaN      NaN\nstateOklahoma       -0.043207        NaN     NaN      NaN\nstateOregon          0.056329        NaN     NaN      NaN\nstatePennsylvania    0.126404        NaN     NaN      NaN\nstateRhode Island    0.187720        NaN     NaN      NaN\nstateSouth Carolina -0.001439        NaN     NaN      NaN\nstateSouth Dakota   -0.176105        NaN     NaN      NaN\nstateTennessee       0.032912        NaN     NaN      NaN\nstateTexas          -0.067259        NaN     NaN      NaN\nstateUtah           -0.090002        NaN     NaN      NaN\nstateVermont         0.208597        NaN     NaN      NaN\nstateVirginia        0.062810        NaN     NaN      NaN\nstateWashington     -0.006842        NaN     NaN      NaN\nstateWest Virginia   0.128481        NaN     NaN      NaN\nstateWisconsin       0.005363        NaN     NaN      NaN\nstateWyoming        -0.127431        NaN     NaN      NaN\nyear                       NA         NA      NA       NA\npolicypriorityscore        NA         NA      NA       NA\necondev                    NA         NA      NA       NA\npldvpag                    NA         NA      NA       NA\nurbrenen                   NA         NA      NA       NA\nnonwhite                   NA         NA      NA       NA\nsoc_capital_ma             NA         NA      NA       NA\nevangelical_pop            NA         NA      NA       NA\npc_inc_ann                 NA         NA      NA       NA\ngini_coef                  NA         NA      NA       NA\nhsdiploma                  NA         NA      NA       NA\nco2                        NA         NA      NA       NA\nideo                       NA         NA      NA       NA\nst_soc                     NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 46 and 0 DF,  p-value: NA\n\n\n\n\nCode\nsd3 = subset(sd2, select = c(year, policypriorityscore, econdev, pldvpag, urbrenen, nonwhite, soc_capital_ma, evangelical_pop, pc_inc_ann, gini_coef, hsdiploma, co2, ideo,st_ec, st_soc))\n\n\n\n\nCode\nlm(st_ec ~ ., data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ ., data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.100478 -0.032490 -0.005094  0.039044  0.098876 \n\nCoefficients: (1 not defined because of singularities)\n                      Estimate Std. Error t value Pr(>|t|)   \n(Intercept)          1.220e+00  3.456e-01   3.531  0.00125 **\nyear                        NA         NA      NA       NA   \npolicypriorityscore -4.394e-01  1.866e-01  -2.355  0.02462 * \necondev             -1.698e-02  1.804e-02  -0.941  0.35331   \npldvpag              3.069e-02  3.116e-02   0.985  0.33179   \nurbrenen             1.308e-02  2.683e-02   0.488  0.62906   \nnonwhite            -2.035e-01  1.042e-01  -1.953  0.05929 . \nsoc_capital_ma      -3.491e-02  2.629e-02  -1.328  0.19324   \nevangelical_pop     -7.226e-04  9.797e-04  -0.738  0.46601   \npc_inc_ann          -2.421e-06  4.374e-06  -0.554  0.58365   \ngini_coef           -4.285e-01  4.835e-01  -0.886  0.38192   \nhsdiploma           -1.004e-02  3.590e-03  -2.798  0.00851 **\nco2                 -1.521e+02  7.283e+02  -0.209  0.83581   \nideo                 1.505e-01  1.641e-01   0.917  0.36572   \nst_soc               3.714e-01  1.279e-01   2.905  0.00651 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05653 on 33 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7675,    Adjusted R-squared:  0.6759 \nF-statistic: 8.378 on 13 and 33 DF,  p-value: 4.133e-07\n\n\nInclusion of policypriorityscore, nonwhite, hsdiploma, and st_soc only has the highest adjusted R squared\n\n\nCode\nlm(st_ec ~ .-econdev -pldvpag -urbrenen -soc_capital_ma -evangelical_pop -pc_inc_ann -gini_coef -co2 -ideo , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ . - econdev - pldvpag - urbrenen - soc_capital_ma - \n    evangelical_pop - pc_inc_ann - gini_coef - co2 - ideo, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.108606 -0.039738 -0.002334  0.031586  0.101660 \n\nCoefficients: (1 not defined because of singularities)\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          1.174528   0.267140   4.397 7.35e-05 ***\nyear                       NA         NA      NA       NA    \npolicypriorityscore -0.585220   0.136907  -4.275 0.000108 ***\nnonwhite            -0.206434   0.076446  -2.700 0.009941 ** \nhsdiploma           -0.013410   0.003006  -4.461 6.00e-05 ***\nst_soc               0.360374   0.054152   6.655 4.54e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05607 on 42 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7089,    Adjusted R-squared:  0.6811 \nF-statistic: 25.57 on 4 and 42 DF,  p-value: 8.847e-11"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section-2",
    "href": "posts/KenDocekal_finalproject2.html#section-2",
    "title": "Final Project Part 2",
    "section": "3",
    "text": "3\nForward selection\n\n\nCode\nlm(st_ec ~ policypriorityscore , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.162156 -0.054892  0.003181  0.042129  0.235594 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.02663    0.01155  -2.305   0.0258 *  \npolicypriorityscore -0.92890    0.17092  -5.435 2.14e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.078 on 45 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.3963,    Adjusted R-squared:  0.3829 \nF-statistic: 29.54 on 1 and 45 DF,  p-value: 2.136e-06\n\n\n\n\nCode\nlm(st_ec ~ policypriorityscore + econdev, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + econdev, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.167145 -0.058159  0.008671  0.040615  0.230549 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.02175    0.01595  -1.364    0.180    \npolicypriorityscore -0.92496    0.17268  -5.357 2.95e-06 ***\necondev             -0.01033    0.02304  -0.448    0.656    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0787 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.399, Adjusted R-squared:  0.3717 \nF-statistic: 14.61 on 2 and 44 DF,  p-value: 1.364e-05\n\n\n\n\nCode\nlm(st_ec ~ policypriorityscore + pldvpag, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + pldvpag, data = sd3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13585 -0.05871  0.00821  0.03793  0.22754 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.07464    0.03463  -2.155   0.0367 *  \npolicypriorityscore -0.88617    0.17126  -5.175 5.41e-06 ***\npldvpag              0.05428    0.03698   1.468   0.1492    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07702 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.4245,    Adjusted R-squared:  0.3983 \nF-statistic: 16.23 on 2 and 44 DF,  p-value: 5.267e-06\n\n\nEvangelical_pop is does not increase p-value\n\n\nCode\nlm(st_ec ~ policypriorityscore + evangelical_pop, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + evangelical_pop, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.175633 -0.037558  0.000244  0.049656  0.188991 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          0.0217794  0.0177642   1.226   0.2267    \npolicypriorityscore -0.7956940  0.1591509  -5.000 9.65e-06 ***\nevangelical_pop     -0.0025410  0.0007552  -3.365   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07035 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.5198,    Adjusted R-squared:  0.498 \nF-statistic: 23.82 on 2 and 44 DF,  p-value: 9.786e-08\n\n\nAll other additions increase p-value\n\n\nCode\nlm(st_ec ~ policypriorityscore + evangelical_pop + pc_inc_ann, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + evangelical_pop + \n    pc_inc_ann, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.173379 -0.041717 -0.000328  0.051878  0.191366 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -7.023e-03  9.369e-02  -0.075   0.9406    \npolicypriorityscore -7.852e-01  1.643e-01  -4.780 2.07e-05 ***\nevangelical_pop     -2.388e-03  9.052e-04  -2.639   0.0115 *  \npc_inc_ann           9.061e-07  2.893e-06   0.313   0.7556    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07108 on 43 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.5209,    Adjusted R-squared:  0.4875 \nF-statistic: 15.59 on 3 and 43 DF,  p-value: 5.269e-07\n\n\n##4\nLog-linear transformation\n\n\nCode\nlm(log(st_ec) ~ .-econdev -pldvpag -urbrenen -soc_capital_ma -evangelical_pop -pc_inc_ann -gini_coef -co2 -ideo , data = sd3) |> summary()\n\n\nWarning in log(st_ec): NaNs produced\n\n\n\nCall:\nlm(formula = log(st_ec) ~ . - econdev - pldvpag - urbrenen - \n    soc_capital_ma - evangelical_pop - pc_inc_ann - gini_coef - \n    co2 - ideo, data = sd3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9592 -0.4953 -0.1379  0.5761  1.1909 \n\nCoefficients: (1 not defined because of singularities)\n                    Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         10.00268    6.53823   1.530  0.15001   \nyear                      NA         NA      NA       NA   \npolicypriorityscore -4.83181    3.11076  -1.553  0.14436   \nnonwhite            -2.51193    1.63967  -1.532  0.14950   \nhsdiploma           -0.15322    0.07597  -2.017  0.06484 . \nst_soc               4.59439    1.49963   3.064  0.00906 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7851 on 13 degrees of freedom\n  (990 observations deleted due to missingness)\nMultiple R-squared:  0.5959,    Adjusted R-squared:  0.4715 \nF-statistic: 4.792 on 4 and 13 DF,  p-value: 0.0135\n\n\n##5\nLinear-log\n\n\nCode\nlm(st_ec ~ log(policypriorityscore) + nonwhite + hsdiploma + st_soc , data = sd3) |> summary()\n\n\nWarning in log(policypriorityscore): NaNs produced\n\n\n\nCall:\nlm(formula = st_ec ~ log(policypriorityscore) + nonwhite + hsdiploma + \n    st_soc, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.115600 -0.050721 -0.000798  0.040976  0.097815 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)   \n(Intercept)               0.990308   0.600184   1.650  0.11729   \nlog(policypriorityscore) -0.006349   0.016818  -0.377  0.71048   \nnonwhite                 -0.182903   0.165846  -1.103  0.28546   \nhsdiploma                -0.011932   0.006463  -1.846  0.08233 . \nst_soc                    0.387463   0.100425   3.858  0.00126 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06913 on 17 degrees of freedom\n  (986 observations deleted due to missingness)\nMultiple R-squared:  0.4971,    Adjusted R-squared:  0.3788 \nF-statistic: 4.201 on 4 and 17 DF,  p-value: 0.01516\n\n\nLinear-log for nonwhite only results in highest adjusted R-squared\n\n\nCode\nlm(st_ec ~ policypriorityscore + log(nonwhite) + hsdiploma + st_soc , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + log(nonwhite) + hsdiploma + \n    st_soc, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.105985 -0.038466  0.001818  0.030985  0.099538 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          0.99820    0.23277   4.288 0.000103 ***\npolicypriorityscore -0.58543    0.13616  -4.300 9.96e-05 ***\nlog(nonwhite)       -0.03483    0.01247  -2.794 0.007814 ** \nhsdiploma           -0.01259    0.00282  -4.464 5.95e-05 ***\nst_soc               0.34288    0.05219   6.570 6.02e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05578 on 42 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7119,    Adjusted R-squared:  0.6844 \nF-statistic: 25.94 on 4 and 42 DF,  p-value: 7.146e-11\n\n\n##6\nTwo Period Panel data comparing 1990 and 2000\n\n\nCode\nsd1990 <- subset(sd2, year = 1990, na.rm = TRUE ) \n\nsd2000 <- subset(sd2, year = 1990, na.rm = TRUE ) \n\n\n\n\nCode\ndiff_ideo <- sd2000$ideo - sd1990$ideo\ndiff_econdev<- sd2000$econdev - sd1990$econdev\n\n\n\n\nCode\ninstall.packages(\"AER\")\n\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\n\nCode\ninstall.packages(\"plm\")\n\n\nInstalling package into '/home/runner/work/_temp/Library'\n(as 'lib' is unspecified)\n\n\nalso installing the dependencies 'miscTools', 'rbibutils', 'bdsmatrix', 'collapse', 'maxLik', 'Rdpack'\n\n\n\n\nCode\nlibrary(AER)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\n\nCode\nlibrary(plm)\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\n\nEstimating a regression using differenced data\n\n\nCode\nideo_mod <- lm(diff_ideo ~ diff_econdev)\n\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)        0          0     NaN      NaN\n\n\n\n\nCode\n# plot the differenced data\nplot(x = diff_econdev, \n     y = diff_ideo, \n     xlab = \"Change in econdev\",\n     ylab = \"Change in ideo\",\n     main = \"Changes in Economic Development Agency Presence and Ideology in 1990-2000\",\n     xlim = c(-0.6, 0.6),\n     ylim = c(-1.5, 1),\n     pch = 20, \n     col = \"steelblue\")\n\n\n\n\n\nCode\n# add the regression line to plot\nabline(ideo_mod, lwd = 1.5)\n\n\nError in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): 'a' and 'b' must be finite\n\n\n##7\nEstimating a combined time and entity fixed effects regression model\n\n\nCode\nideo_lm_mod <- lm(ideo ~ econdev + state + year - 1, data = sd2)\nideo_lm_mod\n\n\n\nCall:\nlm(formula = ideo ~ econdev + state + year - 1, data = sd2)\n\nCoefficients:\n            econdev         stateAlabama         stateArizona  \n          0.0001636           -1.2154481           -1.1040309  \n      stateArkansas      stateCalifornia        stateColorado  \n         -1.1651876           -1.0136333           -1.0488766  \n   stateConnecticut        stateDelaware         stateFlorida  \n         -0.9902822           -1.0545878           -1.0874490  \n       stateGeorgia           stateIdaho        stateIllinois  \n         -1.1408023           -1.2246658           -1.0367910  \n       stateIndiana            stateIowa          stateKansas  \n         -1.1247901           -1.1017004           -1.1408348  \n      stateKentucky       stateLouisiana           stateMaine  \n         -1.1019753           -1.1939602           -1.0447755  \n      stateMaryland   stateMassachusetts        stateMichigan  \n         -0.9994727           -0.9369923           -1.0582279  \n     stateMinnesota     stateMississippi        stateMissouri  \n         -1.0541859           -1.2190042           -1.1057527  \n       stateMontana        stateNebraska          stateNevada  \n         -1.1091557           -1.1442449           -1.0049900  \n stateNew Hampshire      stateNew Jersey      stateNew Mexico  \n         -1.0215205           -0.9929828           -1.1021508  \n      stateNew York  stateNorth Carolina    stateNorth Dakota  \n         -0.9780673           -1.1645759           -1.1695092  \n          stateOhio        stateOklahoma          stateOregon  \n         -1.0620687           -1.2172240           -1.0312511  \n  statePennsylvania    stateRhode Island  stateSouth Carolina  \n         -1.0576267           -0.9651297           -1.1801724  \n  stateSouth Dakota       stateTennessee           stateTexas  \n         -1.2298914           -1.1428635           -1.1739866  \n          stateUtah         stateVermont        stateVirginia  \n         -1.1972355           -0.9604917           -1.1262503  \n    stateWashington   stateWest Virginia       stateWisconsin  \n         -1.0045938           -1.0731507           -1.0857281  \n       stateWyoming                 year  \n         -1.1353952            0.0004764  \n\n\n\n\nCode\nideo_mod <- plm(ideo ~ econdev, \n                      data = sd2,\n                      index = c(\"state\", \"year\"), \n                      model = \"within\", \n                      effect = \"twoways\")\n\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\n\nt test of coefficients:\n\n          Estimate Std. Error t value Pr(>|t|)\necondev -0.0027089  0.0114493 -0.2366    0.813\n\n\nChecking the class of state and year\n\n\nCode\nclass(sd2$state)\n\n\n[1] \"character\"\n\n\nCode\nclass(sd2$year)\n\n\n[1] \"integer\"\n\n\nChanging to factors\n\n\nCode\nstate1 <- as.factor(sd2$state)\n\nyear1 <- as.factor(sd2$year)\n\n\n\n\nCode\nideo_mod <- plm(ideo ~ econdev, \n                      data = sd2,\n                      index = c(\"state\", \"year\"), \n                      model = \"within\", \n                      effect = \"twoways\")\n\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\n\nt test of coefficients:\n\n          Estimate Std. Error t value Pr(>|t|)\necondev -0.0027089  0.0114493 -0.2366    0.813\n\n\n\n\nCode\nclass(ideo_lm_mod)\n\n\n[1] \"lm\"\n\n\nResults in very high p-value\n\n\nCode\ncoeftest(ideo_lm_mod, vcov = vcovHC, type = \"HC1\")[1, ]\n\n\n    Estimate   Std. Error      t value     Pr(>|t|) \n0.0001635623 0.0089096172 0.0183579484 0.9853571217"
  },
  {
    "objectID": "posts/shelton_HW3.html",
    "href": "posts/shelton_HW3.html",
    "title": "HW 3 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nlibrary(GGally)\n\n\nError in library(GGally): there is no package called 'GGally'\n\n\nCode\nknitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/shelton_HW3.html#homework-3",
    "href": "posts/shelton_HW3.html#homework-3",
    "title": "HW 3 Solution",
    "section": "Homework 3",
    "text": "Homework 3\n\nQ1Q2Q3Q4Q5\n\n\nWe are using UN11 data:\n\n\n\n\n\n\nUN11\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1\n\n\n\nPredictor/Explanatory/IV - ppgdp Per Person GDP\nResponse/DV - fertility Fertility Rate per 1000\n\n\n\n\n\n\n\n\n1.1.2\n\n\n\n\n\n\n\nList of 97\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : chr \"Fertility Rate x ppGDP\"\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : NULL\n $ panel.grid.minor          :List of 6\n  ..$ colour       : NULL\n  ..$ linewidth    : 'rel' num 0.5\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.clip                : chr \"inherit\"\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ x                         : chr \"Per Person GDP (USD)\"\n $ y                         : chr \"Fertility   Rate per 1000\"\n $ caption                   : chr \"Fig 1.1.2\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\nNo, our scatter plot appears to have a negative exponential relationship; as ppgdp increase, fertility decreases at a nonlinear rate.\nNOTE: If anyone knows why it is printing the above output as well as the graph please send a comment on Google Classroom or Piazza!\n\n\n\n\n\n\n\n\n\n1.1.3\n\n\n\n\n\n\n\n\n\n\nNow, after using a log transformation on both x and y, our scatter plot appears to have a negative linear relationship; as ppgdp increase, fertility decreases at a linear rate. This transformation makes these variabels appropriate for linear regression.\n\n\n\n\n\n\n\n\n\n\n\nUSD to GBP\n\n\n\na) The slope of the regression line will be divided by 1.33 to represent the adjustment to GBP. \\(1 USD * (1 GBP/1.33 USD)\\) leaves us with just GBP. The slope is reduced by approximately 25 percent.\nb) The correlation between the explanatory variable and the response will not change.\n\n\n\n\n\n\n\n\n\n\nWater Scatterplot Matrix\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nThe rightmost column uses BSAAM as a response variable to the preciptation values at the various sites over the years 1948-1990. Inspecting the plots on this column closer, we can see that BSAAM has a strong positive correlation with OPSLAKE, OPRC, and OBPBC. There is a much weaker positive relationship between BSAAM and APSLAKE, APSAB, and APSMAM. If we were attempting to predict stream runoff volume, it would be best to observe and use the values of OPSLAKE, OPRC, and OBPBC to create our model. These locations’ precipitation values are highly correlated with stream runoff volume each year.\n\n\n\n\n\n\n\n\nScatterplot Matrix Rateprof\n\n\n\n\n\n\n\n\n\n  \n\n\n\nError in loadNamespace(x): there is no package called 'GGally'\n\n\n\n\n\nUsing ggpairs of the GGally package, we can see the variables that correlate the highest with quality are helpfulness and clarity. Clarity and helpfulness have the third highest correlation with each other. If we were attempting to predict the quality rating of a course, we would strongly consider both clarity and helpfulness ratings.\n\n\n\n\n\n\n\n\n\n\n\nPolitical Ideology x Religiosity\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pi_num ~ re_no, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         2.2667     0.3394   6.678 1.18e-08 ***\nre_nooccasionally   0.2506     0.4181   0.599 0.551374    \nre_nomost weeks     2.1619     0.6017   3.593 0.000691 ***\nre_noevery week     2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nFirst using a box plot to visualize the relationship between a numeric and a nominal variable, it’s clear the distributions of those that attend church more frequently skew towards conservative values on the 7-point scale provided by the data. This is confirmed by inspecting the summary of the linear regression model relating Political Ideology to Religiosity. Both group means’ coefficients (scores) for those attending church “most weeks” and “weekly” are approximately 2 times greater that the intercept value that respresents those that attend “never”.\n\n\n\n\n\n\n\n\n\nHigh School GPA x TV\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nObserving the results of the linear regression model relating high-school GPA to hours spent watching TV, we see that for every one hour increase in hours of TV/wk, our model predicts high-school GPA to decrease by 0.018305 points. We see the weak negative linear relationship using both geom_point and geom_smooth to depict the regression line and the original data points."
  },
  {
    "objectID": "posts/HW3_Saaradhaa.html",
    "href": "posts/HW3_Saaradhaa.html",
    "title": "Homework 3",
    "section": "",
    "text": "Qn 1.1.2\n\n# load libraries.\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(alr4)\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\n# load dataset.\ndata(UN11)\n\n# draw scatterplot.\nscatterplot(fertility ~ ppgdp, UN11)\n\n\n\n\nNo, the graph seems curvilinear.\n\n\nQn 1.1.3\n\n# draw scatterplot.\nscatterplot (log(fertility) ~ log(ppgdp), UN11)\n\n\n\n\nYes, the simple linear regression model now seems plausible.\n\n\nQn 2a\nWe can test this using the UN11 dataset since ppgdp is in US dollars.\n\n# create new variable.\nUN11$british <- 1.33*UN11$ppgdp\n\n# check slope.\nsummary(lm(fertility ~ british, UN11))\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, although adjusted R^2 has not.\n\n\nQn 2b\nWe can test this too.\n\n# correlation with US dollars.\ncor(UN11$ppgdp, UN11$fertility)\n\n[1] -0.4399891\n\n# correlation with British pounds.\ncor(UN11$british, UN11$fertility)\n\n[1] -0.4399891\n\n\nSince we multiplied by a constant, the correlation remains the same.\n\n\nQn 3\n\n# load dataset.\ndata(water)\n\n# generate scatterplots.\npairs(water)\n\n\n\n\nStream runoff (BSAAM) seems to have a positive linear relationship with precipitation at OPSLAKE, OPRC and OPBPC; but not with precipitation at APMAM, APSAB or APSLAKE. Stream runoff also seems to be fairly constant (?) over the years.\n\n\nQn 4\n\n# load dataset.\ndata(Rateprof)\n\n# create subset.\nrateprof <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# generate scatterplots.\npairs(rateprof)\n\n\n\n\nQuality, helpfulness and clarity have the clearest linear relationships with one another. Easiness and raterInterest do not seem to have linear relationships with the other variables.\n\n\nQn 5a\n\n# load dataset.\ndata(student.survey)\nglimpse(student.survey)\n\nRows: 60\nColumns: 18\n$ subj <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ ge   <fct> m, f, f, f, m, m, m, f, m, m, m, f, m, m, f, f, f, m, m, f, f, f,…\n$ ag   <int> 32, 23, 27, 35, 23, 39, 24, 31, 34, 28, 23, 27, 36, 28, 28, 25, 4…\n$ hi   <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.5, 3.3, …\n$ co   <dbl> 3.5, 3.5, 3.0, 3.2, 3.5, 3.5, 3.7, 3.0, 3.0, 3.1, 2.6, 3.6, 3.5, …\n$ dh   <int> 0, 1200, 1300, 1500, 1600, 350, 0, 5000, 5000, 900, 253, 190, 245…\n$ dr   <dbl> 5.0, 0.3, 1.5, 8.0, 10.0, 3.0, 0.2, 1.5, 2.0, 2.0, 1.5, 3.0, 1.5,…\n$ tv   <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, 6, 25, 4…\n$ sp   <int> 5, 7, 4, 5, 6, 5, 12, 3, 5, 1, 15, 3, 15, 10, 3, 6, 7, 9, 12, 0, …\n$ ne   <int> 0, 5, 3, 6, 3, 7, 4, 3, 3, 2, 1, 7, 12, 1, 1, 1, 3, 6, 2, 0, 4, 7…\n$ ah   <int> 0, 6, 0, 3, 0, 0, 2, 1, 0, 1, 1, 0, 5, 2, 0, 0, 10, 10, 2, 2, 1, …\n$ ve   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ pa   <fct> r, d, d, i, i, d, i, i, i, i, r, d, d, i, d, i, i, d, i, d, i, i,…\n$ pi   <ord> conservative, liberal, liberal, moderate, very liberal, liberal, …\n$ re   <ord> most weeks, occasionally, most weeks, occasionally, never, occasi…\n$ ab   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ aa   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ld   <lgl> FALSE, NA, NA, FALSE, FALSE, NA, FALSE, FALSE, NA, FALSE, FALSE, …\n\n# generate plots.\nboxplot(pi ~ re, student.survey)\n\n\n\nscatterplot(hi ~ tv, student.survey)\n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\nHigh school GPA and TV-watching seem to have a negative relationship.\n\n\n\nQn 5b\n\n# change pi to numeric variable.\nstudent.survey$pi <- as.numeric(student.survey$pi)\n\n# removing ordering in re and rename it.\nlevels(student.survey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudent.survey$re <- factor(student.survey$re, ordered = FALSE)\n\n# run regression models.\nsummary(lm(pi ~ re, student.survey))\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\nsummary(lm(hi ~ tv, student.survey))\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\nThose who attended religious services most weeks/every week were significantly more likely to be conservative than those who never did, p < .001. There was no significant difference in political ideology between those who occasionally attended religious services and those who never did.\nWatching less hours of TV per week was associated with higher high-school GPAs, p < .05. That being said, as the R2 is fairly low, hours of TV watching is not a great predictor of high school GPA."
  },
  {
    "objectID": "posts/hw3_boonstra.html",
    "href": "posts/hw3_boonstra.html",
    "title": "Homework 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(alr4)\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw3_boonstra.html#section",
    "href": "posts/hw3_boonstra.html#section",
    "title": "Homework 3",
    "section": "1.1.1",
    "text": "1.1.1\nIn this model, ppgdp would be the predictor variable, and fertility would be the response variable."
  },
  {
    "objectID": "posts/hw3_boonstra.html#section-1",
    "href": "posts/hw3_boonstra.html#section-1",
    "title": "Homework 3",
    "section": "1.1.2",
    "text": "1.1.2\n\nun11 %>% \n  ggplot(aes(x=ppgdp,y=fertility)) +\n  geom_point() +\n  geom_smooth(method=lm,se=F)\n\n\n\n\nA linear OLS regression of a linear-linear model of the data does not seem to fit the data very well. While the regression does capture the general downward trend of the data, it is plain to see in the above visualization that it is not a good fit, particularly for larger values of the response variable (ppgdp)."
  },
  {
    "objectID": "posts/hw3_boonstra.html#section-2",
    "href": "posts/hw3_boonstra.html#section-2",
    "title": "Homework 3",
    "section": "1.1.3",
    "text": "1.1.3\n\nun11 %>% \n  ggplot(aes(x=log(ppgdp),y=log(fertility))) +\n  geom_point() +\n  geom_smooth(method=lm,se=F)\n\n\n\n\nThis log-log model of the data is much better suited to a linear OLS regression fit."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-1",
    "href": "posts/HW1_603_Niharikapola.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-data",
    "href": "posts/HW1_603_Niharikapola.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a",
    "href": "posts/HW1_603_Niharikapola.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b",
    "href": "posts/HW1_603_Niharikapola.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c",
    "href": "posts/HW1_603_Niharikapola.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d",
    "href": "posts/HW1_603_Niharikapola.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e",
    "href": "posts/HW1_603_Niharikapola.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f",
    "href": "posts/HW1_603_Niharikapola.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-2",
    "href": "posts/HW1_603_Niharikapola.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "href": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a-1",
    "href": "posts/HW1_603_Niharikapola.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b-1",
    "href": "posts/HW1_603_Niharikapola.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c-1",
    "href": "posts/HW1_603_Niharikapola.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d-1",
    "href": "posts/HW1_603_Niharikapola.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e-1",
    "href": "posts/HW1_603_Niharikapola.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f-1",
    "href": "posts/HW1_603_Niharikapola.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html",
    "href": "posts/HW3_Solutions_OmerYalcin.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nLoad the necessary packages.\n\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\n\nLoad data:\n\ndata(UN11)\n\n\n1.1.1\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n\n\n1.1.2.\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure (or the left half of a U-shape).\n\n\n1.1.2\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n\n\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\ndata(water)\npairs(water)\n\n\n\n\n\nYear appears to be largely unrelated to each of the other variables\nThe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables"
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-4",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\ndata(Rateprof)\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-5",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\na\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\n\n\nb\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\nm1 <- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 <- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\nError in stargazer(m1, m2, type = \"text\", dep.var.labels = c(\"Pol. Ideology\", : could not find function \"stargazer\"\n\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA."
  },
  {
    "objectID": "posts/KPopiela_HW2.html",
    "href": "posts/KPopiela_HW2.html",
    "title": "KPopiela HW2",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(stats)"
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-1",
    "href": "posts/KPopiela_HW2.html#question-1",
    "title": "KPopiela HW2",
    "section": "Question 1",
    "text": "Question 1\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n#Bypass values\nn_bypass <- 539\nx_bypass <- 19\nsd_bypass <- 10\ndf_bypass <- n_bypass-1\nalpha = 0.10\n\n\n#t-score  \ntscore_bypass = qt(p=alpha/2,df=df_bypass,lower.tail = F)\ntscore_bypass\n\n[1] 1.647691\n\n\n\n#standard error  \nse_bypass <- sd_bypass/sqrt(n_bypass)\nse_bypass\n\n[1] 0.4307305\n\n\n\n#margin of error and confidence interval (bypass)\nmargin_error <- tscore_bypass*se_bypass\nlower_CI <- x_bypass - margin_error\nupper_CI <- lower_CI+margin_error\nprint(c(lower_CI,upper_CI))\n\n[1] 18.29029 19.00000\n\n\n\n#Angiography values  \nn_angio <- 847\nx_angio <- 18\nsd_angio <- 9\ndf_angio <- n_angio-1\n#alpha value remains the same at 0.10 \n\n\n#t-score\ntscore_angio <- qt(p=alpha/2,df=df_angio,lower.tail = F)\ntscore_angio\n\n[1] 1.646657\n\n\n\n#standard error\nse_angio <-sd_angio/sqrt(n_angio)\nse_angio\n\n[1] 0.3092437\n\n\n\n#margin of error and confidence interval (angiography)\nmargin_error_angio <- tscore_angio*se_angio\nangio_lowerCI <- x_angio - margin_error_angio\nangio_upperCI <- angio_lowerCI+margin_error_angio\nprint(c(angio_lowerCI,angio_upperCI))\n\n[1] 17.49078 18.00000\n\n\nThe bypass confidence interval for the true mean wait time is 18.290, 19.000, or 0.710.\nThe angiography confidence interval the true mean wait time is 17.491, 18.000, or 0.509.\nThe confidence interval is narrower for angiography."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-2",
    "href": "posts/KPopiela_HW2.html#question-2",
    "title": "KPopiela HW2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n#Sample proportion/Point estimate\nn <- 1031\nx <- 567\nsample_prop <- x/n\nsample_prop\n\n[1] 0.5499515\n\n\n\n#Margin of error\nmargin_error2 <- qnorm(0.975)*sqrt(sample_prop*(1-sample_prop)/n)\nmargin_error2\n\n[1] 0.03036761\n\n\n\n#95% Confidence Interval\nCI_lower <- sample_prop - margin_error2\nCI_upper <- CI_lower + margin_error2\nprint(c(CI_lower,CI_upper))\n\n[1] 0.5195839 0.5499515\n\n\nThe point estimate p, of the proportion of all adult Americans who believe that college is essential for success is 0.549, or ~55%. The margin of error is 0.030, which lines up, since the confidence interval is 0.519, 0.549."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-3",
    "href": "posts/KPopiela_HW2.html#question-3",
    "title": "KPopiela HW2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n#Since most formulas require a value for sample size (n), whichever one I use will have to be reorganized: the confidence interval formula. But because I am looking for n, it has to read z*(s/5)^2=n.\n\nf <-function(n, z = 1.96, s = 42.5) {\n  res <- z*s/sqrt(n)\n  return(res)\n}\n\nvec <- vapply(1:300, FUN = f, FUN.VALUE = 5.0)\nwhich(vec < 5) [1]\n\n[1] 278\n\n#The sample contains at least 278 people."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-4",
    "href": "posts/KPopiela_HW2.html#question-4",
    "title": "KPopiela HW2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nC. Report and interpret the P-value for H a: μ > 500.\n####(Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n#In order to test whether or not the mean income for female employees differs from $500/week, we must first condect a one-sample, two-sided significance test.\n\n#We can also assume the following:  \n#1. The sample is random and the population has a normal distribution  \n#2. The mean income for all senior-level workers = $500/week  \n#3. From the random sample of 9 female employees, the mean income = $410/week  \n#4. Standard deviation = 90  \n#5. Null Hypothesis: H0: μ = 500  \n#6. Alternative Hypothesis: Ha: μ ≠ 500  \n\n\n#Test statistic\nybar <- 410\nmean <- 500\ns <- 90\nn <- 9\n(ybar - mean)/(s/sqrt(n))\n\n[1] -3\n\n#The test statistic value is -3\n\n\n#P-value\nn <- 9 \ndf_n <- (n - 1)  \nt_test <- (410 - 500)/(90/sqrt(9))  \np_value <- pt(t_test, df_n)*2  \np_value\n\n[1] 0.01707168\n\n#P-value is 0.017. If we hold to the assumption that a=0.05, we can easily see that 0.017 < 0.05, which means the null hypothesis can be rejected. Therefore, there is enough statistical evidence to support the claim that the mean income for female employees differs from the overall mean of $500/week.\n\n\n\nB. Report the P-value for Ha : μ < 500. Interpret.\n\n#Hypotheses\n    #H0:mu = $500/week  \n    #Ha:mu = <$500/week  \n    #P-value = p(t<t_test)*p(t<-3)\n\n\n#P-value for Ha:my > 500\nq <- -3\nleft_p_value <- pt(q,df_n,lower.tail=TRUE,log.p=FALSE)\nleft_p_value\n\n[1] 0.008535841\n\n#P-value for Ha:my > 500 is 0.0085. This can be rounded up to 0.01, which indicates that there's strong evidence against the mean weekly income being $500+\n\n\n#P-value for H0:mu < 500\nright_p_value <- pt(q,df_n,lower.tail = FALSE,log.p=FALSE)\nright_p_value\n\n[1] 0.9914642\n\n#The p-value for H0:mu < 500 is 0.99, indicating strong evidence in favor of the null hypothesis. This contradicts the claim that mean mu > 500. To make sure my findings are correct, I must confrim that the sum of each p-value totals to 1. I could code this but it's not hard to tell that 0.01 + 0.99 = 1."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-5",
    "href": "posts/KPopiela_HW2.html#question-5",
    "title": "KPopiela HW2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 ,with se = 10.0\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n#Let's start with Jones and confirming that t=1.95 and the p-value = 0.051\n\n#t-test\nt_testj <- (519.5-500)/10.0\nt_testj\n\n[1] 1.95\n\n#The t-test value, is in fact 1.95\n\n\n#P-value\nn5 <- 1000\ndf_5 <- (n5-1)\n\npvaluej <- pt(t_testj, df_5,lower.tail = FALSE,log.p = FALSE)*2\npvaluej\n\n[1] 0.05145555\n\n#Like the t-test value, the p-value is also accurate to the question at 0.051.\n\n\n#Now lets move onto Smith with t = 1.97 and p-value = 0.049\n\n#t-test\nt_testSmith <- (519.7 - 500)/10.0\nt_testSmith\n\n[1] 1.97\n\n#Smith's t-test value is 1.97.\n\n\n#P-value\n\n#sample size n is the same as in the Jones section: n5 <- 1000  \n#df is also the same as the Jones section: df_5 <- (n5-1)  \n\np_valueSmith <- pt(t_testSmith, df_5,lower.tail = FALSE, log.p = FALSE)*2\np_valueSmith\n\n[1] 0.04911426\n\n#Smith's p-value, like the t-test value, is accurate to what the question presents at 0.049\n\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\n\n#In order for a p-value to be statistically significant, it must be greater than 0.05. Smith’s p-value is 0.049 which, while close, is still less than 0.05. Jones’s p-value, however, is statistically significant at 0.051.\n\n\n\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\n#Smith's and Jones's results were extremely similar, but the difference between the two lies right on the threshold of statistical significance; only Jones's results were statistically significant. But given the result values' closeness (0.051 and 0.049), there is moderate evidence against H0."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-6",
    "href": "posts/KPopiela_HW2.html#question-6",
    "title": "KPopiela HW2",
    "section": "Question 6",
    "text": "Question 6\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n#I'm going to start by calculating the t-score to find the upper and lower values in the gas_taxes interval\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ngas_tax_sample <- 18\ndf_gt <- gas_tax_sample - 1\nmean_gt <- mean(gas_taxes)\ntscore_gt <- qt(p=0.05,df=df_gt,lower.tail=FALSE)\ngas_sd <- sd(gas_taxes)\nme_gas_taxes <- qt(0.05,df = df_gt)*gas_sd/sqrt(18)\n\nlower_int_gt<-(mean_gt+me_gas_taxes)\nlower_int_gt\n\n[1] 37.0461\n\n#The lower interval value is 37.046\n\n\nupper_int_gt <- (mean_gt- me_gas_taxes)\nupper_int_gt\n\n[1] 44.67946\n\n#The upper interval value is 44.679\n\n#The average tax/gallon of gas is less than $0.45, so it is within the upper and lower bounds of the confidence interval. However, we will test an alternate outcome via a t-test"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html",
    "href": "posts/FinalProject_ManiShankerKamarapu.html",
    "title": "Final project part 1",
    "section": "",
    "text": "Churning refers to a customer who leaves one company to go to another company. Customer churn introduces not only some loss in income but also other negative effects on the operation of companies. Churn management is the concept of identifying those customers who are intending to move their custom to a competing service provider.\nRisselada et al. (2010) stated that churn management is becoming part of customer relationship management. It is important for companies to consider it as they try to establish long-term relationships with customers and maximize the value of their customer base.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Does churn-rate depend on the geographical factors of the customer?\nB. Do non-active members are probable to churn or not?\n\n\nThis project will be useful to better understand more about the customer difficulties and factors and also give us a pretty good idea on the factors effecting the customers to exit and also about the dormant state of the customers."
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#hypothesis",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#hypothesis",
    "title": "Final project part 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nCustomer churn analysis has become a major concern in almost every industry that offers products and services. The model developed will help banks identify clients who are likely to be churners and develop appropriate marketing actions to retain their valuable clients. And this model also supports information about similar customer group to consider which marketing reactions are to be provided. Thus, due to existing customers are retained, it will provide banks with increased profits and revenues.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nGeographical factors will not be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nGeographical factors will be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nActive members will not churn.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nActive members will churn."
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#loading-libraries",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#loading-libraries",
    "title": "Final project part 1",
    "section": "Loading libraries",
    "text": "Loading libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#reading-the-data-set",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#reading-the-data-set",
    "title": "Final project part 1",
    "section": "Reading the data set",
    "text": "Reading the data set\n\n\nCode\nChurn <- read_csv(\"_data/Churn_Modelling.csv\")\n\n\nRows: 10000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Surname, Geography, Gender\ndbl (11): RowNumber, CustomerId, CreditScore, Age, Tenure, Balance, NumOfPro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nChurn\n\n\n\n\n  \n\n\n\nThis data set includes 10k bank customer data records with 14 attributes including socio-demographic attributes, account level and behavioural attributes.\nAttribute Description 1. Row Number- Number of customers 2. Customer ID- ID of customer 3. Surname- Customer name 4. Credit Score- Score of credit card usage 5. Geography- Location of customer 6. Gender- Customer gender 7. Age- Age of Customer 8. Tenure- The period of having the account in months 9. Balance- Customer main balance 10. NumOfProducts- No of products used by customer 11. HasCrCard- If the customer has a credit card or not 12. IsActiveMember- Customer account is active or not 13. Estimated Salary- Estimated salary of the customer. 14. Exited- Indicate churned or not\n\n\nCode\nstr(Churn)\n\n\nspc_tbl_ [10,000 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ RowNumber      : num [1:10000] 1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : num [1:10000] 15634602 15647311 15619304 15701354 15737888 ...\n $ Surname        : chr [1:10000] \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : num [1:10000] 619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr [1:10000] \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr [1:10000] \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : num [1:10000] 42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : num [1:10000] 2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num [1:10000] 0 83808 159661 0 125511 ...\n $ NumOfProducts  : num [1:10000] 1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : num [1:10000] 1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : num [1:10000] 1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num [1:10000] 101349 112543 113932 93827 79084 ...\n $ Exited         : num [1:10000] 1 0 1 0 0 1 0 1 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   RowNumber = col_double(),\n  ..   CustomerId = col_double(),\n  ..   Surname = col_character(),\n  ..   CreditScore = col_double(),\n  ..   Geography = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_double(),\n  ..   Tenure = col_double(),\n  ..   Balance = col_double(),\n  ..   NumOfProducts = col_double(),\n  ..   HasCrCard = col_double(),\n  ..   IsActiveMember = col_double(),\n  ..   EstimatedSalary = col_double(),\n  ..   Exited = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#descriptive-statistics",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#descriptive-statistics",
    "title": "Final project part 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n\nCode\nsummary(Churn)\n\n\n   RowNumber       CustomerId         Surname           CreditScore   \n Min.   :    1   Min.   :15565701   Length:10000       Min.   :350.0  \n 1st Qu.: 2501   1st Qu.:15628528   Class :character   1st Qu.:584.0  \n Median : 5000   Median :15690738   Mode  :character   Median :652.0  \n Mean   : 5000   Mean   :15690941                      Mean   :650.5  \n 3rd Qu.: 7500   3rd Qu.:15753234                      3rd Qu.:718.0  \n Max.   :10000   Max.   :15815690                      Max.   :850.0  \n  Geography            Gender               Age            Tenure      \n Length:10000       Length:10000       Min.   :18.00   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:32.00   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median :37.00   Median : 5.000  \n                                       Mean   :38.92   Mean   : 5.013  \n                                       3rd Qu.:44.00   3rd Qu.: 7.000  \n                                       Max.   :92.00   Max.   :10.000  \n    Balance       NumOfProducts    HasCrCard      IsActiveMember  \n Min.   :     0   Min.   :1.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:     0   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 97199   Median :1.00   Median :1.0000   Median :1.0000  \n Mean   : 76486   Mean   :1.53   Mean   :0.7055   Mean   :0.5151  \n 3rd Qu.:127644   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :250898   Max.   :4.00   Max.   :1.0000   Max.   :1.0000  \n EstimatedSalary         Exited      \n Min.   :    11.58   Min.   :0.0000  \n 1st Qu.: 51002.11   1st Qu.:0.0000  \n Median :100193.91   Median :0.0000  \n Mean   :100090.24   Mean   :0.2037  \n 3rd Qu.:149388.25   3rd Qu.:0.0000  \n Max.   :199992.48   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(Churn)\n\n\nRows: 10,000\nColumns: 14\n$ RowNumber       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ CustomerId      <dbl> 15634602, 15647311, 15619304, 15701354, 15737888, 1557…\n$ Surname         <chr> \"Hargrave\", \"Hill\", \"Onio\", \"Boni\", \"Mitchell\", \"Chu\",…\n$ CreditScore     <dbl> 619, 608, 502, 699, 850, 645, 822, 376, 501, 684, 528,…\n$ Geography       <chr> \"France\", \"Spain\", \"France\", \"France\", \"Spain\", \"Spain…\n$ Gender          <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Female\", \"Mal…\n$ Age             <dbl> 42, 41, 42, 39, 43, 44, 50, 29, 44, 27, 31, 24, 34, 25…\n$ Tenure          <dbl> 2, 1, 8, 1, 2, 8, 7, 4, 4, 2, 6, 3, 10, 5, 7, 3, 1, 9,…\n$ Balance         <dbl> 0.00, 83807.86, 159660.80, 0.00, 125510.82, 113755.78,…\n$ NumOfProducts   <dbl> 1, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, …\n$ HasCrCard       <dbl> 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …\n$ IsActiveMember  <dbl> 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, …\n$ EstimatedSalary <dbl> 101348.88, 112542.58, 113931.57, 93826.63, 79084.10, 1…\n$ Exited          <dbl> 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html",
    "title": "Final Project Update",
    "section": "",
    "text": "Code\n# load libraries.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\nlibrary(interactions)\nlibrary(lmtest)\nlibrary(sandwich)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-intro",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-intro",
    "title": "Final Project Update",
    "section": "Part 1: Intro",
    "text": "Part 1: Intro\nPrior research literature in the social sciences has continually stressed the need for more research on the Global South. However, few papers actually focus on it. Hence, I am interested to learn more about this region. A data source that lends itself useful for this is the World Values Survey, a global survey with an easily accessible database.\nI am specifically interested in understanding what drives subjective well-being, which can be interpreted via happiness and life satisfaction (Addai et al., 2013).\n\n\n\n\n\n\nPart 1: Research Questions\n\n\n\nA. What predicts happiness and life satisfaction in the Global South?\nB. Do predictors of happiness and life satisfaction differ between the Global North and South?\n\n\nThis project will be useful to better understand motivations and desires in the Global South, reduce inter-cultural tensions and enhance cross-cultural cohesion. Governments can also benefit from this research in terms of policy prioritization to maximize citizens’ well-being."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-hypotheses-edited-based-on-feedback",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-hypotheses-edited-based-on-feedback",
    "title": "Final Project Update",
    "section": "Part 1: Hypotheses (Edited Based on Feedback)",
    "text": "Part 1: Hypotheses (Edited Based on Feedback)\nPast researchers have studied happiness and life satisfaction in the Global South via the World Values Survey (Addai et al., 2013; Ngamaba, 2016). The studies focused on Ghana and Rwanda respectively. The common predictors of happiness and life satisfaction across both countries were satisfaction with health and income.\nAlba (2019) found that happiness was generally greater in the Global North than the Global South, and indicated that future research should attempt to cover the factors behind this, which gave me the impetus for this project.\nEdited based on feedback: I refer to Maslow’s hierarchy of needs, where physical and safety needs come first. My thinking is that happiness and well-being in the Global North may depend on more subjective measures, given that health and income-related problems should be relatively more accounted for.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nHealth and financial satisfaction will not positively predict happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nHealth and financial satisfaction will positively predict happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nHealth and financial satisfaction will not have a greater impact on happiness and life satisfaction on the Global South than the Global North.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nHealth and financial satisfaction will have a greater impact on happiness and life satisfaction on the Global South than the Global North."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-read-in-data",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-read-in-data",
    "title": "Final Project Update",
    "section": "Part 1: Read In Data",
    "text": "Part 1: Read In Data\nI will be working with the most recent wave of the World Values Survey, Wave 7, which was conducted from 2017 to 2022. The data is freely available for non-profit purposes. It must be cited properly and not re-distributed (Haerpfer et al., 2022).\nRepresentative samples of the population aged 18 and above were collected from 59 countries. Data was mostly collected by interviewing respondents at their homes (“WVS Database”, 2022).\nI am using the version of Wave 7 released in May 2022.\nI will indicate my comments in each code chunk to keep track of my progress.\n\n\nCode\n# read in dataset.\nwvs <- read_csv(\"~/Desktop/2022_Fall/DACSS 603/General/Final Project/WVS/4. Data/WVS_Cross-National_Wave_7_csv_v4_0.csv\", show_col_types = FALSE) %>% select(\"B_COUNTRY_ALPHA\", \"G_TOWNSIZE\", \"H_SETTLEMENT\", \"H_URBRURAL\", \"O1_LONGITUDE\", \"O2_LATITUDE\", \"Q1\", \"Q2\", \"Q3\", \"Q6\", \"Q46\", \"Q47\", \"Q48\", \"Q49\", \"Q50\", \"Q57\", \"Q171\", \"Q260\", \"Q262\", \"Q263\", \"Q269\", \"Q270\", \"Q271\", \"Q273\", \"Q274\", \"Q275\", \"Q279\", \"Q287\", \"Q288\", \"Q288R\", \"Q289\", \"I_WOMJOB\", \"I_WOMPOL\", \"I_WOMEDU\", \"Q182\", \"Q184\")\n\n\nThe dataset originally had 552 columns. I have selected a subset of columns based on variables used in past papers, as well as some variables I am interested to examine. These include place/area of residence, literacy, demographics, importance of various social aspects, happiness and wellbeing indicators, trust, religiosity, equality of gender/sexual orientation and abortion attitudes.\nI will first create a dummy variable for Global North/South. The Global South comprises low- and lower-middle income countries, as defined by the World Bank (“World Bank Country and Lending Groups”, 2022). Global South countries surveyed include Ethiopia, Philippines, Indonesia, Bangladesh, Iran, Kenya, Bolivia, Kyrgyzstan, Lebanon, Tajikistan, Tunisia, Ukraine, Mongolia, Morocco, Egypt, Myanmar, Vietnam, Nicaragua, Zimbabwe, Nigeria and Pakistan.\n\n\nCode\n# create dummy.\nwvs <- mutate(wvs, NS = case_when(B_COUNTRY_ALPHA == \"ETH\" | B_COUNTRY_ALPHA == \"PHL\" | B_COUNTRY_ALPHA == \"IDN\" | B_COUNTRY_ALPHA == \"BGD\" | B_COUNTRY_ALPHA == \"IRN\" | B_COUNTRY_ALPHA == \"KEN\" | B_COUNTRY_ALPHA == \"BOL\" | B_COUNTRY_ALPHA == \"KGZ\" | B_COUNTRY_ALPHA == \"LBN\" | B_COUNTRY_ALPHA == \"TJK\" | B_COUNTRY_ALPHA == \"TUN\" | B_COUNTRY_ALPHA == \"MOR\" | B_COUNTRY_ALPHA == \"UKR\" | B_COUNTRY_ALPHA == \"MNG\" | B_COUNTRY_ALPHA == \"EGY\" | B_COUNTRY_ALPHA == \"MMR\" | B_COUNTRY_ALPHA == \"VNM\" | B_COUNTRY_ALPHA == \"NIC\" | B_COUNTRY_ALPHA == \"ZWE\" | B_COUNTRY_ALPHA == \"NGA\" | B_COUNTRY_ALPHA == \"PAK\" ~ \"1\"))\n\n# replace \"NA\" with \"O\" (for Global North).\nwvs$NS <- replace_na(wvs$NS, \"0\")\n\n# change to factor.\nwvs$NS <- as.factor(wvs$NS)\n\n# check counts of levels.\nwvs %>% select(NS) %>% summary()\n\n\n NS       \n 0:59178  \n 1:28644  \n\n\nCode\n# sanity check.\nwvs %>% filter(B_COUNTRY_ALPHA == \"ETH\" | B_COUNTRY_ALPHA == \"PHL\" | B_COUNTRY_ALPHA == \"IDN\" | B_COUNTRY_ALPHA == \"BGD\" | B_COUNTRY_ALPHA == \"IRN\" | B_COUNTRY_ALPHA == \"KEN\" | B_COUNTRY_ALPHA == \"BOL\" | B_COUNTRY_ALPHA == \"KGZ\" | B_COUNTRY_ALPHA == \"LBN\" | B_COUNTRY_ALPHA == \"TJK\" | B_COUNTRY_ALPHA == \"TUN\" | B_COUNTRY_ALPHA == \"MOR\" | B_COUNTRY_ALPHA == \"UKR\" | B_COUNTRY_ALPHA == \"MNG\" | B_COUNTRY_ALPHA == \"EGY\" | B_COUNTRY_ALPHA == \"MMR\" | B_COUNTRY_ALPHA == \"VNM\" | B_COUNTRY_ALPHA == \"NIC\" | B_COUNTRY_ALPHA == \"ZWE\" | B_COUNTRY_ALPHA == \"NGA\" | B_COUNTRY_ALPHA == \"PAK\") %>% nrow()\n\n\n[1] 28644\n\n\nCode\n# rename columns.\nnames(wvs) <- c(\"B_COUNTRY_ALPHA\", \"G_TOWNSIZE\", \"H_SETTLEMENT\", \"H_URBRURAL\", \"Long\", \"Lat\", \"FamImpt\", \"FriendsImpt\", \"LeisureImpt\", \"ReligionImpt\", \"Happiness\", \"PerceivedHealth\", \"FOC\", \"LS\", \"FS\", \"Trust\", \"AttendReligious\", \"Sex\", \"Age\", \"Immigrant\", \"Citizen\", \"HHSize\", \"Parents\", \"Married\", \"Kids\", \"Edu\", \"Job\", \"SocialClass\", \"Income\", \"IncomeR\", \"Religion\", \"I_WOMJOB\", \"I_WOMPOL\", \"I_WOMEDU\", \"homolib\", \"abortlib\", \"NS\")\n\n\nThe sanity check shows that the creation of the dummy was successful, with 28,644 data points from the Global South."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-exploratory-analysis",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-exploratory-analysis",
    "title": "Final Project Update",
    "section": "Part 1: Exploratory Analysis",
    "text": "Part 1: Exploratory Analysis\n\n\nCode\n# check rows, columns and variable types.\nstr(wvs)\n\n\ntibble [87,822 × 37] (S3: tbl_df/tbl/data.frame)\n $ B_COUNTRY_ALPHA: chr [1:87822] \"CYP\" \"CYP\" \"CYP\" \"CYP\" ...\n $ G_TOWNSIZE     : num [1:87822] 6 6 6 6 6 6 6 6 6 6 ...\n $ H_SETTLEMENT   : num [1:87822] 4 4 4 4 4 4 4 4 4 4 ...\n $ H_URBRURAL     : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ Long           : num [1:87822] 34.8 34.8 34.8 34.8 34.8 ...\n $ Lat            : num [1:87822] 32.4 32.4 32.4 32.4 32.5 ...\n $ FamImpt        : num [1:87822] 1 1 1 1 1 1 2 1 1 1 ...\n $ FriendsImpt    : num [1:87822] 1 3 2 2 NA 1 2 2 1 2 ...\n $ LeisureImpt    : num [1:87822] 1 1 1 1 2 1 2 1 1 2 ...\n $ ReligionImpt   : num [1:87822] 1 1 1 1 1 3 2 1 3 1 ...\n $ Happiness      : num [1:87822] 2 1 2 2 3 2 2 1 2 3 ...\n $ PerceivedHealth: num [1:87822] 4 2 1 3 3 1 1 1 1 4 ...\n $ FOC            : num [1:87822] 10 5 5 5 3 7 5 5 5 NA ...\n $ LS             : num [1:87822] 8 7 9 5 5 8 4 7 8 9 ...\n $ FS             : num [1:87822] 8 5 5 5 5 7 3 5 8 4 ...\n $ Trust          : num [1:87822] 2 2 2 2 2 2 2 2 2 2 ...\n $ AttendReligious: num [1:87822] 7 2 3 2 4 4 2 4 4 2 ...\n $ Sex            : num [1:87822] 1 2 2 2 2 1 1 2 1 2 ...\n $ Age            : num [1:87822] 61 61 42 64 52 39 61 25 36 77 ...\n $ Immigrant      : num [1:87822] 1 1 2 1 2 2 1 1 1 1 ...\n $ Citizen        : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ HHSize         : num [1:87822] 2 4 6 2 8 1 2 3 2 3 ...\n $ Parents        : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ Married        : num [1:87822] 1 1 1 1 5 3 1 1 2 1 ...\n $ Kids           : num [1:87822] 2 2 4 2 3 2 2 1 2 3 ...\n $ Edu            : num [1:87822] 1 1 4 3 3 3 2 3 1 0 ...\n $ Job            : num [1:87822] 1 1 5 1 1 1 1 7 1 5 ...\n $ SocialClass    : num [1:87822] 3 4 3 3 5 5 5 5 NA 4 ...\n $ Income         : num [1:87822] 5 5 3 5 3 5 3 5 7 3 ...\n $ IncomeR        : num [1:87822] 2 2 1 2 1 2 1 2 2 1 ...\n $ Religion       : num [1:87822] 3 3 3 3 3 3 3 3 3 3 ...\n $ I_WOMJOB       : num [1:87822] 0.75 0.5 1 0.75 0.5 0.5 0.5 0.75 0.5 0.5 ...\n $ I_WOMPOL       : num [1:87822] 0.66 NA 1 0.66 NA 0.33 0.66 0.66 NA 0.33 ...\n $ I_WOMEDU       : num [1:87822] 0.66 1 1 0.66 0.33 1 0.66 0.66 0.66 0.66 ...\n $ homolib        : num [1:87822] 1 1 5 1 1 3 1 4 1 1 ...\n $ abortlib       : num [1:87822] 1 1 1 1 1 3 1 3 1 1 ...\n $ NS             : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\n# check basic descriptive statistics.\nsummary(wvs)\n\n\n B_COUNTRY_ALPHA      G_TOWNSIZE     H_SETTLEMENT     H_URBRURAL   \n Length:87822       Min.   :1.000   Min.   :1.000   Min.   :1.000  \n Class :character   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:1.000  \n Mode  :character   Median :6.000   Median :3.000   Median :1.000  \n                    Mean   :5.312   Mean   :3.066   Mean   :1.318  \n                    3rd Qu.:8.000   3rd Qu.:5.000   3rd Qu.:2.000  \n                    Max.   :8.000   Max.   :5.000   Max.   :2.000  \n                    NA's   :1274    NA's   :207     NA's   :32     \n      Long              Lat            FamImpt       FriendsImpt   \n Min.   :-156.34   Min.   :-43.26   Min.   :1.000   Min.   :1.000  \n 1st Qu.:   7.66   1st Qu.:  6.99   1st Qu.:1.000   1st Qu.:1.000  \n Median :  39.94   Median : 24.75   Median :1.000   Median :2.000  \n Mean   :  36.16   Mean   : 21.35   Mean   :1.112   Mean   :1.721  \n 3rd Qu.: 100.27   3rd Qu.: 35.70   3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   : 156.89   Max.   :100.35   Max.   :4.000   Max.   :4.000  \n NA's   :27098     NA's   :27094    NA's   :146     NA's   :289    \n  LeisureImpt     ReligionImpt     Happiness     PerceivedHealth\n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.788   Mean   :1.938   Mean   :1.857   Mean   :2.194  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:3.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :5.000  \n NA's   :473     NA's   :831     NA's   :574     NA's   :254    \n      FOC               LS               FS             Trust      \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   :1.000  \n 1st Qu.: 6.000   1st Qu.: 6.000   1st Qu.: 5.000   1st Qu.:2.000  \n Median : 7.000   Median : 7.000   Median : 6.000   Median :2.000  \n Mean   : 7.203   Mean   : 7.043   Mean   : 6.172   Mean   :1.765  \n 3rd Qu.: 9.000   3rd Qu.: 9.000   3rd Qu.: 8.000   3rd Qu.:2.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :2.000  \n NA's   :800      NA's   :393      NA's   :545      NA's   :1198   \n AttendReligious      Sex             Age           Immigrant    \n Min.   :1.000   Min.   :1.000   Min.   : 16.00   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.: 29.00   1st Qu.:1.000  \n Median :4.000   Median :2.000   Median : 41.00   Median :1.000  \n Mean   :4.139   Mean   :1.525   Mean   : 42.85   Mean   :1.059  \n 3rd Qu.:6.000   3rd Qu.:2.000   3rd Qu.: 55.00   3rd Qu.:1.000  \n Max.   :7.000   Max.   :2.000   Max.   :103.00   Max.   :2.000  \n NA's   :1034    NA's   :62      NA's   :339      NA's   :344    \n    Citizen          HHSize          Parents         Married    \n Min.   :1.000   Min.   : 1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.: 2.000   1st Qu.:1.000   1st Qu.:1.00  \n Median :1.000   Median : 4.000   Median :1.000   Median :1.00  \n Mean   :1.022   Mean   : 3.945   Mean   :1.353   Mean   :2.65  \n 3rd Qu.:1.000   3rd Qu.: 5.000   3rd Qu.:2.000   3rd Qu.:5.00  \n Max.   :2.000   Max.   :63.000   Max.   :4.000   Max.   :6.00  \n NA's   :5164    NA's   :852      NA's   :1438    NA's   :504   \n      Kids             Edu             Job        SocialClass   \n Min.   : 0.000   Min.   :0.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.: 0.000   1st Qu.:2.000   1st Qu.:1.00   1st Qu.:3.000  \n Median : 2.000   Median :3.000   Median :3.00   Median :3.000  \n Mean   : 1.766   Mean   :3.546   Mean   :3.13   Mean   :3.255  \n 3rd Qu.: 3.000   3rd Qu.:5.000   3rd Qu.:5.00   3rd Qu.:4.000  \n Max.   :24.000   Max.   :8.000   Max.   :8.00   Max.   :5.000  \n NA's   :1201     NA's   :818     NA's   :1143   NA's   :2302   \n     Income          IncomeR         Religion        I_WOMJOB     \n Min.   : 1.000   Min.   :1.000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:0.2500  \n Median : 5.000   Median :2.000   Median :3.000   Median :0.5000  \n Mean   : 4.859   Mean   :1.841   Mean   :3.005   Mean   :0.5075  \n 3rd Qu.: 6.000   3rd Qu.:2.000   3rd Qu.:5.000   3rd Qu.:0.7500  \n Max.   :10.000   Max.   :3.000   Max.   :9.000   Max.   :1.0000  \n NA's   :2330     NA's   :2330    NA's   :2485    NA's   :648     \n    I_WOMPOL         I_WOMEDU         homolib          abortlib      NS       \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.000   Min.   : 1.000   0:59178  \n 1st Qu.:0.3300   1st Qu.:0.6600   1st Qu.: 1.000   1st Qu.: 1.000   1:28644  \n Median :0.6600   Median :0.6600   Median : 2.000   Median : 2.000            \n Mean   :0.5427   Mean   :0.6649   Mean   : 3.864   Mean   : 3.407            \n 3rd Qu.:0.6600   3rd Qu.:1.0000   3rd Qu.: 6.000   3rd Qu.: 5.000            \n Max.   :1.0000   Max.   :1.0000   Max.   :10.000   Max.   :10.000            \n NA's   :2222     NA's   :1250     NA's   :5691     NA's   :1979              \n\n\nCode\nprint(dfSummary(wvs, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.30, style = \"grid\", valid.col = FALSE), \n      method = 'render', table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nwvs\nDimensions: 87822 x 37\n  Duplicates: 45\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      B_COUNTRY_ALPHA\n[character]\n      1. CAN2. IDN3. CHN4. USA5. TUR6. NLD7. HKG8. BOL9. SGP10. PAK[ 49 others ]\n      4018(4.6%)3200(3.6%)3036(3.5%)2596(3.0%)2415(2.7%)2145(2.4%)2075(2.4%)2067(2.4%)2012(2.3%)1995(2.3%)62263(70.9%)\n      \n      0\n(0.0%)\n    \n    \n      G_TOWNSIZE\n[numeric]\n      Mean (sd) : 5.3 (2.4)min ≤ med ≤ max:1 ≤ 6 ≤ 8IQR (CV) : 5 (0.5)\n      1:8337(9.6%)2:8407(9.7%)3:7237(8.4%)4:6240(7.2%)5:10100(11.7%)6:8447(9.8%)7:15461(17.9%)8:22319(25.8%)\n      \n      1274\n(1.5%)\n    \n    \n      H_SETTLEMENT\n[numeric]\n      Mean (sd) : 3.1 (1.5)min ≤ med ≤ max:1 ≤ 3 ≤ 5IQR (CV) : 3 (0.5)\n      1:18302(20.9%)2:17840(20.4%)3:14116(16.1%)4:14500(16.5%)5:22857(26.1%)\n      \n      207\n(0.2%)\n    \n    \n      H_URBRURAL\n[numeric]\n      Min  : 1Mean : 1.3Max  : 2\n      1:59862(68.2%)2:27928(31.8%)\n      \n      32\n(0.0%)\n    \n    \n      Long\n[numeric]\n      Mean (sd) : 36.2 (68.1)min ≤ med ≤ max:-156.3 ≤ 39.9 ≤ 156.9IQR (CV) : 92.6 (1.9)\n      5482 distinct values\n      \n      27098\n(30.9%)\n    \n    \n      Lat\n[numeric]\n      Mean (sd) : 21.4 (20)min ≤ med ≤ max:-43.3 ≤ 24.8 ≤ 100.3IQR (CV) : 28.7 (0.9)\n      3911 distinct values\n      \n      27094\n(30.9%)\n    \n    \n      FamImpt\n[numeric]\n      Mean (sd) : 1.1 (0.4)min ≤ med ≤ max:1 ≤ 1 ≤ 4IQR (CV) : 0 (0.3)\n      1:78979(90.1%)2:7722(8.8%)3:793(0.9%)4:182(0.2%)\n      \n      146\n(0.2%)\n    \n    \n      FriendsImpt\n[numeric]\n      Mean (sd) : 1.7 (0.7)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:38026(43.4%)2:37657(43.0%)3:10105(11.5%)4:1745(2.0%)\n      \n      289\n(0.3%)\n    \n    \n      LeisureImpt\n[numeric]\n      Mean (sd) : 1.8 (0.8)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:35509(40.7%)2:37328(42.7%)3:12046(13.8%)4:2466(2.8%)\n      \n      473\n(0.5%)\n    \n    \n      ReligionImpt\n[numeric]\n      Mean (sd) : 1.9 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.6)\n      1:42455(48.8%)2:18300(21.0%)3:15396(17.7%)4:10840(12.5%)\n      \n      831\n(0.9%)\n    \n    \n      Happiness\n[numeric]\n      Mean (sd) : 1.9 (0.7)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:27071(31.0%)2:47564(54.5%)3:10659(12.2%)4:1954(2.2%)\n      \n      574\n(0.7%)\n    \n    \n      PerceivedHealth\n[numeric]\n      Mean (sd) : 2.2 (0.9)min ≤ med ≤ max:1 ≤ 2 ≤ 5IQR (CV) : 1 (0.4)\n      1:19021(21.7%)2:38932(44.5%)3:24210(27.6%)4:4434(5.1%)5:971(1.1%)\n      \n      254\n(0.3%)\n    \n    \n      FOC\n[numeric]\n      Mean (sd) : 7.2 (2.3)min ≤ med ≤ max:1 ≤ 7 ≤ 10IQR (CV) : 3 (0.3)\n      1:2162(2.5%)2:1243(1.4%)3:2445(2.8%)4:3500(4.0%)5:10992(12.6%)6:9521(10.9%)7:13833(15.9%)8:16714(19.2%)9:7937(9.1%)10:18675(21.5%)\n      \n      800\n(0.9%)\n    \n    \n      LS\n[numeric]\n      Mean (sd) : 7 (2.3)min ≤ med ≤ max:1 ≤ 7 ≤ 10IQR (CV) : 3 (0.3)\n      1:2484(2.8%)2:1284(1.5%)3:2875(3.3%)4:4064(4.6%)5:10620(12.1%)6:10040(11.5%)7:14730(16.8%)8:17517(20.0%)9:8939(10.2%)10:14876(17.0%)\n      \n      393\n(0.4%)\n    \n    \n      FS\n[numeric]\n      Mean (sd) : 6.2 (2.4)min ≤ med ≤ max:1 ≤ 6 ≤ 10IQR (CV) : 3 (0.4)\n      1:4989(5.7%)2:2834(3.2%)3:5059(5.8%)4:6455(7.4%)5:14206(16.3%)6:11865(13.6%)7:13932(16.0%)8:13170(15.1%)9:5783(6.6%)10:8984(10.3%)\n      \n      545\n(0.6%)\n    \n    \n      Trust\n[numeric]\n      Min  : 1Mean : 1.8Max  : 2\n      1:20326(23.5%)2:66298(76.5%)\n      \n      1198\n(1.4%)\n    \n    \n      AttendReligious\n[numeric]\n      Mean (sd) : 4.1 (2.2)min ≤ med ≤ max:1 ≤ 4 ≤ 7IQR (CV) : 4 (0.5)\n      1:11941(13.8%)2:16094(18.5%)3:8890(10.2%)4:13579(15.6%)5:4512(5.2%)6:10835(12.5%)7:20937(24.1%)\n      \n      1034\n(1.2%)\n    \n    \n      Sex\n[numeric]\n      Min  : 1Mean : 1.5Max  : 2\n      1:41654(47.5%)2:46106(52.5%)\n      \n      62\n(0.1%)\n    \n    \n      Age\n[numeric]\n      Mean (sd) : 42.9 (16.4)min ≤ med ≤ max:16 ≤ 41 ≤ 103IQR (CV) : 26 (0.4)\n      85 distinct values\n      \n      339\n(0.4%)\n    \n    \n      Immigrant\n[numeric]\n      Min  : 1Mean : 1.1Max  : 2\n      1:82299(94.1%)2:5179(5.9%)\n      \n      344\n(0.4%)\n    \n    \n      Citizen\n[numeric]\n      Min  : 1Mean : 1Max  : 2\n      1:80826(97.8%)2:1832(2.2%)\n      \n      5164\n(5.9%)\n    \n    \n      HHSize\n[numeric]\n      Mean (sd) : 3.9 (2.2)min ≤ med ≤ max:1 ≤ 4 ≤ 63IQR (CV) : 3 (0.6)\n      33 distinct values\n      \n      852\n(1.0%)\n    \n    \n      Parents\n[numeric]\n      Mean (sd) : 1.4 (0.6)min ≤ med ≤ max:1 ≤ 1 ≤ 4IQR (CV) : 1 (0.4)\n      1:61004(70.6%)2:20796(24.1%)3:4048(4.7%)4:536(0.6%)\n      \n      1438\n(1.6%)\n    \n    \n      Married\n[numeric]\n      Mean (sd) : 2.6 (2.1)min ≤ med ≤ max:1 ≤ 1 ≤ 6IQR (CV) : 4 (0.8)\n      1:49193(56.3%)2:6782(7.8%)3:3614(4.1%)4:1909(2.2%)5:4770(5.5%)6:21050(24.1%)\n      \n      504\n(0.6%)\n    \n    \n      Kids\n[numeric]\n      Mean (sd) : 1.8 (1.7)min ≤ med ≤ max:0 ≤ 2 ≤ 24IQR (CV) : 3 (1)\n      23 distinct values\n      \n      1201\n(1.4%)\n    \n    \n      Edu\n[numeric]\n      Mean (sd) : 3.5 (2)min ≤ med ≤ max:0 ≤ 3 ≤ 8IQR (CV) : 3 (0.6)\n      0:4690(5.4%)1:10721(12.3%)2:12179(14.0%)3:22178(25.5%)4:8235(9.5%)5:7446(8.6%)6:15158(17.4%)7:5402(6.2%)8:995(1.1%)\n      \n      818\n(0.9%)\n    \n    \n      Job\n[numeric]\n      Mean (sd) : 3.1 (2.1)min ≤ med ≤ max:1 ≤ 3 ≤ 8IQR (CV) : 4 (0.7)\n      1:31351(36.2%)2:7467(8.6%)3:12868(14.8%)4:10126(11.7%)5:12073(13.9%)6:5034(5.8%)7:6783(7.8%)8:977(1.1%)\n      \n      1143\n(1.3%)\n    \n    \n      SocialClass\n[numeric]\n      Mean (sd) : 3.3 (1)min ≤ med ≤ max:1 ≤ 3 ≤ 5IQR (CV) : 1 (0.3)\n      1:1453(1.7%)2:18048(21.1%)3:33302(38.9%)4:22649(26.5%)5:10068(11.8%)\n      \n      2302\n(2.6%)\n    \n    \n      Income\n[numeric]\n      Mean (sd) : 4.9 (2.1)min ≤ med ≤ max:1 ≤ 5 ≤ 10IQR (CV) : 3 (0.4)\n      1:6904(8.1%)2:5163(6.0%)3:9612(11.2%)4:11839(13.8%)5:20733(24.3%)6:13216(15.5%)7:9950(11.6%)8:4966(5.8%)9:1417(1.7%)10:1692(2.0%)\n      \n      2330\n(2.7%)\n    \n    \n      IncomeR\n[numeric]\n      Mean (sd) : 1.8 (0.6)min ≤ med ≤ max:1 ≤ 2 ≤ 3IQR (CV) : 1 (0.3)\n      1:21679(25.4%)2:55738(65.2%)3:8075(9.4%)\n      \n      2330\n(2.7%)\n    \n    \n      Religion\n[numeric]\n      Mean (sd) : 3 (2.6)min ≤ med ≤ max:0 ≤ 3 ≤ 9IQR (CV) : 4 (0.9)\n      0:19919(23.3%)1:16027(18.8%)2:6508(7.6%)3:7762(9.1%)4:235(0.3%)5:23807(27.9%)6:569(0.7%)7:5556(6.5%)8:2777(3.3%)9:2177(2.6%)\n      \n      2485\n(2.8%)\n    \n    \n      I_WOMJOB\n[numeric]\n      Mean (sd) : 0.5 (0.3)min ≤ med ≤ max:0 ≤ 0.5 ≤ 1IQR (CV) : 0.5 (0.7)\n      5 distinct values\n      \n      648\n(0.7%)\n    \n    \n      I_WOMPOL\n[numeric]\n      Mean (sd) : 0.5 (0.3)min ≤ med ≤ max:0 ≤ 0.7 ≤ 1IQR (CV) : 0.3 (0.6)\n      4 distinct values\n      \n      2222\n(2.5%)\n    \n    \n      I_WOMEDU\n[numeric]\n      Mean (sd) : 0.7 (0.3)min ≤ med ≤ max:0 ≤ 0.7 ≤ 1IQR (CV) : 0.3 (0.4)\n      4 distinct values\n      \n      1250\n(1.4%)\n    \n    \n      homolib\n[numeric]\n      Mean (sd) : 3.9 (3.3)min ≤ med ≤ max:1 ≤ 2 ≤ 10IQR (CV) : 5 (0.9)\n      1:37570(45.7%)2:4967(6.0%)3:4037(4.9%)4:3211(3.9%)5:9115(11.1%)6:3866(4.7%)7:2893(3.5%)8:3366(4.1%)9:2090(2.5%)10:11016(13.4%)\n      \n      5691\n(6.5%)\n    \n    \n      abortlib\n[numeric]\n      Mean (sd) : 3.4 (2.9)min ≤ med ≤ max:1 ≤ 2 ≤ 10IQR (CV) : 4 (0.9)\n      1:39931(46.5%)2:6502(7.6%)3:5802(6.8%)4:4207(4.9%)5:10290(12.0%)6:4357(5.1%)7:3450(4.0%)8:3748(4.4%)9:2019(2.4%)10:5537(6.5%)\n      \n      1979\n(2.3%)\n    \n    \n      NS\n[factor]\n      1. 02. 1\n      59178(67.4%)28644(32.6%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-11-11\n\n\n\nThe dataset has 87,822 rows, each representing one participant, and 37 columns. Referring to the codebook, these are some noteworthy descriptive statistics:\n\nRespondents tended to come from more urban settings (H_URBRURAL).\nOn average, family was more likely to be perceived as important than friends, leisure time and religion (FamImpt, FriendsImpt, LeisureImpt, ReligionImpt).\nOn average, people were “quite happy” (the second-highest option for Happiness).\nLife satisfaction tended to be 7/10 (LS).\nPeople tended to err on the side of caution when it came to trusting others (Trust).\nHouseholds had 4 people on average, with maximum household size being 63 (HHSize)!\nThe interquartile range for education was lower secondary to short-cycle tertiary education (Edu).\nFor the survey variables (FamImpt to abortlib), missing data ranged from 0.2% to 6.5%, which is acceptable.\n67.4% of respondents were from the Global North (NS).\n\nLet’s check if life satisfaction and happiness differ between the Global North and South.\n\n\nCode\nt.test(Happiness ~ NS, wvs)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Happiness by NS\nt = 4.1272, df = 49878, p-value = 3.677e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.01164249 0.03270103\nsample estimates:\nmean in group 0 mean in group 1 \n       1.863945        1.841774 \n\n\nCode\nt.test(LS ~ NS, wvs)\n\n\n\n    Welch Two Sample t-test\n\ndata:  LS by NS\nt = 13.283, df = 47990, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.1962795 0.2642322\nsample estimates:\nmean in group 0 mean in group 1 \n       7.117885        6.887629 \n\n\nThe Welch’s two-sample t-tests show that there is a significant difference in happiness and life satisfaction between the Global North and South, where the former has higher mean values for both, p < .001. This echoes Alba (2019)’s finding on happiness, and adds new knowledge to the literature regarding life satisfaction.\nWe can also create graphs to visualize the latitude and longitude of countries in the Global North and Global South.\n\n\nCode\nggplot(wvs) + geom_bin2d(mapping = aes(x = Long, y = Lat)) + facet_wrap(vars(NS))\n\n\nWarning: Removed 27098 rows containing non-finite values (stat_bin2d).\n\n\n\n\n\nThe graph above shows that the Global North (“0”) and South (“1”) are not neatly divided by physical location, due to the existence of developed countries physically located in the South (e.g., South Korea) and developing countries physically located in the North (e.g., Ukraine)."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-intro",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-intro",
    "title": "Final Project Update",
    "section": "Part 2: Intro",
    "text": "Part 2: Intro\nIn the previous post, I discussed the prior literature on the topic (predictors of happiness and life satisfaction in Global South vs. North), my RQs and hypotheses, and explored the dataset (WVS Wave 7). I was given feedback to explain my hypotheses better and have edited that section accordingly.\nI have added one variable, SocialClass, which was a significant predictor in past papers that I missed out previously. I have also removed several variables I feel are not useful for my RQs (e.g., number of women in parliament).\nThe response variables are Happiness and LS (life satisfaction). They will be measured separately, as done in prior papers.\nThe main explanatory variables are PerceivedHealth, FS (financial satisfaction) and NS (country type: North vs. South). A potential interaction between the explanatory variables will be included.\nSome notes before commencing analysis:\n\nIt is important to note that NS cannot be transformed, since it is categorical. However, if required, log/quadratic transformations can be done for PerceivedHealth or FS.\nSince each row represents one participant, the unit of analysis is at the participant level. The NS dummy refers to where the participant comes from, either the Global North or the Global South.\nI will be treating FamImpt, FriendsImpt, LeisureImpt, ReligionImpt and Happiness as continuous. I will attempt to verify this for RQ B by running two regressions with FamImpt as categorical vs. continuous.\nI went through the variables again and realized I have to change many of them to factors, and reverse code some of them. I will do this in the code chunk below before generating the models.\n\n\n\nCode\n# change the following variables to factor type.\nwvs <- wvs %>% mutate(across(c(B_COUNTRY_ALPHA,H_SETTLEMENT,H_URBRURAL,Trust,Sex,Immigrant,Citizen,Parents,Married,Job,Religion), as.factor))\n\n# reverse code the following variables, such that the largest number reflects agreement.\nwvs$FamImpt <- 5-wvs$FamImpt\nwvs$FriendsImpt <- 5-wvs$FriendsImpt\nwvs$LeisureImpt <- 5-wvs$LeisureImpt\nwvs$ReligionImpt <- 5-wvs$ReligionImpt\nwvs$Happiness <- 5-wvs$Happiness\nwvs$PerceivedHealth <- 6-wvs$PerceivedHealth\nwvs$AttendReligious <-8-wvs$AttendReligious\nwvs$SocialClass <- 6-wvs$SocialClass"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-a",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-a",
    "title": "Final Project Update",
    "section": "Part 2: Regression Models (RQ A)",
    "text": "Part 2: Regression Models (RQ A)\n\n\n\n\n\n\nRecap: H1A\n\n\n\nHealth and financial satisfaction will positively predict happiness and life satisfaction in the Global South.\n\n\n\nTo test this hypothesis, I will:\n\nFilter the dataset to only include observations from the Global South.\nCreate plots of health and financial satisfaction against happiness and life satisfaction.\nRun a regression model with these variables. Previous papers did not test an interaction between the variables, so I will not do so. There also does not seem to be a meaningful reason to do so.\n\n\n\n\nCode\n# create subset of dataset.\nsubset <- wvs %>% filter(NS == \"1\")\n\n# i first made bar plots with facet wrapping, but this was not ideal. i also tried a jitter plot (code below), but it also didn't work. i'll use boxplots in the end.\n# ggplot(subset, aes(x = FS, y = LS)) + geom_jitter(stat = \"identity\", width = 0.2, height = 0.1, na.rm = T)\n\n# generate boxplots.\nboxplot(LS ~ PerceivedHealth, subset)\n\n\n\n\n\nCode\nboxplot(LS ~ FS, subset)\n\n\n\n\n\nCode\nboxplot(Happiness ~ PerceivedHealth, subset)\n\n\n\n\n\nCode\nboxplot(Happiness ~ FS, subset)\n\n\n\n\n\nLooking at the boxplots, there seems to be a roughly linear positive relationship between all 4 variables. No transformations should be required to run the regression models.\n\n\nCode\n# run regression model for Happiness.\nModelA_H <- lm(Happiness ~ PerceivedHealth + FS, subset)\nsummary(ModelA_H)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth + FS, data = subset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.78824 -0.37097  0.01499  0.47464  1.95803 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.701889   0.018399   92.50   <2e-16 ***\nPerceivedHealth 0.262885   0.004639   56.67   <2e-16 ***\nFS              0.077193   0.001582   48.78   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6944 on 28499 degrees of freedom\n  (142 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  3618 on 2 and 28499 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for Happiness with demographic controls.\nModelA_H_controls <- lm(Happiness ~ PerceivedHealth + FS + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion, subset)\nsummary(ModelA_H_controls)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth + FS + Sex + Age + Immigrant + \n    Citizen + HHSize + Parents + Married + Kids + Edu + Job + \n    SocialClass + Income + Religion, data = subset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85651 -0.38630  0.02411  0.50703  2.13673 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.629e+00  3.488e-02  46.707  < 2e-16 ***\nPerceivedHealth  2.608e-01  4.956e-03  52.615  < 2e-16 ***\nFS               7.508e-02  1.700e-03  44.172  < 2e-16 ***\nSex2             8.090e-02  9.718e-03   8.324  < 2e-16 ***\nAge             -4.815e-05  4.261e-04  -0.113 0.910044    \nImmigrant2       5.271e-02  5.848e-02   0.901 0.367467    \nCitizen2        -8.387e-02  8.628e-02  -0.972 0.331025    \nHHSize           6.807e-03  2.053e-03   3.316 0.000914 ***\nParents2         6.084e-03  1.274e-02   0.478 0.632937    \nParents3         2.492e-02  1.878e-02   1.327 0.184554    \nParents4        -7.208e-02  5.673e-02  -1.270 0.203939    \nMarried2        -2.291e-02  1.926e-02  -1.190 0.234094    \nMarried3        -1.878e-01  2.980e-02  -6.302 2.99e-10 ***\nMarried4        -1.990e-01  3.518e-02  -5.658 1.55e-08 ***\nMarried5        -1.065e-01  2.120e-02  -5.024 5.09e-07 ***\nMarried6        -7.052e-02  1.496e-02  -4.715 2.43e-06 ***\nKids             4.017e-03  3.149e-03   1.276 0.202050    \nEdu             -1.390e-02  2.447e-03  -5.679 1.37e-08 ***\nJob2            -3.250e-03  1.648e-02  -0.197 0.843711    \nJob3            -1.926e-02  1.238e-02  -1.555 0.119967    \nJob4             2.946e-02  2.234e-02   1.318 0.187377    \nJob5            -8.471e-02  1.505e-02  -5.630 1.82e-08 ***\nJob6            -2.189e-02  2.068e-02  -1.058 0.289894    \nJob7            -5.892e-02  1.607e-02  -3.667 0.000246 ***\nJob8            -3.689e-02  4.336e-02  -0.851 0.394827    \nSocialClass      3.316e-02  4.753e-03   6.977 3.10e-12 ***\nIncome          -1.856e-04  2.287e-03  -0.081 0.935317    \nReligion1        6.950e-02  1.742e-02   3.991 6.61e-05 ***\nReligion2        7.609e-02  2.102e-02   3.620 0.000295 ***\nReligion3       -1.818e-02  2.265e-02  -0.803 0.422249    \nReligion4       -1.593e-01  8.440e-02  -1.887 0.059135 .  \nReligion5        1.240e-02  1.533e-02   0.809 0.418618    \nReligion6       -3.304e-02  4.661e-02  -0.709 0.478414    \nReligion7       -1.247e-02  2.091e-02  -0.597 0.550772    \nReligion8        1.105e-01  2.569e-02   4.302 1.70e-05 ***\nReligion9        3.676e-02  4.022e-02   0.914 0.360654    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.69 on 26907 degrees of freedom\n  (1701 observations deleted due to missingness)\nMultiple R-squared:  0.2118,    Adjusted R-squared:  0.2107 \nF-statistic: 206.5 on 35 and 26907 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for LS. for whatever reason, logging PerceivedHealth produced a slightly higher adjusted R^2, but i did not include that because (1) it did not improve the diagnostic plots and (2) PerceivedHealth is not count data (e.g., population/income) that traditionally improves with logging.\nModelA_LS <- lm(LS ~ PerceivedHealth + FS, subset)\nsummary(ModelA_LS)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth + FS, data = subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3728 -1.1136  0.0639  1.0965  6.6294 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.515388   0.054654   46.02   <2e-16 ***\nPerceivedHealth 0.338881   0.013790   24.57   <2e-16 ***\nFS              0.516304   0.004706  109.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.064 on 28509 degrees of freedom\n  (132 observations deleted due to missingness)\nMultiple R-squared:  0.3396,    Adjusted R-squared:  0.3395 \nF-statistic:  7329 on 2 and 28509 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for LS with demographic controls.\nModelA_LS_controls <- lm(LS ~ PerceivedHealth + FS + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion, subset)\nsummary(ModelA_LS_controls)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth + FS + Sex + Age + Immigrant + \n    Citizen + HHSize + Parents + Married + Kids + Edu + Job + \n    SocialClass + Income + Religion, data = subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0351 -1.1901  0.0939  1.1606  7.0551 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      2.252840   0.103753  21.714  < 2e-16 ***\nPerceivedHealth  0.374906   0.014743  25.429  < 2e-16 ***\nFS               0.501032   0.005058  99.058  < 2e-16 ***\nSex2             0.111760   0.028898   3.867  0.00011 ***\nAge              0.005052   0.001267   3.987 6.71e-05 ***\nImmigrant2       0.024794   0.172774   0.144  0.88589    \nCitizen2         0.012178   0.256561   0.047  0.96214    \nHHSize          -0.003993   0.006105  -0.654  0.51306    \nParents2         0.137374   0.037897   3.625  0.00029 ***\nParents3         0.127139   0.055866   2.276  0.02287 *  \nParents4        -0.175944   0.169331  -1.039  0.29879    \nMarried2         0.038214   0.057252   0.667  0.50448    \nMarried3        -0.281920   0.088449  -3.187  0.00144 ** \nMarried4        -0.363767   0.104647  -3.476  0.00051 ***\nMarried5        -0.136939   0.062998  -2.174  0.02974 *  \nMarried6        -0.234748   0.044475  -5.278 1.31e-07 ***\nKids             0.003537   0.009349   0.378  0.70518    \nEdu             -0.010311   0.007279  -1.417  0.15663    \nJob2             0.125980   0.049015   2.570  0.01017 *  \nJob3            -0.061988   0.036841  -1.683  0.09247 .  \nJob4            -0.043148   0.066305  -0.651  0.51521    \nJob5            -0.108559   0.044760  -2.425  0.01530 *  \nJob6            -0.098398   0.061520  -1.599  0.10973    \nJob7            -0.220107   0.047815  -4.603 4.18e-06 ***\nJob8             0.376925   0.128739   2.928  0.00342 ** \nSocialClass      0.003458   0.014134   0.245  0.80673    \nIncome           0.019709   0.006803   2.897  0.00377 ** \nReligion1        0.153145   0.051783   2.957  0.00310 ** \nReligion2       -0.323565   0.062492  -5.178 2.26e-07 ***\nReligion3       -0.405155   0.067274  -6.022 1.74e-09 ***\nReligion4       -0.218467   0.251068  -0.870  0.38423    \nReligion5       -0.039050   0.045606  -0.856  0.39187    \nReligion6        0.236925   0.138646   1.709  0.08749 .  \nReligion7        0.021554   0.062184   0.347  0.72888    \nReligion8        0.397010   0.076415   5.195 2.06e-07 ***\nReligion9        0.440675   0.119625   3.684  0.00023 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.053 on 26916 degrees of freedom\n  (1692 observations deleted due to missingness)\nMultiple R-squared:  0.3474,    Adjusted R-squared:  0.3466 \nF-statistic: 409.4 on 35 and 26916 DF,  p-value: < 2.2e-16\n\n\nTo summarise, I ran 4 regression models above - 2 each for Happiness and LS (1 with just the main predictors, and 1 with demographic controls). What’s important to note is that in all the models, even with the addition of demographic control variables, PerceivedHealth and FS positively predict Happiness and LS in the Global South, p < .001. The addition of demographic controls also did not improve adjusted R2 by much - just ~0.01. For RQ A, we can reject the null hypothesis.\n\n\nCode\n# diagnostic plots for the models above.\npar(mfrow = c(2,3)); plot(ModelA_H_controls, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelA_H_controls)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelA_H_controls\nBP = 1111.1, df = 35, p-value < 2.2e-16\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelA_LS_controls, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelA_LS_controls)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelA_LS_controls\nBP = 1753, df = 35, p-value < 2.2e-16\n\n\nThe diagnostic plots generally seem fine except the scale-location plot, which indicates heteroskedasticity. The Breusch-Pagan test helps to confirm this. One way to correct this would be to use robust standard errors.\n\n\nCode\n# obtain robust standard errors for models.\ncoeftest(ModelA_H_controls, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                   Estimate  Std. Error t value  Pr(>|t|)    \n(Intercept)      1.6290e+00  3.6811e-02 44.2526 < 2.2e-16 ***\nPerceivedHealth  2.6076e-01  5.5823e-03 46.7122 < 2.2e-16 ***\nFS               7.5078e-02  1.8435e-03 40.7247 < 2.2e-16 ***\nSex2             8.0897e-02  9.7539e-03  8.2938 < 2.2e-16 ***\nAge             -4.8146e-05  4.3017e-04 -0.1119 0.9108859    \nImmigrant2       5.2707e-02  5.7875e-02  0.9107 0.3624530    \nCitizen2        -8.3868e-02  8.0561e-02 -1.0411 0.2978586    \nHHSize           6.8075e-03  2.3891e-03  2.8494 0.0043833 ** \nParents2         6.0841e-03  1.3093e-02  0.4647 0.6421611    \nParents3         2.4917e-02  1.8159e-02  1.3722 0.1700264    \nParents4        -7.2076e-02  6.2383e-02 -1.1554 0.2479456    \nMarried2        -2.2913e-02  2.0262e-02 -1.1308 0.2581384    \nMarried3        -1.8780e-01  3.2263e-02 -5.8210 5.918e-09 ***\nMarried4        -1.9903e-01  4.1600e-02 -4.7843 1.725e-06 ***\nMarried5        -1.0652e-01  2.2111e-02 -4.8175 1.462e-06 ***\nMarried6        -7.0524e-02  1.5482e-02 -4.5552 5.256e-06 ***\nKids             4.0168e-03  3.3643e-03  1.1939 0.2325112    \nEdu             -1.3898e-02  2.4639e-03 -5.6406 1.711e-08 ***\nJob2            -3.2496e-03  1.5661e-02 -0.2075 0.8356224    \nJob3            -1.9258e-02  1.2231e-02 -1.5744 0.1153957    \nJob4             2.9459e-02  2.2008e-02  1.3386 0.1807214    \nJob5            -8.4713e-02  1.4778e-02 -5.7324 1.001e-08 ***\nJob6            -2.1886e-02  2.0707e-02 -1.0569 0.2905639    \nJob7            -5.8923e-02  1.7249e-02 -3.4160 0.0006365 ***\nJob8            -3.6892e-02  5.0343e-02 -0.7328 0.4636812    \nSocialClass      3.3162e-02  4.9316e-03  6.7244 1.799e-11 ***\nIncome          -1.8561e-04  2.3702e-03 -0.0783 0.9375819    \nReligion1        6.9505e-02  1.8209e-02  3.8170 0.0001354 ***\nReligion2        7.6088e-02  2.2839e-02  3.3315 0.0008650 ***\nReligion3       -1.8177e-02  2.1665e-02 -0.8390 0.4014735    \nReligion4       -1.5928e-01  8.7627e-02 -1.8177 0.0691182 .  \nReligion5        1.2403e-02  1.5230e-02  0.8144 0.4154368    \nReligion6       -3.3038e-02  3.8184e-02 -0.8652 0.3869116    \nReligion7       -1.2473e-02  2.0610e-02 -0.6052 0.5450570    \nReligion8        1.1052e-01  2.5547e-02  4.3261 1.523e-05 ***\nReligion9        3.6764e-02  4.3284e-02  0.8494 0.3956905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(ModelA_LS_controls, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                  Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)      2.2528402  0.1064129 21.1707 < 2.2e-16 ***\nPerceivedHealth  0.3749064  0.0162830 23.0244 < 2.2e-16 ***\nFS               0.5010321  0.0062128 80.6452 < 2.2e-16 ***\nSex2             0.1117604  0.0289195  3.8645 0.0001116 ***\nAge              0.0050525  0.0013206  3.8258 0.0001306 ***\nImmigrant2       0.0247937  0.1779629  0.1393 0.8891987    \nCitizen2         0.0121777  0.2740275  0.0444 0.9645543    \nHHSize          -0.0039932  0.0065037 -0.6140 0.5392264    \nParents2         0.1373743  0.0386397  3.5553 0.0003782 ***\nParents3         0.1271390  0.0562443  2.2605 0.0237995 *  \nParents4        -0.1759444  0.1602356 -1.0980 0.2721988    \nMarried2         0.0382137  0.0594300  0.6430 0.5202275    \nMarried3        -0.2819201  0.0976985 -2.8856 0.0039096 ** \nMarried4        -0.3637666  0.1143297 -3.1817 0.0014656 ** \nMarried5        -0.1369391  0.0649089 -2.1097 0.0348923 *  \nMarried6        -0.2347479  0.0454759 -5.1620 2.460e-07 ***\nKids             0.0035369  0.0101206  0.3495 0.7267341    \nEdu             -0.0103113  0.0071848 -1.4352 0.1512529    \nJob2             0.1259797  0.0478372  2.6335 0.0084555 ** \nJob3            -0.0619882  0.0358968 -1.7268 0.0842075 .  \nJob4            -0.0431482  0.0646541 -0.6674 0.5045419    \nJob5            -0.1085591  0.0442199 -2.4550 0.0140953 *  \nJob6            -0.0983980  0.0588659 -1.6716 0.0946227 .  \nJob7            -0.2201066  0.0511284 -4.3050 1.676e-05 ***\nJob8             0.3769250  0.1471414  2.5617 0.0104230 *  \nSocialClass      0.0034580  0.0152135  0.2273 0.8201950    \nIncome           0.0197092  0.0074973  2.6289 0.0085720 ** \nReligion1        0.1531451  0.0503513  3.0415 0.0023560 ** \nReligion2       -0.3235648  0.0648191 -4.9918 6.019e-07 ***\nReligion3       -0.4051554  0.0654854 -6.1870 6.222e-10 ***\nReligion4       -0.2184666  0.2857574 -0.7645 0.4445656    \nReligion5       -0.0390497  0.0411714 -0.9485 0.3429004    \nReligion6        0.2369245  0.1113743  2.1273 0.0334058 *  \nReligion7        0.0215541  0.0577008  0.3735 0.7087426    \nReligion8        0.3970097  0.0738658  5.3747 7.733e-08 ***\nReligion9        0.4406754  0.1037644  4.2469 2.175e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI generated robust standard errors for the models above. Although the t-values have reduced, the predictors are still positive and significant. Hence, we can still reject the null hypothesis."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-b",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-b",
    "title": "Final Project Update",
    "section": "Part 2: Regression Models (RQ B)",
    "text": "Part 2: Regression Models (RQ B)\n\n\n\n\n\n\nRecap: H1B\n\n\n\nHealth and financial satisfaction will have a greater impact on happiness and life satisfaction on the Global South than the Global North.\n\n\nI will first run a correlation matrix with all potential numeric variables that might be relevant.\n\n\nCode\n# run correlations for numeric variables (except DVs, which are Happiness and LS).\nmatrix <- wvs %>% select(PerceivedHealth, FS, G_TOWNSIZE, FamImpt, FriendsImpt, LeisureImpt, ReligionImpt, FOC, AttendReligious, Age, HHSize, Kids, Edu, SocialClass, IncomeR, I_WOMJOB, I_WOMPOL, I_WOMEDU, homolib, abortlib)\ncor <- cor(matrix, use=\"complete.obs\")\n\n\nIn the correlation matrix, I am concerned about correlations where r ≥ 0.5 (more conservative than a cut-off of r ≥ 0.7). This is observed between AttendReligious and ReligionImpt; and homolib and abortlib. I will try a few different models to figure out which combination of variables might work best.\n\n\nCode\n# try model with \"AttendReligious\" and \"abortlib\".\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + FOC + Trust + AttendReligious + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + FOC + \n    Trust + AttendReligious + Sex + Age + Immigrant + Citizen + \n    HHSize + Parents + Married + Kids + Edu + Job + SocialClass + \n    Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, \n    data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84562 -0.35131 -0.02747  0.46085  2.53907 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1.0023646  0.0485735  20.636  < 2e-16 ***\nPerceivedHealth         0.2417281  0.0091906  26.302  < 2e-16 ***\nFS                      0.0752496  0.0055272  13.614  < 2e-16 ***\nNS1                    -0.3760471  0.0494237  -7.609 2.81e-14 ***\nG_TOWNSIZE             -0.0109366  0.0013678  -7.996 1.31e-15 ***\nH_URBRURAL2             0.0398046  0.0071450   5.571 2.54e-08 ***\nFamImpt                 0.1207622  0.0070036  17.243  < 2e-16 ***\nFriendsImpt             0.0249529  0.0034003   7.339 2.18e-13 ***\nLeisureImpt             0.0239952  0.0032088   7.478 7.64e-14 ***\nFOC                     0.0324218  0.0011304  28.682  < 2e-16 ***\nTrust2                 -0.0242134  0.0061949  -3.909 9.29e-05 ***\nAttendReligious         0.0077179  0.0013120   5.882 4.06e-09 ***\nSex2                    0.0448632  0.0053685   8.357  < 2e-16 ***\nAge                    -0.0003398  0.0002341  -1.452 0.146603    \nImmigrant2              0.0048528  0.0121882   0.398 0.690517    \nCitizen2                0.0346332  0.0198507   1.745 0.081045 .  \nHHSize                  0.0049448  0.0013326   3.711 0.000207 ***\nParents2               -0.0080024  0.0073853  -1.084 0.278566    \nParents3                0.0269953  0.0116900   2.309 0.020932 *  \nParents4               -0.0226720  0.0315412  -0.719 0.472263    \nMarried2                0.0392866  0.0093657   4.195 2.74e-05 ***\nMarried3               -0.1071227  0.0130624  -8.201 2.43e-16 ***\nMarried4               -0.0757703  0.0165779  -4.571 4.87e-06 ***\nMarried5               -0.0653700  0.0119429  -5.474 4.43e-08 ***\nMarried6               -0.0475473  0.0081482  -5.835 5.39e-09 ***\nKids                    0.0104236  0.0019030   5.477 4.33e-08 ***\nEdu                    -0.0049675  0.0014208  -3.496 0.000472 ***\nJob2                    0.0321825  0.0093302   3.449 0.000562 ***\nJob3                    0.0065385  0.0076058   0.860 0.389974    \nJob4                    0.0400777  0.0098986   4.049 5.15e-05 ***\nJob5                    0.0308835  0.0085066   3.631 0.000283 ***\nJob6                    0.0239344  0.0116517   2.054 0.039965 *  \nJob7                   -0.0086893  0.0095815  -0.907 0.364470    \nJob8                    0.0333899  0.0243857   1.369 0.170928    \nSocialClass             0.0325389  0.0029396  11.069  < 2e-16 ***\nIncome                  0.0007032  0.0014035   0.501 0.616341    \nReligion1               0.0519833  0.0083318   6.239 4.43e-10 ***\nReligion2               0.0046457  0.0107478   0.432 0.665564    \nReligion3              -0.0727053  0.0104960  -6.927 4.34e-12 ***\nReligion4              -0.1164567  0.0458200  -2.542 0.011036 *  \nReligion5              -0.0820015  0.0084759  -9.675  < 2e-16 ***\nReligion6              -0.0568803  0.0292758  -1.943 0.052031 .  \nReligion7              -0.0326901  0.0113023  -2.892 0.003825 ** \nReligion8               0.0617715  0.0152881   4.040 5.34e-05 ***\nReligion9               0.0269946  0.0157294   1.716 0.086132 .  \nI_WOMJOB                0.0183741  0.0090170   2.038 0.041582 *  \nI_WOMPOL               -0.0080608  0.0094226  -0.855 0.392295    \nI_WOMEDU               -0.0738107  0.0093836  -7.866 3.72e-15 ***\nabortlib               -0.0091560  0.0009459  -9.679  < 2e-16 ***\nPerceivedHealth:FS     -0.0042135  0.0014208  -2.966 0.003022 ** \nPerceivedHealth:NS1     0.0915541  0.0132100   6.931 4.23e-12 ***\nFS:NS1                  0.0522839  0.0080080   6.529 6.67e-11 ***\nPerceivedHealth:FS:NS1 -0.0115482  0.0020588  -5.609 2.04e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68392 degrees of freedom\n  (19377 observations deleted due to missingness)\nMultiple R-squared:  0.2346,    Adjusted R-squared:  0.2341 \nF-statistic: 403.2 on 52 and 68392 DF,  p-value: < 2.2e-16\n\n\nCode\n# try model with \"ReligionImpt\" and \"abortlib\".\nModelB_H <- lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs)\nsummary(ModelB_H)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + \n    FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + \n    Parents + Married + Kids + Edu + Job + SocialClass + Income + \n    Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.83838 -0.35003 -0.02486  0.45985  2.58146 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             0.9653591  0.0487346  19.808  < 2e-16 ***\nPerceivedHealth         0.2427013  0.0091832  26.429  < 2e-16 ***\nFS                      0.0753615  0.0055248  13.641  < 2e-16 ***\nNS1                    -0.3875722  0.0493653  -7.851 4.18e-15 ***\nG_TOWNSIZE             -0.0104199  0.0013679  -7.617 2.62e-14 ***\nH_URBRURAL2             0.0409348  0.0071384   5.734 9.82e-09 ***\nFamImpt                 0.1139240  0.0070337  16.197  < 2e-16 ***\nFriendsImpt             0.0239321  0.0033986   7.042 1.92e-12 ***\nLeisureImpt             0.0218539  0.0032127   6.802 1.04e-11 ***\nReligionImpt            0.0359167  0.0031719  11.323  < 2e-16 ***\nFOC                     0.0322169  0.0011295  28.522  < 2e-16 ***\nTrust2                 -0.0291083  0.0062043  -4.692 2.72e-06 ***\nSex2                    0.0415730  0.0053662   7.747 9.52e-15 ***\nAge                    -0.0002997  0.0002339  -1.281 0.200055    \nImmigrant2              0.0069104  0.0121815   0.567 0.570520    \nCitizen2                0.0388779  0.0198208   1.961 0.049829 *  \nHHSize                  0.0045529  0.0013320   3.418 0.000631 ***\nParents2               -0.0086985  0.0073777  -1.179 0.238390    \nParents3                0.0216617  0.0116667   1.857 0.063356 .  \nParents4               -0.0282459  0.0314608  -0.898 0.369288    \nMarried2                0.0394323  0.0093613   4.212 2.53e-05 ***\nMarried3               -0.1125812  0.0130569  -8.622  < 2e-16 ***\nMarried4               -0.0795349  0.0165963  -4.792 1.65e-06 ***\nMarried5               -0.0668593  0.0119429  -5.598 2.17e-08 ***\nMarried6               -0.0500088  0.0081394  -6.144 8.09e-10 ***\nKids                    0.0101190  0.0019017   5.321 1.04e-07 ***\nEdu                    -0.0039301  0.0014214  -2.765 0.005694 ** \nJob2                    0.0299734  0.0093275   3.213 0.001312 ** \nJob3                    0.0051187  0.0076043   0.673 0.500861    \nJob4                    0.0367885  0.0099071   3.713 0.000205 ***\nJob5                    0.0263369  0.0084946   3.100 0.001933 ** \nJob6                    0.0255340  0.0116272   2.196 0.028091 *  \nJob7                   -0.0123139  0.0095726  -1.286 0.198321    \nJob8                    0.0260058  0.0243860   1.066 0.286237    \nSocialClass             0.0331230  0.0029360  11.282  < 2e-16 ***\nIncome                  0.0007745  0.0014028   0.552 0.580883    \nReligion1               0.0356580  0.0084815   4.204 2.62e-05 ***\nReligion2              -0.0148305  0.0108456  -1.367 0.171498    \nReligion3              -0.0949489  0.0107237  -8.854  < 2e-16 ***\nReligion4              -0.1352025  0.0458731  -2.947 0.003207 ** \nReligion5              -0.1116511  0.0090220 -12.375  < 2e-16 ***\nReligion6              -0.0804296  0.0294147  -2.734 0.006252 ** \nReligion7              -0.0484974  0.0114371  -4.240 2.23e-05 ***\nReligion8               0.0410340  0.0154060   2.664 0.007735 ** \nReligion9               0.0127811  0.0157734   0.810 0.417775    \nI_WOMJOB                0.0209233  0.0090190   2.320 0.020349 *  \nI_WOMPOL               -0.0063906  0.0094151  -0.679 0.497292    \nI_WOMEDU               -0.0730143  0.0093696  -7.793 6.65e-15 ***\nabortlib               -0.0075929  0.0009610  -7.901 2.82e-15 ***\nPerceivedHealth:FS     -0.0042494  0.0014202  -2.992 0.002771 ** \nPerceivedHealth:NS1     0.0932809  0.0131979   7.068 1.59e-12 ***\nFS:NS1                  0.0535637  0.0080021   6.694 2.19e-11 ***\nPerceivedHealth:FS:NS1 -0.0117965  0.0020574  -5.734 9.86e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68517 degrees of freedom\n  (19252 observations deleted due to missingness)\nMultiple R-squared:  0.2363,    Adjusted R-squared:  0.2357 \nF-statistic: 407.7 on 52 and 68517 DF,  p-value: < 2.2e-16\n\n\nCode\n# run model with \"FamImpt\" as categorical.\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + as.factor(FamImpt) + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + as.factor(FamImpt) + FriendsImpt + LeisureImpt + \n    ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + \n    HHSize + Parents + Married + Kids + Edu + Job + SocialClass + \n    Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, \n    data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.83836 -0.34998 -0.02473  0.45999  2.60795 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1.1072339  0.0681742  16.241  < 2e-16 ***\nPerceivedHealth         0.2425875  0.0091842  26.414  < 2e-16 ***\nFS                      0.0752720  0.0055256  13.622  < 2e-16 ***\nNS1                    -0.3882081  0.0493691  -7.863 3.79e-15 ***\nG_TOWNSIZE             -0.0104176  0.0013681  -7.615 2.68e-14 ***\nH_URBRURAL2             0.0409434  0.0071384   5.736 9.75e-09 ***\nas.factor(FamImpt)2     0.0592525  0.0602280   0.984 0.325216    \nas.factor(FamImpt)3     0.2033328  0.0549154   3.703 0.000214 ***\nas.factor(FamImpt)4     0.3138016  0.0544706   5.761 8.40e-09 ***\nFriendsImpt             0.0239401  0.0033987   7.044 1.89e-12 ***\nLeisureImpt             0.0218856  0.0032129   6.812 9.72e-12 ***\nReligionImpt            0.0359340  0.0031724  11.327  < 2e-16 ***\nFOC                     0.0322200  0.0011296  28.524  < 2e-16 ***\nTrust2                 -0.0291466  0.0062056  -4.697 2.65e-06 ***\nSex2                    0.0415226  0.0053664   7.738 1.03e-14 ***\nAge                    -0.0002972  0.0002339  -1.270 0.203936    \nImmigrant2              0.0069407  0.0121817   0.570 0.568840    \nCitizen2                0.0387761  0.0198213   1.956 0.050435 .  \nHHSize                  0.0045466  0.0013320   3.413 0.000642 ***\nParents2               -0.0088173  0.0073794  -1.195 0.232147    \nParents3                0.0217277  0.0116670   1.862 0.062561 .  \nParents4               -0.0280973  0.0314612  -0.893 0.371818    \nMarried2                0.0393610  0.0093636   4.204 2.63e-05 ***\nMarried3               -0.1123870  0.0130579  -8.607  < 2e-16 ***\nMarried4               -0.0794961  0.0165964  -4.790 1.67e-06 ***\nMarried5               -0.0668742  0.0119429  -5.599 2.16e-08 ***\nMarried6               -0.0499130  0.0081398  -6.132 8.73e-10 ***\nKids                    0.0101127  0.0019017   5.318 1.05e-07 ***\nEdu                    -0.0039281  0.0014214  -2.764 0.005719 ** \nJob2                    0.0300083  0.0093277   3.217 0.001295 ** \nJob3                    0.0051541  0.0076045   0.678 0.497914    \nJob4                    0.0368074  0.0099073   3.715 0.000203 ***\nJob5                    0.0263615  0.0084947   3.103 0.001915 ** \nJob6                    0.0255713  0.0116272   2.199 0.027863 *  \nJob7                   -0.0122479  0.0095730  -1.279 0.200754    \nJob8                    0.0259292  0.0243864   1.063 0.287665    \nSocialClass             0.0331054  0.0029361  11.275  < 2e-16 ***\nIncome                  0.0007985  0.0014030   0.569 0.569271    \nReligion1               0.0355768  0.0084819   4.194 2.74e-05 ***\nReligion2              -0.0147994  0.0108457  -1.365 0.172402    \nReligion3              -0.0949489  0.0107240  -8.854  < 2e-16 ***\nReligion4              -0.1347230  0.0458750  -2.937 0.003318 ** \nReligion5              -0.1115545  0.0090242 -12.362  < 2e-16 ***\nReligion6              -0.0803182  0.0294174  -2.730 0.006329 ** \nReligion7              -0.0486226  0.0114391  -4.251 2.14e-05 ***\nReligion8               0.0408256  0.0154072   2.650 0.008057 ** \nReligion9               0.0128689  0.0157737   0.816 0.414593    \nI_WOMJOB                0.0209234  0.0090190   2.320 0.020348 *  \nI_WOMPOL               -0.0064471  0.0094153  -0.685 0.493502    \nI_WOMEDU               -0.0729682  0.0093718  -7.786 7.02e-15 ***\nabortlib               -0.0075775  0.0009611  -7.884 3.22e-15 ***\nPerceivedHealth:FS     -0.0042300  0.0014204  -2.978 0.002902 ** \nPerceivedHealth:NS1     0.0934470  0.0131990   7.080 1.46e-12 ***\nFS:NS1                  0.0537264  0.0080034   6.713 1.92e-11 ***\nPerceivedHealth:FS:NS1 -0.0118354  0.0020577  -5.752 8.87e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68515 degrees of freedom\n  (19252 observations deleted due to missingness)\nMultiple R-squared:  0.2363,    Adjusted R-squared:  0.2357 \nF-statistic: 392.6 on 54 and 68515 DF,  p-value: < 2.2e-16\n\n\nCode\n# try model with \"ReligionImpt\" and \"homolib\": adjusted R^2 was lower, so i'm sticking to ModelB_H. significance of main predictors doesn't change either.\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + IncomeR + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + homolib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + \n    FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + \n    Parents + Married + Kids + Edu + Job + SocialClass + IncomeR + \n    Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + homolib, data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82527 -0.34776 -0.02813  0.46170  2.57719 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             9.352e-01  4.939e-02  18.934  < 2e-16 ***\nPerceivedHealth         2.391e-01  9.295e-03  25.728  < 2e-16 ***\nFS                      7.412e-02  5.575e-03  13.296  < 2e-16 ***\nNS1                    -3.655e-01  5.080e-02  -7.194 6.34e-13 ***\nG_TOWNSIZE             -7.377e-03  1.393e-03  -5.295 1.20e-07 ***\nH_URBRURAL2             4.442e-02  7.274e-03   6.107 1.02e-09 ***\nFamImpt                 1.179e-01  7.079e-03  16.661  < 2e-16 ***\nFriendsImpt             2.194e-02  3.486e-03   6.293 3.14e-10 ***\nLeisureImpt             1.999e-02  3.324e-03   6.013 1.83e-09 ***\nReligionImpt            4.532e-02  3.186e-03  14.224  < 2e-16 ***\nFOC                     3.203e-02  1.156e-03  27.711  < 2e-16 ***\nTrust2                 -1.941e-02  6.334e-03  -3.064 0.002184 ** \nSex2                    3.663e-02  5.473e-03   6.693 2.21e-11 ***\nAge                    -6.294e-04  2.394e-04  -2.629 0.008555 ** \nImmigrant2              8.568e-03  1.221e-02   0.702 0.482983    \nCitizen2                3.916e-02  1.985e-02   1.973 0.048524 *  \nHHSize                  2.562e-03  1.377e-03   1.861 0.062696 .  \nParents2               -6.188e-03  7.519e-03  -0.823 0.410536    \nParents3                3.009e-02  1.212e-02   2.483 0.013020 *  \nParents4               -2.584e-02  3.217e-02  -0.803 0.421753    \nMarried2                3.385e-02  9.428e-03   3.590 0.000331 ***\nMarried3               -1.082e-01  1.326e-02  -8.162 3.34e-16 ***\nMarried4               -7.639e-02  1.688e-02  -4.525 6.06e-06 ***\nMarried5               -6.989e-02  1.231e-02  -5.676 1.39e-08 ***\nMarried6               -5.380e-02  8.288e-03  -6.491 8.58e-11 ***\nKids                    9.797e-03  1.952e-03   5.019 5.21e-07 ***\nEdu                    -6.213e-03  1.460e-03  -4.256 2.09e-05 ***\nJob2                    3.939e-02  9.561e-03   4.120 3.80e-05 ***\nJob3                    1.194e-02  7.739e-03   1.543 0.122926    \nJob4                    3.650e-02  1.012e-02   3.607 0.000310 ***\nJob5                    3.441e-02  8.731e-03   3.941 8.11e-05 ***\nJob6                    2.356e-02  1.187e-02   1.985 0.047189 *  \nJob7                   -2.361e-02  9.918e-03  -2.381 0.017287 *  \nJob8                    2.882e-02  2.483e-02   1.161 0.245692    \nSocialClass             2.947e-02  2.934e-03  10.046  < 2e-16 ***\nIncomeR                 6.624e-03  4.955e-03   1.337 0.181299    \nReligion1               3.819e-02  8.529e-03   4.478 7.56e-06 ***\nReligion2              -1.602e-02  1.094e-02  -1.464 0.143063    \nReligion3              -9.595e-02  1.093e-02  -8.777  < 2e-16 ***\nReligion4              -1.403e-01  4.582e-02  -3.061 0.002207 ** \nReligion5              -1.144e-01  9.290e-03 -12.318  < 2e-16 ***\nReligion6              -8.127e-02  2.935e-02  -2.769 0.005632 ** \nReligion7              -4.454e-02  1.148e-02  -3.881 0.000104 ***\nReligion8               4.505e-02  1.560e-02   2.888 0.003875 ** \nReligion9               1.857e-02  1.585e-02   1.172 0.241385    \nI_WOMJOB                8.150e-03  9.281e-03   0.878 0.379913    \nI_WOMPOL               -1.148e-02  9.708e-03  -1.182 0.237049    \nI_WOMEDU               -6.899e-02  9.770e-03  -7.062 1.66e-12 ***\nhomolib                 2.897e-05  9.231e-04   0.031 0.974960    \nPerceivedHealth:FS     -3.838e-03  1.433e-03  -2.678 0.007418 ** \nPerceivedHealth:NS1     9.339e-02  1.355e-02   6.891 5.60e-12 ***\nFS:NS1                  4.705e-02  8.226e-03   5.719 1.08e-08 ***\nPerceivedHealth:FS:NS1 -1.121e-02  2.111e-03  -5.309 1.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.622 on 65328 degrees of freedom\n  (22441 observations deleted due to missingness)\nMultiple R-squared:  0.231, Adjusted R-squared:  0.2304 \nF-statistic: 377.4 on 52 and 65328 DF,  p-value: < 2.2e-16\n\n\nLooking at adjusted R2, ReligionImpt is preferable to AttendReligious, and abortlib is preferable to homolib.\nI also ran another model to see how things change when FamImpt is treated as categorical: adjusted R2 remained the same, but the standard errors rose and t-values went down. Hence, I will stick to treating it as numeric (as well as other ordered variables with 4 levels).\nAdditionally, in past papers, adjusted R2 ranged between 0.15 to 0.3 for happiness. Ours is in the higher part of that range (adjusted R2 = 0.24).\nFor the final model I settled on (labelled as ModelB_H above), a three-way interaction between PerceivedHealth, FS and NS was observed. I will plot this graphically.\n\n\nCode\ninteract_plot(ModelB_H, pred = PerceivedHealth, modx = FS, modx.values = c(1,10), mod2 = NS)\n\n\n\n\n\nFor people in the Global North (NS = 0), it seems like the magnitude of the relationship between PerceivedHealth and Happiness does not change for different values of FS. However, for those in the Global South, the relationship seems to become less steep for greater values of FS. As FS increases, the impact of PerceivedHealth on Happiness reduces. I think at least partially, this shows that Happiness depends more on PerceivedHealth and FS in the Global South (but I am not 100% sure that my interpretation is correct).\nTo fully answer the RQ, I now need to re-run the same model with LS as the DV.\n\n\nCode\n# run model.\nModelB_LS <- lm(LS ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs)\nsummary(ModelB_LS)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth * FS * NS + G_TOWNSIZE + H_URBRURAL + \n    FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + \n    Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + \n    Married + Kids + Edu + Job + SocialClass + Income + Religion + \n    I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, data = wvs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0761 -0.9137  0.0669  0.9278  8.1280 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             0.118394   0.135781   0.872 0.383240    \nPerceivedHealth         0.517614   0.025575  20.239  < 2e-16 ***\nFS                      0.489514   0.015392  31.803  < 2e-16 ***\nNS1                    -0.329556   0.137541  -2.396 0.016575 *  \nG_TOWNSIZE             -0.024345   0.003814  -6.384 1.74e-10 ***\nH_URBRURAL2            -0.028291   0.019904  -1.421 0.155220    \nFamImpt                 0.167848   0.019595   8.566  < 2e-16 ***\nFriendsImpt            -0.018183   0.009471  -1.920 0.054868 .  \nLeisureImpt             0.018509   0.008955   2.067 0.038750 *  \nReligionImpt            0.058195   0.008839   6.584 4.62e-11 ***\nFOC                     0.241036   0.003148  76.576  < 2e-16 ***\nTrust2                  0.030117   0.017293   1.742 0.081591 .  \nSex2                    0.055853   0.014955   3.735 0.000188 ***\nAge                     0.004196   0.000652   6.436 1.23e-10 ***\nImmigrant2              0.048708   0.033967   1.434 0.151591    \nCitizen2               -0.005785   0.055315  -0.105 0.916707    \nHHSize                 -0.012284   0.003713  -3.308 0.000940 ***\nParents2                0.005514   0.020563   0.268 0.788601    \nParents3                0.026205   0.032533   0.805 0.420553    \nParents4               -0.232785   0.087756  -2.653 0.007988 ** \nMarried2                0.209746   0.026089   8.040 9.15e-16 ***\nMarried3               -0.128045   0.036375  -3.520 0.000432 ***\nMarried4               -0.107591   0.046250  -2.326 0.020005 *  \nMarried5               -0.093412   0.033257  -2.809 0.004974 ** \nMarried6               -0.101216   0.022679  -4.463 8.10e-06 ***\nKids                    0.019528   0.005297   3.687 0.000227 ***\nEdu                    -0.011649   0.003962  -2.940 0.003285 ** \nJob2                    0.095599   0.025989   3.678 0.000235 ***\nJob3                   -0.028597   0.021202  -1.349 0.177418    \nJob4                    0.069394   0.027600   2.514 0.011932 *  \nJob5                    0.052595   0.023680   2.221 0.026348 *  \nJob6                    0.054279   0.032410   1.675 0.093984 .  \nJob7                   -0.109330   0.026686  -4.097 4.19e-05 ***\nJob8                    0.125121   0.068019   1.840 0.065846 .  \nSocialClass             0.019626   0.008183   2.398 0.016474 *  \nIncome                  0.005628   0.003911   1.439 0.150099    \nReligion1               0.111241   0.023636   4.706 2.53e-06 ***\nReligion2              -0.155857   0.030234  -5.155 2.54e-07 ***\nReligion3              -0.269519   0.029872  -9.023  < 2e-16 ***\nReligion4              -0.290707   0.127955  -2.272 0.023093 *  \nReligion5              -0.314493   0.025148 -12.506  < 2e-16 ***\nReligion6              -0.042414   0.081963  -0.517 0.604825    \nReligion7              -0.152603   0.031879  -4.787 1.70e-06 ***\nReligion8               0.233037   0.042930   5.428 5.71e-08 ***\nReligion9              -0.001271   0.043950  -0.029 0.976926    \nI_WOMJOB                0.034039   0.025140   1.354 0.175740    \nI_WOMPOL                0.081730   0.026240   3.115 0.001842 ** \nI_WOMEDU                0.036452   0.026119   1.396 0.162834    \nabortlib               -0.025280   0.002678  -9.439  < 2e-16 ***\nPerceivedHealth:FS     -0.027077   0.003957  -6.843 7.81e-12 ***\nPerceivedHealth:NS1    -0.027232   0.036778  -0.740 0.459037    \nFS:NS1                  0.085490   0.022307   3.832 0.000127 ***\nPerceivedHealth:FS:NS1 -0.004343   0.005735  -0.757 0.448892    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.737 on 68623 degrees of freedom\n  (19146 observations deleted due to missingness)\nMultiple R-squared:  0.411, Adjusted R-squared:  0.4106 \nF-statistic: 920.9 on 52 and 68623 DF,  p-value: < 2.2e-16\n\n\nFor the model with LS as the DV, the adjusted R2 of 0.41 for life satisfaction is in line with past papers (range: 0.16 to 0.5).\nThere is no three-way interaction. Only 2 of the two-way interactions are significant - PerceivedHealth*FS and FS*NS. I will plot these graphs to interpret them.\n\n\nCode\n# plot interactions.\ninteract_plot(ModelB_LS, pred = PerceivedHealth, modx = FS, modx.values = c(1,10))\n\n\n\n\n\nCode\ninteract_plot(ModelB_LS, pred = FS, modx = NS)\n\n\n\n\n\n\nPerceivedHealth*FS: FS seems to limit the effect of PerceivedHealth on LS, regardless of whether the person is from the Global North/South.\nFS*NS: There seems to be a stronger effect of FS on LS for the Global South (NS = 1).\n\nNow I will generate the diagnostic plots for ModelB_H and ModelB_LS.\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelB_H, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelB_H)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelB_H\nBP = 3520.9, df = 52, p-value < 2.2e-16\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelB_LS, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelB_LS)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelB_LS\nBP = 6066.6, df = 52, p-value < 2.2e-16\n\n\nSimilar to what was observed for RQ A, the scale-location plot indicates heteroskedasticity (confirmed by the Breusch-Pagan test). I will generate robust standard errors for both models.\n\n\nCode\n# obtain robust standard errors for models.\ncoeftest(ModelB_H, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                          Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)             0.96535914  0.05634466  17.1331 < 2.2e-16 ***\nPerceivedHealth         0.24270127  0.01144805  21.2002 < 2.2e-16 ***\nFS                      0.07536150  0.00666454  11.3078 < 2.2e-16 ***\nNS1                    -0.38757224  0.06503656  -5.9593 2.546e-09 ***\nG_TOWNSIZE             -0.01041991  0.00139078  -7.4922 6.856e-14 ***\nH_URBRURAL2             0.04093482  0.00730926   5.6004 2.147e-08 ***\nFamImpt                 0.11392400  0.00748668  15.2169 < 2.2e-16 ***\nFriendsImpt             0.02393210  0.00367876   6.5055 7.798e-11 ***\nLeisureImpt             0.02185385  0.00344548   6.3428 2.271e-10 ***\nReligionImpt            0.03591670  0.00323280  11.1101 < 2.2e-16 ***\nFOC                     0.03221690  0.00130500  24.6873 < 2.2e-16 ***\nTrust2                 -0.02910827  0.00572731  -5.0824 3.737e-07 ***\nSex2                    0.04157302  0.00534434   7.7789 7.418e-15 ***\nAge                    -0.00029973  0.00023855  -1.2565  0.208953    \nImmigrant2              0.00691044  0.01077333   0.6414  0.521240    \nCitizen2                0.03887792  0.01796379   2.1642  0.030449 *  \nHHSize                  0.00455286  0.00154128   2.9539  0.003138 ** \nParents2               -0.00869855  0.00767204  -1.1338  0.256883    \nParents3                0.02166174  0.01212325   1.7868  0.073975 .  \nParents4               -0.02824595  0.03476356  -0.8125  0.416498    \nMarried2                0.03943227  0.00968852   4.0700 4.707e-05 ***\nMarried3               -0.11258117  0.01339883  -8.4023 < 2.2e-16 ***\nMarried4               -0.07953490  0.01853927  -4.2901 1.789e-05 ***\nMarried5               -0.06685929  0.01270208  -5.2636 1.416e-07 ***\nMarried6               -0.05000883  0.00828032  -6.0395 1.554e-09 ***\nKids                    0.01011896  0.00206435   4.9018 9.520e-07 ***\nEdu                    -0.00393011  0.00142772  -2.7527  0.005912 ** \nJob2                    0.02997341  0.00914822   3.2764  0.001052 ** \nJob3                    0.00511872  0.00782594   0.6541  0.513068    \nJob4                    0.03678854  0.00968219   3.7996  0.000145 ***\nJob5                    0.02633692  0.00851043   3.0947  0.001971 ** \nJob6                    0.02553402  0.01142623   2.2347  0.025441 *  \nJob7                   -0.01231387  0.01055162  -1.1670  0.243209    \nJob8                    0.02600579  0.02677949   0.9711  0.331498    \nSocialClass             0.03312301  0.00311454  10.6349 < 2.2e-16 ***\nIncome                  0.00077450  0.00147361   0.5256  0.599183    \nReligion1               0.03565801  0.00846033   4.2147 2.504e-05 ***\nReligion2              -0.01483054  0.01087377  -1.3639  0.172609    \nReligion3              -0.09494892  0.01080759  -8.7854 < 2.2e-16 ***\nReligion4              -0.13520247  0.04550734  -2.9710  0.002969 ** \nReligion5              -0.11165106  0.00919236 -12.1461 < 2.2e-16 ***\nReligion6              -0.08042960  0.02698805  -2.9802  0.002882 ** \nReligion7              -0.04849738  0.01087110  -4.4611 8.166e-06 ***\nReligion8               0.04103402  0.01560114   2.6302  0.008536 ** \nReligion9               0.01278109  0.01514373   0.8440  0.398680    \nI_WOMJOB                0.02092325  0.00951658   2.1986  0.027909 *  \nI_WOMPOL               -0.00639059  0.01005827  -0.6354  0.525198    \nI_WOMEDU               -0.07301433  0.01000503  -7.2978 2.958e-13 ***\nabortlib               -0.00759285  0.00097437  -7.7926 6.657e-15 ***\nPerceivedHealth:FS     -0.00424937  0.00167731  -2.5334  0.011297 *  \nPerceivedHealth:NS1     0.09328088  0.01707000   5.4646 4.655e-08 ***\nFS:NS1                  0.05356371  0.00996608   5.3746 7.700e-08 ***\nPerceivedHealth:FS:NS1 -0.01179646  0.00251070  -4.6985 2.626e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(ModelB_LS, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                          Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)             0.11839390  0.15647523   0.7566 0.4492740    \nPerceivedHealth         0.51761407  0.03262995  15.8632 < 2.2e-16 ***\nFS                      0.48951395  0.01878447  26.0595 < 2.2e-16 ***\nNS1                    -0.32955582  0.19624677  -1.6793 0.0930995 .  \nG_TOWNSIZE             -0.02434490  0.00386763  -6.2945 3.102e-10 ***\nH_URBRURAL2            -0.02829065  0.02086088  -1.3562 0.1750534    \nFamImpt                 0.16784807  0.02052719   8.1769 2.962e-16 ***\nFriendsImpt            -0.01818347  0.01028087  -1.7687 0.0769533 .  \nLeisureImpt             0.01850893  0.00977097   1.8943 0.0581924 .  \nReligionImpt            0.05819543  0.00862565   6.7468 1.523e-11 ***\nFOC                     0.24103567  0.00427540  56.3773 < 2.2e-16 ***\nTrust2                  0.03011720  0.01557963   1.9331 0.0532263 .  \nSex2                    0.05585260  0.01478280   3.7782 0.0001581 ***\nAge                     0.00419594  0.00067759   6.1925 5.957e-10 ***\nImmigrant2              0.04870756  0.02822134   1.7259 0.0843677 .  \nCitizen2               -0.00578498  0.04589833  -0.1260 0.8997014    \nHHSize                 -0.01228380  0.00417933  -2.9392 0.0032919 ** \nParents2                0.00551351  0.02111411   0.2611 0.7939937    \nParents3                0.02620458  0.03506813   0.7472 0.4549165    \nParents4               -0.23278509  0.08357550  -2.7853 0.0053488 ** \nMarried2                0.20974578  0.02636583   7.9552 1.815e-15 ***\nMarried3               -0.12804540  0.03611182  -3.5458 0.0003917 ***\nMarried4               -0.10759093  0.04846076  -2.2202 0.0264107 *  \nMarried5               -0.09341168  0.03561537  -2.6228 0.0087232 ** \nMarried6               -0.10121559  0.02296190  -4.4080 1.045e-05 ***\nKids                    0.01952766  0.00589338   3.3135 0.0009219 ***\nEdu                    -0.01164860  0.00398015  -2.9267 0.0034272 ** \nJob2                    0.09559875  0.02580836   3.7042 0.0002122 ***\nJob3                   -0.02859702  0.02178343  -1.3128 0.1892589    \nJob4                    0.06939360  0.02637211   2.6313 0.0085072 ** \nJob5                    0.05259493  0.02402791   2.1889 0.0286067 *  \nJob6                    0.05427933  0.03125783   1.7365 0.0824793 .  \nJob7                   -0.10932964  0.03054728  -3.5790 0.0003451 ***\nJob8                    0.12512155  0.07189406   1.7404 0.0818003 .  \nSocialClass             0.01962597  0.00901434   2.1772 0.0294694 *  \nIncome                  0.00562853  0.00438793   1.2827 0.1995906    \nReligion1               0.11124113  0.02298752   4.8392 1.306e-06 ***\nReligion2              -0.15585713  0.02994334  -5.2051 1.945e-07 ***\nReligion3              -0.26951940  0.02976146  -9.0560 < 2.2e-16 ***\nReligion4              -0.29070744  0.12026620  -2.4172 0.0156430 *  \nReligion5              -0.31449248  0.02519223 -12.4837 < 2.2e-16 ***\nReligion6              -0.04241378  0.07059987  -0.6008 0.5480000    \nReligion7              -0.15260268  0.02876183  -5.3057 1.126e-07 ***\nReligion8               0.23303695  0.04394884   5.3025 1.146e-07 ***\nReligion9              -0.00127116  0.03863632  -0.0329 0.9737539    \nI_WOMJOB                0.03403916  0.02709696   1.2562 0.2090483    \nI_WOMPOL                0.08173040  0.02906210   2.8123 0.0049208 ** \nI_WOMEDU                0.03645214  0.02900196   1.2569 0.2087994    \nabortlib               -0.02528014  0.00273160  -9.2547 < 2.2e-16 ***\nPerceivedHealth:FS     -0.02707689  0.00475419  -5.6954 1.236e-08 ***\nPerceivedHealth:NS1    -0.02723172  0.05270169  -0.5167 0.6053572    \nFS:NS1                  0.08549005  0.02984951   2.8640 0.0041841 ** \nPerceivedHealth:FS:NS1 -0.00434316  0.00765981  -0.5670 0.5707115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor ModelB_H, zooming in on the three-way interaction between PerceivedHealth*FS*NS: the magnitude of the t-value reduces, but remains significant, p < .001.\nFor ModelB_LS, the results are similar: t-values go up, but both PerceivedHealth*FS and FS*NS remain significant.\nSummarizing the results for both happiness and life satisfaction:\n\nPerceived health and financial satisfaction seem to have a greater impact on happiness in the Global South.\nFinancial satisfaction has a greater impact on life satisfaction in the Global South.\nHowever, perceived health did not have a different effect on life satisfaction in the Global North vs. South.\n\nGiven the third point, we cannot reject the null hypothesis for RQ B. However, I want to caution that I am not sure if I am interpreting the results correctly.\nI am also wondering…is there a function/package in R that can generate the model equations easily for me, especially since I included so many controls?"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#moving-forward",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#moving-forward",
    "title": "Final Project Update",
    "section": "Moving Forward",
    "text": "Moving Forward\nThis is definitely a work-in-progress and will be edited further upon receiving feedback. Other things I want to try before submitting the final product in December:\n\nI had included variables measuring equality of gender/sexual orientation and abortion attitudes in the regression, just out of curiosity. I would like to interpret whether they had significant effects on happiness and life satisfaction.\nThere are some outliers indicated in the diagnostic plots. Removing them could potentially improve my final 4 models (ModelA_H_controls, ModelA_LS_controls, ModelB_H and ModelB_LS)."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#bibliography",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#bibliography",
    "title": "Final Project Update",
    "section": "Bibliography",
    "text": "Bibliography\nAddai, I., Opoku-Agyeman, C., & Amanfu, S. (2013). Exploring Predictors of Subjective Well-Being in Ghana: A Micro-Level Study. Journal Of Happiness Studies, 15(4), 869-890.\nAlba, C. (2019). A Data Analysis of the World Happiness Index and its Relation to the North-South Divide. Undergraduate Economic Review, 16(1).\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen (eds.). 2022. World Values Survey: Round Seven - Country-Pooled Datafile Version 4.0. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat.\nNgamaba, K. (2016). Happiness and life satisfaction in Rwanda. Journal Of Psychology In Africa, 26(5), 407-414.\nWorld Bank Country and Lending Groups. World Bank Data Help Desk. (2022). Retrieved from https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups.\nWVS Database. World Values Survey. (2022). Retrieved from https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp."
  },
  {
    "objectID": "posts/hw1_boonstra.html",
    "href": "posts/hw1_boonstra.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a",
    "href": "posts/hw1_boonstra.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlungcap <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(lungcap$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b",
    "href": "posts/hw1_boonstra.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThese are the boxplots of the distributions for the lung capacity of males and females in the sample:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Gender,y=LungCap)) +\n  geom_boxplot()\n\n\n\n\n\nAccording to these boxplots, it appears that males and females have similar median lung capacities, but that males may be more likely to have a higher lung capacity than females."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c",
    "href": "posts/hw1_boonstra.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\nlungcap %>% \n  group_by(Smoke) %>% \n  summarise(mean_lungcap=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nAccording to this sample, it would appear that smokers have a higher lung capacity than non-smokers. This would appear to be counter-intuitive, as one would likely expect smoking to reduce lung functionality and, by extension, capacity."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d",
    "href": "posts/hw1_boonstra.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nIn order to complete this examination by group, we must create a new nominal variable that groups observations by age; this can be accomplished fairly simply using the mutate() and case_when() functions:\n\n\nCode\nlungcap_age <- lungcap %>% \n  mutate(age_group = case_when(\n    Age <= 13 ~ \"13 and under\",\n    Age == 14 | Age == 15 ~ \"14 to 15\",\n    Age == 16 | Age == 17 ~ \"16 to 17\",\n    Age >= 18 ~ \"18 and older\"\n  ))\n\n\nWith this new dataframe, we can use the group_by() function to calculate mean lung capacity by age group and smoker status:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group,Smoke) %>% \n  summarise(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke `mean(LungCap)`\n  <chr>        <chr>           <dbl>\n1 13 and under no               6.36\n2 13 and under yes              7.20\n3 14 to 15     no               9.14\n4 14 to 15     yes              8.39\n5 16 to 17     no              10.5 \n6 16 to 17     yes              9.38\n7 18 and older no              11.1 \n8 18 and older yes             10.5 \n\n\nAccording to these data, it appears that lung capacity generally increases with age. Interestingly, lung capacity is worse for smokers than it is for non-smokers in every age group except for “13 and under”. This is surprising on the surface, given that, when the data are ungrouped, smokers have a higher lung capacity than non-smokers (see part c). However, this begins to make more sense when we see how much better the “13 and under” group is represented compared to the others in this dataset:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group) %>% \n  count()\n\n\n# A tibble: 4 × 2\n# Groups:   age_group [4]\n  age_group        n\n  <chr>        <int>\n1 13 and under   428\n2 14 to 15       120\n3 16 to 17        97\n4 18 and older    80\n\n\nThis high number of observations compared to other age groups likely plays a significant role in skewing the mean of the entire dataset."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e",
    "href": "posts/hw1_boonstra.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nIt is not clear to me how this part is different from part d; from what I do understand, I believe the question being asked here is addressed in that part."
  },
  {
    "objectID": "posts/hw1_boonstra.html#f",
    "href": "posts/hw1_boonstra.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nIt would appear that lung capacity and age covary together positively, such that a higher age means a higher lung capacity. We can confirm this with a simple visualization:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Age,y=LungCap)) +\n  geom_point() +\n  geom_smooth(method='lm')"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a-1",
    "href": "posts/hw1_boonstra.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 160 / 810 = 0.1975309."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b-1",
    "href": "posts/hw1_boonstra.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThe probability that a randomly selected inmate has less than 2 prior convictions is (128+434) / 810 = 0.6938272."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c-1",
    "href": "posts/hw1_boonstra.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is (128+434+160) / 810 = 0.891358."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d-1",
    "href": "posts/hw1_boonstra.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe probability that a randomly selected inmate has more than 2 prior convictions is (64+24) / 810 = 0.108642."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e-1",
    "href": "posts/hw1_boonstra.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nBefore calculating expected value, we should put together a probability mass function for the prisoners data.\n\n\nCode\nprisoners <- prisoners %>% \n  mutate(prob=freq/810) %>% \n  mutate(expect=prob*priors)\n\nprisoners %>% \n  summarise(sum(expect))\n\n\n  sum(expect)\n1     1.28642\n\n\nThe expected value for the number of prior convictions is about 1.29 priors.\nEDIT: There is a much simpler way to compute this! Rather than using the dataframe I created, storing values and their frequencies, I can create one vector that stores each value a certain number of times, according to the given frequencies:\n\n\nCode\nprisoners_full <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nprisoners_full\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n\nBecause each value now appears as frequently as its “probability” of appearing, taking the mean of this vector also provides the correct expected value.\n\n\nCode\nmean(prisoners_full)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/hw1_boonstra.html#f-1",
    "href": "posts/hw1_boonstra.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCreating this numerical vector also makes the standard deviation calculation extremely simple in R.\n\n\nCode\nsd(prisoners_full)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html",
    "href": "posts/HW2_EthanCampbell.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a",
    "href": "posts/HW2_EthanCampbell.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nHere we can get the t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\n# Calculating the t statistic\nT_statistic = (410-500)/(90/(sqrt(9)))\nT_statistic\n\n\n[1] -3\n\n\nCode\n# calculating the p value\n\npvalue = 2* pt(T_statistic, df=8)\n\npvalue\n\n\n[1] 0.01707168\n\n\nCode\n# the p value is showing evidence that we would reject the null hypothesis here since it is < .05."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b",
    "href": "posts/HW2_EthanCampbell.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nTesting to see p value of it being less than 500\n\n\nCode\npvalue_left <- pt(T_statistic, df = 8, lower.tail = TRUE)\npvalue_left\n\n\n[1] 0.008535841\n\n\nCode\n# this is also showing a value smaller than the 5% given which means it is more evidence to reject the null hypothesis"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c",
    "href": "posts/HW2_EthanCampbell.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nTesting to see the p value greater than 500\n\n\nCode\npvalue_right <- pt(T_statistic, df = 8, lower.tail = FALSE)\n\npvalue_right\n\n\n[1] 0.9914642\n\n\nCode\n# Making sure the two values equal 1\nsum(pvalue_left, pvalue_right)\n\n\n[1] 1\n\n\nthis is showing a 99.14% chance of observing if the population mean was less than that 500 mark. This is interesting and we would fail to reject the null hypothesis here since it exceeds the amount specified. This would indicate that they are not getting paid the same amount."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a-1",
    "href": "posts/HW2_EthanCampbell.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n# Lets run the test and see whats going on\n\nJones <- (519.5-500)/(10)\nSmith= (519.7-500)/(10)\n\n# Here we can see the t stat they are both getting so looking good so far\nJones\n\n\n[1] 1.95\n\n\nCode\nSmith\n\n\n[1] 1.97\n\n\nCode\n# Now to get the P-value\n\nJones_p <- 2*pt(Jones, df= 999, lower.tail = FALSE)\nSmith_p <- 2*pt(Smith, df= 999, lower.tail = FALSE)\n\n# Observing the p values\n\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b-1",
    "href": "posts/HW2_EthanCampbell.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBased on the basic test of this with a CI of 95% we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c.",
    "href": "posts/HW2_EthanCampbell.html#c.",
    "title": "Homework 2",
    "section": "C.",
    "text": "C.\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-1",
    "href": "posts/NiyatiSharma_HW3.html#answer-1",
    "title": "Homework 3",
    "section": "Answer 1",
    "text": "Answer 1\n\n\nCode\n# Load dataset\ndata(UN11) \nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\nCode\n# Select variables of focus\nUN11 <- UN11 %>%\nselect(c(ppgdp, fertility)) \n\n# Preview data\nhead(UN11)\n\n\n              ppgdp fertility\nAfghanistan   499.0     5.968\nAlbania      3677.2     1.525\nAlgeria      4473.0     2.142\nAngola       4321.9     5.135\nAnguilla    13750.1     2.000\nArgentina    9162.1     2.172\n\n\n#a. Predictor variable = ppgdp (gross national product per person, in US dollars) Response variable = fertility (birth rate per 1000 females).\n#b. ::: {.cell}\n\nCode\nmm1<-lm((UN11$fertility)~(UN11$ppgdp))\n\n:::\n\n\nCode\nplot((UN11$fertility) ~ (UN11$ppgdp), data=UN11)\nabline(mm1,col=\"blue\")\n\n\n\n\n\nThe graph shows there is a negative relation betweeb fertility and ppgdp. It is not exactly linear because increasing ppgdp only decreases fertility about the 10,000 point. #c. ::: {.cell}\n:::\n\n\nCode\nmm2<-lm(log(UN11$fertility)~log(UN11$ppgdp))\nsummary(mm1)\n\n\n\nCall:\nlm(formula = (UN11$fertility) ~ (UN11$ppgdp))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nUN11$ppgdp  -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\n\n\n\n\n\nCode\nplot(log(UN11$fertility) ~ log(UN11$ppgdp), data=UN11)\nabline(mm2,col=\"blue\")\n\n\n\n\n\nThe log scatterplot shows a relationship that also looks negative slop but a bit closer to the linear regression."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-2",
    "href": "posts/NiyatiSharma_HW3.html#answer-2",
    "title": "Homework 3",
    "section": "Answer 2",
    "text": "Answer 2\n#a Change the currency from American dollars to British pounds i.e 1.66 * American dollar shifts the mean of the x-axis (explanatory variable) to increase while the mean of the y-axis (response variable) remains the same shows that the slope of the prediction equation would change.\n#b The correlation doesn’t change,as the relative values of the variables remain unchanged."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-3",
    "href": "posts/NiyatiSharma_HW3.html#answer-3",
    "title": "Homework 3",
    "section": "Answer 3",
    "text": "Answer 3\n\n\nCode\n# load dataset \ndata(water)\n#create scatterplot matrix\npairs(water)\n\n\n\n\n\nCode\n#calculate the summary\nsummary(water)\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\n\nLooking at scatterplots , we can say that APMAN,APSLAKE,APSAB lakes shows positive linear relationship with precipitation while the OPBPC,OPRC,OPSLAKE lakes seems to have one as well with each other. Also, it seems that the stream run-off variable BSAAM has a postive relationship to the OPBPC,OPRC,OPSLAKE lakes but no real notable relationship to the APMAN,APSLAKE,APSAB lakes."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-4",
    "href": "posts/NiyatiSharma_HW3.html#answer-4",
    "title": "Homework 3",
    "section": "Answer 4",
    "text": "Answer 4\n\n\nCode\n# load dataset and review\ndata(Rateprof)\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\n# select the columns\nRP <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n# generate scatterplots.\npairs(RP)\n\n\n\n\n\nReferring to the scatterplot matrix of the average professor ratings for Quality, helpfulness and clarity have the positive have strong positive correlations with each other. Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-5",
    "href": "posts/NiyatiSharma_HW3.html#answer-5",
    "title": "Homework 3",
    "section": "Answer 5",
    "text": "Answer 5\n\n\nCode\n# load and preview data\ndata(student.survey)\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA"
  },
  {
    "objectID": "posts/shelton_HW1.html",
    "href": "posts/shelton_HW1.html",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "1.) Using LungCapData, answer descriptive questions about the data and its distributions.\n2.) Use the given distribution to answer questions about the probability of discrete events.\n\n\n\n\n\n\nCode\n#| include: false\n#| label: Loading in LungCap\n\n og_lungcap <- readxl::read_xls(\"_data/LungCapData.xls\")\n\n# Quick look at dataset\n# glimpse(og_lungcap)\n\n# Variables - 3<dbl> ratio 3<char> (can coerce to logical if needed), \n\n# length(which(is.na(og_lungcap)))\n\n# No missing values to consider\n\n# Descriptive\n# summarytools::dfSummary(og_lungcap)\n\n\nLungCapData: Describes the lung capacity of a population of 725 children aged 3 - 19. It further categorizes the subjects by height, sex, smoking habits, and whether they were birthed using the Caesarean section technique.\nIn the following sections, we’ll use select(), group_by(), filter(), and summarize() to further explore the data and find important relations between variables.\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nLungCap looks to be approximately normally distributed (unimodal, symmetric) with most observations centered around the mean (7.86).\n\n\n\n\n\nCode\nhist_gender <- ggplot(og_lungcap, aes(x=LungCap, y=..density.., fill=Gender)) +\n  geom_histogram(alpha=.5, position=\"identity\", bins=20)+\n  geom_vline(aes(xintercept=mean(LungCap)))\nhist_gender\n\n\n\n\n\nPackage ggplot2 functions ggplot() and geom_histogram() are used to display the LungCap distribution filled by the Gender variable. Both density plots center on the mean, indicating both male and female lung capacity observations are highly concentrated around the mean. The male distribution is shifted slightly to the right of the female distribution, meaning male observations had a higher upper range value than female observations. Males had more observations concentrated to the right of the mean, and the female distribution reciprocated this effect to the left of the mean.\n\n\n\n\n\nCode\nsmokers <- group_by(og_lungcap, Smoke)\nsmokers %>%\n  summarize(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nAfter creating a new dataset smokers by using group_by() on our original data, smokers is piped into a summarize() call. The results surprisingly show that the smoking group had a higher mean lung capacity than the nonsmoking group. This is likely due to a mean age difference within the groups.\n\n\n\n\n\nCode\n# Creating Age Groups Using Case When\n\nsmokers_age <- smokers %>%\n  mutate(AgeGroup = case_when(Age >= 18 ~ \"18+\", \n            Age == 16 | Age == 17 ~ \"16-17\",\n            Age == 14 | Age == 15 ~ \"14-15\",\n            Age <= 13~ \"Under 13\"))\n\n# Mean LungCap by Age and Smoke\n# Must regroup by Smoke again\nsmokers_age %>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(LungCap)`\n  <chr>    <chr>           <dbl>\n1 14-15    no               9.14\n2 14-15    yes              8.39\n3 16-17    no              10.5 \n4 16-17    yes              9.38\n5 18+      no              11.1 \n6 18+      yes             10.5 \n7 Under 13 no               6.36\n8 Under 13 yes              7.20\n\n\nAfter using mutate() to add a column AgeGroup to a copy of smokers, group_by() groups the new dataset by AgeGroup and Smoke before piping it into a summarize() command to find the grouped means of LungCap by AgeGroup and Smoke.\nThe results show that for children above the age of 13, smokers had a lower mean lung capacity than non-smokers. However, for the 13 and under group, we again see results that imply smokers have greater lung capacity than nonsmokers. Let’s investigate further into the relationship between age and lung capacity to explain this quizzical result.\n\n\n\n\n\nCode\ncov(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 8.738289\n\n\nCode\ncor(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 0.8196749\n\n\nCode\n#GGPlot of Age vs Lung\nggplot(og_lungcap, aes(x=Age, y=LungCap)) + geom_point()\n\n\n\n\n\nAge and LungCap have a high covariance which leads to a high correlation (p=0.82). This strong positive value (-1<p<1) indicates these variables “vary greatly” together: when Age is high in the data, so is LungCap. We cannot say that an increase in Age causes an increase Lung capacity without first showing this through regression; however, our results show the variables are highly correlated.\nWe can use knowledge of the human body to infer that as our body ages, our lungs mature. The ages of smokers of the Under 13 group are likely highly left skewed, as I don’t expect many children under 10 to be smoking. This underlying age distribution explains our puzzling results from the previous section.\n\n\nCode\nsmokers_age%>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(Age))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(Age)`\n  <chr>    <chr>       <dbl>\n1 14-15    no          14.5 \n2 14-15    yes         14.6 \n3 16-17    no          16.4 \n4 16-17    yes         16.6 \n5 18+      no          18.5 \n6 18+      yes         18.1 \n7 Under 13 no           9.49\n8 Under 13 yes         11.7 \n\n\n\n\n\n\nFirst, let’s create two vectors: x_val and freq. Then, we’ll use rbind() to create a table.\n\n\nCode\nx_val <-c(0,1,2,3,4)\nfreq <- c(128,434,160,64,24)\nprob <- freq/sum(freq)\n\nxdist <- rbind(x_val,prob)\n\nxdist\n\n\n           [,1]      [,2]      [,3]       [,4]       [,5]\nx_val 0.0000000 1.0000000 2.0000000 3.00000000 4.00000000\nprob  0.1580247 0.5358025 0.1975309 0.07901235 0.02962963\n\n\n\n\n\n\nCode\n# Finding probability of inmate having exactly 2 prior convictions\n\n#Column Index is 3 as the first column is 0\n\n#Surely there is a cleaner way to do this using tidyverse functions rather than base?\n\n# a\na <- xdist['prob',3] \na\n\n\n     prob \n0.1975309 \n\n\n\n\n\n\n\nCode\n#b\nb <- sum(xdist['prob',1:2])\nb\n\n\n[1] 0.6938272\n\n\n\n\n\n\n\nCode\n# c\nc <- a + b\nc\n\n\n    prob \n0.891358 \n\n\n\n\n\n\n\nCode\n#d\nd <- 1 - c\nd\n\n\n    prob \n0.108642 \n\n\n\n\n\n\n\n[1] 1.28642\n\n\n\n\n\n\n\n\n\nCode\n# Var= E(X^2) - E(X)^2\n# Again using brute force because cannot use var() function on the object xdist correctly\nvar_x <-sum((x_val^2)*prob) - ex^2\nvar_x\n\n\n[1] 0.8562353\n\n\n\n\n\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/Emily Duryea Homework 1.html",
    "href": "posts/Emily Duryea Homework 1.html",
    "title": "Duryea Homework 1",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")  \n\nhist(df$LungCap)\n\n\n\n\n\nPart A: Plotting probability density histogram\n\n\nCode\nhist(df$LungCap, \n     col=\"yellow\",\n     border=\"black\",\n     prob = TRUE,\n     xlab = \"LungCap\",\n     main = \"Density Plot\")\n\nlines(density(df$LungCap),\n      lwd = 2,\n      col = \"chocolate3\")\n\n\n\n\n\nPart B: Compare the probability distribution of the LungCap with respect to Males and Females\n\n\nCode\nggplot(df, aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"LungCap Probability Distribution for Males and Females\", y = \"Probability density\")\n\n\n\n\n\nPart C: Compare the mean lung capacities for smokers and non-smokers\n\n\nCode\nmean_smoking <- df %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nmean_smoking\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThe means of smokers vs non smokers does not make sense since non smokers have a lower mean lung cap, when one would think it would be the other way around. However, limited data is provided on the sample, so there could be other factors in play.\nPart D: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”\n\n\nCode\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\ndf %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGroup)) +\n  theme_classic() + \n  labs(title = \"LungCap and Smoke based on age groups\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nBased on the histograms, Part D seems to contrast with Part C, since the plots seem to demonstrate non-smokers having higher lung capacity than smokers in all age groups. Additionally, lung capacity appears to decrease with age based on the graph.\nPart E: Compare the lung capacities for smokers and non-smokers within each age group\n\n\nCode\ndf %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(title = \"LungCap and Smoke based on age and smoker vs nonsmoker\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nBased on information gained in Part D and Part E, it appears that lung capacity decreases with age, and, despite the means in Part C, lung capacity is higher for non-smokers.\nPart F: Calculate the correlation and covariance between Lung Capacity and Age\n\n\nCode\nCov_lungcapage <- cov(df$LungCap, df$Age)\nCor_lungcapeage <- cor(df$LungCap, df$Age)\nCov_lungcapage\n\n\n[1] 8.738289\n\n\nCode\nCor_lungcapeage\n\n\n[1] 0.8196749\n\n\nBecause both the covariance and correlation are positive numbers, the relationship between lung capacity and age are positively related, meaning as one increases, the other also increases in a proportional manner.\nQuestion 2\n\n\nCode\nPrior_Convictions <- c(0:4)\nInmate_Number <- c(128, 434, 160, 64, 24)\nip <- tibble(Prior_Convictions, Inmate_Number)\n\nip <- mutate(ip, Probability = Inmate_Number/sum(Inmate_Number))\nip\n\n\n# A tibble: 5 × 3\n  Prior_Convictions Inmate_Number Probability\n              <int>         <dbl>       <dbl>\n1                 0           128      0.158 \n2                 1           434      0.536 \n3                 2           160      0.198 \n4                 3            64      0.0790\n5                 4            24      0.0296\n\n\nPart A: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nip %>%\n  filter(Prior_Convictions == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability that a randomly selected inmate has exactly two prior convictions is 0.1975309.\nPart B: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\npartb <- ip %>%\n  filter(Prior_Convictions < 2)\nsum(partb$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than two prior convictions is 0.6938272.\nPart C: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\npartc <- ip %>%\n  filter(Prior_Convictions <= 2)\nsum(partc$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has two or fewer prior convictions is 0.891358.\nPart D: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\npartd <- ip %>%\n  filter(Prior_Convictions > 2)\nsum(partd$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than two prior convictions is 0.108642.\nPart E: What is the expected value for the number of prior convictions?\n\n\nCode\nip <- mutate(ip, vl = Prior_Convictions*Probability)\nparte <- sum(ip$vl)\nparte\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.28642.\nPart F: Calculate the variance and the standard deviation for the Prior Convictions\n\n\nCode\nip_var <-sum(((ip$Prior_Convictions-parte)^2)*ip$Probability)\nip_var\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(ip_var)\n\n\n[1] 0.9253298\n\n\nThe variance for prior convictions is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html",
    "href": "posts/HW2_KarenKimble.html",
    "title": "Kimble HW 2",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(stats)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-1",
    "href": "posts/HW2_KarenKimble.html#question-1",
    "title": "Kimble HW 2",
    "section": "Question 1",
    "text": "Question 1\n\nAngiography\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 9 / sqrt(847)\n\nt_score <- qt(p = 1-tail_area, df = 846)\n\nCI_1 <- c(18 - t_score * standard_error, 18 + t_score * standard_error)\n\nprint(CI_1)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nBypass Surgery\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 10 / sqrt(539)\n\nt_score <- qt(p = 1-tail_area, df = 538)\n\nCI_2 <- c(19 - t_score * standard_error, 19 + t_score * standard_error)\n\nprint(CI_2)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval for angiography (range of about 1.1) is smaller than the confidence interval for bypass surgery (range of about 1.5) at a confidence level of 90%."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-2",
    "href": "posts/HW2_KarenKimble.html#question-2",
    "title": "Kimble HW 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\n# Point estimate\n\np <- 567/1031\n\np\n\n\n[1] 0.5499515\n\n\nThe point estimate p for the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n# Confidence Interval\n\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_deviation <- sqrt((p * (1-p))/1031)\n\nstandard_error <- standard_deviation / sqrt(1031)\n\nt_score <- qt(p = 1-tail_area, df = 1030)\n\nCI_3 <- c(p - t_score * standard_error, p + t_score * standard_error)\n\nprint(CI_3)\n\n\n[1] 0.5490046 0.5508984\n\n\nThrough this test, I am 95% confident that the true proportion of all adult Americans who believe a college education is essential for success lies between 0.549 and 0.551."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-3",
    "href": "posts/HW2_KarenKimble.html#question-3",
    "title": "Kimble HW 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nsd <- (200-30)/4\n\nx <- 5/(2.26 * sd)\n\nsample_size = (1/x)^2\n\nsample_size\n\n\n[1] 369.0241\n\n\nThe size of the sample should be at least 370 people. Since they want a confidence interval within 5 dollars and they assume the standard deviation is a quarter of the range of 30 dollars to 200 dollars, I was able to use the confidence interval equation to find the missing variable of sample size. The confidence level is 95%, meaning that with a large sample size, the t-score would be around 2.26, allowing me to use the equation and isolate the sample size."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-4",
    "href": "posts/HW2_KarenKimble.html#question-4",
    "title": "Kimble HW 2",
    "section": "Question 4",
    "text": "Question 4\n\nPart A\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\n\n\nCode\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 90 / sqrt(9)\n\nt_score <- qt(p = 1-tail_area, df = 8)\n\nCI_4A <- c(410 - t_score * standard_error, 410 + t_score * standard_error)\n\np_value = 2 * pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05\n\n\nCode\nprint(CI_4A)\n\n\n[1] 340.8199 479.1801\n\n\nThe p-value is exactly 0.05, meaning it is not smaller than the alpha value of 0.05 and thus is not statistically significant. We do not have enough evidence to reject the null hypothesis. The confidence interval shows that we are 95% confident the true mean income of female employees lies between 340.82 dollars/week and 479.18 dollars/week.\n\n\nPart B\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = TRUE)\n\np_value\n\n\n[1] 0.975\n\n\nThe p-value for the alternate hypothesis that the true mean income of female employees is greater than 500 dollars/week is 0.975. This value is extremely large and greater than the 0.05 alpha level, meaning there is not statistically significant evidence and thus do not reject the null hypothesis.\n\n\nPart C\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.025\n\n\nThe p-value for the alternative hypothesis that the true mean income of female employees is less than 500 dollars/week is 0.025. This value is less than the alpha value of 0.05, meaning it is statistically significant and we can reject the null hypothesis. The true mean income of female employees is likely less than 500 dollars/week."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-5",
    "href": "posts/HW2_KarenKimble.html#question-5",
    "title": "Kimble HW 2",
    "section": "Question 5",
    "text": "Question 5\n\nPart A\n\n\nCode\n# Jones\n\nt_score <- (519.5-500)/(10)\n\nt_score\n\n\n[1] 1.95\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Smith\n\nt_score <- (519.7-500)/(10)\n\nt_score\n\n\n[1] 1.97\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.04911426\n\n\n\n\nPart B\nFor Jones’s study, the results were not statistically significant because the p-value of 0.51 is greater than the alpha value of 0.05. For Smith’s study, the results were statistically signficiant because the p-value of 0.49 is less than the alpha value of 0.05.\n\n\nPart C\nReporting the result of a test as P being greater or less than the alpha value can be misleading if the p value is not reported. In both studies, the p-value was .01 away from 0.05, yet in only one study was the result statistically significant. A small p-value may still be meaningful to report because it still shows that there was a relatively small probability of getting the result that one did. Not reporting the p-value when reporting the result and whether or not a hypothesis is rejected leaves an important part of the study out. Someone simply reading that a hypothesis was not rejected without knowing the p-value may assume the p-value was very large even when it was small (such as in the case of Smith vs. Jones), thus leaving out a major aspect of the study."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-6",
    "href": "posts/HW2_KarenKimble.html#question-6",
    "title": "Kimble HW 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, alternative = c(\"less\"), mu = 45)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value of this test is 0.038, which is less than the alpha value of 0.05. This means that there is statistically significant evidence and we can reject the null hypothesis, that the true average gas tax per gallon in the United States is 45 cents. There is significant evidence to suggest that the true average gas tax per gallon in the United States is less than 45 cents."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html",
    "href": "posts/DACSS 603 Final Part 1.html",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv' does not exist.\n\n\nCode\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\n\nError in number(x = x, accuracy = accuracy, scale = scale, prefix = prefix, : object 'NYC_2019' not found\n\n\nCode\nNYC_2021 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv' does not exist.\n\n\nCode\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n\nError in rbind(NYC_2019, NYC_2021): object 'NYC_2019' not found\n\n\nCode\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n\nError in recode(school_data$`% Poverty`, `Above 95%` = \"95%\", `Below 5%` = \"5%\"): object 'school_data' not found\n\n\nCode\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\n\nError in lapply(X = X, FUN = FUN, ...): object 'school_data' not found\n\n\nCode\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found\n\n\nCode\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found"
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#research-question",
    "href": "posts/DACSS 603 Final Part 1.html#research-question",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "href": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "href": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nError in glimpse(school_data): object 'school_data' not found\n\n\n\n\nCode\nsummary(school_data)\n\n\nError in summary(school_data): object 'school_data' not found\n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#references",
    "href": "posts/DACSS 603 Final Part 1.html#references",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/MeghaJoseph_HW2.html",
    "href": "posts/MeghaJoseph_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "##QUESTION1\n\n\nCode\nprocedure <- c('Bypass', 'Angiography')\nsamplesize <- c(539, 847)\nmeanwait <- c(19, 18)\nstandev <- c(10, 9)\n\nsurgdata <- data.frame(procedure, samplesize, meanwait, standev)\n\nsurgdata\n\n\n    procedure samplesize meanwait standev\n1      Bypass        539       19      10\n2 Angiography        847       18       9\n\n\n##QUESTION2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n##QUESTION3\n\n\nCode\nstdevBooks <- (200-30)/4\nmargerrorBooks <- (10/2)\nzBooks <- 1.96\n\nstdevBooks^2 * (zBooks/margerrorBooks)^2\n\n\n[1] 277.5556\n\n\n##QUESTION4 ##A\n\n\nCode\n(410-500)/(90/sqrt(9))\n\n\n[1] -3\n\n\nCode\npt(-3, 8)*2 \n\n\n[1] 0.01707168\n\n\n##B\n\n\nCode\n pt(-3, 8, lower.tail = TRUE)\n\n\n[1] 0.008535841\n\n\n##C\n\n\nCode\n pt(-3, 8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\n##QUESTION5 ##A1\n\n\nCode\n JonesT <- (519.5-500)/10\nJonesT\n\n\n[1] 1.95\n\n\n##A2\n\n\nCode\n JonesP <- pt(1.95, 999, lower.tail = FALSE)*2\nJonesP\n\n\n[1] 0.05145555\n\n\n##A3\n\n\nCode\nSmithT <- (519.7-500)/10\nSmithT\n\n\n[1] 1.97\n\n\n##A4\n\n\nCode\nSmithP <- pt(1.97, 999, lower.tail = FALSE)*2\nSmithP\n\n\n[1] 0.04911426\n\n\n##B\nWith an α-level of .05, the p-values that both Jones (P=.051) and Smith (P=.049) found are very close to equivalent. Although Jones’ P-value is slightly greater than α=.05 and Smith’s P-value is slightly less than α=.05, the proximity of the results should yield the same conclusion. Both P-values provide moderate evidence to reject the null hypothesis and indicate that the mean is not equal to 500. If we were to technically interpret the P-values, then Jones’ test would fail to reject the null hypothesis, and Smith’s test would reject the null hypothesis.\n##C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. For example, a P-value of .009 for a significance level of .05 provides much stronger evidence to reject the null than a P-value of .045, however both values allow for rejection of the null at the significance level .05. In the Jones/Smith example, reporting the results only as “P ≤ 0.05” versus “P > 0.05” will lead to different conclusions about very similar results (rejecting versus failing to reject the null).\n##QUESTION6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html",
    "href": "posts/KalimahMuhammad_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html#questions",
    "href": "posts/KalimahMuhammad_hw2.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\n1.Cardiac Care Network - Wait Times for Cardiac Surgeries\nPrompt: The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\n\nBypass Surgery Confidence Interval\n\n\nCode\n#calculate confidence interval for bypass surgery\nmean<- 19 #mean wait time\nsd<-10 #standard deviation\nn <-539 #sample size\nbypass_se <- (sd/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score<- qt(p=1-tail_area, df=n-1) #determine t-score\nbypass_CI <- c(mean - t_score* bypass_se,\n               mean + t_score* bypass_se) #calculate confidence interval\nprint(bypass_CI)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval (CI) for the average wait time for bypass surgery is between 18.29 and 19.71 days.\n\n\nAngiography Confidence Interval\n\n\nCode\n#Calculate cofidence interval for angiography\n#mean= 18, sd=9, n=847\nmean_ag<- 18 #mean wait time\nsd_ag<-9 #standard deviation\nn_ag <-847 #sample size\nag_se <- (sd_ag/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score_ag<- qt(p=1-tail_area, df=n-1) #determine t-score\nag_CI <- c(mean_ag - t_score_ag* ag_se,\n               mean_ag + t_score_ag* ag_se) #calculate confidence interval\nprint(ag_CI)\n\n\n[1] 17.36126 18.63874\n\n\nMeanwhile, the CI for the angiography mean wait time is between 17.36 and 18.63 days.\n\n\nIs the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n19.70971-18.29029 #difference in bypass surgery CI range\n\n\n[1] 1.41942\n\n\nCode\n18.63874-17.36126 #difference in angiography CI range\n\n\n[1] 1.27748\n\n\nThe range in the confidence interval for angiography is 1.28 narrower than the bypass surgery, 1.42.\n\n\n\n\n2. National Center for Public Policy - Is college essential for success?\nPrompt: A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#proportion of US adults who believe college is essential for success\nprop.test(567,1031,conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate of the proportion of all adult Americans who believe that a college education is essential for success is 0.55. The confidence interval set at 95% ranges between 0.52 and 0.58.\n\n\n\n3. Student Sample Size\nPrompt: Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within 5 dollars of the true population mean (i.e. they want the confidence interval to have a length of 10 dollars or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between 30 dollars and 200 dollars. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculate the sample size\npop_sd<-(200-30)/4\ncritical_value<-1.96 #based off signigicance level of 5\nsample_size<- ((pop_sd*critical_value)/5)^2 \nprint(sample_size)\n\n\n[1] 277.5556\n\n\nThe sample size should be 278 students to estimate the mean cost of textbooks per semester.\n\n\n\n4. Income for Union Workers\nPrompt: According to a union agreement, the mean income for all senior-level workers in a large service company equals 500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nsam_mean<-410\nmu<-500\nsam_sd<-90\nn<-9\n\nt_score<- (sam_mean-mu)/(sam_sd/(sqrt(n)))\nprint(t_score)\n\n\n[1] -3\n\n\nCode\nupper_tail<- pt(t_score, df=n-1, lower.tail = FALSE)\nprint(upper_tail)\n\n\n[1] 0.9914642\n\n\nCode\nlower_tail<- pt(t_score, df=n-1, lower.tail = TRUE)\nprint(lower_tail)\n\n\n[1] 0.008535841\n\n\nCode\np_value<- upper_tail + lower_tail\nprint(p_value)\n\n\n[1] 1\n\n\n\n\n\n5. Jones and Smith\nPrompt: Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 with se = 10.0.\n\n\nCode\nmu<- 500 #hypothesized population mean\nj_mean<-519.5 #Jones's mean\ns_mean<-519.7 #Smith's mean\nn=1000 #sample size\nse<-10 #standard error\n\n\nShow that t = 1.95 and P-value = 0.051 for Jones.\n\n\nCode\n#calculate the t-score for Jones\nj_tscore<-(j_mean - mu)/se\nprint(j_tscore)\n\n\n[1] 1.95\n\n\nCode\n#calculate p-value for Jones\nj_pvalue<- pt(j_tscore, df=n-1, lower.tail = FALSE) *2\nprint(j_pvalue)\n\n\n[1] 0.05145555\n\n\nShow that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#calculate the t-score for Smith\ns_tscore<-(s_mean - mu)/se\nprint(s_tscore)\n\n\n[1] 1.97\n\n\nCode\n#calculate p-value for Smith\ns_pvalue<- pt(s_tscore, df=n-1, lower.tail = FALSE) *2\nprint(s_pvalue)\n\n\n[1] 0.04911426\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIn Smith’s test, the p-value of .049, less than 0.05, coupled with the significance level of 0.05 indicate a statistically significant result to reject the null hypothesis. However, the results from the Jones’s test with a p-value of 0.052, greater than 0.05, indicates the results were not statistically significant and the null was retained.\nTheses results can be misleading as the significance level impacts how the p-values are referenced when under 0.05. P-values over 0.05 will typically retain the null, however p-values under 0.05 are influenced by the significance level to determine whether results are statistically significant to reject the null hypothesis.\n\n\n\n6. US Gas Tax\nPrompt:Are the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#t-test of average gas taxes from sample cities\nt.test(gas_taxes, alternative = c(\"less\"), mu=45, conf.level = 0.95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes, using the t-test to compare the sample mean (40.86) to the hypothesized population mean (45) at the confidence level of 95% resulted in a favorable conclusion that the sample average was less than 45 cents."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html",
    "href": "posts/HW2_RoyYoon.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Bypass Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Bypass Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nbypass_sample_size <- 539\nbypass_mean <- 19\nbypass_sd <- 10"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Standard Error for Bypass",
    "text": "Standard Error for Bypass\n\n\nCode\nbypass_standard_error <- bypass_sd / sqrt(bypass_sample_size)\n\nbypass_standard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "title": "Homework 2",
    "section": "T-Value Steps for Bypass",
    "text": "T-Value Steps for Bypass\n\n\nCode\nbypass_confidence_level <- 0.90\nbypass_tail_area <- (1 - bypass_confidence_level)/2\n\nbypass_tail_area\n\n\n[1] 0.05\n\n\nCode\nbypass_t_score <- qt(p = 1 - bypass_tail_area, df = bypass_sample_size - 1)\n\nbypass_t_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Bypass Confidence Interval",
    "text": "Calculate Bypass Confidence Interval\n\n\nCode\nbypass_confidence_interval <- c(bypass_mean - (bypass_t_score * bypass_standard_error),\n                                bypass_mean + (bypass_t_score * bypass_standard_error))\n\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Margin of Error for bypass",
    "text": "Margin of Error for bypass\n\n\nCode\nbypass_margin_of_error <- bypass_t_score * bypass_standard_error\n\nbypass_margin_of_error\n\n\n[1] 0.7097107"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Angiography Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Angiography Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nangiography_sample_size <- 847\nangiography_mean <- 18\nangiography_sd <- 9"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "title": "Homework 2",
    "section": "Standard Error for Angiography",
    "text": "Standard Error for Angiography\n\n\nCode\nangiography_standard_error <- angiography_sd / sqrt(angiography_sample_size)\n\nangiography_standard_error\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "title": "Homework 2",
    "section": "T-Value Steps for Angiography",
    "text": "T-Value Steps for Angiography\n\n\nCode\nangiography_confidence_level <- 0.90\nangiography_tail_area <- (1 - angiography_confidence_level)/2\n\nangiography_tail_area\n\n\n[1] 0.05\n\n\nCode\nangiography_t_score <- qt(p = 1 - angiography_tail_area, df = angiography_sample_size - 1)\n\nangiography_t_score\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Angiography Confidence Interval",
    "text": "Calculate Angiography Confidence Interval\n\n\nCode\nangiography_confidence_interval <- c(angiography_mean - (angiography_t_score * angiography_standard_error),\n                                angiography_mean + (angiography_t_score * angiography_standard_error))\n\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Margin of Error for Angiography",
    "text": "Margin of Error for Angiography\n\n\nCode\nangiography_margin_of_error <- angiography_t_score * angiography_standard_error\n\nangiography_margin_of_error\n\n\n[1] 0.5092182"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "href": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "title": "Homework 2",
    "section": "Comparing Bypass and Angiogrpahy Confidence Intervals",
    "text": "Comparing Bypass and Angiogrpahy Confidence Intervals\n\n\nCode\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922\n\n\nCode\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nAngiography has a more narrow confidence interval"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#point-estimate",
    "href": "posts/HW2_RoyYoon.html#point-estimate",
    "title": "Homework 2",
    "section": "Point Estimate",
    "text": "Point Estimate\n\n\nCode\nsample_size <- 1031\n\neducation_essential <- 567\n\npoint_estimate <- education_essential / sample_size\n\npoint_estimate \n\n\n[1] 0.5499515\n\n\nCode\nprop.test(education_essential, sample_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  education_essential out of sample_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate for adult Americans who believe college education is essential is 0.5499515. The 95 percent confidence interval is 0.5189682, 0.5805580. 95% of confidence intervals calculated with this procedure would contain the true mean. With the sampling method repeated, about 95% of the intervals would contain the true mean."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a.",
    "href": "posts/HW2_RoyYoon.html#a.",
    "title": "Homework 2",
    "section": "A.",
    "text": "A.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\n\nData is normally distributed; significance level is 5%\nNull Hypothesis (H0): μ = 500\nAlternative Hypothesis(H1): μ < 500, μ > 500"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-standard-error",
    "href": "posts/HW2_RoyYoon.html#a-standard-error",
    "title": "Homework 2",
    "section": "A: Standard Error",
    "text": "A: Standard Error\n\n\nCode\nfemale_standard_dev <- 90\nfemale_sample_size <- 9\nfemale_sample_mean <- 410 \nnull_hypothesis_mean <- 500\n\nfemale_standard_error <- female_standard_dev / sqrt(female_sample_size)\n\nfemale_standard_error\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-t-test",
    "href": "posts/HW2_RoyYoon.html#a-t-test",
    "title": "Homework 2",
    "section": "A: t-test",
    "text": "A: t-test\n\n\nCode\nt_stat <- (female_sample_mean - null_hypothesis_mean) / female_standard_error\n\nt_stat\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-p-value",
    "href": "posts/HW2_RoyYoon.html#a-p-value",
    "title": "Homework 2",
    "section": "A: p-value",
    "text": "A: p-value\n\n\nCode\np_value <- (pt(t_stat, df = 8)) * 2\n\np_value\n\n\n[1] 0.01707168\n\n\np-value (0.01707168) is smaller than the 5% significance level, so we are able to reject the null hypothesis and favor the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "href": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "title": "Homework 2",
    "section": "B. Report the P-value for Ha : μ < 500. Interpret.",
    "text": "B. Report the P-value for Ha : μ < 500. Interpret.\n\n\nCode\nlow_p_value <- (pt(t_stat, df = 8, lower.tail = TRUE))\nlow_p_value\n\n\n[1] 0.008535841"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "href": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "title": "Homework 2",
    "section": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)",
    "text": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nup_p_value <- (pt(t_stat, df = 8, lower.tail = FALSE))\nup_p_value\n\n\n[1] 0.9914642\n\n\nCode\n#sanity check from hint: The P-values for the two possible one-sided tests must sum to 1\nlow_p_value + up_p_value\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a",
    "href": "posts/HW2_RoyYoon.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#Jones\n\njones_t_stat <- (jones_sample_mean - h0_mean) / jones_standard_error\n\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- (pt(jones_t_stat, df = 999, lower.tail=FALSE)) * 2\n\njones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n\nsmith_t_stat <- (smith_sample_mean - h0_mean) / smith_standard_error\n\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- (pt(smith_t_stat, df = 999, lower.tail=FALSE) ) * 2\n\nsmith_p_value\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b",
    "href": "posts/HW2_RoyYoon.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nif the p value is less than the significance level, the null hypothesis is rejected.\nsignificance level: α = 0.05\nJones p-value: 0.05145555; Jones p-value > significance level, so the study is not statistically significant\nsmith p-value: 0.04911426; Smith p-value < significance level, so the study is statistically significant"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c",
    "href": "posts/HW2_RoyYoon.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIf the p value is less than the significance level, the null hypothesis is rejected and we can consider the finding/result statistically insignificant. So blanketing Jone’s study as not statistically significant and and Smith’s study as significant may make sense. However, when you look at the sample means from Jones and Smith, there is not an outstanding difference between them. By the way the mathematics of calculating the p-value worked out, it can seem as though there is a great difference between the numbers reported by Jones and Smith, when their reported numbers are actually very narrow to each other."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#question-6",
    "href": "posts/HW2_RoyYoon.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nWith a sample mean of 40.86278 and a p-value of 0.03827 which is less than the assumes 0.05 significance level, the null hypothesis is rejected and the alternative hypothesis is favored.\nThus, there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html",
    "href": "posts/FinalPart2_CalebHill.html",
    "title": "Final Part 2",
    "section": "",
    "text": "Multiple research reports state that there is a relationship between re-hospitalization rates and social characteristics, such as demographic and economic identifiers, (Barnett, Hsu & McWilliams, 2015; Murray, Allen, Clark, Daly & Jacobs, 2021). Specifically, racial characteristics play a large role in predicting re-hospitalization in a population (Li, Cai & Glance, 2015). While some articles examine economic and health factors contributing to these disparities, very few dig deep into environmental factors that influence this phenomenon, (Spatz, Bernheim, Horwitz & Herrin, 2020). With your zipcode affecting up to 60% of your health outcomes, this research is relevant to better improving one of our most costly health expenditures: hospitalization.\nRe-hospitalization is a substantially costlier expenditure, as readmitting a patient further increases costs – especially if the diagnosis was untreated, poorly treated, or incorrectly treated. Most inpatient episodes characterized as a re-hospitalization when the patient is readmitted to the hospital 60 days after discharge. If the cause is different, sometimes that is counted as a re-hospitalization; other times, not so much, (Bhosale, K., Nath, R., Pandit, N., Agarwal, P., Khairnar, S., Yadav, B. & Chandrakar, S., 2020).\n\n\nThis paper aims to explore how different environmental variables impact re-hospitalization rates on a county-by-county level. Due to the nature of this project, we will not be controlling for racial, ethnic, and sex variables. These environmental factors will include both common environmental concerns, such as heat index, average temperature, precipitation, and natural disasters, along with the built environment, mean travel time to work, renter burden, and population density. We will also stratify by rural/urban classification, to determine if counties above or below 250,000 population experience differences in re-hospitalization rates, dependent upon these explanatory variables.\nThe data-set chosen for this analysis is taken from the Agency for Healthcare Research and Quality, Social Determinants of Health (SDOH) Database. This data-set has over 300 variables to explore each SDOH domain: social context, economic context, education, healthcare, and the environment. We shall pull data from three of these five domains: social, economic, and environmental.\nHow re-hospitalization is measured is not clarified per this data-set’s codebook. However, the Center for Medicare and Medicaid (CMS) 30-day Risk-Standardized Readmission Rate (RSRR) measures re-hospitalization as an unplanned readmission to inpatient services. It does stratify and specify based upon diagnosis. As the AHRQ is a federal agency alongside CMS, it is likely that they are pulling from CMS for this measure and aggregating various diagnoses into one county rate.\n\n\n\nThe hypothesis for this research report is:\n\nEnvironmental factors increase rates of re-hospitalization in the United States.\n\nTherefore, the null hypothesis is:\n\nEnvironmental factors do not increase rates of re-hospitalization in the United States.\n\nVarious regression analyses shall be employed to determine the relationship – or lack thereof – between these variables.\nFirst I’ll import the relevant libraries.\nThen I’ll import the dataset and view the first six rows.\n\n\nCode\ndf <- SDOH_2020_COUNTY_1_0 <- read_excel(\"_data/SDOH_2020_COUNTY_1_0.xlsx\", \n                                         sheet = \"Data\")\n\n\nWarning: Expecting logical in OA1673 / R1673C391: got '46123'\n\n\nWarning: Expecting logical in OA1765 / R1765C391: got '32510'\n\n\nWarning: Expecting logical in OB1765 / R1765C392: got '41025'\n\n\nWarning: Expecting logical in OC1765 / R1765C393: got '41037'\n\n\nWarning: Expecting logical in OA2799 / R2799C391: got '49017'\n\n\nWarning: Expecting logical in OB2799 / R2799C392: got '49019'\n\n\nWarning: Expecting logical in OC2799 / R2799C393: got '49025'\n\n\nWarning: Expecting logical in OD2799 / R2799C394: got '49055'\n\n\nWarning: Expecting logical in OA2844 / R2844C391: got '51760'\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 685\n   YEAR COUNTYFIPS STATEFIPS STATE COUNTY REGION TERRI…¹ ACS_T…² ACS_T…³ ACS_T…⁴\n  <dbl> <chr>      <chr>     <chr> <chr>  <chr>    <dbl>   <dbl>   <dbl>   <dbl>\n1  2020 01001      01        Alab… Autau… South        0   55639   54929   52404\n2  2020 01003      01        Alab… Baldw… South        0  218289  216518  206329\n3  2020 01005      01        Alab… Barbo… South        0   25026   24792   23694\n4  2020 01007      01        Alab… Bibb … South        0   22374   22073   21121\n5  2020 01009      01        Alab… Bloun… South        0   57755   57164   54250\n6  2020 01011      01        Alab… Bullo… South        0   10173   10143    9579\n# … with 675 more variables: ACS_TOT_POP_ABOVE15 <dbl>,\n#   ACS_TOT_POP_ABOVE16 <dbl>, ACS_TOT_POP_16_19 <dbl>,\n#   ACS_TOT_POP_ABOVE25 <dbl>, ACS_TOT_CIVIL_POP_ABOVE18 <dbl>,\n#   ACS_TOT_CIVIL_VET_POP_ABOVE25 <dbl>, ACS_TOT_OWN_CHILD_BELOW17 <dbl>,\n#   ACS_TOT_WORKER_NWFH <dbl>, ACS_TOT_WORKER_HH <dbl>,\n#   ACS_TOT_CIVILIAN_LABOR <dbl>, ACS_TOT_CIVIL_EMPLOY_POP <dbl>,\n#   ACS_TOT_POP_POV <dbl>, ACS_TOT_CIVIL_NONINST_POP_POV <dbl>, …\n\n\nNext I want to verify the class is a dataframe. Otherwise, I’ll need to transform the data to make it easier to work with.\n\n\nCode\nclass(df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nAll good here.\nNow on to data transformation. We will need to select only the relevant columns for this analysis.\n\n\nCode\ndf_new <- df %>%\n  select(COUNTYFIPS,\n         STATE,\n         COUNTY,\n         AHRF_USDA_RUCC_2013,\n         CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         ACS_PCT_COMMT_60MINUP,\n         ACS_PCT_RENTER_HU_COST_50PCT,\n         LTC_AVG_OBS_REHOSP_RATE) \nnrow(df_new)\n\n\n[1] 3229\n\n\nCode\nhead(df_new)\n\n\n# A tibble: 6 × 14\n  COUNTYF…¹ STATE COUNTY AHRF_…² CEN_P…³ NEPHT…⁴ NOAAC…⁵ NOAAC…⁶ NOAAS…⁷ SAIPE…⁸\n  <chr>     <chr> <chr>  <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 01001     Alab… Autau… 2          93.6       4    66.2    5.72      17   67565\n2 01003     Alab… Baldw… 3         137.        0    68.9    5.27      58   71135\n3 01005     Alab… Barbo… 6          28.3       3    66.4    5.75      24   38866\n4 01007     Alab… Bibb … 1          35.9       0    64.1    5.61      23   50907\n5 01009     Alab… Bloun… 1          89.6       0    62.7    5.96      45   55203\n6 01011     Alab… Bullo… 6          16.3       2    66.2    5.40      12   33124\n# … with 4 more variables: SAIPE_PCT_POV <dbl>, ACS_PCT_COMMT_60MINUP <dbl>,\n#   ACS_PCT_RENTER_HU_COST_50PCT <dbl>, LTC_AVG_OBS_REHOSP_RATE <dbl>, and\n#   abbreviated variable names ¹​COUNTYFIPS, ²​AHRF_USDA_RUCC_2013,\n#   ³​CEN_POPDENSITY_COUNTY, ⁴​NEPHTN_HEATIND_105, ⁵​NOAAC_AVG_TEMP_YEARLY,\n#   ⁶​NOAAC_PRECIPITATION_AVG_YEARLY, ⁷​NOAAS_TOT_NATURAL_DISASTERS,\n#   ⁸​SAIPE_MEDIAN_HH_INCOME\n\n\nOut of 1400+ variables, we’ve whittled them down to 14. Of those 14, we have four (4) that are unique identifiers (FIPS, State, County, and Rural-Urban Continuation Code), four (4) environmental, two (2) economic, one (1) housing, two (2) built-enviornment, and one (1) healthcare outcome.\nBefore we launch into exploring these variables via descriptive statistics, first we need to determine where the NAs are and see if any of the variables will have a substantial amount of missing data.\n\n\nCode\nkable(colSums(is.na(df_new)))\n\n\n\n\n\n\nx\n\n\n\n\nCOUNTYFIPS\n0\n\n\nSTATE\n0\n\n\nCOUNTY\n0\n\n\nAHRF_USDA_RUCC_2013\n9\n\n\nCEN_POPDENSITY_COUNTY\n8\n\n\nNEPHTN_HEATIND_105\n121\n\n\nNOAAC_AVG_TEMP_YEARLY\n123\n\n\nNOAAC_PRECIPITATION_AVG_YEARLY\n123\n\n\nNOAAS_TOT_NATURAL_DISASTERS\n0\n\n\nSAIPE_MEDIAN_HH_INCOME\n87\n\n\nSAIPE_PCT_POV\n87\n\n\nACS_PCT_COMMT_60MINUP\n8\n\n\nACS_PCT_RENTER_HU_COST_50PCT\n8\n\n\nLTC_AVG_OBS_REHOSP_RATE\n410\n\n\n\n\n\nPlenty of variables with missing data. Some are minor, such as population density, housing cost, and commute time variables with 8. Some are concerning, such as Heat Index, Average Yearly Temperature, and Average Yearly Precipitation, all around 120+.\nThe most concerning is – of course – our outcome variable, Re-Hospitalization Rates. This is not ideal. However, 410 / 3229 (12.6%) is not bad. That still leaves us with plenty of counties to review.\n\n\nCode\ndf_new <- df_new %>%\n  drop_na() %>%\n  print(nrow(df_new))\n\n\n# A tibble: 2,814 × 14\n   COUNTYFIPS STATE   COUNTY          AHRF_USDA_RUCC_2013 CEN_POPDENSITY_COUNTY\n   <chr>      <chr>   <chr>           <chr>                               <dbl>\n 1 01001      Alabama Autauga County  2                                    93.6\n 2 01003      Alabama Baldwin County  3                                   137. \n 3 01005      Alabama Barbour County  6                                    28.3\n 4 01007      Alabama Bibb County     1                                    35.9\n 5 01009      Alabama Blount County   1                                    89.6\n 6 01011      Alabama Bullock County  6                                    16.3\n 7 01013      Alabama Butler County   6                                    25.4\n 8 01015      Alabama Calhoun County  3                                   189. \n 9 01017      Alabama Chambers County 6                                    56.0\n10 01019      Alabama Cherokee County 6                                    47.0\n   NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_YEARLY NOAAC_PRECIPITATION_AVG_YEARLY\n                <dbl>                 <dbl>                          <dbl>\n 1                  4                  66.2                           5.72\n 2                  0                  68.9                           5.27\n 3                  3                  66.4                           5.75\n 4                  0                  64.1                           5.61\n 5                  0                  62.7                           5.96\n 6                  2                  66.2                           5.40\n 7                  0                  67.1                           5.37\n 8                  0                  63.2                           5.74\n 9                  0                  63.3                           5.93\n10                  0                  62.3                           5.85\n   NOAAS_TOT_NATURAL_DISASTERS SAIPE_MEDIAN_HH_INCOME SAIPE_PCT_POV\n                         <dbl>                  <dbl>         <dbl>\n 1                          17                  67565          11.2\n 2                          58                  71135           8.9\n 3                          24                  38866          25.5\n 4                          23                  50907          17.8\n 5                          45                  55203          13.1\n 6                          12                  33124          30.8\n 7                          18                  42268          20.6\n 8                          14                  50259          14.5\n 9                          18                  39318          16.3\n10                          38                  50388          14.7\n   ACS_PCT_COMMT_60MINUP ACS_PCT_RENTER_HU_COST_50PCT LTC_AVG_OBS_REHOSP_RATE\n                   <dbl>                        <dbl>                   <dbl>\n 1                  6.06                         26.6                    0.14\n 2                  7.53                         20.8                    0.14\n 3                 11.8                          22.4                    0.22\n 4                 10.4                          27.4                    0.16\n 5                 18.6                          22.6                    0.14\n 6                 12.7                          34                      0.05\n 7                  9.14                         29.5                    0.11\n 8                  7.21                         20.5                    0.13\n 9                  5.93                         13.0                    0.15\n10                 10.2                          12.2                    0.19\n# … with 2,804 more rows\n\n\n2,814 x 14 is a good place to start."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#descriptive-statistics",
    "href": "posts/FinalPart2_CalebHill.html#descriptive-statistics",
    "title": "Final Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nFor our preliminary analysis, we’re going to provide summary statistics analyzing the 10 variables relevant to our research question, from Population Density to the end of the data-set, and a visualization for each.\n\n\nCode\nkable(describe(df_new))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nCOUNTYFIPS*\n1\n2814\n1.407500e+03\n8.124762e+02\n1407.50000\n1.407500e+03\n1043.009100\n1.000000e+00\n2814.00000\n2.813000e+03\n0.0000000\n-1.2012794\n15.3161135\n\n\nSTATE*\n2\n2814\n2.442928e+01\n1.359471e+01\n23.00000\n2.441874e+01\n16.308600\n1.000000e+00\n48.00000\n4.700000e+01\n0.0584273\n-1.2662366\n0.2562759\n\n\nCOUNTY*\n3\n2814\n8.369051e+02\n4.694844e+02\n831.50000\n8.339396e+02\n584.885700\n1.000000e+00\n1674.00000\n1.673000e+03\n0.0526610\n-1.1127183\n8.8503222\n\n\nAHRF_USDA_RUCC_2013*\n4\n2814\n4.772210e+00\n2.619826e+00\n6.00000\n4.717140e+00\n2.965200\n1.000000e+00\n9.00000\n8.000000e+00\n0.0039932\n-1.2985960\n0.0493867\n\n\nCEN_POPDENSITY_COUNTY\n5\n2814\n2.893180e+02\n1.877174e+03\n50.74500\n8.620448e+01\n55.404762\n5.000000e-01\n71895.54000\n7.189504e+04\n25.8518141\n850.3767977\n35.3868910\n\n\nNEPHTN_HEATIND_105\n6\n2814\n4.098792e+00\n8.186776e+00\n0.00000\n1.963144e+00\n0.000000\n0.000000e+00\n59.00000\n5.900000e+01\n2.9358498\n9.6132423\n0.1543302\n\n\nNOAAC_AVG_TEMP_YEARLY\n7\n2814\n5.627572e+01\n8.030342e+00\n55.95833\n5.617415e+01\n8.784405\n3.541667e+01\n78.49167\n4.307500e+01\n0.1339838\n-0.5932113\n0.1513812\n\n\nNOAAC_PRECIPITATION_AVG_YEARLY\n8\n2814\n3.611196e+00\n1.628908e+00\n3.62375\n3.638924e+00\n1.887844\n2.241667e-01\n9.74500\n9.520833e+00\n-0.0899446\n-0.7490480\n0.0307068\n\n\nNOAAS_TOT_NATURAL_DISASTERS\n9\n2814\n3.678074e+01\n4.541085e+01\n25.00000\n2.851510e+01\n19.273800\n0.000000e+00\n662.00000\n6.620000e+02\n5.3277337\n46.7140570\n0.8560470\n\n\nSAIPE_MEDIAN_HH_INCOME\n10\n2814\n5.738628e+04\n1.442985e+04\n55107.00000\n5.584910e+04\n11735.520300\n2.599700e+04\n155362.00000\n1.293650e+05\n1.4172637\n3.6789671\n272.0193680\n\n\nSAIPE_PCT_POV\n11\n2814\n1.371606e+01\n5.320367e+00\n12.80000\n1.323637e+01\n4.744320\n3.000000e+00\n39.60000\n3.660000e+01\n1.0397109\n1.6688697\n0.1002951\n\n\nACS_PCT_COMMT_60MINUP\n12\n2814\n8.155924e+00\n4.864799e+00\n6.85500\n7.487016e+00\n3.810282\n0.000000e+00\n35.91000\n3.591000e+01\n1.5015546\n3.0016425\n0.0917071\n\n\nACS_PCT_RENTER_HU_COST_50PCT\n13\n2814\n2.060974e+01\n6.649144e+00\n20.78000\n2.057608e+01\n6.093486\n0.000000e+00\n49.26000\n4.926000e+01\n0.1136840\n0.5492796\n0.1253440\n\n\nLTC_AVG_OBS_REHOSP_RATE\n14\n2814\n1.449645e-01\n8.432610e-02\n0.14000\n1.422425e-01\n0.059304\n0.000000e+00\n1.00000\n1.000000e+00\n1.6404370\n12.5443704\n0.0015896\n\n\n\n\n\nWe should note for many of these analyses that the Urban / Rural Continuum Code runs from 1.00 to 9.00. Anything 4.00 or higher would be classified as Rural, with an urban population of less than 250,000.\n\nPopulation Density\n\n\nCode\npop_den <- df_new %>%\n  filter(CEN_POPDENSITY_COUNTY < 5000)\n\nggplot(pop_den, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\nWe’ve surely got some out-liers. The mean is 291, but the median is 46. The max is 70,000. We’ve filtered those out for this visualization and set the bins close to the median. A left-skewed variable is expected, as the majority of counties in the United States would be classified as rural and therefore have low population densities.\nLet’s plot a facet-grid on these codes.\n\n\nCode\nggplot(pop_den, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50) +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n\n\n\nAs expected, a large part of the distribution is 4.0 or higher. 1.0 also has a high variation of population density, which may cause issues with the regression.\n\n\nHeat Index Over 105F\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_boxplot()\n\n\n\n\n\nDue to the wide range in climate for the United States, it’s not surprising that there’s a large variety of out-liers. The median number of days a county experienceed a heat index of over 105F each year is 4 days per year. One county even reached 59 days – a Texas county!\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe data-set has a very left skewed distribution, similar to Population Density. Most counties experience under 10 days with a Heat Index over 105.\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis distribution stays fairly constant, regardless of UR classification. 2.00 and 6.00 may have interesting insights, as their right tails are more pronounced, but that would be better suited to a map for quick reference. That is outside the scope of this project.\n\n\nAverage Yearly Temperature\n\n\nCode\nggplot(df_new, aes(NOAAC_AVG_TEMP_YEARLY)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere’s a good distribution. Average temperature each month is between 50 to 60 for most of the counties. The range (45) is also fairly large and shows the multiple climates within its borders.\n\n\nAverage Yearly Precipitation\n\n\nCode\nggplot(df_new, aes(NOAAC_PRECIPITATION_AVG_YEARLY)) +\n  geom_boxplot()\n\n\n\n\n\nAverage precipitation each month is fairly uniform, with the mean at 3.49 inches of rain, on average, each month. This variable will most likely provide less variation in the analysis compared to others, such as population density and heat index. This can be both a good and a bad thing, as variations in precipitation was one of the variables I was most interested in exploring for this project. Oh well.\n\n\nTotal Natural Disasters\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_boxplot()\n\n\n\n\n\nMany high out-liers over 100; some even reaching over 600. Let’s plot a histogram to get a better look at the data’s distribution.\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA left skewed variable, with observations dropping off dramatically once we reach 50 total recorded natural disasters.\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLittle to no difference in UR classification for natural disaster out-liers.\n\n\nMedian Household Income\n\n\nCode\nggplot(df_new, aes(SAIPE_MEDIAN_HH_INCOME)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVery close to a normal distribution, if barely left-skewed. A couple of high out-liers, hovering around $90,000+ in median household income, but the mean holds at $57,465.\n\n\nPercent in Poverty\n\n\nCode\nggplot(df_new, aes(SAIPE_PCT_POV)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother close to normal distribution. Most counties have poverty rates ranging from 10% to 20%. There are of course out-liers, especially a good number below 10%, but those are rare.\n\n\nCode\nggplot(df_new, aes(SAIPE_PCT_POV)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHighly urbanized counties (1.00) have substantially less percentage poverty compared to their rural counterparts. 7.00, 8.00, and 9.00 have the highest spread, with some counties reaching 40% poverty rates! We barely see the urban areas (1.00 - 3.00) reach 30% poverty.\n\n\nPercent Commuting Alone, Over 60 Minutes\n\n\nCode\nggplot(df_new, aes(ACS_PCT_COMMT_60MINUP)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe majority of counties fall below 10% of their population commuting up to and more than 60 minutes for work. Let’s do another facet grid to see if there’s a relationship between UR codes.\n\n\nCode\nggplot(df_new, aes(ACS_PCT_COMMT_60MINUP)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNot particularly. The only codes that appear different than the rest include 1.00 (highly urban, over 1 million population) and 8.00 (completely rural, fewer than 2,500 population). 7.00 and higher is surprising, as these are counties with very little population and often not adjacent to metro areas. Therefore, populations are most likely condensed around “urban” centers for economic purposes.\n\n\nPercent Renter Housing Costs Over 50 Percent of Income\n\n\nCode\nggplot(df_new, aes(ACS_PCT_RENTER_HU_COST_50PCT)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is a startling figure. On average, 20% of counties have renters where 50% or more of their income goes toward housing costs. These leaves little to no room for other expenses and drives economic instability. The data is normally distributed and barely left-skewed – but still an item to consider with further analysis.\n\n\nRe-hospitalization Rate\n\n\nCode\nggplot(df_new, aes(LTC_AVG_OBS_REHOSP_RATE)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother right skewed variable. Lots of counties with 0.00 rates of re-hospitalization, and few, if any, above 0.50 per 100,000 people. From a health perspective, this is good news! From a research perspective, that’s going to make analysis a little trickier. However, the somewhat normal and/or bimodal distribution should be fairly easy to work with. While needing some transformation for a linear regression, we can test multiple models per each variable to determine which amendment provides the most robust inference."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#analysis",
    "href": "posts/FinalPart2_CalebHill.html#analysis",
    "title": "Final Part 2",
    "section": "Analysis",
    "text": "Analysis\n\nHypothesis Testing\nRemember that the hypothesis for this research report is:\n\nEnvironmental factors increase rates of re-hospitalization in the United States.\n\nWe have nine (9) explanatory variables to work with, so we can run different regressions to determine what variables influence re-hospitalization rates the most – if at all – and how they interact with other variables.\nReminder that the nine (9) explanatory variable are broken down into three domains: environmental, economic, and built environment.\nEnvironmental entails:\n\nDay with Heat Index over 105F\nAverage Annual Precipitation\nAverage Annual Precipitation\nTotal Natural Disasters Per Year\n\nEconomic is:\n\nMedian Household Income\nPercent Poverty\n\nAnd the Built Environment includes:\n\nPopulation Density\nPercent Rental Housing Cost, over 50%\nPercent Commuting Alone, over 60 minutes\n\nWe will run four models to test the hypothesis. They shall examine each environmental variable’s impact on the dependent variable, re-hospitalization rates. The control variables will be the economic and built environment variables, five (5) in total.\n\n\nCode\nmodel1 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NEPHTN_HEATIND_105 +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n\nmodel2 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAC_AVG_TEMP_YEARLY +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n  \nmodel3 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAC_PRECIPITATION_AVG_YEARLY +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n  \nmodel4 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAS_TOT_NATURAL_DISASTERS +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n\n\nLet’s plot these regressions, removing the control variables to get a better visualization. For two of the models, we will employ log transformations for data that is skewed. These variables were identified during the Descriptive Statistic section. This shall include a log transformation for the response variable in particular.\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105,\n       log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 1\",\n       x = \"Heat Index Over 105F\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\nCode\nggplot(df_new, aes(log(NOAAC_AVG_TEMP_YEARLY),\n       log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 2\",\n       x = \"Average Annual Temperature\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\nCode\nggplot(df_new, aes(NOAAC_PRECIPITATION_AVG_YEARLY,\n                   log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 3\",\n       x = \"Average Annual Precipitation\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS,\n                   log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 4\",\n       x = \"Total Natural Disasters\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n\nOf these four, it looks like temperature and precipitation have the best fit. All but Total Natural Disasters have a positive relationship with the response variable, so that helps us in determining if we should accept or reject the null hypothesis.\nWe will reject the null hypothesis. While it looks like, at first glance, that there is little positive relationship, we can at least note that there is some positive relationship. In the next two sections, we will dig deeper into each model, examining the p-value and R-Squared value, to see what level of relationship is present.\n\n\nModel Comparisons\nNow we will compare the four (4) models in more depth.\n\n\nCode\nsummary(model1)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NEPHTN_HEATIND_105 + CEN_POPDENSITY_COUNTY + \n    SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + ACS_PCT_COMMT_60MINUP + \n    ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24905 -0.04273 -0.00201  0.03850  0.86257 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   5.582e-02  1.553e-02   3.595 0.000330 ***\nNEPHTN_HEATIND_105            6.909e-04  1.959e-04   3.526 0.000428 ***\nCEN_POPDENSITY_COUNTY         6.250e-07  8.575e-07   0.729 0.466180    \nSAIPE_MEDIAN_HH_INCOME        4.223e-07  1.825e-07   2.313 0.020777 *  \nSAIPE_PCT_POV                 3.331e-03  5.166e-04   6.449 1.32e-10 ***\nACS_PCT_COMMT_60MINUP        -4.504e-05  3.321e-04  -0.136 0.892129    \nACS_PCT_RENTER_HU_COST_50PCT  8.043e-04  2.517e-04   3.195 0.001413 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0824 on 2807 degrees of freedom\nMultiple R-squared:  0.04725,   Adjusted R-squared:  0.04521 \nF-statistic:  23.2 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model2)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAC_AVG_TEMP_YEARLY + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22744 -0.04216 -0.00227  0.03810  0.87918 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  -3.293e-03  1.691e-02  -0.195 0.845614    \nNOAAC_AVG_TEMP_YEARLY         1.721e-03  2.236e-04   7.697 1.91e-14 ***\nCEN_POPDENSITY_COUNTY         6.974e-07  8.498e-07   0.821 0.411866    \nSAIPE_MEDIAN_HH_INCOME        2.163e-07  1.835e-07   1.178 0.238760    \nSAIPE_PCT_POV                 2.082e-03  5.440e-04   3.827 0.000132 ***\nACS_PCT_COMMT_60MINUP        -4.280e-04  3.338e-04  -1.282 0.199842    \nACS_PCT_RENTER_HU_COST_50PCT  6.661e-04  2.498e-04   2.667 0.007701 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08172 on 2807 degrees of freedom\nMultiple R-squared:  0.06281,   Adjusted R-squared:  0.06081 \nF-statistic: 31.35 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model3)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAC_PRECIPITATION_AVG_YEARLY + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24203 -0.04423 -0.00280  0.03765  0.86494 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     3.851e-02  1.539e-02   2.502  0.01240 *  \nNOAAC_PRECIPITATION_AVG_YEARLY  8.861e-03  1.046e-03   8.470  < 2e-16 ***\nCEN_POPDENSITY_COUNTY           4.913e-07  8.473e-07   0.580  0.56206    \nSAIPE_MEDIAN_HH_INCOME          5.067e-07  1.801e-07   2.813  0.00495 ** \nSAIPE_PCT_POV                   2.954e-03  5.089e-04   5.805 7.15e-09 ***\nACS_PCT_COMMT_60MINUP          -5.921e-04  3.358e-04  -1.763  0.07794 .  \nACS_PCT_RENTER_HU_COST_50PCT    4.636e-04  2.514e-04   1.844  0.06534 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08154 on 2807 degrees of freedom\nMultiple R-squared:  0.06688,   Adjusted R-squared:  0.06489 \nF-statistic: 33.53 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model4)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAS_TOT_NATURAL_DISASTERS + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24509 -0.04423 -0.00198  0.03901  0.85940 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   4.845e-02  1.564e-02   3.099  0.00196 ** \nNOAAS_TOT_NATURAL_DISASTERS  -5.117e-05  3.596e-05  -1.423  0.15490    \nCEN_POPDENSITY_COUNTY         4.512e-07  8.578e-07   0.526  0.59891    \nSAIPE_MEDIAN_HH_INCOME        5.359e-07  1.876e-07   2.856  0.00432 ** \nSAIPE_PCT_POV                 3.750e-03  5.097e-04   7.358 2.44e-13 ***\nACS_PCT_COMMT_60MINUP        -2.872e-05  3.331e-04  -0.086  0.93131    \nACS_PCT_RENTER_HU_COST_50PCT  7.913e-04  2.526e-04   3.132  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08255 on 2807 degrees of freedom\nMultiple R-squared:  0.04372,   Adjusted R-squared:  0.04168 \nF-statistic: 21.39 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nThree of the four explanatory variables meet the statistical significance threshold (0.001). Total Natural Disasters Per Year do not. This helps in finalizing whether to accept or reject the null hypothesis. The p-value is most significant for precipitation, model 3.\nFor the model fit, the adjusted R squared ranges from 0.04 to 0.06. The highest is for precipitation, model 3.\nPrecipitation also has the lowest residual standard error, at 0.081.\nFinally, we’re going to calculate the PRESS statistic (Predicted Residual Sum of Squares) to best determine which model can predict the response variable based upon the explanatory variables.\n\n\nCode\nPRESS <- function(model) {\n  i <- residuals(model)/(1 - lm.influence(model)$hat)\n  sum(i^2)\n}\n\nPRESS(model1)\n\n\n[1] 19.17066\n\n\nCode\nPRESS(model2)\n\n\n[1] 18.86294\n\n\nCode\nPRESS(model3)\n\n\n[1] 18.77811\n\n\nCode\nPRESS(model4)\n\n\n[1] 19.23593\n\n\nModel 3 has the lowest PRESS score.\nDue to a strong p-value, PRESS score, and model fit compared to the other three models, model 3 will be chosen as the final model for the diagnostic exploration. While the adjusted R squared value is not strong when controlling for economic and built environment factors, there is still a positive relationship, and therefore some influence on the dependent variable.\n\n\nDiagnostics\nFinally, we’ll plot the diagnostics to best understand the model.\n\n\nCode\npar(mfrow = c(2,3));\nplot(model3, \n     which = 1:6)\n\n\n\n\n\nOf the six plots, Cook’s distance is the most striking and relevant, as there are three out-liers: 1679, 2189, and 2455.\n1679 is New York County, New York.\n2189 is Mellette County, South Dakota.\n2455 is Real County, Texas.\nOtherwise, the plot looks good.\nNormal Q-Q violates this test, as the points at the right tail of the plot do not generally fall along the line. This is very apparent for our three out-liers. The remaining plots do not violate their tested assumptions and further cement the model’s reliability."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#conclusion",
    "href": "posts/FinalPart2_CalebHill.html#conclusion",
    "title": "Final Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper explored the relationship between re-hospitalization rates and four environmental variables, when controlling for common variables that regularly influence the dependent variable. These four variables included days with a heat index over 105F, average annual temperature, average annual precipitation, and total natural disasters.\nFour models were selected, one for each variable, to best determine which measure best impacted re-hospitalization rates. Three of the four variables were statistically significant and two had a larger adjusted R squared value than the others. Precipitation was selected as the variable with the best model fit to explain re-hospitalization rate impact. While the adjusted R-squared value is negligible (0.06), there is a positive relationship that is statistically significant. Therein we see some form of an influence large amounts of annual precipitation has on re-hospitalization rates.\nI would have liked to tighten the analysis further instead of including multiple (10) variables, by focusing on some key measurements: Precipitation as the explanatory, Poverty as the control, and filtering by Rurality to determine the relationship with re-hospitalization rates. I could then fit different models (Simple Linear, Poisson, Polynomial, etc.) to see which worked best. I may not have time to do so for the poster presentation, nor would that perhaps be within the scope of this project. Either way, I have something for future classes, perhaps via time series analysis or machine learning (prediction over inference). Either way, this helped me better understand the robustness of linear regression models."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#references",
    "href": "posts/FinalPart2_CalebHill.html#references",
    "title": "Final Part 2",
    "section": "References",
    "text": "References\nBarnett, M., Hsu, J. & McWilliams, M. (2015). “Patient Characteristics and Differences in Hospital Readmission Rates.” JAMA Intern Med., 175(11): 1803-1812.\nBhosale KH, Nath RK, Pandit N, Agarwal P, Khairnar S, Yadav B, & Chandrakar S. (2020). “Rate of Rehospitalization in 60 Days of Discharge and It’s Determinants in Patients with Heart Failure with Reduced Ejection Fraction in a Tertiary Care Centre in India.” Int J Heart Fail. 21;2(2):131-144.\nLi, Y., Cai, X. & Glance, L. (2015). “Disparities in 30-day rehospitalization rates among Medicare skilled nursing facility residents by race and site of care.” Med Care, 53(12): 1058-1065.\nMurray, F., Allen, M., Clark, C., Daly, C. & Jacobs, D. (2021). “Socio-demographic and -economic factors associated with 30-day readmission for conditions targeted by the hospital readmissions reduction program: a population-based study.” BMC Public Health, 21.\nSpatz, E., Bernheim, S., Horwitz, L. & Herrin, J. (2020). Community factors and hospital wide readmission rates: Does context matter? PLoS One, 15(10)."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html",
    "href": "posts/HW3_ManiShankerKamarapu.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a",
    "href": "posts/HW3_ManiShankerKamarapu.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b",
    "href": "posts/HW3_ManiShankerKamarapu.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first, then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#c",
    "href": "posts/HW3_ManiShankerKamarapu.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-4",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-5",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html",
    "href": "posts/HW2_SteveONeill.html",
    "title": "Homework 2",
    "section": "",
    "text": "“The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (”Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population”\n\n\n\nCode\nbypass <- data.frame(sample_size = 539,\n                     mean_wait_time = 19,\n                     standard_dev = 10)\nbypass \n\n\n  sample_size mean_wait_time standard_dev\n1         539             19           10\n\n\nCode\nangiography <- data.frame(sample_size = 847,\n                     mean_wait_time = 18,\n                     standard_dev = 9)\nangiography\n\n\n  sample_size mean_wait_time standard_dev\n1         847             18            9\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-2",
    "href": "posts/HW2_SteveONeill.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\n\nCode\ncollege_n <- 1031\ncollege_k <- 567\ncollege_p <- college_k/college_n\ncollege_p\n\n\n[1] 0.5499515\n\n\nThe point estimate for the proportion of all adult Americans who believe that a college education is essential for success is simply 0.5499515.\n\n\nCode\ncollege_moe <- qnorm(0.975)*sqrt(college_p*(1-college_p)/college_n)\ncollege_moe\n\n\n[1] 0.03036761\n\n\nCode\ncollege_CI_low <- college_p - college_moe\ncollege_CI_high <- college_p + college_moe\ncollege_CI_low\n\n\n[1] 0.5195839\n\n\nCode\ncollege_CI_high\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the point estimate is [0.5195839, 0.5803191]."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-3",
    "href": "posts/HW2_SteveONeill.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\n\nCode\n#The estimate will be useful if it is within $5 of the true population mean\nbooks_moe <- 5\n#The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200.\nbooks_range <- (200-30)\n#They think that the population standard deviation is about a quarter of this range\nbooks_sd <- books_range / 4\nbooks_sd\n\n\n[1] 42.5\n\n\nA 5% alpha means a 95% confidence level. Conventionally we know the ‘critical value’ for the 95% confidence interval is 1.96\nUnfortunately, I’m missing the equation that lets us find the sample size from these values!"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-4",
    "href": "posts/HW2_SteveONeill.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\n\nCode\n#the mean income for all senior-level workers in a large service company equals $500 per week.\nunion_pop_mean = 500\n#For a random sample of nine female employees,\nunion_sample_size = 9\n# ȳ = $410\nunion_sample_mean = 410\n# and s = 90\nunion_sample_sd = 90\n\n\n\nT-statistic\nAssuming the sample is normally distributed, the formula for a t-statistic is: (sample mean - population mean) / (sample standard deviation / square root of sample size)\n\n\nCode\ntstatistic <- (union_sample_mean - union_pop_mean) / (union_sample_sd / sqrt(union_sample_size))\ntstatistic\n\n\n[1] -3\n\n\nThe t-statistic is -3.\n\n\nHypotheses\nIn this case, the null hypothesis is that the mean income for female employees matches $500/wk. The alternate hypothesis is that it does not equal $500/wk.\npt takes the T-statistic and degrees of freedom and returns the p-value to the left. If we multiply it by two, it becomes ‘two-tailed’:\n\n\nCode\n2 * pt(-3, (union_sample_size - 1))\n\n\n[1] 0.01707168\n\n\nThis two-tailed test returns a p-value of 0.01707168 his is well under the significance level of 5%, meaning that there is only a 1.7 percent chance of receiving as extreme a result as this under the null hypothesis. Therefore the null hypothesis is said to be rejected."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-5",
    "href": "posts/HW2_SteveONeill.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0\n\n\n\nCode\njones_smith_pop_mean = 500\njones_smith_sample_size = 10000\n\njones_sample_mean = 519.5\nsmith_sample_mean = 519.7\n\njones_smith_se = 10\n\n\n\n\nCode\njones_t_statistic <- (jones_sample_mean - jones_smith_pop_mean) / jones_smith_se\njones_t_statistic\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- 2*pt(jones_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\njones_p_value\n\n\n[1] 0.05120403\n\n\nCode\nsmith_t_statistic <- (smith_sample_mean - jones_smith_pop_mean) / jones_smith_se\nsmith_t_statistic\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- 2*pt(smith_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\nsmith_p_value\n\n\n[1] 0.04886592\n\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\n\nUsing an alpha of .05, Jones’ p value is not ‘statistically significant’, but Smith’s is.\nBoth Jones and Smith’s results should be presented with the P-values included so that knowledgeable people can see how borderline they are. Alternately, different significance levels could be adopted - or the sample size could be increased."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-6",
    "href": "posts/HW2_SteveONeill.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\ngas_mean <- gas_taxes %>% mean()\n\n\nError in gas_taxes %>% mean(): could not find function \"%>%\"\n\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\n\nCode\nt.test(gas_taxes, mu = gas_mean, alternative = 'less')\n\n\nError in t.test.default(gas_taxes, mu = gas_mean, alternative = \"less\"): object 'gas_mean' not found\n\n\nWith the information we have, I am not sure if we have enough evidence to conclude about the average tax per gallon in 2005. Otherwise, the p-value is within bounds at .5 and yes, this would be statistically significant."
  },
  {
    "objectID": "posts/Homework 1 LJones.html",
    "href": "posts/Homework 1 LJones.html",
    "title": "Homework 1",
    "section": "",
    "text": "First I’ll load the libraries and read in the data.\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlc <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nThe distribution of lung capacity is as follows:\n\n\nCode\nhist(lc$LungCap)\n\n\n\n\n\nThe histogram appears close to the normal distribution.\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data=lc)\n\n\n\n\n\n\n\n\n\n\nCode\nlc %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nInterestingly, the mean lung capacity is higher for smokers than it is for non-smokers.\n\n\n\n\n\nCode\nlcbyagegrp <- lc %>% \n  mutate(age_group = case_when(\n    Age <=13 ~ \"13 and Under\",\n    Age >=14 & Age <=15 ~\"14-15\",\n    Age >=16 & Age <=17 ~\"16 - 17\",\n    Age >=18 ~\"18+\")) %>% \n  arrange(age_group, Age)\n\nggplot(lcbyagegrp, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(age_group ~ Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nCode\nlcbyagegrp %>%\n  group_by(age_group, Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke  Mean\n  <chr>        <chr> <dbl>\n1 13 and Under no     6.36\n2 13 and Under yes    7.20\n3 14-15        no     9.14\n4 14-15        yes    8.39\n5 16 - 17      no    10.5 \n6 16 - 17      yes    9.38\n7 18+          no    11.1 \n8 18+          yes   10.5 \n\n\n\n\nThe mean lung capacity for smokers aged 13 and under is higher than that of non-smokers in the same age group, which defies expectation. The rest of the age groups meet that expectation. There may be an error or extreme outlier in the data for smokers aged 13 and under.\n\n\n\n\n\n\nCode\nlc %>% cov(Age, LungCap)\n\n\nError in pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", : object 'LungCap' not found\n\n\n\n\nCode\n#correlation\ncor(lc$LungCap,lc$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#covariance\ncov(lc$LungCap, lc$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is very close to positive 1, indicating a strong positive correlation between between lung capacity and age. The covariance being a positive number indicates a positive relationship.\n\n\n\n\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(X, Frequency)\n\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\n\n\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\ndf2\n\n\n  X Frequency Probability\n1 0       128  0.15802469\n2 1       434  0.53580247\n3 2       160  0.19753086\n4 3        64  0.07901235\n5 4        24  0.02962963\n\n\nThe probability is about 19.75%.\n\n\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\nsum(b2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability is about 69%.\n\n\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\nsum(c2$Probability)\n\n\n[1] 0.891358\n\n\nThe probability is about 89%.\n\n\n\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\nsum(d2$Probability)\n\n\n[1] 0.108642\n\n\nThe probability is about 10.9%.\n\n\n\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\ne\n\n\n[1] 1.28642\n\n\nThe expected number of prior convictions is about 1.286.\n\n\n\n\n\nCode\n#variance\nvariance <- (sum(Frequency*((X-e)^2)))/(sum(Frequency)-1)\nvariance\n\n\n[1] 0.8572937\n\n\nCode\n#standard deviation\nsd <- sqrt(variance)\nsd\n\n\n[1] 0.9259016\n\n\nThe variance of prior convictions is about 0.857, and the standard deviation (simply, the square root of the variance) is about 0.926."
  },
  {
    "objectID": "posts/KenDocekal_HW2.html",
    "href": "posts/KenDocekal_HW2.html",
    "title": "HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q1",
    "href": "posts/KenDocekal_HW2.html#q1",
    "title": "HW2",
    "section": "Q1",
    "text": "Q1\n90% Confidence Interval for Bypass:\n18.29 - 19.71\n\n\nCode\nn <- 539\nxbar <- 19 \ns <- 10\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 18.29029\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 19.70971\n\n\n90% Confidence Interval for Angiography:\n17.49 - 18.51\n\n\nCode\nn <- 847\nxbar <- 18 \ns <- 9\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 17.49078\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 18.50922\n\n\nThe confidence interval is narrower for Angiography - 1.02 difference, compared to Bypass - 1.42 difference."
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q2",
    "href": "posts/KenDocekal_HW2.html#q2",
    "title": "HW2",
    "section": "Q2",
    "text": "Q2\nThe proportion point estimate for adult Americans who believe that a college education is essential for success is .55, based on 567 out of the representative sample of 1031 adult Americans surveyed.\nA 95% confidence interval shows that in 95% of cases the observed mean proportion of adult Americans who believe that a college education is essential for success will be between 52% and 58%.\n\n\nCode\nprop.test(567,1031)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q3",
    "href": "posts/KenDocekal_HW2.html#q3",
    "title": "HW2",
    "section": "Q3",
    "text": "Q3\nBased on the range of 30 to 200 we can determine the standard deviation using s = (Maximum – Minimum)/4 resulting in s=42.5.With a 95% significance level we will use 1.96 for the z score. To find the minimum sample size needed - n, we solve for (Zscore*s/margin of error)^2. Our sample size needs to be at least 277.56.\n\n\nCode\nn <- ((1.96)*(42.5)/5)^2\n\nn\n\n\n[1] 277.5556"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q4",
    "href": "posts/KenDocekal_HW2.html#q4",
    "title": "HW2",
    "section": "Q4",
    "text": "Q4"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q5",
    "href": "posts/KenDocekal_HW2.html#q5",
    "title": "HW2",
    "section": "Q5",
    "text": "Q5"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q6",
    "href": "posts/KenDocekal_HW2.html#q6",
    "title": "HW2",
    "section": "Q6",
    "text": "Q6\nCreate a data frame with a column for tax values.\n\n\nCode\ngas_taxes <-  data.frame (first_column  = c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)) \n\n\n\n\nCode\ngas_taxes\n\n\n   first_column\n1         51.27\n2         47.43\n3         38.89\n4         41.95\n5         28.61\n6         41.29\n7         52.19\n8         49.48\n9         35.02\n10        48.13\n11        39.28\n12        54.41\n13        41.66\n14        30.28\n15        18.49\n16        38.72\n17        33.41\n18        45.02\n\n\nCode\nnames(gas_taxes) <- c(\"tax\")\n\n\nUsing a one sample t-test where null hypothesis is mean tax is 45 and alternative hypothesis is true mean is less than 45 we are able to reject the null hypothesis at the 95% confidence level. Results indicate that mean tax was 40.86 and 95% of all observations will find a mean tax less than 44.68; therefore, while a few cities at the upper end of the range had prices near 45 cents per gallon, this was not usual and the average tax per gallon of gas in the US in 2005 was less than 45 cents.\n\n\nCode\nt.test(gas_taxes$tax, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes$tax\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/HW1_StephRoberts.html",
    "href": "posts/HW1_StephRoberts.html",
    "title": "Homework 1",
    "section": "",
    "text": "Homework 1\n##1. Use the LungCapData to answer the following questions. (Hint: Using dplyr, especiallygroup_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nCode\ndf<- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n#Summarize\n\n\nCode\nsummary(df)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\n\nCode\nmean(df$LungCap)\n\n\n[1] 7.863148\n\n\n\n\nCode\nmedian(df$LungCap)\n\n\n[1] 8\n\n\n\n\nCode\nvar(df$LungCap)\n\n\n[1] 7.086288\n\n\n\n\nCode\nsd(df$LungCap)\n\n\n[1] 2.662008\n\n\n\n\nCode\nmin(df$LungCap)\n\n\n[1] 0.507\n\n\nCode\nmax(df$LungCap)\n\n\n[1] 14.675\n\n\n#a. What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=0.5,col='black',fill='gray')\n\n\n\n\n\nThe histogram follows a distribution close to normal distibution. In fact, if we change binwidth slightly, it appears even closer to normal distribution.\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')\n\n\n\n\n\nThis helps illustrate the importance of binwidth and what it can do to our visualization interpretations.\n#b. Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nggplot(df, aes(x = LungCap, y = Gender)) +        \n  geom_boxplot()\n\n\n\n\n\nThe distribution of male lung capacity is larger and longer than females’.\n#c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean() \n\n\n[1] 8.645455\n\n\nCode\ndf %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\n\n[1] 7.770188\n\n\nIt does not make sense at face value. In this sample, smokers have a higher mean lung capacity than non-smokers. Let’s check how big each subsample is.\n\n\nCode\nlength(which(df$Smoke == 'yes'))\n\n\n[1] 77\n\n\nCode\nlength(which(df$Smoke == 'no'))\n\n\n[1] 648\n\n\nAs suspected, there are far more, almost 10 times as many, non-smokers. If we could gather data from all the smokers, perhaps our means would look a lot different. Maybe our sample was taken from young people whose lungs have not been long affected by the smoking.\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(Age) %>%\n  median() \n\n\n[1] 15\n\n\nAgain, as suspected, our sample of smokers is a young age. Therefore, the lack of difference in lung capacity between smokers and non-smokers is not too surprising.\n#d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Create age groups\ndf <- df %>% \n  mutate(agegroup = case_when(\n    Age <= 13  ~ \"less than or equal to 13\",\n    Age >= 14 & Age <= 15 ~ \"14 to 15\",\n    Age >= 16 & Age <= 17 ~ \"16 TO 17\",\n    Age >= 18 ~ \"greater than or equal to 18\"))\n\ntable(df$agegroup)\n\n\n\n                   14 to 15                    16 TO 17 \n                        120                          97 \ngreater than or equal to 18    less than or equal to 13 \n                         80                         428 \n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  ggplot(aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')+\n  facet_wrap(~agegroup)\n\n\n\n\n\nThese histograms suggest that participants 13 or younger have smaller lung capacity. The Lung capacity seems to generally increase with age as children grow.\n#e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(df, aes(x = LungCap, \n           fill = agegroup)) +\n  geom_density(alpha = 0.4)+\n  facet_wrap(~Smoke)\n\n\n\n\n\nThis visualization starts to explain furthermore why there is an unexpected result for lung capacity in smokers vs. non-smokers. As we have deducted, lung capacity generally improves with age (in growing years). However, teenagers approaching adulthood are also a group more likely to have access or influence to smoking cigarettes. It is likely that our smokers account for some of the older participants, who happen to be closer to normal smoking age.\n#f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\ncov(df$LungCap, df$Age) #calculate covariance\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age) #calculate correlation\n\n\n[1] 0.8196749\n\n\nA positive coraviance (8.74) indicates lung capacity and age tend to increase together. The positive correlation relatively close to 1 (0.82) indicates there is a fairly strong correlation between the variables.\n##2. Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n#create the sample\nx<-rep(c(0,1,2,3,4),times=c(128, 434, 160, 64, 24))\nsample(x, 10)\n\n\n [1] 1 1 0 3 3 3 1 1 1 2\n\n\nCode\n#Verify n of sample\nsum(128, 434, 160, 64, 24)\n\n\n[1] 810\n\n\n\n\nCode\n#Calculate the mean\nmean(x)\n\n\n[1] 1.28642\n\n\nCode\n#Verify the mean\nsample_mean <- (((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/810)\nprint(sample_mean)\n\n\n[1] 1.28642\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016\n\n\n#a. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n#probability of 2 convictions?\ndnorm.convict <- dnorm(2, mean(x), sd(x))\nprint(dnorm.convict)\n\n\n[1] 0.3201613\n\n\nThe probability of 2 convications in 0.32.\n#b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#probability of <2 convictions\nless.than <- pnorm(2, mean(x), sd(x)) - dnorm.convict\nprint(less.than)\n\n\n[1] 0.4593924\n\n\nThe probability of <2 convictions is 0.46.\n#c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n#probability of =<2 convictions?\npnorm.convict <- pnorm(2, mean(x), sd(x))\nprint(pnorm.convict)\n\n\n[1] 0.7795537\n\n\nThe probability of less than or equal to 2 convictions is 0.78.\n#d. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n#probability of >2 convictions?\ngreater.than <- 1 - pnorm.convict\nprint(greater.than)\n\n\n[1] 0.2204463\n\n\nThe probability of greater than 2 convictions is 0.22.\n\n\nCode\n#Verify all probabilities add to 1\nless.than + dnorm.convict + greater.than\n\n\n[1] 1\n\n\n#e. What is the expected value for the number of prior convictions?\n\n\nCode\n# Expected value of a probability distribution  can be found with μ = Σx * P(x), where x = data value and P(x) = probability of data. \n\n#Calculate probabilities of data\np0 <- dnorm(0, mean(x), sd(x))\np0\n\n\n[1] 0.1641252\n\n\nCode\np1 <- dnorm(1, mean(x), sd(x))\np1\n\n\n[1] 0.410739\n\n\nCode\np2 <- dnorm(2, mean(x), sd(x))\np2\n\n\n[1] 0.3201613\n\n\nCode\np3 <- dnorm(3, mean(x), sd(x))\np3\n\n\n[1] 0.07772916\n\n\nCode\np4 <- dnorm(4, mean(x), sd(x))\np4\n\n\n[1] 0.005877753\n\n\nCode\n#Calculate expected value\nev <- sum((0*p0), (1*p1), (2*p2), (3*p3), (4*p4))\nev\n\n\n[1] 1.30776\n\n\nCode\n#The expected value should be close to the mean in a normal distribution\nmean(x)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.31.\n#f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n#Calculate variance\nvar(x)\n\n\n[1] 0.8572937\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html",
    "href": "posts/HW1_KarenDetter.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "href": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "title": "Homework 1",
    "section": "Plot histogram with probability density on the y axis",
    "text": "Plot histogram with probability density on the y axis\n\n\nCode\nhist(LungCapData$LungCap, freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution - most of the observations are close to the mean, with very few close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "href": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "title": "Homework 1",
    "section": "Create boxplots separated by gender",
    "text": "Create boxplots separated by gender\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, horizontal = TRUE)\n\n\n\n\n\nThe boxplots show that male lung capacity has a wider range than that of females; however, the minimum, median, and maximum values are all higher than those of females. This implies that, as a group, men are likely to have higher lung capacity than women."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "href": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "title": "Homework 1",
    "section": "Group by smoking status and summarize mean lung capacities",
    "text": "Group by smoking status and summarize mean lung capacities\n\n\nCode\nlibrary(dplyr)\nLungCapData %>%\ngroup_by(Smoke) %>%\nsummarize(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\nIn this dataset, the mean lung capacity of smokers is actually higher than that of non-smokers. Since this is counter to what would be expected, there is likely another variable exerting a confounding effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "href": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "title": "Homework 1",
    "section": "Create new data frame with age group category variables",
    "text": "Create new data frame with age group category variables\n\n\nCode\nLungCapData_AgeGroups <- LungCapData %>%\nmutate(AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\", \n            Age == 14 | Age == 15 ~ \"14 to 15\",\n            Age == 16 | Age == 17 ~ \"16 to 17\",\n            Age >= 18 ~ \"greater than or equal to 18\"))"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "href": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "title": "Homework 1",
    "section": "Summarize mean lung capacities by age group and smoking status",
    "text": "Summarize mean lung capacities by age group and smoking status\n\n\nCode\nLungCapData_AgeGroups %>%\ngroup_by(AgeGroup, Smoke) %>%\nsummarize(MeanLungCap = mean(LungCap), n = n())\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   AgeGroup [4]\n  AgeGroup                    Smoke MeanLungCap     n\n  <chr>                       <chr>       <dbl> <int>\n1 14 to 15                    no           9.14   105\n2 14 to 15                    yes          8.39    15\n3 16 to 17                    no          10.5     77\n4 16 to 17                    yes          9.38    20\n5 greater than or equal to 18 no          11.1     65\n6 greater than or equal to 18 yes         10.5     15\n7 less than or equal to 13    no           6.36   401\n8 less than or equal to 13    yes          7.20    27"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "href": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "title": "Homework 1",
    "section": "Calculate correlation and covariance between lung capacity and age",
    "text": "Calculate correlation and covariance between lung capacity and age\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 8.738289\n\n\nSince the correlation coefficient is close to 1, there is a high degree of correlation between lung capacity and age. The covariance of 8.7, being a positive number, indicates that as age increases, lung capacity increases."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-data-frame",
    "href": "posts/HW1_KarenDetter.html#create-data-frame",
    "title": "Homework 1",
    "section": "Create data frame",
    "text": "Create data frame\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nFreq <- c(128,434,160,64,24)\nPrisonerData <- data.frame (PriorConv, Freq)\nPrisonerData\n\n\n  PriorConv Freq\n1         0  128\n2         1  434\n3         2  160\n4         3   64\n5         4   24"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has == 2 prior convictions",
    "text": "Calculate probability that an inmate has == 2 prior convictions\nprobability = frequency/n\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has < 2 prior convictions",
    "text": "Calculate probability that an inmate has < 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n\n\n\nCode\n(128/810) + (434/810)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has <= 2 prior convictions",
    "text": "Calculate probability that an inmate has <= 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n + frequency(2)/n\n\n\nCode\n(128/810) + (434/810) + (160/810)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has > 2 prior convictions",
    "text": "Calculate probability that an inmate has > 2 prior convictions\nprobability = frequency(3)/n + frequency(4)/n\n\n\nCode\n(64/810) + (24/810)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate expected value for number of prior convictions",
    "text": "Calculate expected value for number of prior convictions"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "href": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "title": "Homework 1",
    "section": "Create a matrix of prior conviction values and their probabilities",
    "text": "Create a matrix of prior conviction values and their probabilities\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nProbs <- c(0.1580247, 0.5358025, 0.1975309, 0.07901235, 0.02962963)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "title": "Homework 1",
    "section": "Calculate expected value",
    "text": "Calculate expected value\n\n\nCode\nc(PriorConv %*% Probs)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate variance and standard deviation for prior convictions",
    "text": "Calculate variance and standard deviation for prior convictions\n\n\nCode\nvar(PriorConv)\n\n\n[1] 2.5\n\n\nCode\nsd(PriorConv)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#double-check-values",
    "href": "posts/HW1_KarenDetter.html#double-check-values",
    "title": "Homework 1",
    "section": "Double-check values",
    "text": "Double-check values\n\n\nCode\nsqrt(var(PriorConv)) == sd(PriorConv)\n\n\n[1] TRUE"
  },
  {
    "objectID": "posts/Homework2.html",
    "href": "posts/Homework2.html",
    "title": "Homework 2 - Emily Duryea",
    "section": "",
    "text": "Uploading packages to be used for this assignment:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\n\n\n\nBelow is the code used for setting up the degrees of freedom (df = sample size - 1). Thus, Bypass df would be 539 - 1 = 538, and, Angiography would be 847 - 1 = 846.\n\n\nCode\nBypass_df = 538\nAngio_df = 846\n\n\nBelow is the code for setting up the mean and standard deviation.\n\n\nCode\nBypass_mean = 19\nBypass_sd = 10\nAngio_mean = 18\nAngio_sd = 9\n\n\nThe code for calculating t-score with 90% confidence interval is below.\n\n\nCode\nBypass_tscore <- qt(p = 0.9, df = Bypass_df)\nAngio_tscore <- qt(p = 0.9, df = Angio_df)\n\n\nThe code for calculating the standard error (sd/sqrt(sample size)) is below.\n\n\nCode\nBypass_se <- Bypass_sd/sqrt(539)\nAngio_se <- Angio_sd/sqrt(847)\n\n\nThe code for calculating the margin of error (T-score multiplied by the standard error) is below.\n\n\nCode\nBypass_me <- Bypass_tscore*Bypass_se\nAngio_me <- Angio_tscore*Angio_se\n\n\nBelow is the code for calculating the upper and lower ranges (add mean to margin of error for upper, subtract for lower).\n\n\nCode\nBypass_low <- Bypass_mean - Bypass_me\nBypass_up <- Bypass_mean + Bypass_me\nAngio_low <- Angio_mean - Angio_me\nAngio_up <- Angio_mean + Angio_me\nBypass <- c(Bypass_low, Bypass_up)\nAngio <- c(Angio_low, Angio_up)\nBypass\n\n\n[1] 18.44732 19.55268\n\n\nCode\nAngio\n\n\n[1] 17.60338 18.39662\n\n\nThe 90% confidence interval for Bypass is [18.45, 19.55], and for Angio it is [17.60, 18.40]. Thus, Angio has the narrower confidence interval, which is logical to conclude because it has a larger sample size (847 > 539), which reduces the margin of error. Additionally, the standard deviation is smaller (9<10), which signifies less variance.\n\n\n\n\n\nCode\n# Number who believed college ed is essential for success\nN <- 567\n\n# Total sample size\nS <- 1031\n\n# Calculating point estimate\nPE <- N/S\n\n# Using the function prop.test() to find the confidence interval range & p-value\nprop.test(N, S, PE)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  N out of S, null probability PE\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval is [0.519, 0.580], with a p-value of 0.550.\n\n\n\n\n\nCode\n# Calculating confidence interval of 95%\nCI95 <- qnorm(0.025, lower.tail = F)\n# Calculating sample size needed using confidence interval equation\nStudent_sample <- ((170*0.25/5)*CI95)^2\nStudent_sample\n\n\n[1] 277.5454\n\n\nBased on these calculations, the needed sample size would be 278 students for a significance level of 5%.\n\n\n\n\n\n\n\nCode\n# Calculating the standard error (sd = 90 sample = 9)\ncompany_se <- 90/sqrt(9)\ncompany_se\n\n\n[1] 30\n\n\nCode\n# Calculating the t-score\ncompany_tscore <- (410-500)/company_se\ncompany_tscore\n\n\n[1] -3\n\n\nCode\n# Calculating the p-value (df = 9-1 = 8)\ncompany_pvalue <- (pt(q=-3, df=8))*2\ncompany_pvalue\n\n\n[1] 0.01707168\n\n\nIt is possible to reject the null hypothesis, as the p-value is statistically significant (0.017), less than 0.05.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or less\nless_company <- pt(-3, 8)\nless_company\n\n\n[1] 0.008535841\n\n\nThe p-value for the lower tail is 0.00854.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or more\nmore_company <- pt(-3, 8, lower.tail = F)\nmore_company\n\n\n[1] 0.9914642\n\n\nThe p-value for the upper tail is 0.991.\n\n\nCode\ntotal_company <- less_company + more_company\ntotal_company\n\n\n[1] 1\n\n\nThe total of both tails is equal to 1.\n\n\n\n\n\n\n\n\nCode\n# Calculating t-scores\nJones_tscore <- (519.5-500)/10\nJones_tscore\n\n\n[1] 1.95\n\n\nCode\nSmith_tscore <- (519.7-500)/10\nSmith_tscore\n\n\n[1] 1.97\n\n\nCode\n# Calculating p-values\nJones_pvalue <- (pt(q=1.95, df=999, lower.tail=FALSE))*2\nJones_pvalue\n\n\n[1] 0.05145555\n\n\nCode\nSmith_pvalue <- (pt(q=1.97, df=999, lower.tail=FALSE))*2\nSmith_pvalue\n\n\n[1] 0.04911426\n\n\n\n\n\nIf the significance level is 0.05, then Smith has statistically significant study findings, while Jones does not.\n\n\n\nBoth of the studies could be statistically significant, depending on the significance level. For example, a 0.1 significance level would mean Jones also had statistically significant results. In this case, since the t-scores were so similar (0.2 off), it would not be unreasonable for both studies to reject the null hypothesis.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# Getting the mean\nmean_gtaxes <- mean(gas_taxes)\nmean_gtaxes\n\n\n[1] 40.86278\n\n\nCode\n# Conducting t-test\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nBecause the p-value is 0.03827 on a 95% confidence interval, on the 0.05 significance level, it is possible the null hypothesis that gas prices are equal to or greater than $0.45."
  },
  {
    "objectID": "posts/FinalProject_EthanCampbell.html",
    "href": "posts/FinalProject_EthanCampbell.html",
    "title": "Final Project",
    "section": "",
    "text": "Extensive research has been done on climate change and economic changes respectively but there is not a significant amount of research about their relation towards one another. There are research papers that touch on this but in different aspects and focus more on other factors like political aspects. I would like to look a little broader and look at the difference between each climate zone and their economic differences. This can be taken with a grain of salt as there are many factors that could effect the economic situation being left out. The data was pulled from NASA’s POWER data access viewer; here I pulled the data by region since pulling the whole country in one go was unavailable. Thus, I will conduct research on each region respectively and then compare the results. This study will be conducted on the Köppen climate classification scale to determine climate types for study. I will keep the this one the first level of the scale as further scaling would take significantly more time to process.\n\n\nThere are three levels to this climate classification the first scale is the 5 main climate groups A(tropical), B(Arid), C(Temperate), D(Continental), and E(Polar), the second layer is the seasonal precipitation type, and the final layer indicates the heat levels. Through this three layer system that was created by Wladimir Köppen in 1884 we are able to accurately dial in on a specific climate type. getwd() \n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Is there a relation between climate zone and economic growth?\nB. Do Southern climates have the largest economic growth?"
  },
  {
    "objectID": "posts/FinalProject_EthanCampbell.html#reading-in-the-data",
    "href": "posts/FinalProject_EthanCampbell.html#reading-in-the-data",
    "title": "Final Project",
    "section": "Reading in the data",
    "text": "Reading in the data\nData was collected\n\n\nCode\n# Reading in all the weather data \n\nAmherst <- read.csv(\"_data/amherst.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/amherst.csv': No such file\nor directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nFlorida <- read.csv(\"_data/flordia.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/flordia.csv': No such file\nor directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nIllinois <- read.csv(\"_data/illinois.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/illinois.csv': No such file\nor directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nMiddle <- read.csv(\"_data/middle.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/middle.csv': No such file\nor directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nNewmexico <- read.csv(\"_data/Newmexico.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/Newmexico.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nNorth <- read.csv(\"_data/North.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/North.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nSouth <- read.csv(\"_data/South.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/South.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nSouthCali <- read.csv(\"_data/SouthCali.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/SouthCali.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nTexas <- read.csv(\"_data/Texas.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/Texas.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nWashington <- read.csv(\"_data/washington.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/washington.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nWestV <- read.csv(\"_data/WestV.csv\", skip = 14)\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/WestV.csv': No such file or\ndirectory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\nAmherst\nHad trouble with pivot_wider since it would split the values up by each name but then would fill in the values with NA for the other sections. This added a ton of NA values that one looked bad and were hard to deal with. I had to go a more manual way and do it for each part of PARAMETER to get the exact number of rows I needed. This stopped the NA values and got them all lined up so it reduced the size of the document from 500k+ rows to 89290 rows. This is huge in terms of running the data and working with it. Finally, I just merged the data together and then I was able to rename all the columns and start regression analysis.\n\n\nCode\n# Bringing all the month columns into one column\nregion <- Amherst %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Amherst' not found\n\n\nCode\n# trying to create a function that would apply to all regions\ntidy_function <- function(region, Tidy_region, t2m, rh2m, wh10m, wh50m, PRECTOTCORR){\n  Tidy_region <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH) %>%\n  filter(PARAMETER == 'PS')\n  Tidy_region <- Tidy_region %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  Tidy_region <- Tidy_region %>%\n  select(PS, YEAR, MONTH, LAT, LON)\n  t2m <- region %>%\n  select(PARAMETER, Month_Average, YEAR) %>%\n  filter(PARAMETER == 'T2M')\n  t2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  t2m <- t2m %>%\n  select(T2M, YEAR)\n  Tidy_region$T2M <- t2m$T2M\n  rh2m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n  rh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  rh2m <- rh2m %>%\n  select(RH2M, YEAR)\n  Tidy_region$RH2M <- rh2m$RH2M\n  wh10m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n  wh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  wh10m <- wh10m %>%\n  select(WS10M, YEAR)\n  Tidy_region$WS10M <- wh10m$WS10M\n  wh50m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n  wh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  wh50m <- wh50m %>%\n  select(WS50M, YEAR)\n  Tidy_region$WS50M <- wh50m$WS50M\n  PRECTOTCORR <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n  PRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  PRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n  Tidy_region$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n  # renaming all the variables to easier to digest names\n  Tidy_region <- Tidy_region %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n}\n\nAmherst <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\n## Getting them in clean looking order\nAmherst <- Amherst %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Amherst' not found\n\n\nCode\nAmherst <- Amherst %>%\n  mutate(Temperature = Temperature* 9/5 + 32)\n\n\nError in is.data.frame(.data): object 'Amherst' not found\n\n\nCode\nview_amherst <- Amherst %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Amherst' not found\n\n\nCode\nkable(view_amherst, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\n# aggregate the months into 1 year\n# Converting abbreviation to normal word\nWeather_region_Amherst$Month <- mapvalues(Weather_region_Amherst$Month, from = c(\"NOV\", \"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEP\", \"OCT\", \"DEC\"), to = c(\"November\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"December\"))\n\n\nError in mapvalues(Weather_region_Amherst$Month, from = c(\"NOV\", \"JAN\", : object 'Weather_region_Amherst' not found\n\n\nCode\n# Change word to numeric value\nWeather_region_Amherst <- Weather_region_Amherst %>%\n  mutate(Month = recode(Month,\n                        January = 1,\n                        February = 2,\n                        March = 3,\n                        April = 4,\n                        May = 5,\n                        June = 6,\n                        July = 7,\n                        August = 8,\n                        September = 9,\n                        October = 10,\n                        November = 11,\n                        December = 12))\n\n\nError in is.data.frame(.data): object 'Weather_region_Amherst' not found\n\n\nCode\n# Changing from year month to date column\n\nWeather_region_Amherst$Date <- with(Weather_region_Amherst, ym(sprintf('%04d%02d', Year, Month)))\n\n\nError in with(Weather_region_Amherst, ym(sprintf(\"%04d%02d\", Year, Month))): object 'Weather_region_Amherst' not found\n\n\nCode\nYear <- format(as.Date(Weather_region_Amherst$Date), format = \"%Y\")\n\n\nError in as.Date(Weather_region_Amherst$Date): object 'Weather_region_Amherst' not found\n\n\nCode\nWeather_region_Amherst %>%\n  group_by(Latitude, Longitude) %>%\n  mutate(Annual_Temperature = mean(Temperature),\n      Annual_Humidity = mean(Humidity),\n      Annual_Precipitation = sum(Precipitation))\n\n\nError in group_by(., Latitude, Longitude): object 'Weather_region_Amherst' not found\n\n\nCode\n# creating the mean value for each long and lat for each variable\nMeans_variables <- ddply(Weather_region_Amherst, .(Year, Latitude, Longitude), summarise,\n      Annual_Temperature = mean(Temperature),\n      Annual_Humidity = mean(Humidity),\n      Annual_Precipitation = sum(Precipitation),\n      Average_Pressure = mean(Surface_Pressure),\n      Average_Wind_10Meter = mean(Wind_10_meter),\n      Average_Wind_50Meter = mean(Wind_50_meter))\n\n\nError in empty(.data): object 'Weather_region_Amherst' not found\n\n\nCode\n# Creating the regions based on longitude and latitude \nWeather_region_Amherst <- Means_variables %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Means_variables' not found\n\n\nCode\ndata <- merge(Means_variables, Economy)\n\n\nError in merge(Means_variables, Economy): object 'Means_variables' not found\n\n\nCode\ndata <- data %>%\n  distinct(Annual_Temperature, Annual_Humidity, Annual_Precipitation, Year_Money_Millions)\n\n\nError in UseMethod(\"distinct\"): no applicable method for 'distinct' applied to an object of class \"function\"\n\n\nCode\ncor(data$Year_Money_Millions ~ data$Annual_Temperature)\n\n\nError in cor(data$Year_Money_Millions ~ data$Annual_Temperature): supply both 'x' and 'y' or a matrix-like 'x'\n\n\nCode\ntest <- lm(Year_Money_Millions ~ log(Annual_Temperature), data = data)\n\n\nError in model.frame.default(formula = Year_Money_Millions ~ log(Annual_Temperature), : 'data' must be a data.frame, environment, or list\n\n\nCode\nsummary(test)\n\n\nError in summary(test): object 'test' not found\n\n\nCode\nplot(test)\n\n\nError in plot(test): object 'test' not found\n\n\nCode\ncorrplot(data)\n\n\nError in corrplot(data): could not find function \"corrplot\"\n\n\nCode\nplot(Year_Money_Millions ~ Annual_Temperature + Annual_Humidity + Annual_Precipitation, \n     data = data, \n     col = \"steelblue\", \n     pch = 20, \n     xlim = c(0, 100),\n     cex.main = 0.9,\n     main = \"Percentage of English language learners\")\n\n\nError in FUN(X[[i]], ...): invalid 'envir' argument of type 'closure'\n\n\nCode\n# Creating weather types\n\n# Creating the 4 regions \n\n\n\n\nFlorida\n\n\nCode\nregion <- Florida %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Florida' not found\n\n\nCode\nFlorida <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nFlorida <- Florida %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Florida' not found\n\n\nCode\nsummary(Florida)\n\n\nError in summary(Florida): object 'Florida' not found\n\n\nCode\nview_Florida <- Florida %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Florida' not found\n\n\nCode\nkable(view_Florida, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Florida Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Florida <- Florida %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Florida' not found\n\n\nCode\nWeather_region_Florida\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Florida' not found\n\n\n\n\nIllinois\n\n\nCode\nregion <- Illinois %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Illinois' not found\n\n\nCode\nIllinois <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nIllinois <- Illinois %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Illinois' not found\n\n\nCode\nsummary(Illinois)\n\n\nError in summary(Illinois): object 'Illinois' not found\n\n\nCode\nview_Illinois <- Illinois %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Illinois' not found\n\n\nCode\nkable(view_Illinois, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Illinois Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Illinois <- Illinois %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Illinois' not found\n\n\nCode\nWeather_region_Illinois\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Illinois' not found\n\n\n\n\nMiddle\n\n\nCode\nregion <- Middle %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Middle' not found\n\n\nCode\nMiddle <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nMiddle <- Middle %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Middle' not found\n\n\nCode\nsummary(Middle)\n\n\nError in summary(Middle): object 'Middle' not found\n\n\nCode\nview_Middle <- Middle %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Middle' not found\n\n\nCode\nkable(view_Middle, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Middle Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Middle <- Middle %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Middle' not found\n\n\nCode\nWeather_region_Middle\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Middle' not found\n\n\n\n\nNew Mexico\n\n\nCode\nregion <- Newmexico %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Newmexico' not found\n\n\nCode\nNew_Mexico <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nNew_Mexico <- New_Mexico %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'New_Mexico' not found\n\n\nCode\nsummary(New_Mexico)\n\n\nError in summary(New_Mexico): object 'New_Mexico' not found\n\n\nCode\nview_Newmexico <- New_Mexico %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'New_Mexico' not found\n\n\nCode\nkable(view_Newmexico, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Newmexico <- New_Mexico %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'New_Mexico' not found\n\n\nCode\nWeather_region_Newmexico\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Newmexico' not found\n\n\n\n\nNorth\n\n\nCode\nregion <- North %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'North' not found\n\n\nCode\nNorth <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nNorth <- North %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'North' not found\n\n\nCode\nsummary(North)\n\n\nError in summary(North): object 'North' not found\n\n\nCode\nview_North <- North %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'North' not found\n\n\nCode\nkable(view_North, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Northern Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_North <- North %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'North' not found\n\n\nCode\nWeather_region_North\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_North' not found\n\n\n\n\nSouth\n\n\nCode\nregion <- South %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'South' not found\n\n\nCode\nSouth <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nSouth <- South %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'South' not found\n\n\nCode\nsummary(South)\n\n\nError in summary(South): object 'South' not found\n\n\nCode\nview_South <- South %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'South' not found\n\n\nCode\nkable(view_South, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Southern Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_South <- South %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'South' not found\n\n\nCode\nWeather_region_South\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_South' not found\n\n\n\n\nSouth California\n\n\nCode\nregion <- SouthCali %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'SouthCali' not found\n\n\nCode\nSouth_California <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nSouth_California <- South_California %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'South_California' not found\n\n\nCode\nsummary(South_California)\n\n\nError in summary(South_California): object 'South_California' not found\n\n\nCode\nview_SouthCali <- South_California %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'South_California' not found\n\n\nCode\nkable(view_SouthCali, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"South California Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_SouthCali <- South_California %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'South_California' not found\n\n\nCode\nWeather_region_SouthCali\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_SouthCali' not found\n\n\n\n\nTexas\n\n\nCode\nregion <- Texas %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Texas' not found\n\n\nCode\nTexas <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nTexas <- Texas %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Texas' not found\n\n\nCode\nsummary(Texas)\n\n\nError in summary(Texas): object 'Texas' not found\n\n\nCode\nview_Texas <- Texas %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Texas' not found\n\n\nCode\nkable(view_Texas, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Texas Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Texas <- Texas %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Texas' not found\n\n\nCode\nWeather_region_Texas\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Texas' not found\n\n\n\n\nWashington\n\n\nCode\nregion <- Washington %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Washington' not found\n\n\nCode\nWashington <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nWashington <- Washington %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Washington' not found\n\n\nCode\nsummary(Washington)\n\n\nError in summary(Washington): object 'Washington' not found\n\n\nCode\nview_Washington <- Washington %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'Washington' not found\n\n\nCode\nkable(view_Washington, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Washington Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_Washington <- Washington %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Washington' not found\n\n\nCode\nWeather_region_Washington\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_Washington' not found\n\n\n\n\nWest Virgina\n\n\nCode\nregion <- WestV %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'WestV' not found\n\n\nCode\nWest_Virginia <- tidy_function(region)\n\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH): object 'region' not found\n\n\nCode\nWest_Virginia <- West_Virginia %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'West_Virginia' not found\n\n\nCode\nsummary(West_Virginia)\n\n\nError in summary(West_Virginia): object 'West_Virginia' not found\n\n\nCode\nview_WestV <- West_Virginia %>%\n  slice(1:10)\n\n\nError in slice(., 1:10): object 'West_Virginia' not found\n\n\nCode\nkable(view_WestV, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"West Virginia Data\") %>%\n  kable_styling(font_size = 16)\n\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\nCode\nWeather_region_WestV <- West_Virginia %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'West_Virginia' not found\n\n\nCode\nWeather_region_WestV\n\n\nError in eval(expr, envir, enclos): object 'Weather_region_WestV' not found\n\n\n\n\nState Economy data\nThe economic data is pulled from the Bureau of Economic Analysis (Analysis, n.d.). This data is the ins, outs, and the difference between the former two in income by state. The data ranges from 1990 to 2020 and covers every state in the US.\n\n\nCode\n# Reading in economic data\n\nEconomy <- read.csv(\"_data/Economy.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file '_data/Economy.csv': No such file\nor directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\n# Renaming the columns to remove the X\nEconomy <- Economy %>%\n  dplyr::rename('1990' = X1990) %>%\n  dplyr::rename('1991' = X1991) %>%\n  dplyr::rename('1992' = X1992) %>%\n  dplyr::rename('1993' = X1993) %>%\n  dplyr::rename('1994' = X1994) %>%\n  dplyr::rename('1995' = X1995) %>%\n  dplyr::rename('1996' = X1996) %>%\n  dplyr::rename('1997' = X1997) %>%\n  dplyr::rename('1998' = X1998) %>%\n  dplyr::rename('1999' = X1999) %>%\n  dplyr::rename('2000' = X2000) %>%\n  dplyr::rename('2001' = X2001) %>%\n  dplyr::rename('2002' = X2002) %>%\n  dplyr::rename('2003' = X2003) %>%\n  dplyr::rename('2004' = X2004) %>%\n  dplyr::rename('2005' = X2005) %>%\n  dplyr::rename('2006' = X2006) %>%\n  dplyr::rename('2007' = X2007) %>%\n  dplyr::rename('2008' = X2008) %>%\n  dplyr::rename('2009' = X2009) %>%\n  dplyr::rename('2010' = X2010) %>%\n  dplyr::rename('2011' = X2011) %>%\n  dplyr::rename('2012' = X2012) %>%\n  dplyr::rename('2013' = X2013) %>%\n  dplyr::rename('2014' = X2014) %>%\n  dplyr::rename('2015' = X2015) %>%\n  dplyr::rename('2016' = X2016) %>%\n  dplyr::rename('2017' = X2017) %>%\n  dplyr::rename('2018' = X2018) %>%\n  dplyr::rename('2019' = X2019) %>%\n  dplyr::rename('2020' = X2020) \n\n\nError in dplyr::rename(., `1990` = X1990): object 'Economy' not found\n\n\nCode\n# Pivoting to combine all the years into one column\nEconomy <- Economy %>%\npivot_longer(\n  cols = c('1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'),\n  names_to = \"Year\",\n  values_to = \"Yearly_Fianace\",\n)\n\n\nError in pivot_longer(., cols = c(\"1990\", \"1991\", \"1992\", \"1993\", \"1994\", : object 'Economy' not found\n\n\nCode\n## Change from char to numeric\nEconomy$Year <- as.numeric(Economy$Year)\n\n\nError in eval(expr, envir, enclos): object 'Economy' not found\n\n\nCode\n# Changing the finance column to be in millions\nEconomy <- Economy %>%\n  mutate(Year_Money_Millions = Yearly_Fianace/1000)\n\n\nError in is.data.frame(.data): object 'Economy' not found\n\n\nCode\nEconomy <- Economy %>%\n  select(State, Year, Year_Money_Millions, Description) %>%\n  filter(Description == \"Adjustment for residence\")\n\n\nError in select(., State, Year, Year_Money_Millions, Description): object 'Economy' not found\n\n\nCode\nEconomy <- Economy %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Economy' not found"
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html",
    "href": "posts/FinalProjectPart1_DonnySnyder.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question",
    "text": "Research Question\nAffective polarization describes a heightened state of animosity between partisans that has steadily grown from the 1970s to today (Iyengar et al., 2019). Identifying antecedents of affective polarization is essential to creating intervention strategies into this negative state of politics. Levendusky (2009) proposes a social model where individuals making sense of simplified elite cues enables people to understand the relevant identities of the political landscape, which may lead to downstream affective polarization. I intend to expand on this model, testing a construct of construal level, or the level of abstraction to concreteness (Trope & Liberman, 2010) with which partisans perceive partisan groups and group cues. Prior studies suggest that lower construal may serve as an antecedent to affective polarization when partisans view issues in more concrete, group terms (Snyder, Unpublished). This study will expand these models into extant, large scale, political science datasets. Additionally, this project will employ supervised machine learning models to qualitatively code a large-n sample of free response questions."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "title": "Final Project Part 1",
    "section": "Hypotheses",
    "text": "Hypotheses\nI hypothesize that partisans who are qualitatively coded as having a lower construal level will demonstrate higher levels of group/affective polarization, as measured on a feeling thermometer or measures of feelings about political groups - whichever is available in the datasets.\nI hypothesize that using a sentiment analysis, these tendencies may be moderated by valence of their free response, with stronger valence enhancing the effect of construal level on affective polarization."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "title": "Final Project Part 1",
    "section": "Datasets",
    "text": "Datasets\nI intend to use ANES and/or NAES free response data to provide an initial exploratory analysis. I will qualitatively code these data using a novel construal level paradigm (Snyder, unpublished). i will then use this qualitative coding process to train a supervised machine learning algorithm."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). The origins and consequences of affective polarization in the United States. Annual Review of Political Science, 22(1), 129-146. Levendusky, M. (2009). The partisan sort: How liberals became Democrats and conservatives became Republicans. University of Chicago Press. Snyder, D. (2022). Keep It Simple Stupid: How Individual Differences in Cue Construal Explain Variations in Affective Polarization. Unpublished Manuscript Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. Psychological review, 117(2), 440."
  },
  {
    "objectID": "posts/HW_1_603.html",
    "href": "posts/HW_1_603.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_603.html#question-1",
    "href": "posts/HW_1_603.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nLung_data<- read_excel(\"C:/Users/manik/Desktop/LungCapData.xls\")\n\n\nError: `path` does not exist: 'C:/Users/manik/Desktop/LungCapData.xls'\n\n\nCode\nLung_data\n\n\nError in eval(expr, envir, enclos): object 'Lung_data' not found\n\n\nGiven data consists of 725 rows and 6 columns\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nLung_data %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram() +\n  geom_density(color = \"Red\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lung_data' not found\n\n\nBased on above histogram , we can say the distribution is very close to the normal distribution\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nLung_data %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lung_data' not found\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nMean_smokers <- Lung_data %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lung_data' not found\n\n\nCode\nMean_smokers\n\n\nError in eval(expr, envir, enclos): object 'Mean_smokers' not found\n\n\nThe mean of the lung capacity who smokes is greater than the people who doesnt smoke which doesnt make any sense in practical\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nLung_data <- mutate(Lung_data, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lung_data, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lung_data' not found\n\n\nCode\nLung_data %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + coord_flip()\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lung_data' not found\n\n\nCode\n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n$y\n[1] \"Lung Capacity\"\n\n$x\n[1] \"Frequency\"\n\n$title\n[1] \"Relationship of LungCap and Smoke based on age categories\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\nLung_data %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lung_data' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke.\nCalculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nCovariance_LA <- cov(Lung_data$LungCap, Lung_data$Age)\n\n\nError in is.data.frame(y): object 'Lung_data' not found\n\n\nCode\nCorrelation_LA <- cor(Lung_data$LungCap, Lung_data$Age)\n\n\nError in is.data.frame(y): object 'Lung_data' not found\n\n\nCode\nCovariance_LA\n\n\nError in eval(expr, envir, enclos): object 'Covariance_LA' not found\n\n\nCode\nCorrelation_LA\n\n\nError in eval(expr, envir, enclos): object 'Correlation_LA' not found\n\n\nFrom the above result we can say that both covariance and correlation is positive and which indicates direct relationship that means Lungcapacity increases as age increases"
  },
  {
    "objectID": "posts/HW_1_603.html#question-2",
    "href": "posts/HW_1_603.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nIP\n\n\n\n\n  \n\n\n\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nIP %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\np_2 <- IP %>%\n  filter(Prior_convitions < 2)\nsum(p_2$Probability)\n\n\n[1] 0.6938272\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\np <- IP %>%\n  filter(Prior_convitions <= 2)\nsum(p$Probability)\n\n\n[1] 0.891358\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nP_3 <- IP %>%\n  filter(Prior_convitions > 2)\nsum(P_3$Probability)\n\n\n[1] 0.108642\n\n\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nIP <- mutate(IP, Wm = Prior_convitions*Probability)\nexpe<- sum(IP$Wm)\nexpe\n\n\n[1] 1.28642\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar_ <-sum(((IP$Prior_convitions-expe)^2)*IP$Probability)\nvar_\n\n\n[1] 0.8562353\n\n\nstandard deviation:\n\n\nCode\nsqrt(var_)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html",
    "href": "posts/HW2_ManiGogula.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-1",
    "href": "posts/HW2_ManiGogula.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-2",
    "href": "posts/HW2_ManiGogula.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-3",
    "href": "posts/HW2_ManiGogula.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-4",
    "href": "posts/HW2_ManiGogula.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a",
    "href": "posts/HW2_ManiGogula.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "href": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-p-value",
    "href": "posts/HW2_ManiGogula.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b",
    "href": "posts/HW2_ManiGogula.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c",
    "href": "posts/HW2_ManiGogula.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-5",
    "href": "posts/HW2_ManiGogula.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a-1",
    "href": "posts/HW2_ManiGogula.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b-1",
    "href": "posts/HW2_ManiGogula.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c-1",
    "href": "posts/HW2_ManiGogula.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-6",
    "href": "posts/HW2_ManiGogula.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW3_CalebHill.html",
    "href": "posts/HW3_CalebHill.html",
    "title": "Homework 3",
    "section": "",
    "text": "First, let’s load the relevant libraries and set all the graph themes to minimal.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\ntheme_minimal()\n\n\nList of 94\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : NULL\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : NULL\n $ panel.grid.minor          :List of 6\n  ..$ colour       : NULL\n  ..$ linewidth    : 'rel' num 0.5\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.clip                : chr \"inherit\"\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\nFor Question 1, let’s load in the UN11 data-set and view the first few variables to verify we have loaded in the data.\n\n\nCode\ndata(UN11)\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nFor the sake of the following two subsections, the predictor is “ppgdp” and the response is “fertility.” We are attempting to predict fertility but a country’s PPGDP.\n\n\n\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) +\n  geom_point()\n\n\n\n\n\nA straight-line mean function does seem possible if you remove ppgdp that is less than $5,000. Otherwise, there is a sharp concentration of points at this range that might distort a straight-line mean.\n\n\n\n\n\nCode\nggplot(UN11, aes(log(ppgdp), log(fertility))) +\n  geom_point() + \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\nThe simple linear regression would be much more plausible with this graph. We now see a negative association between fertility and ppgdp.The higher the fertility, the lower the ppgdp, and vice-versa."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#a",
    "href": "posts/HW3_CalebHill.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nggplot(UN11, aes(log(ppgdp*1.33), log(fertility))) +\n  geom_point() +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\nI don’t think the slope has changed, but we can call a quick lm to see.\n\n\nCode\nlm(log(ppgdp) ~ log(fertility), UN11)\n\n\n\nCall:\nlm(formula = log(ppgdp) ~ log(fertility), data = UN11)\n\nCoefficients:\n   (Intercept)  log(fertility)  \n        10.780          -2.539  \n\n\nCode\nlm(log(ppgdp*1.33) ~ log(fertility), UN11)\n\n\n\nCall:\nlm(formula = log(ppgdp * 1.33) ~ log(fertility), data = UN11)\n\nCoefficients:\n   (Intercept)  log(fertility)  \n        11.065          -2.539  \n\n\nA little for the intercept, but not when mapped onto the outcome variable (ppgdp does not change its effect on fertility when currency changes)."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#b",
    "href": "posts/HW3_CalebHill.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nTherefore, the correlation does not change when adjusted from US dollars to British pounds."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#a-1",
    "href": "posts/HW3_CalebHill.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\ndata(student.survey)\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\nCode\nggplot(student.survey, aes(re, pi)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nggplot(student.survey, aes(tv, hi)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDue to the categorical nature for (i), it is very difficult to draw conclusions from this graph on how the explanatory variable relates to the outcome variable. There is a hint of a positive relationship, but that is just how the graph is coded. If political ideology was flipped (liberal to conservative from top to bottom), we would see an inverse relationship. Conservatives seem more likely to attend every week, while liberals never do.\nFor (ii), we see that as an individual observes more tv per week, the lower the high school GPA is. If we removed the outliers, we might see a regression line where the explanatory variable explains more for the outcome variable, but that is not within the scope of this homework, nor is it immediately a best practice."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#b-1",
    "href": "posts/HW3_CalebHill.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nlm(re ~ pi, student.survey)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\n\nCall:\nlm(formula = re ~ pi, data = student.survey)\n\nCoefficients:\n(Intercept)         pi.L         pi.Q         pi.C         pi^4         pi^5  \n     2.6071       2.0552       0.4501       0.1361      -0.2283      -0.3046  \n       pi^6  \n     0.4565  \n\n\nCode\nlm (tv ~ hi, student.survey)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nCoefficients:\n(Intercept)           hi  \n     20.200       -3.909  \n\n\nThe LM models are a little difficult to interpret, but I shall attempt to explain.\nFor (i), the outcome variable (pi), is explained by the intercept codes. So a change in re (religiosity), accounts for varying differences for pi (political ideology). This can be a massive change, such as 2.05 for L or -0.30 for pi^5. I attempted to switch the variables in the LM model, as I was unsure if this was correct, and the issue persisted due to the categorical nature of the simple linear regression. Either I am still completing this wrong or the model is incorrect for the data provided - better a logistic regression perhaps.\nFor (ii), one hour of TV on average explains a change of -3.91 in high school GPA. This shows a strong negative association between the two variables."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html",
    "href": "posts/HW2_ManiShankerKamarapu.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "href": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "title": "Homework 2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a",
    "href": "posts/HW2_ManiShankerKamarapu.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b",
    "href": "posts/HW2_ManiShankerKamarapu.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c",
    "href": "posts/HW2_ManiShankerKamarapu.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html",
    "href": "posts/HW3_Yakub Rabiutheen.html",
    "title": "Homework 3",
    "section": "",
    "text": "The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\n\nCode\n##load data\ndata(UN11)"
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#question-2.b",
    "href": "posts/HW3_Yakub Rabiutheen.html#question-2.b",
    "title": "Homework 3",
    "section": "Question-2.b",
    "text": "Question-2.b\nHow, if at all, does the correlation change?\n\n\nCode\ncor.test(usdollar,pound)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  usdollar and pound\nt = 189812531, df = 8, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1 \n\n\nCurrency Changes do not affect correlation."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#question-3",
    "href": "posts/HW3_Yakub Rabiutheen.html#question-3",
    "title": "Homework 3",
    "section": "Question-3",
    "text": "Question-3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water_supply,main = \"Sierra Southern California Water Supply Runoff\",\n      pch = 21, bg = \"green\")\n\n\nError in pairs(water_supply, main = \"Sierra Southern California Water Supply Runoff\", : object 'water_supply' not found\n\n\n\n\nCode\nlm_water_supply<-lm(BSAAM~APMAM+APSAB+APSLAKE+OPBPC+OPRC+OPSLAKE,data = water)\nsummary(lm_water_supply)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nThe following variables OPBPC, OPRC, OPSLAKE are correlated with each other."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#q.4",
    "href": "posts/HW3_Yakub Rabiutheen.html#q.4",
    "title": "Homework 3",
    "section": "Q.4",
    "text": "Q.4\n\n\nCode\ndata(\"Rateprof\")\npairs(~Rateprof$quality+Rateprof$helpfulness+Rateprof$clarity+Rateprof$easiness+Rateprof$raterInterest, lwd=2, labels = c(\"QUALITY\", \"HELPFULNESS\", \"CLARITY\", \"EASINESS\", \"Rater INTEREST\"), pch=19, cex = 0.75, col = \"blue\")\n\n\n\n\n\nThe following variables “quality”, “clarity”, and “helpfulnes are correlated with each other."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#q.5",
    "href": "posts/HW3_Yakub Rabiutheen.html#q.5",
    "title": "Homework 3",
    "section": "Q.5",
    "text": "Q.5\n\n\nCode\n##load data\ndata(student.survey)\npi_conv <- as.numeric(student.survey$pi)\nre_conv <- as.numeric(student.survey$re)\n##run regression analysis\nmodel1 <- lm(pi_conv ~ re_conv, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_conv ~ re_conv, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre_conv       0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nCode\n##run regression analysis\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\nCode\nlibrary(smss)\ndata(\"student.survey\")\nggplot(data=student.survey,aes(x=re,fill=pi))+\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\n\n\n\n\n\nAs shown in the graph,there is a strong correlation association between religiosity and Political Idealogy.\n\n\nCode\ndata(\"student.survey\")\nggplot(data=student.survey,aes(x=hi, y=tv)) +\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")  \n\n\n\n\n\nThere is very little relationship betweeh watching TV and High School GPA.\n\n\nCode\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n\n                     pi                re           hi              tv        \n very liberal         : 8   never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24   occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10   every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                     3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                     Max.   :4.000   Max.   :37.000  \n very conservative    : 2"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html",
    "href": "posts/EmmaRasmussenFinalPart1.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(googlesheets4)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "href": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question:",
    "text": "Research Question:\nDoes political partisanship correlate with COVID-19 death rates?\nThe COVID-19 pandemic became a political matter. Behaviors associated with COVID-19 prevention were adopted on partisan lines (masking, social distancing, and vaccine uptake). Early in the pandemic, mask mandates were protested in some communities. My research question is have these behaviors affected COVID-19 death rates along partisan lines? If so, public health interventions could target communities that may be higher risk for COVID-19 deaths based on political partisanship.\nI am thinking death toll would make the most sense to measure than infection rates as infection rates are constantly changing (other studies have looked at infection rates over waves of the pandemic, see this study from the Pew Research Center (Jones 2022)). I also think that one way to measure partisanship will be the 2020 county-level election results (% voting for Trump). In other words, my research is looking to see if (county-level) Trump support correlates with COVID-19 death rates. Both these variables can be found in county-level data sets so I can join multiple dataset with county name (or FIPS code) as the “key”.\nOther variables to consider at the county-level (confounding variables): vaccine (and booster) uptake, average age of population"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "href": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "title": "Final Project Part 1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\nWhile I came up with this research idea on my own, other organizations such as NPR (Wood and Brumfiel 2021) and the Pew Research Center ()have already tested this. For this project, I will use the most recent data I can find. I was hoping to consider the confounding variable of population density, for instance I am guessing more urban populations will tend to vote democratic but these more densely populated places may also have higher infection rates. However, I cannot find any county level population density data sets, so I may use the “Urban Rural Description” variable in one of my datasets.\nH0: B1 (and all beta values) is zero. There is no correlation Ha: B1 (or any beta value) is not zero. There is a correlation between partisanship and COVID-19 death rates."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "href": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics:",
    "text": "Descriptive Statistics:\n\n#Reading in the data from google sheets\ngs4_deauth()\n\nvotedf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1fmxoA_bibvsxsvgRdVPCgMA7DkmJNZfxiWgLgCLcsOY/edit#gid=937778872\")\n\ncoviddf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1Hy2O3HxhZGF_fhu6jgmoC2ibWwJTlI7pQOESBOd4hTU/edit#gid=787918384\")\n\n\n#Changing fips code to character format and adding in leading zeros\ncoviddf$\"FIPS Code\" <- as.character(coviddf$\"FIPS Code\")\ncoviddf<-mutate(coviddf, FIPSNEW=str_pad(coviddf$\"FIPS Code\", 5, pad = \"0\"))\nhead(coviddf, 12)\n\n# A tibble: 12 × 22\n   `Data as of`        `Start Date`        `End Date`          State County Na…¹\n   <dttm>              <dttm>              <dttm>              <chr> <chr>      \n 1 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 2 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 3 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 4 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 5 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 6 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 7 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 8 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 9 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n10 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n11 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n12 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n# … with 17 more variables: `Urban Rural Code` <dbl>, `FIPS State` <dbl>,\n#   `FIPS County` <dbl>, `FIPS Code` <chr>, Indicator <chr>,\n#   `Total deaths` <dbl>, `COVID-19 Deaths` <dbl>, `Non-Hispanic White` <dbl>,\n#   `Non-Hispanic Black` <dbl>,\n#   `Non-Hispanic American Indian or Alaska Native` <dbl>,\n#   `Non-Hispanic Asian` <dbl>,\n#   `Non-Hispanic Native Hawaiian or Other Pacific Islander` <dbl>, …\n# ℹ Use `colnames()` to see all variable names\n\nvotedf$county_fips <- as.character(votedf$county_fips)\nvotedf<-mutate(votedf, county_fipsNEW=str_pad(votedf$county_fips, 5, pad = \"0\"))\nhead(votedf, 12)\n\n# A tibble: 12 × 13\n    year state   state_po county_…¹ count…² office candi…³ party candi…⁴ total…⁵\n   <dbl> <chr>   <chr>    <chr>     <chr>   <chr>  <chr>   <chr>   <dbl>   <dbl>\n 1  2000 ALABAMA AL       AUTAUGA   1001    US PR… AL GORE DEMO…    4942   17208\n 2  2000 ALABAMA AL       AUTAUGA   1001    US PR… GEORGE… REPU…   11993   17208\n 3  2000 ALABAMA AL       AUTAUGA   1001    US PR… RALPH … GREEN     160   17208\n 4  2000 ALABAMA AL       AUTAUGA   1001    US PR… OTHER   OTHER     113   17208\n 5  2000 ALABAMA AL       BALDWIN   1003    US PR… AL GORE DEMO…   13997   56480\n 6  2000 ALABAMA AL       BALDWIN   1003    US PR… GEORGE… REPU…   40872   56480\n 7  2000 ALABAMA AL       BALDWIN   1003    US PR… RALPH … GREEN    1033   56480\n 8  2000 ALABAMA AL       BALDWIN   1003    US PR… OTHER   OTHER     578   56480\n 9  2000 ALABAMA AL       BARBOUR   1005    US PR… AL GORE DEMO…    5188   10395\n10  2000 ALABAMA AL       BARBOUR   1005    US PR… GEORGE… REPU…    5096   10395\n11  2000 ALABAMA AL       BARBOUR   1005    US PR… RALPH … GREEN      46   10395\n12  2000 ALABAMA AL       BARBOUR   1005    US PR… OTHER   OTHER      65   10395\n# … with 3 more variables: version <dbl>, mode <chr>, county_fipsNEW <chr>, and\n#   abbreviated variable names ¹​county_name, ²​county_fips, ³​candidate,\n#   ⁴​candidatevotes, ⁵​totalvotes\n# ℹ Use `colnames()` to see all variable names\n\n\n\nsummary(votedf)\n\n      year         state             state_po         county_name       \n Min.   :2000   Length:72617       Length:72617       Length:72617      \n 1st Qu.:2004   Class :character   Class :character   Class :character  \n Median :2012   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2011                                                           \n 3rd Qu.:2020                                                           \n Max.   :2020                                                           \n county_fips           office           candidate            party          \n Length:72617       Length:72617       Length:72617       Length:72617      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n candidatevotes      totalvotes         version             mode          \n Min.   :      0   Min.   :      0   Min.   :20220315   Length:72617      \n 1st Qu.:    115   1st Qu.:   5175   1st Qu.:20220315   Class :character  \n Median :   1278   Median :  11194   Median :20220315   Mode  :character  \n Mean   :  10782   Mean   :  42514   Mean   :20220315                     \n 3rd Qu.:   5848   3rd Qu.:  29855   3rd Qu.:20220315                     \n Max.   :3028885   Max.   :4264365   Max.   :20220315                     \n county_fipsNEW    \n Length:72617      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nsummary(coviddf)\n\n   Data as of           Start Date            End Date         \n Min.   :2022-10-05   Min.   :2020-01-01   Min.   :2022-10-01  \n 1st Qu.:2022-10-05   1st Qu.:2020-01-01   1st Qu.:2022-10-01  \n Median :2022-10-05   Median :2020-01-01   Median :2022-10-01  \n Mean   :2022-10-05   Mean   :2020-01-01   Mean   :2022-10-01  \n 3rd Qu.:2022-10-05   3rd Qu.:2020-01-01   3rd Qu.:2022-10-01  \n Max.   :2022-10-05   Max.   :2020-01-01   Max.   :2022-10-01  \n                                                               \n    State           County Name        Urban Rural Code   FIPS State   \n Length:3495        Length:3495        Min.   :1.000    Min.   : 1.00  \n Class :character   Class :character   1st Qu.:2.000    1st Qu.:18.00  \n Mode  :character   Mode  :character   Median :4.000    Median :33.00  \n                                       Mean   :3.645    Mean   :30.47  \n                                       3rd Qu.:5.000    3rd Qu.:42.00  \n                                       Max.   :6.000    Max.   :56.00  \n                                                                       \n  FIPS County      FIPS Code          Indicator          Total deaths   \n Min.   :  1.00   Length:3495        Length:3495        Min.   :   621  \n 1st Qu.: 31.00   Class :character   Class :character   1st Qu.:  1690  \n Median : 71.00   Mode  :character   Mode  :character   Median :  3284  \n Mean   : 99.37                                         Mean   :  7163  \n 3rd Qu.:121.00                                         3rd Qu.:  6990  \n Max.   :840.00                                         Max.   :220829  \n                                                                        \n COVID-19 Deaths   Non-Hispanic White Non-Hispanic Black\n Min.   :  101.0   Min.   :0.0270     Min.   :0.0010    \n 1st Qu.:  176.0   1st Qu.:0.6677     1st Qu.:0.0230    \n Median :  364.0   Median :0.8300     Median :0.0690    \n Mean   :  852.7   Mean   :0.7742     Mean   :0.1242    \n 3rd Qu.:  844.0   3rd Qu.:0.9290     3rd Qu.:0.1800    \n Max.   :31013.0   Max.   :1.0000     Max.   :0.7610    \n                   NA's   :3          NA's   :592       \n Non-Hispanic American Indian or Alaska Native Non-Hispanic Asian\n Min.   :0.0000                                Min.   :0.0010    \n 1st Qu.:0.0020                                1st Qu.:0.0070    \n Median :0.0040                                Median :0.0130    \n Mean   :0.0214                                Mean   :0.0261    \n 3rd Qu.:0.0100                                3rd Qu.:0.0280    \n Max.   :0.8610                                Max.   :0.5170    \n NA's   :1701                                  NA's   :1360      \n Non-Hispanic Native Hawaiian or Other Pacific Islander    Hispanic     \n Min.   :0.0000                                         Min.   :0.0030  \n 1st Qu.:0.0000                                         1st Qu.:0.0220  \n Median :0.0010                                         Median :0.0480  \n Mean   :0.0023                                         Mean   :0.0987  \n 3rd Qu.:0.0010                                         3rd Qu.:0.1090  \n Max.   :0.2000                                         Max.   :0.9870  \n NA's   :2183                                           NA's   :740     \n     Other        Urban Rural Description   Footnote           FIPSNEW         \n Min.   :0.0010   Length:3495             Length:3495        Length:3495       \n 1st Qu.:0.0090   Class :character        Class :character   Class :character  \n Median :0.0150   Mode  :character        Mode  :character   Mode  :character  \n Mean   :0.0174                                                                \n 3rd Qu.:0.0220                                                                \n Max.   :0.2410                                                                \n NA's   :1633                                                                  \n\n\nThis data is going to require some tidying before merging. In the coviddf, each county is listed 3 times, (once per indicator) so I will likely filter out just the indicator “Distribution of COVID-19 deaths (%)” so each county is listed only once. Similarly, the votedf contains extra years. For my research, I am only concerned with 2016 data so I will filter out % voting for Trump in 2016 as a measure of political affiliation/partisanship. Then I will merge the two dfs based on county names (will also require some data tidying).\nThe votedf was compiled by the MIT Election Data and Science Lab. It was first published in 2018 and has been updated with the 2020 election. It contains county-level presidential election data beginning in 2000 and going up to the 2020 election. The data has 12 columns, and 72,617 rows (many of which I will filter out before conducting analysis.) There are 1,892 distinct county names in the data set.\nThe coviddf only has 857 unique county names in the data frame. This may be because not all counties reported COVID-19 death counts. When I join the data sets, I will join so as to only include observations that we have information from both data frames. The coviddf is provisional, meaning that it is consistently updated (I believe on a weekly basis) with current COVID-19 death toll data. It is likely compiled by counties/towns reporting these numbers to the CDC. This data has limitations, not all counties report this, and not all report it accurately/ attribute COVID-19 as the true cause of death in all circumstances. Using the summary function, we can see the “mean” COVID-19 deaths by county is 852.7, however this isn’t super meaningful given each county has this reported 3 times in the data and the median is significantly lower. Statistics provided by the summary function will be more meaningful once the data is tidied."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#references",
    "href": "posts/EmmaRasmussenFinalPart1.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nJones, B. (2022). The Changing Political Geography of COVID-19 Over the Last Two Years. Pew Research Center. March 3, 2022. https://www.pewresearch.org/politics/2022/03/03/the-changing-political-geography-of-covid-19-over-the-last-two-years/\nMIT Election Data and Science Lab. (2021) County Presidential Election Returns 2000-2020. Accessed from the Harvard Dataverse [October 11, 2022]. https://doi.org/10.7910/DVN/VOQCHQ\nNational Center for Health Statistics. (2022). Provisional COVID-19 Deaths by County, and Race and Hispanic Origin. Accessed from the Centers for Disease Control [October 11, 2022]. https://data.cdc.gov/d/k8wy-p9cg\nWood, D. and Brumfiel, G. (2021). Pro-Trump counties now have far higher COVID death rates. Misinformation is to blame. NPR. December 5, 2021. https://www.npr.org/sections/health-shots/2021/12/05/1059828993/data-vaccine-misinformation-trump-counties-covid-death-rate\n[Need to add italics to references]"
  },
  {
    "objectID": "posts/KPopiela_HW3.html",
    "href": "posts/KPopiela_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "library(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stats)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-1",
    "href": "posts/KPopiela_HW3.html#question-1",
    "title": "HW3",
    "section": "Question 1",
    "text": "Question 1\n\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n1.1. Identify the predictor and the response.\nPredictor: ppgdp\nResponse: fertility\n\n\n1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\ndata(UN11)\nggplot(data=UN11, aes(x=ppgdp,y=fertility))+geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA straight-line mean function would not be plausible here as the data is not presented linearly\n\n\n1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change\n\nggplot(data = UN11, aes(x=log(ppgdp),y=log(fertility))) + geom_point() +\n  geom_smooth(method=\"lm\",se=FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA simple linear regression model would be much more appropriate"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-2",
    "href": "posts/KPopiela_HW3.html#question-2",
    "title": "HW3",
    "section": "Question 2",
    "text": "Question 2\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n2.1 How, if at all, does the slope of the prediction equation change?\nTo account for the conversion rate from USD to GBP, the response and the slope must be divided by 1.33\n\n\n2.2 How, if at all, does the correlation change?\nCorrelation wouldn’t change in this scenario since it isn’t affected by units of measurement"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-3",
    "href": "posts/KPopiela_HW3.html#question-3",
    "title": "HW3",
    "section": "Question 3",
    "text": "Question 3\n\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\ndata(water)\npairs(water)\n\n\n\n\n\nThe following variables appear to be correlated with each other: OPBPC, OPRC, OPSLAKE. All parts of the matrix with 2 of these variables exhibit a dependence among themselves that is not present between OPBPC, OPRC, and OPSLAKE and APMAM, APSAB, APSLAKE. That being said, though, there also appears to be a correlation among APMAM, APSAB, APSLAKE.\nBSAAM is more closely related to OPBPC, OPRC, and OPSLAKE than to APMAM, APSAB, APSLAKE."
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-4",
    "href": "posts/KPopiela_HW3.html#question-4",
    "title": "HW3",
    "section": "Question 4",
    "text": "Question 4\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\ndata(Rateprof)\npairs(Rateprof[c(\"quality\",\"clarity\",\"helpfulness\",\"easiness\",\"raterInterest\")])\n\n\n\n\nThere is a strong positive relationship among the variables “quality”, “clarity”, and “helpfulness.” There appears to be some correlation between “helpfulness” and “easiness”, but the data is more dispersed."
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-5",
    "href": "posts/KPopiela_HW3.html#question-5",
    "title": "HW3",
    "section": "Question 5",
    "text": "Question 5\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable). (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\n\n(i) y = political ideology and x = religiosity\n\n\n(ii) y = high school GPA and x = hours of TV watching\n\n5.1 Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n5.2 Summarize and interpret results of inferential analyses\n\ndata(\"student.survey\")\ncolnames(student.survey)\n\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"   \"ne\"  \n[11] \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \n\n\nAs per the question, I will be focusing on the following variables: “re” (religiosity) and “pi” (political ideology) for subsection i, and “hi” (high school GPA) and “tv” (hours watching TV) for subsection ii.\n\n#subsection i\nggplot(data=student.survey,aes(x=re,fill=pi))+\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\n\n\n\n\nThe above graph is one of several that could have been used; I just used a bar graph with fill colors since it was simple and suited my purposes. I didn’t know how to get additional information about what the variables actually mean, but I’m assuming “religiosity” refers to how often respondents attend religious services or practice their religion. Based on this graph, as “religiousness” increases, conservatism does as well. While not a majority by any means, it is still significant to note that those who identify as very conservative only appear in the bar labelled “every week,” whereas those who identify as very liberal are not even present on the graph to the right of “occasionally.” This, therefore, indicates that those who are heavily liberal-leaning in political ideology are far less likely to go to church/temple/mosque/etc. regularly/frequently than those who are more conservative.\n\n#subsection ii\nggplot(data=student.survey,aes(x=hi, y=tv)) +\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")\n\n\n\n\nOnce again, this graph is just one of several visualizations that could be used. I chose a scatterplot to reflect individual responses and lessen the visual impact of outliers. While this graph does not show a linear relationship between the two variables, there is a higher concentration of responses with higher GPA’s and lower # of hours watching TV. I will conduct a simple regression model to test whether a linear relationship exists.\nAnd here are some summary statistics for context:\n\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n                     pi                re           hi              tv        \n very liberal         : 8   never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24   occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10   every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                     3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                     Max.   :4.000   Max.   :37.000  \n very conservative    : 2"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html",
    "href": "posts/NiyatiSharma_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\n\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-1",
    "href": "posts/NiyatiSharma_HW1.html#question-1",
    "title": "HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "href": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "title": "HW1",
    "section": "Read the data from xls file",
    "text": "Read the data from xls file\n\n\nCode\nRE <- read_excel(\"_data/LungCapData.xls\")\nRE\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n##A\n\n\nCode\nRE %>% \n  ggplot(aes(LungCap))+\n  geom_histogram(bins=20)\n\n\n\n\n\nThe histogram looks close to normal distributed."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b",
    "href": "posts/NiyatiSharma_HW1.html#b",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\nRE %>%\n  ggplot(aes (LungCap, color=Gender)) +\n  geom_boxplot() +\n  theme_classic() \n\n\n\n\n\nThe probability density of the female is higher than the males."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c",
    "href": "posts/NiyatiSharma_HW1.html#c",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\nMean_Smoker <- RE %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_Smoker\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nggplot(RE, aes(LungCap,Smoke))+\n  geom_boxplot()\n\n\n\n\n\nFrom this sample, it appears that smokers have a higher mean lung capacity than non-smokers."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d",
    "href": "posts/NiyatiSharma_HW1.html#d",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\nRE<-RE %>% \n  mutate(Category = as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\nRE\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean Category    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nRE %>%\n  ggplot(aes( LungCap, color = Smoke)) +\n  geom_histogram()+\n  facet_grid(Smoke ~ Category)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe people who smoke are few in age group of “less than or equal to 13”. From the result we can say age is inversely proportional to the lung capacity."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e",
    "href": "posts/NiyatiSharma_HW1.html#e",
    "title": "HW1",
    "section": "E",
    "text": "E\nForm the above data we can say the output are pretty similar that smokers have a lower lung capacity compared to non-smokers"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f",
    "href": "posts/NiyatiSharma_HW1.html#f",
    "title": "HW1",
    "section": "F",
    "text": "F\ncorrelation and covariance between lung capacity and age\n\n\nCode\ncov(RE$LungCap,RE$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(RE$LungCap,RE$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is positive and indicates that age and lung capacity are directly related. Correlation is also positive,from these results we can conclude that the lung capacity increases with age."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-2",
    "href": "posts/NiyatiSharma_HW1.html#question-2",
    "title": "HW1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nx <- c(0:4)\nfreq <- c(128, 434, 160, 64, 24)\nconvictions <- data_frame(x, freq)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nconvictions\n\n\n# A tibble: 5 × 2\n      x  freq\n  <int> <dbl>\n1     0   128\n2     1   434\n3     2   160\n4     3    64\n5     4    24\n\n\n\n\nCode\nconvictions <- convictions %>% mutate(probability = freq/sum(freq))\nconvictions\n\n\n# A tibble: 5 × 3\n      x  freq probability\n  <int> <dbl>       <dbl>\n1     0   128      0.158 \n2     1   434      0.536 \n3     2   160      0.198 \n4     3    64      0.0790\n5     4    24      0.0296"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#a",
    "href": "posts/NiyatiSharma_HW1.html#a",
    "title": "HW1",
    "section": "A",
    "text": "A\nProbability of exactly 2 is 19.75%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b-1",
    "href": "posts/NiyatiSharma_HW1.html#b-1",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\na <-head(convictions,2)\nsum(a$probability)\n\n\n[1] 0.6938272\n\n\nProbability that a randomly selected inmate has fewer than 2 prior convictions : 69.38%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c-1",
    "href": "posts/NiyatiSharma_HW1.html#c-1",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\na <-head(convictions,3)\nsum(a$probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions : 89.13%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d-1",
    "href": "posts/NiyatiSharma_HW1.html#d-1",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\na <-tail(convictions,2)\nsum(a$probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions? : 10.86%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e-1",
    "href": "posts/NiyatiSharma_HW1.html#e-1",
    "title": "HW1",
    "section": "E",
    "text": "E\n\n\nCode\nWE <- weighted.mean(convictions$x,convictions$probability)\nWE\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions : 1.28"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f-1",
    "href": "posts/NiyatiSharma_HW1.html#f-1",
    "title": "HW1",
    "section": "F",
    "text": "F\nThe variance is 0.857 and the standard deviation is 0.925\n\n\nCode\nAB <- (sum(freq*((x-WE)^2)))/(sum(freq)-1)\nAB\n\n\n[1] 0.8572937\n\n\nCode\nsqrt(AB)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/CalebHill_HW1.html",
    "href": "posts/CalebHill_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nNext, let’s compare the probability distribution of the LungCap with respect to Males and Females, using a boxplot.\n\n\nCode\nboxplot(LungCap ~ Gender, df)\n\n\n\n\n\nThe minimum and mean are very similar to each other, with the minimum around 1 and the mean around 8. The maximum does differ though by gender, at 13 to 14/15 respectively.\n\n\n\nFor the third question, we’re going to compare the mean lung capacities for smokers and non-smokers. To compare the mean, we’ll again use the box-plot.\n\n\nCode\nboxplot(LungCap ~ Smoke, df)\n\n\n\n\n\nWhile the mean is very similar, hovering between 8 and 9, the range is what is substantial. A smoker’s lung capacity has a much smaller range, 4 - 13, compared to non-smokers, at 1 - 15. This makes sense, as a smoker’s lungs would start to have less capacity through consistent substance abuse.\n\n\n\nFor question four, we need to create a new variable, Age Group, followed by comparing the relationship between Smoking and Lung Capacity, broken down by Age Group. First, we’ll create the new column, referencing the Age column to determine groups.\n\n\nCode\ndf_new <- df %>%\n  mutate(\n    Age_Group = dplyr::case_when(\n      Age <= 13 ~ \"Less than or equal to 13\",\n      Age == 14 | Age == 15 ~ \"14 or 15\",\n      Age == 16 | Age == 17 ~ \"16 or 17\",\n      Age >= 18 ~ \"Greater than or equal to 18\"\n    ),\n    Age_Group = factor(\n      Age_Group,\n      level = c(\"Less than or equal to 13\", \"14 or 15\", \"16 or 17\", \"Greater than or equal to 18\")\n    )\n  )\nhead(df_new)\n\n\n# A tibble: 6 × 7\n  LungCap   Age Height Smoke Gender Caesarean Age_Group                  \n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>                      \n1    6.48     6   62.1 no    male   no        Less than or equal to 13   \n2   10.1     18   74.7 yes   female no        Greater than or equal to 18\n3    9.55    16   69.7 no    female yes       16 or 17                   \n4   11.1     14   71   no    male   no        14 or 15                   \n5    4.8      5   56.9 no    male   no        Less than or equal to 13   \n6    6.22    11   58.7 no    female no        Less than or equal to 13   \n\n\nGood. Now we can place a histogram to better understand the relationship between LungCap and Smoking status. To view it by age group, we’ll add a facet wrap to the visualization.\n\n\nCode\nggplot(df_new, aes(LungCap, color=Smoke)) +\n  geom_histogram() +\n  facet_wrap(~Age_Group)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThose that are smokers have a smaller sample size than non-smokers. Looking purely at the distribution of each, we can see that three of the four age groups follow a normal distribution, save the 14 or 15 group that has a somewhat “two hump” distribution.\nEven so, smoking status does seem to mirror the non-smoker distribution, when it comes to the overall sample count and LungCap.\n\n\n\nFor the fifth question, we’ll compare the lung capacities for smokers and non-smokers within each age group. We’ll use a box-plot and facet wrap this visualization again by Age Group.\n\n\nCode\nggplot(df_new, aes(LungCap, Smoke)) +\n  geom_boxplot() +\n  facet_wrap(~Age_Group)\n\n\n\n\n\nWe can readily see that smokers, irrespective of age, have a substantially smaller lung capacity range compared to non-smokers. While the mean might be similar, sometimes even smaller for “13 years old or less”, the length of each capacity varies for non-smokers where it doesn’t for smokers.\n\n\n\nFor the sixth question, we shall calculate the covariance and correlation between LungCap and Age.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is the relationship between a pair of random variables where change in one variable causes change in another variable. With a covariance of 8.73, that means that there is a positive relationship between the two variables and that, by every 1 point change of Age, that can result in an average of 8.73 point change in LungCap.\nCorrelations show whether and how strongly pairs or variables are related to one another. Correlation can range from 0.0 to 1.0. With a result of 0.81, that means there is a high correlation between LungCap and Age."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#a-1",
    "href": "posts/CalebHill_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nLet’s calculate the probability that a randomly selected inmate has EXACTLY 2 prior convictions.\n\n\nCode\ndbinom(2, 810, 0.1975)\n\n\n[1] 7.90917e-74\n\n\nSo 7.9%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#b-1",
    "href": "posts/CalebHill_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nLet’s calculate the probability that a randomly selected inmate has FEWER THAN 2 prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975, lower.tail=FALSE)\n\n\n[1] 1\n\n\nNot sure why it’s pulling 1."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#c-1",
    "href": "posts/CalebHill_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLet’s calculate the probability that a randomly selected inmate has 2 OR FEWER prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975)\n\n\n[1] 7.989018e-74\n\n\nSo 7.98%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#d-1",
    "href": "posts/CalebHill_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nLet’s calculate the probability that a randomly selected inmate has MORE THAN 2 prior convictions."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#e-1",
    "href": "posts/CalebHill_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nLet’s calculate the expected value for the number of prior convictions. As I am unable to calculate sections B and D, I’m unable to determine the expected value."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#f-1",
    "href": "posts/CalebHill_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nLet’s calculate the variance and the standard deviation for the Prior Convictions.As I am unable to determine the expected value, I cannot calculate the variance and standard deviation either."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html",
    "href": "posts/HW1_ManiShankerKamarapu.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"_data/LungCapData.xls\")\nLc\n\n\n\n\n  \n\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a",
    "href": "posts/HW1_ManiShankerKamarapu.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\n\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b",
    "href": "posts/HW1_ManiShankerKamarapu.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c",
    "href": "posts/HW1_ManiShankerKamarapu.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d",
    "href": "posts/HW1_ManiShankerKamarapu.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e",
    "href": "posts/HW1_ManiShankerKamarapu.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f",
    "href": "posts/HW1_ManiShankerKamarapu.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\nCorrelation <- cor(Lc$LungCap, Lc$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW 2 - Eris Dodds.html",
    "href": "posts/HW 2 - Eris Dodds.html",
    "title": "",
    "section": "",
    "text": "Question 1\nSee Code\n\nbypass_mean<-19\nbypass_sd<-10\nbypass_size<-539\nstandard_error_bypass<-bypass_sd/sqrt(bypass_size)\nconfidence_level<-0.9\ntail_area<-(1-confidence_level)/2\nt_score<-qt(p = 1-tail_area, df = bypass_size-1)\nCI_bypass<-c(bypass_mean - t_score * standard_error_bypass, bypass_mean + t_score * standard_error_bypass)\nprint(CI_bypass)\n\n[1] 18.29029 19.70971\n\nanio_mean<-18\nanio_sd<-9\nanio_size<-847\nstandard_error_anio<-anio_sd/sqrt(anio_size)\ntail_area_anio<-(1-confidence_level)/2\nt_score_anio<-qt(p = 1-tail_area_anio, df = anio_size)\nCI_anio<-c(anio_mean - t_score_anio * standard_error_anio, anio_mean + t_score_anio * standard_error_anio)\nprint(CI_anio)\n\n[1] 17.49078 18.50922\n\n\n\n\nQuestion 2\nPE<-.55 Lower Limit = .52 Upper Limit = .58\n\npop<-567\nsize<-1031\nPE<-pop/size\nprop.test(pop,size)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  pop out of size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n\n\nQuestion 3\n\n(1.96)^2 *(42.5)^2/ (5)^2\n\n[1] 277.5556\n\n\n\n\nQuestion 4\nThe p value is .049, showing it is statisitcally significant and we can reject the null. Other aspects to this question are verified within the code\n\nfem_size<-9\nfem_mean<-410\nnull<-500\nsd<-90\n\nSE<-sd/sqrt(fem_size)\nt_score<-(fem_mean-null)/SE\np_value<-(pt(t_score, df=9))*2\nupper_p<-(pt(t_score, df=8, lower.tail = FALSE))\nupper_p\n\n[1] 0.9914642\n\nlower_p<-(pt(t_score, df=8, lower.tail = FALSE))\nlower_p\n\n[1] 0.9914642\n\n\n\n\nQuestion 5\n\nsee code\nJones = .051 not significant, cannot reject null. Smith = .049 significant, reject null.\nBeing broad about the direction of the p value, in this case, would overshadow how marginally significant and insignificant the p values actually came out to in this case.\n\n\njones_mean<-519.5\nsmith_mean<-519.7\njones_se<-10\nsmith_se<-10\nnull<-500\njones_t<-(jones_mean-null)/jones_se\njones_t\n\n[1] 1.95\n\nsmith_t<-(smith_mean-null)/smith_se\nsmith_t\n\n[1] 1.97\n\njones_p<-pt(jones_t, df=999, lower.tail = FALSE)*2\njones_p\n\n[1] 0.05145555\n\nsmith_p<-pt(smith_t, df=999, lower.tail = FALSE)*2\nsmith_p\n\n[1] 0.04911426\n\n\n\n\nQuestion 6\nThe results of a t test show that the mean is less that 45, with a relatively small p value. We can have more confidence, then, that the average gas tax per gallon was less than .45 cents. ::: {.cell}\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n:::"
  },
  {
    "objectID": "posts/HW_1_QH.html",
    "href": "posts/HW_1_QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a",
    "href": "posts/HW_1_QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/HW_1_QH.html#d",
    "href": "posts/HW_1_QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/HW_1_QH.html#e",
    "href": "posts/HW_1_QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/HW_1_QH.html#f",
    "href": "posts/HW_1_QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a-1",
    "href": "posts/HW_1_QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/HW_1_QH.html#b",
    "href": "posts/HW_1_QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/HW_1_QH.html#c",
    "href": "posts/HW_1_QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/HW_1_QH.html#d-1",
    "href": "posts/HW_1_QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/HW_1_QH.html#e-1",
    "href": "posts/HW_1_QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/HW_1_QH.html#f-1",
    "href": "posts/HW_1_QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html",
    "href": "posts/HW2_ToryBartelloni.html",
    "title": "DACSS 603: Homework 2",
    "section": "",
    "text": "Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nlibrary(dplyr)\nsurgery <- data.frame(Procedure=c(\"Bypass\",\"Angiography\"), \n                      Sample_Size=c(539,847),\n                      Mean_Wait_Time=c(19,18),\n                      Standard_Deviation=c(10,9))\n\nbypass <- surgery %>% filter(Procedure == \"Bypass\")\n\nt_score_bypass <- qt(p=1-0.05, df=bypass$Sample_Size-1)\n\nse_bypass <- bypass$Standard_Deviation / sqrt(bypass$Sample_Size)\n\nCI_bypass <- c(bypass$Mean_Wait_Time - t_score_bypass * se_bypass,\n    bypass$Mean_Wait_Time + t_score_bypass * se_bypass)\n\nangio <- surgery %>% filter(Procedure == \"Angiography\")\n\nt_score_angio <- qt(p=1-0.05, df=angio$Sample_Size-1)\n\nse_angio <- angio$Standard_Deviation / sqrt(angio$Sample_Size)\n\nCI_angio <- c(angio$Mean_Wait_Time - t_score_angio * se_angio,\n    angio$Mean_Wait_Time + t_score_angio * se_angio)\n\nCI <- data.frame(Procedure = c(\"Bypass\", \"Angiography\"),\n                 Lower_Limit = c(CI_bypass[1],CI_angio[1]),\n                 Upper_Limit = c(CI_bypass[2],CI_angio[2]))\n\nknitr::kable(CI, caption = \"90% Confidence Levels for Cardiac Procedures\")\n\n\n\n90% Confidence Levels for Cardiac Procedures\n\n\nProcedure\nLower_Limit\nUpper_Limit\n\n\n\n\nBypass\n18.29029\n19.70971\n\n\nAngiography\n17.49078\n18.50922\n\n\n\n\n\nThe confidence interval is more narrow for angiogrphy because the larger sample size reduces t-score, and the larger sample and lower standard deviation together reduce the standard error."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#a.",
    "href": "posts/HW2_ToryBartelloni.html#a.",
    "title": "DACSS 603: Homework 2",
    "section": "A.",
    "text": "A.\nAbove we have shown that the difference in the results leads to the t-statistics and p-values provided."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#b.",
    "href": "posts/HW2_ToryBartelloni.html#b.",
    "title": "DACSS 603: Homework 2",
    "section": "B.",
    "text": "B.\nThe p-values observed show that at a 95% confidence level Jones’ results are non-significant while Smith’s results are significant."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#c.",
    "href": "posts/HW2_ToryBartelloni.html#c.",
    "title": "DACSS 603: Homework 2",
    "section": "C.",
    "text": "C.\nThis is a good example of why not reporting p-values can be insufficient or misleading because the results we observed are extremely close, but the arbitrary boundary we agreed upon prior to the test distinguishes them into different categories. This would not be so important if the difference between those categories were not important. Without the context of the specific results we could see the two extremely similar results treated and acted upon in starkly different ways.\nI would argue this is one good reason why we should be reporting p-values up until .001 so researchers and users of the data can fully understand the context they would be applying the result within. Good to note that reporting extremely small p-values (<.001) has it’s drawbacks as well and we do not want to overemphasize results that may be the result of methodology rather than a real and important distinction for instance."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html",
    "href": "posts/Homework2_Kaushika Potluri.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "title": "Homework 2",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stats)\n\n\n##Question 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for Angiography or Bypass surgery?\n\nAngiography\n\n\nCode\nang_mean <- 18\nang_sd <- 9\nang_ss <- 847\n\nang_se <- ang_sd/sqrt(ang_ss)\n\nang_cl <- 0.90  \nang_tail <- (1-ang_cl)/2\nang_tscore <- qt(p = 1-ang_tail, df = ang_ss-1)\n\nang_ci <- c(ang_mean - ang_tscore * ang_se,\n        ang_mean + ang_tscore * ang_se)\nprint(ang_ci)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nCode\n#assessing Confidence interval\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_ang <- ang_tscore * ang_se\nMargin_of_error_ang * 1.01\n\n\n[1] 0.5143103\n\n\nWe can be 90% confident that the population mean wait time for the Angiography procedure is between 17.49078 and 18.50922 days with margin of error +/-0.51\n\n\nBypass\n\n\nCode\nbypass_mean <- 19\nbypass_sd <- 10\nbypass_ss <- 539\n\nbypass_se <- bypass_sd/sqrt(bypass_ss)\n\nbypass_cl <- 0.90  \nbypass_tail <- (1-bypass_cl)/2\nbypass_tscore <- qt(p = 1-bypass_tail, df = bypass_ss-1)\n\nbypass_ci <- c(bypass_mean - bypass_tscore * bypass_se,\n        bypass_mean + bypass_tscore * bypass_se)\nprint(bypass_ci)\n\n\n[1] 18.29029 19.70971\n\n\nWe can be 90% confident that the population mean wait time for the Bypass procedure is between 18.29029 and 19.70971 days.\n\n\nCode\n#assessing Confidence interval\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_bypass <- bypass_tscore * bypass_se\nMargin_of_error_bypass * 1.41\n\n\n[1] 1.000692\n\n\nTherefore, the confidence interval is more narrow for Angiographies."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-2",
    "href": "posts/Homework2_Kaushika Potluri.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = Number of American adults (population), x = sample (surveyed)\nn = 1031\nx = 567 #(believed that college education is essential for success)\n\n#Using prop.test to find p (The CI is 95% by default)\n#This  function will return the range for the point estimate at 95% CI.\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe percentage of adult Americans who think a college education is necessary for success is p, which is 0.5499515. We have a confidence interval of 95 percent confidence interval that equals, [0.5189682, 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-3",
    "href": "posts/Homework2_Kaushika Potluri.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#Evaluating standard deviation using the given values\nUMassSD <- (200-30)/4\nUMassSD\n\n\n[1] 42.5\n\n\nSince the significance level is at 5% our Confidence level is 95%. A 95% confidence level has a z-score of 1.96. With this ideal sample size can be calculated.\n\n\nCode\n#samplesize = ((UMassSD * zscore)/5)^2\nsamplesize <- ((UMassSD * 1.96)/5)^2\nprint(samplesize)\n\n\n[1] 277.5556\n\n\nThe size necessary for the sample is 278."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-4",
    "href": "posts/Homework2_Kaushika Potluri.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\np_mean <- 500\ns_meanfemale <- 410\ns_sizefemale = 9\nsd = 90\n\n#find standard error\nstandarderrorfemale<- sd/sqrt(s_sizefemale)\nstandarderrorfemale\n\n\n[1] 30\n\n\n\n\nCode\n#calculating t-score\nt_stat<- (s_meanfemale-p_mean)/standarderrorfemale\nt_stat\n\n\n[1] -3\n\n\n\n\nCode\n#calculating p value\ndf <- 9-1\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nSince the p value is less than .05 we can reject the null hypothesis\n\nReport the P-value for Ha : μ < 500. Interpret.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\npvalue_lower <- pt(-t_stat, df, lower.tail = FALSE)\npvalue_lower\n\n\n[1] 0.008535841\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500.\n\nReport and interpret the P-value for H a: μ > 500.\n\n\n\nCode\npvalue_upper <- pt(t_stat, df, lower.tail = FALSE)\npvalue_upper\n\n\n[1] 0.9914642\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500.\n\n\nCode\n#checking if sum = 1\npvalue_upper + pvalue_lower\n\n\n[1] 1"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-5",
    "href": "posts/Homework2_Kaushika Potluri.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nJones\n\n\nCode\n#first calculating t-value for Jones\nt_stat_Jones <- (519.5 - 500)/(10)\nt_stat_Jones\n\n\n[1] 1.95\n\n\nCode\ndf <- 1000-1\n#now we calculate p value for Jones\n\n\np_value_Jones <- 2*pt(t_stat_Jones,df, lower.tail = FALSE)\np_value_Jones\n\n\n[1] 0.05145555\n\n\n\n\nSmith\n\n\nCode\n#first calculating t-value for Smith\nt_stat_Smith <- (519.7 - 500)/(10)\nt_stat_Smith\n\n\n[1] 1.97\n\n\nCode\ndf <- 1000-1\n\n#now we calculate p value for Smith\np_value_Smith <- 2*pt(t_stat_Smith,df, lower.tail = FALSE)\np_value_Smith\n\n\n[1] 0.04911426\n\n\nb)Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAnswer : When they say ‘statistically significant’ it means the p-value is smaller than the 0.05. For Jones, the p-value is 0.051 which is greater than the 0.05 significance level. This means that it is not statistically significant and we cannot reject the null hypothesis.\nFor Smith, the p-value is 0.049 which is smaller than the significance level. This means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\nAnswer : One cannot assess the validity of the result if we do not provide the P-value and you cannot tell how close the p-value is to being significant. Since the values of Jones and Smith’s is barely greater and lesser than 0.05 respectively, it is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected. This could draw very different conclusions."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-6",
    "href": "posts/Homework2_Kaushika Potluri.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer :\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\n#Mean of taxes\nMean_gastaxes <- mean(gas_taxes)\nMean_gastaxes\n\n\n[1] 40.86278\n\n\n\n\nCode\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value is 0.03 at 95% confidence level. This is lesser than the 5% significance level. Therefore, this proves that we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. We can say that the average tax per gallon of gas in the US in 2005 was less than 45 cents with 95% confidence."
  },
  {
    "objectID": "posts/HW2_Saaradhaa.html",
    "href": "posts/HW2_Saaradhaa.html",
    "title": "Homework 2",
    "section": "",
    "text": "Qn 2\n\nset.seed(0)\nprop <- prop.test(x=567, n=1031)\nprop\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% CI is [0.5189682, 0.580558], which includes the point estimate 0.5499515 and excludes 0.5. Hence, we can reject the null hypothesis that the true probability is 0.5 at the 5% significance level, p = 0.0014898.\n\n\nQn 3\n\n# calculate population SD.\nPSD <- (200-30)/4\n\n# calculate sample size.\nn <- round(((1.96*PSD)/5)^2)\n\nThe minimum required sample size is 278.\n\n\nQn 4a\nAssumptions: H0 is true, observations are independent of one another, y is continuous and sample is approximately normally distributed. H0: μ = 500 Ha: μ ≠ 500\n\n# calculate t-statistic.\nt <- (410-500)/(90/sqrt(9))\n\n# calculate p-value.\np <- 2*pt(q=abs(t), df=8, lower.tail=FALSE)\np\n\n[1] 0.01707168\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8) = 3, p = 0.0170717. Female employees’ mean income significantly differs from $500 per week.\n[I have a question - I am confused on whether I was right to use the absolute value here, and when we should use absolute values.]\n\n\nQn 4b\n\n# calculate p-value.\np2 <- pt(q=t, df=8, lower.tail=TRUE)\np2\n\n[1] 0.008535841\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.0085358. Female employees’ mean income is significantly less than $500 per week.\n\n\nQn 4c\n\n# calculate p-value.\np3 <- pt(q=t, df=8, lower.tail=FALSE)\np3\n\n[1] 0.9914642\n\n\nWe fail to reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.9914642. Female employees’ mean income is not significantly more than $500 per week.\n\n\nQn 5a\n\n# calculate SD for Jones and Smith.\nSD <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_j <- ((519.5-500)/SD) * sqrt(1000)\nt_j\n\n[1] 1.95\n\n# calculate p-value for Jones.\np_j <- 2*(pt(q=t_j, df=999, lower.tail=FALSE))\np_j\n\n[1] 0.05145555\n\n# calculate t for Smith.\nt_s <- ((519.7-500)/SD) * sqrt(1000)\nt_s\n\n[1] 1.97\n\n# calculate p-value for Smith.\np_s <- 2*(pt(q=t_s, df=999, lower.tail=FALSE))\np_s\n\n[1] 0.04911426\n\n\n\n\nQn 5b\nThe result is statistically significant for Smith, but not Jones.\n\n\nQn 5c\nIt is useful to report the exact p-value in cases like this, when the p-value is very close to alpha. It helps the reader to understand (1) why it was/was not rejected, and (2) how much evidence there is against the null hypothesis.\n\n\nQn 6\n\n#create variable.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# do t-test.\ntax <- t.test(gas_taxes, alternative=\"less\",mu=45)\ntax\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% CI is [-, 44.6794598], which includes the estimated mean 40.8627778 and excludes 45. Hence, we can reject the null hypothesis at the 5% significance level, t(17)= -1.8857058, p = 0.0382708. The average tax per gallon in the US in 2005 was significantly less than 45 cents."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html",
    "href": "posts/MeghaJoseph_hw1.html",
    "title": "HOME WORK1 603",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1",
    "href": "posts/MeghaJoseph_hw1.html#answer-1",
    "title": "HOME WORK1 603",
    "section": "Answer 1",
    "text": "Answer 1\n\n\nCode\nreadD <- read_excel(\"_data/LungCapData.xls\")\nreadD\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (a)",
    "text": "Answer 1 (a)\nDistribution of LungCap:\n\n\nCode\nhist(readD$LungCap)\n\n\n\n\n\nThe distribution is a normal distribution. ## Answer 1 (b)\n\n\nCode\nboxplot(readD$LungCap ~ readD$Gender)\n\n\n\n\n\nThe mean of males appear higher than females."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (c)",
    "text": "Answer 1 (c)\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(readD, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean of smokers is higher than the mean of non smokers and therefore it is not sensible."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (d)",
    "text": "Answer 1 (d)\n\n\nCode\nclass(readD$Age)\n\n\n[1] \"numeric\"\n\n\nCode\nreadD <- mutate(readD, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", \n                                            Age == 14 | Age == 15 ~ \"14 to 15\", \n                                            Age == 16 | Age == 17 ~ \"16 to 17\", \n                                            Age >= 18 ~ \"18 and above\"))\nggplot(readD, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nreadD %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nFrom the above results we can say that people from age group 10 and above smoke."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (f)",
    "text": "Answer 1 (f)\n\n\nCode\ncor(readD$LungCap,readD$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(readD$LungCap,readD$Age)\n\n\n[1] 8.738289\n\n\nFrom the data we can see that the covariance is positive and it shows that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. Therefore as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-2",
    "href": "posts/MeghaJoseph_hw1.html#answer-2",
    "title": "HOME WORK1 603",
    "section": "Answer 2",
    "text": "Answer 2\n\n\nCode\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nC<- data.frame(X, Frequency)\nC\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nC<-rename(C, PriorConvictions=X)\nC\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(C, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\n\n\nCode\nPO<-Frequency/810\nPO\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, PO)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\nAnswer\na: 19.75% b :9.38% c :89.14% d :10.86% e :1.28642 f: variance: 0.8572937 standard deviation: 0.9259016"
  },
  {
    "objectID": "posts/Homework3.html",
    "href": "posts/Homework3.html",
    "title": "Homework 3 - Emily Duryea",
    "section": "",
    "text": "United Nations (Data file: UN11in alr4)\nThe data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nQuestion: Identify the predictor and the response.\nAnswer: The predictor is ppgdp, and the response is ferility, since we are looking at how ppgdp (the independent variable) is affecting fertility (dependent variable).\n\n\n\nQuestion: Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\n# Importing needed libraries\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n# Importing the UN11 dataset\ndata(UN11)\n\n# Creating a scatterplot\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point(color = 'black') +\n  labs(title = \"PPGDP and Fertility\")\n\n\n\n\n\nAnswer: This graph does not look like it could represented by a linear function. Rather, it looks like it would be represented by a nonlinear (curvilinear) function.\n\n\n\nQuestion: Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\n# Creating a scatterplot\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = 'black') +\n  geom_smooth(method = lm) +\n  labs(title = \"PPGDP and Fertility\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnswer: After taking the logarithm of each variable, based on the graph, it is now plausible to use a simple linear regression.\n\n\n\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\nQuestion: How, if at all, does the slope of the prediction equation change?\n\n\nCode\n# Creating a variable for the British pound\nUN11$Britishpound <- 1.33*UN11$ppgdp\n\n# Examining the slope\nsummary(lm(fertility ~ Britishpound, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ Britishpound, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.178e+00  1.048e-01  30.331  < 2e-16 ***\nBritishpound -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nggplot(data = UN11, aes(x = log(Britishpound), y = log(fertility))) +\n  geom_point(color = 'black') +\n  geom_smooth(method = lm) +\n  labs(title = \"British Pound and Fertility\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n# Comparing the slope\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nAnswer: The slope has changed slightly due to the 1.33 increase adjustment for British pounds, but according to the results of the summary function, the adjusted R-squared is the same for both (0.1895).\n\n\n\nQuestion: How, if at all, does the correlation change?\n\n\nCode\n# Finding the correlation with US dollars\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\n# Finding the correlation with British pounds\ncor(UN11$Britishpound, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nAnswer: The correlations of fertility with US dollars AND British pounds are the same, because, although British pounds are of a different value from US dollars, the values are multiplied by a constant (1.33).\n\n\n\n\nWater runoff in the Sierras (Data file: water in alr4)\nCan Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\n# Loading dataset\ndata(water)\n\n# Creating scatterplots\npairs(water)\n\n\n\n\n\nCode\n# Conducting regression analysis\nwater1 <- lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\nsummary(water1)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nAnswer: This graph does not look like it could represented by a linear function. Rather, it looks like it would be represented by a nonlinear (curvilinear) function.\n\n\n\nProfessor ratings (Data file: Rateprof in alr4)\nIn the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\n# Importing dataset\ndata(Rateprof)\n\n# Creating a subset of the dataset with the five variables of interest\nRateprof5 <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# Creating the scatterplots\npairs(Rateprof5)\n\n\n\n\n\nAnswer: All 5 of the variables of interest have positive correlations. However, some relationships are stronger than others. Quality, helpfulness, and clarity all have stronger positive relationships, while easiness and raterInterest are very weak positive relationships.\n\n\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\n\nQuestion: Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\nCode\n# Importing dataset\ndata(student.survey)\nstudentsurvey <- student.survey\n\n# Creating subset of data with variables needed\nstudentsurvey <- studentsurvey %>%\n  select(hi, tv, pi, re)\n\n# Creating a plot to compare political ideology with religious service attendance\nplot(pi ~ re, data = studentsurvey)\n\n\n\n\n\nCode\n# Creating a plot comparing High School GPA (hi) and average number of hours watching tb a week (tv)\nggplot(data = studentsurvey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnswer: Based on the plots generated, religious service attendance is correlated with conservatism, and hours of TV watched per week has a negative relationship with high school GPA.\n\n\n\nQuestion: Summarize and interpret results of inferential analyses.\n\n\nCode\n# Changing the pi variable to a numeric one\nstudentsurvey$pi <- as.numeric(studentsurvey$pi)\n\n# Removing ordering from the re variable\nlevels(studentsurvey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudentsurvey$re <- factor(studentsurvey$re, ordered = FALSE)\n\n# Conducting regression analyses for pi and re\nsummary(lm(pi ~ re, studentsurvey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = studentsurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nCode\n# Conducting regression analyses for hi and tv\nsummary(lm(hi ~ tv, studentsurvey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = studentsurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nAnswer: According to this dataset, people who attended religious services most weeks or every week are significantly more likely to report as conservative (p < 0.001). Additionally, people who watch less hours of tv are significantly more likely to have a higher GPA (p < 0.05)."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html",
    "href": "posts/DACSS 603 HW 1.html",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   0.3.5\n✔ tibble  3.1.8     ✔ stringr 1.4.1\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n✔ readr   2.1.3     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n\n# Reading in File\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram above shows that the Lung Cap data is roughly normally distributed because a majority of the observations are centered around the mean. There are fewer observations at the tail ends of the histogram.\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, main = \"Lung Capacity by Gender\",\n        xlab = \"Gender\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the box-plots above, it appears that males in this study had slightly higher lung capacities than females, with the median for males at 9 and the median for females at 8. However, both genders had large ranges, but these ranges reflected the overall pattern of males having slightly higher lung capacities.\n\n\n\n\n\nCode\nsmokers <- filter(LungCapData, Smoke == \"yes\")\nmean(smokers$LungCap)\n\n\n[1] 8.645455\n\n\nCode\nnonsmokers <- filter(LungCapData, Smoke == \"no\")\nmean(nonsmokers$LungCap)\n\n\n[1] 7.770188\n\n\nThe mean lung capacity for smokers (8.65) is higher than the mean lung capacity for non-smokers (7.77). Based on what we now know about how smoking affects the lungs, these results don’t seem to make sense. However, there is the possibility that smokers may be more used to deep inhales/exhales and therefore could have better lung capacity until the substance has more of an effect on their lungs. There may also be external factors that led to these results that aren’t clear from the data right now.\n\n\n\n\n\nCode\nLungCapData <- within(LungCapData, {\n  Age.group <- NA\n  Age.group[Age <= 13] <- \"13 and Under\"\n  Age.group[Age >= 14 & Age <= 15] <- \"14-15\"\n  Age.group[Age >= 16 & Age <= 17] <- \"16-17\"\n  Age.group[Age >= 18] <- \"18 and Over\"\n} )\n\n\n\n\n\n\nCode\n# Boxplots\n\nsmoking_age <- filter(LungCapData, Smoke == \"yes\")\n\nboxplot(LungCap ~ Age.group, data = smoking_age,\n        main = \"Lung Capacity of Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the boxplot above, we can see that smokers’ lung capacities reach about a maximum of 12 as age increases, but there is not very much improvement in the maximums. The medians move a bit more as age increases, but still not very dramatically after ages 14 and 15. Smokers that are 18 and over have higher lung capacities overall, but this may just be because of natural aging processes and development.\n\n\nCode\n# Means\n\nsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  7.20\n2 14-15         8.39\n3 16-17         9.38\n4 18 and Over  10.5 \n\n\nWe see the same trend in means as in the medians: mean lung capacity to increases as the age increases.\n\n\n\n\n\nCode\n# Boxplot\n\nnonsmoking_age <- filter(LungCapData, Smoke == \"no\")\n\nboxplot(LungCap ~ Age.group, data = nonsmoking_age,\n        main = \"Lung Capacity of Non-Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nIn non-smokers, we see the same trend of increasing lung capacities as age increases, but the median lung capacities in the two older age groups in the non-smoking group are higher than those in the smoking group. There are also more outliers for non-smokers, especially in the 14-15 category.\n\n\nCode\n# Means\n\nnonsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  6.36\n2 14-15         9.14\n3 16-17        10.5 \n4 18 and Over  11.1 \n\n\nThe means of the non-smoking group by age follow the same trend as the medians, as well as in the smoking group. However, the mean lung capacity for the oldest two age groups in the non-smoking category are higher than the means for those groups in the smoking category.\n\n\n\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"13 and Under\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     6.36\n2 yes    7.20\n\n\nThe mean lung capacity for smokers is higher than the mean lung capacity for non-smokers in the age group 13 and under, which mirrors the general means we found earlier. However, from the boxplot of Smokers by Age Group, we can see that there is a very low outlier in this age group, which might be affecting the mean for this group as well as overall smokers.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"14-15\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nIn this age group, the mean lung capacity for non-smokers is higher than the mean lung capacity for smokers–unlike the younger group.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"16-17\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nThe same trend continues in this age group, with the mean lung capacity in non-smokers ages 16 and 17 higher than the mean lung capacity of smokers in this group. Yet as the ages increase, the mean lung capacities for non-smokers and smokers increase about the same amount (by 1).\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"18 and Over\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     11.1\n2 yes    10.5\n\n\nIn this oldest age group, the same trend continues: the mean lung capacity for non-smokers is higher than that of smokers. This pattern in the groups 18+, 16-17, and 14-15 are not found in the overall means for smokers and nonsmokers, suggesting that the outlier in the 13 and Under group might have brought down the overall mean for smokers.\n\n\n\n\n\n\nCode\n# Correlation\n\ncor(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 0.8196749\n\n\nThe correlation between lung capacity and age is positive and strong. As age increases, lung capacity also increases. The value of 0.8 is close to 1, meaning there is a somewhat strong relationship between the two variables.\n\n\nCode\n# Covariance\n\ncov(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 8.738289\n\n\nThe covariance is positive, meaning that there is a positive relationship between the varaibles, which is also clear from the correlation (since the correlation coefficient is a function of the covariance). Age and lung capacity have an overall positive relationship: as age increases, so does lung capacity."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-a",
    "href": "posts/DACSS 603 HW 1.html#part-a",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part A",
    "text": "Part A\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-b",
    "href": "posts/DACSS 603 HW 1.html#part-b",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part B",
    "text": "Part B\n\n\nCode\n(434 + 128)/810\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-c",
    "href": "posts/DACSS 603 HW 1.html#part-c",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part C",
    "text": "Part C\n\n\nCode\n(160 + 434 + 128)/810\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-d",
    "href": "posts/DACSS 603 HW 1.html#part-d",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part D",
    "text": "Part D\n\n\nCode\n(64 + 24)/810\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-e",
    "href": "posts/DACSS 603 HW 1.html#part-e",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part E",
    "text": "Part E\n\n\nCode\n# Creating vector\nconvict <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\n\nweighted.mean(convict)\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.27–but since prior convictions have to be a whole number, that would be rounded to 1."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-f",
    "href": "posts/DACSS 603 HW 1.html#part-f",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part F",
    "text": "Part F\n\n\nCode\nvar(convict)\n\n\n[1] 0.8572937\n\n\nCode\nsd(convict)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html",
    "href": "posts/HW2_Solutions_OmerYalcin.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\nThe confidence intervals:\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n[1] 17.49078 18.50922\n\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n2 * bypass_me\n\n[1] 1.419421\n\n2 * angio_me\n\n[1] 1.018436\n\n\nThe confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\none-step solution:\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAlternatively:\n\np_hat <- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n[1] 0.5195839 0.5803191\n\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\nbinom.test(k, n)\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515"
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nrange = 200-30\npopulation_sd = range/4\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\] (We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\] \\[ zs = 5 \\sqrt n\\] \\[ \\frac{zs}{5} = \\sqrt n\\] \\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n[1] 277.5454\n\n\nRounding up, we need a sample of 278."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-4",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\nget_t_stat <- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\nFind the t-statistic:\n\nt_stat <- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\nA\nTwo-tailed test\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n[1] 0.01707168\n\n\nWe can reject the hypothesis that population mean is 500.\n\n\nB\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n[1] 0.008535841\n\n\nWe can reject the hypothesis that population mean is greater than 500.\n\n\nC\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n[1] 0.9914642\n\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n1 - pval_lower_tail\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-5",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Smith: 0.0491 \n\n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-6",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-6",
    "title": "Homework 2",
    "section": "Question 6:",
    "text": "Question 6:\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nIn the one sided test, we are able to reject the null in favor of the alternative that the gas taxes are less than 45 cents.\nNote that a two-sided test at the same level would not have resulted in the rejection of the null.\nHowever, a two-sided 90% confidence interval gives the same upper bound, since now there is a 5% rejection are on two sides:\n\nt.test(gas_taxes, mu = 45, alternative = 'two.sided', conf.level = 0.9)\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.07654\nalternative hypothesis: true mean is not equal to 45\n90 percent confidence interval:\n 37.04610 44.67946\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-1",
    "href": "posts/Niharika_HW1.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-data",
    "href": "posts/Niharika_HW1.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/Niharika_HW1.html#a",
    "href": "posts/Niharika_HW1.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/Niharika_HW1.html#b",
    "href": "posts/Niharika_HW1.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/Niharika_HW1.html#c",
    "href": "posts/Niharika_HW1.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/Niharika_HW1.html#d",
    "href": "posts/Niharika_HW1.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/Niharika_HW1.html#e",
    "href": "posts/Niharika_HW1.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/Niharika_HW1.html#f",
    "href": "posts/Niharika_HW1.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-2",
    "href": "posts/Niharika_HW1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-the-table",
    "href": "posts/Niharika_HW1.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/Niharika_HW1.html#a-1",
    "href": "posts/Niharika_HW1.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/Niharika_HW1.html#b-1",
    "href": "posts/Niharika_HW1.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/Niharika_HW1.html#c-1",
    "href": "posts/Niharika_HW1.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/Niharika_HW1.html#d-1",
    "href": "posts/Niharika_HW1.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#e-1",
    "href": "posts/Niharika_HW1.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#f-1",
    "href": "posts/Niharika_HW1.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html",
    "href": "posts/KenDocekal_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a",
    "href": "posts/KenDocekal_HW1.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nRead in the data from the Excel file:\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)\n\n\nWarning in View(LungCapData): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b",
    "href": "posts/KenDocekal_HW1.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability distribution of the LungCap, Males and Females, in a box plot:\n\n\nCode\nboxplot(LungCapData$LungCap ~ LungCapData$Gender)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c",
    "href": "posts/KenDocekal_HW1.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLung capacities for smokers and non-smokers, mean and standard deviation:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(mean = mean(LungCap, na.rm = TRUE), sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     7.77  2.73\n2 yes    8.65  1.88\n\n\nResults seem to point to smokers having greater lung capacity which is odd and could indicate factors other than age are influencing lung capacity"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d",
    "href": "posts/KenDocekal_HW1.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”:\nage 13 and lower:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age <=13)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     6.36  2.21\n2 yes    7.20  1.58\n\n\nage 14 to 15:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 14:15)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     8.84 1.36 \n2 yes    8.91 0.865\n\n\nage 16 to 17:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 16:17)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no    10.4   1.73\n2 yes    9.60  1.41\n\n\nage 18 and over:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age >=18)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     11.1  1.56\n2 yes    10.5  1.25"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e",
    "href": "posts/KenDocekal_HW1.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nWhen looking at mean lung capacity of smokers versus non-smokers by age groups we can see lung capacity increasing consistently as age increases. For the two lowest age groups mean capacity is lower for non-smokers although the difference decreases as age increases; this trend is reversed from age 16 onwards as non-smokers overtake smokers in lung capacity. Across all age groups non-smokers also have a greater standard deviation in lung capacity compared to smokers with the age 13 and under non-smoker group having the greatest standard deviation. It is likely that the greater number of age 13 and under respondents is the reason why overall results mirror the distribution seen in the youngest age group."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f",
    "href": "posts/KenDocekal_HW1.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCovariance between lung capacity and age:\n\n\nCode\ncov(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 8.738289\n\n\nA positive covariance is shown which lets us know that as age increases lung capacity also increases.\nCorrelation between lung capacity and age:\n\n\nCode\ncor(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 0.8196749\n\n\nThe correlation coefficient is also positive; similar to the covariance this lets us know that there is a positive relationship between age and lung capacity. Additionally, since .819 is a relatively high score, as a score of 1 would indicate a perfect positive relationship, we know there is a strong relationship where a older respondent would be highly likely to have higher lung capacity and a younger respondent would likely have lower lung capacity."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a-1",
    "href": "posts/KenDocekal_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions:\nCreate data frame:\n\n\nCode\nconvictions<- c(0,1,2,3,4)\nprisoners<- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(convictions, prisoners)\n\ntibble(df)\n\n\n# A tibble: 5 × 2\n  convictions prisoners\n        <dbl>     <dbl>\n1           0       128\n2           1       434\n3           2       160\n4           3        64\n5           4        24\n\n\nProbability of exactly 2 prior convictions:\n\n\nCode\n160/sum(prisoners)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b-1",
    "href": "posts/KenDocekal_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability of fewer than 2 prior convictions (total # of prisoners with less than 2 prior convictions = 562):\n\n\nCode\n562/sum(prisoners)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c-1",
    "href": "posts/KenDocekal_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nProbability of 2 or fewer prior convictions (total # of prisoners with 2 or fewer prior convictions = 722):\n\n\nCode\n722/sum(prisoners)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d-1",
    "href": "posts/KenDocekal_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nProbability of more than 2 prior convictions (total # of prisoners with more than 2 prior convictions = 88):\n\n\nCode\n88/sum(prisoners)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e-1",
    "href": "posts/KenDocekal_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nThe expected value for the number of prior convictions (using the probability of observing each prisoner prior conviction group):\n\n\nCode\ncon1<- c(0,1,2,3,4)\npprob<- c(.158,.536,.198,.079,.028)\n\n\nsum(con1*pprob)\n\n\n[1] 1.281"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f-1",
    "href": "posts/KenDocekal_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nVariance and standard deviation for prior convictions:\n\n\nCode\nvar(prisoners)\n\n\n[1] 25948\n\n\nCode\nsd(prisoners)\n\n\n[1] 161.0838"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html",
    "href": "posts/Buck_Yoon_finalpart2.html",
    "title": "finalpart2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(gapminder)\n\n\nError in library(gapminder): there is no package called 'gapminder'\n\n\nCode\nlibrary(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#research-question",
    "href": "posts/Buck_Yoon_finalpart2.html#research-question",
    "title": "finalpart2",
    "section": "Research Question",
    "text": "Research Question\nwe are going to be using the National Longitudinal Study of Adolescent to Adult Health, 1994-2018 we are interested in exploring the relation between education levels and health."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#hypothesis",
    "href": "posts/Buck_Yoon_finalpart2.html#hypothesis",
    "title": "finalpart2",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are going to be using the hypothesis from researchers Eric R. Ride and Mark H. Showalter, but using the data from the National Longitudinal Study\nThere hypothesis was: ’The empirical link between education and health is firmly established. Numerous studies document that higher levels of education are positively associated with longer life and better health throughout the lifespan…But measuring the causal links between education and health is a more challenging task.” Estimating the relation between health and education: what do we know and what do we need to know?\nWe are hypothesizing that a positive correlation exists between education and health; the more education an individual receives, the better health the individual may have.\nWe want to look at the National Longitudinal Study of Adolescent to Adult Health 1992-2018 and observe what other factors beyond education there is that can affect the correlation to health. What are the potential moderating or mediating variables?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#descriptive-statistics",
    "href": "posts/Buck_Yoon_finalpart2.html#descriptive-statistics",
    "title": "finalpart2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis is an overview of the entire data set we are still determining which specific sections we want to analyze for our final project.\nAccording to ICPSR:\nStudy Purpose: Add Health was developed in response to a mandate from the U.S. Congress to fund a study of adolescent health. Waves I and II focused on the forces that may influence adolescents’ health and risk behaviors, including personal traits, families, friendships, romantic relationships, peer groups, schools, neighborhoods, and communities. As participants aged into adulthood, the scientific goals of the study expanded and evolved. Wave III explored adolescent experiences and behaviors related to decisions, behavior, and health outcomes in the transition to adulthood. Wave IV expanded to examine developmental and health trajectories across the life course of adolescence into young adulthood, using an integrative study design which combined social, behavioral, and biomedical measures data collection. Wave V aimed to track the emergence of chronic disease as the cohort aged into their 30s and early 40s.\nStudy Design: Add health is a school-based longitudinal study of a nationally-representative sample of adolescents in grates 7-12 in the United States in 1945-45. Over more than 20 years of data collection, data have been collected from adolescents, their fellow students, school administrators, parents, siblings, friends, and romantic partners through multiple data collection components. In addition, existing databases with information about respondents’ neighborhoods and communities have been merged with Add Health data, including variables on income poverty, unemployment, availability and utilization of health services, crime, church membership, and social programs and policies.\nSample:\n\nWave I: The Stage 1 in-school sample was a stratified, random sample of all high schools in the United States. A school was eligible for the sample if it included an 11th grade and had a minimum enrollment of 30 students. A feeder school – a school that sent graduates to the high school and that included a 7th grade – was also recruited from the community. The in-school questionnaire was administered to more than 90,000 students in grades 7 through 12. The Stage 2 in-home sample of 27,000 adolescents consisted of a core sample from each community, plus selected special over samples. Eligibility for over samples was determined by an adolescent’s responses on the in-school questionnaire. Adolescents could qualify for more than one sample.\nWave II: The Wave II in-home interview surveyed almost 15,000 of the same students one year after Wave I.\nWave III: The in-home Wave III sample consists of over 15,000 Wave I respondents who could be located and re-interviewed six years later.\nWave IV: All original Wave I in-home respondents were eligible for in-home interviews at Wave IV. At Wave IV, the Add Health sample was dispersed across the nation with respondents living in all 50 states. Administrators were able to locate 92.5% of the Wave IV sample and interviewed 80.3% of eligible sample members.\nWave V: All Wave I respondents who were still living were eligible at Wave V, yielding a pool of 19,828 persons. This pool was split into three stratified random samples for the purposes of survey design testing.\nTime Method: Longitudinal:Panel\nUniverse: Adolescents in grades 7 through 12 during the 1994-1995 school year. Respondents were geographically located in the United States.\nUnits of Observation: Individual\nData Types: Survey Data\nTime periods: 1994 - 2018\nDate of Collections: Wave 1(1994-01 - 1995-12), Wave II(1996-04 - 1996-09), Wave III(2001-04 - 2002 -04), Wave IV(2007-04 - 2009-01), Wave V(2016-03 - 2018-11)\nResponse Rates: Wave 1(79%), Wave 2(88.6%), Wave III(77.4%), Wave IV(80.3%), Wave V(71.8%)."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#part-2",
    "href": "posts/Buck_Yoon_finalpart2.html#part-2",
    "title": "finalpart2",
    "section": "Part 2",
    "text": "Part 2"
  },
  {
    "objectID": "posts/hw2_boonstra.html",
    "href": "posts/hw2_boonstra.html",
    "title": "Homework 2",
    "section": "",
    "text": "This question involves calculating 90% confidence intervals for data on mean wait time between heart surgery procedures being scheduled and the procedures being conducted for individuals in Ontario, Canada.\nThe equation for calculating confidence intervals using the Student’s t-distribution is as follows:\n\\(CI = \\overline{x} \\pm (t \\times \\frac{\\sigma}{\\sqrt{n}})\\)\n\n\nStarting with the Bypass subset, we can fill in some of these values:\n\\(CI_{bypass} = 19 \\pm (t \\times \\frac{10}{\\sqrt{539}})\\)\nThe t-quantile for the 90% confidence interval at 538 degrees of freedom is equal to 1.6476908, leaving us with the equation:\n\\(CI_{bypass} = 19 \\pm (1.648 \\times \\frac{10}{\\sqrt{539}})\\)\nThis maths out to 18.2902893 and 19.7097107.\n\n\n\nSimilarly, for the Angiography subset:\n\\(CI_{angiography} = 18 \\pm (t \\times \\frac{9}{847} )\\)\nThe t-quantile for the 90% confidence interval at 84 degrees of freedom is equal to 1.6466568, leaving us with the equation:\n\\(CI_{angiography} = 18 \\pm (1.647 \\times \\frac{9}{847} )\\)\nThis maths out to 17.4907818 and 18.5092182.\n\n\n\nBetween these two subsets, the 90% confidence interval is narrower for the Angiography subset:\n\\(CI_{bypass\\_range} = CI_{bypass\\_high} - CI_{bypass\\_low}=\\) 1.4194214\n\\(CI_{angiography\\_range}=CI_{angiography\\_high} - CI_{angiography\\_low}=\\) 1.0184363"
  },
  {
    "objectID": "posts/hw2_boonstra.html#t-tests",
    "href": "posts/hw2_boonstra.html#t-tests",
    "title": "Homework 2",
    "section": "t-tests",
    "text": "t-tests\nThese tests operate under the assumption that female employees’ pay data are randomly sampled and normally distributed.\n\nt.test(fem_pay,mu=500) # two-sided\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.01707\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 340.8199 479.1801\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"less\") # one-sided, H_0 mean is not less\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.008536\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 465.7864\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"greater\") # one-sided, H_0 mean is not greater\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.9915\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 354.2136      Inf\nsample estimates:\nmean of x \n      410 \n\n\n\nPart A\nFrom the first test, we can reject the null hypothesis that mean pay for female employees is equal to $500 per week. This holds at the 5% significance level, with a p-value of less than 0.02, and a t-statistic of -3.\n\n\nPart B\nFrom the second test, we get a p-value of less than 0.009, which enables us at the 5% significance level to reject the null hypothesis that mean pay for female employees is not less than $500, and accept the alternative hypothesis that mean pay is less than $500.\n\n\nPart C\nFrom the third test, we get a p-value of greater than 0.99, which mean that we fail to reject the null hypothesis that mean pay for female employees is not greater than $500."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html",
    "href": "posts/nboonstra_final_603_proposal.html",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "",
    "text": "Code\nrm(list=ls())\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#theory",
    "href": "posts/nboonstra_final_603_proposal.html#theory",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Theory",
    "text": "Theory\nA review of even such a small sample of the literature as the works mentioned above will clearly demonstrate that, beyond disagreement over the presence of partisan turnout bias, there is little consensus on the theoretical aspect of such a phenomenon. Before offering my hypothesis, therefore, I would like to briefly address this theoretical side of the argument.\nShaw and Petrocik (2020) take issue with a notion found in turnout bias literature, the notion being “that turnout is endogenous to candidate preference” (p. 53). They cite Downs’ (1957) famous equation, \\(V=(P*B)-C\\), as evidence that it is the intensity of one’s political beliefs, and not their direction, that determines the decision to vote or not, and that therefore turnout is not endogenous to candidate preference.\nI believe this argument misses a subtle nuance that is key to the turnout bias debate. Suppose that not all individuals in a given polity face the same costs to voting; assume, in other words, that a more accurate rendition of Downs’ equation would be \\(V_i=(P*B_i)-C_i\\), in which both cost of voting and the perceived benefit of a preferred candidate’s victory are unique to the individual. For the sake of this argument, the manner in which these costs are distributed is not important; only the fact that there are unequal costs matters. Suppose further that one of the parties in this polity has established itself as being the party that lobbies for a reduction in the cost of voting, particularly for those who face disproportionately high barriers. In a world of rational actors and perfect information, it would follow, ceteris paribus, that an individual who faced disproportionately high costs to voting would support this party, since this party would lobby to improve opportunities for this group. However, the very higher cost of voting that would motivate this individual to support this party could also prevent them from ultimately voting for that candidate in an election. Thus, it could be said that turnout is endogenous to candidate preference – or, more accurately, that the cost of voting is endogenous to both candidate preference and turnout.\nWe can apply this theoretical model to the American case. Certain individuals do face higher barriers to voting; unfortunately, unlike in the model, these barriers do tend to be distributed in a certain manner, often inequitably by race and socioeconomic status. Additionally, it would not be difficult to argue that, of the two major parties, the Democrats have placed themselves in the position of the party lobbying for expanded voting access and reduction of barriers to the ballot box, starting with their role in the Civil Rights movement and corresponding legislation, and continuing to the start of the present Congress and the introduction of H.R. 1, a bill explicitly aimed at expanding voting rights. Thus, while our world is not one of completely perfect information or completely rational actors, and while a number of factors contribute to partisan identity and vote choice, there is a reasonable case to be made that individuals who face barriers to voting, ceteris paribus, would be more likely to support the Democratic Party. Once again, these very barriers to voting that would push individuals toward the Democrats also can restrict them from expressing that preference at the ballot box. Thus, we have our situation of endogeneity between partisan preference and turnout."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "href": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Hypotheses",
    "text": "Hypotheses\nWith the theoretical argument out of the way, I can proceed to out line some of the hypotheses I would like to test with this project.\n\\(H_1\\): Higher turnout will benefit Democrats in state-level Presidential elections.\n\\(H_2\\): Democrats will perform better in state-level Presidential elections as turnout increases relative to the previous election in that state.\nThe distinction of state-level elections is an important one; Shaw and Petrocik (2020) tend to aggregate their data, either by assessing elections on the national level or by aggregating county-level data. In the United States, Presidential elections are conducted at the state level, and I believe that this is the appropriate level of analysis for this analysis."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "href": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Election Data, 1976-2020",
    "text": "Election Data, 1976-2020\nObtained from the MIT Election Project on 10/10/2022.\n\n\nCode\nelection_full <- read_csv(\"./_data/mit_election_1976_2020.csv\")\n\nelection_full <- election_full %>% \n  mutate(party_simplified2 = case_when(\n    party_detailed == \"DEMOCRAT\" ~ \"DEMOCRAT\",\n    party_detailed == \"REPUBLICAN\" ~ \"REPUBLICAN\",\n    party_detailed == \"LIBERTARIAN\" ~ \"LIBERTARIAN\",\n    party_detailed == \"GREEN\" ~ \"GREEN\",\n    party_detailed == \"INDEPENDENT\" ~ \"INDEPENDENT\",\n    TRUE ~ \"OTHER\"\n  )) %>% \n  mutate(party_dem = case_when(\n    party_detailed == \"DEMOCRAT\" ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(election_full, n=20)\n\n\n# A tibble: 20 × 17\n    year state    state…¹ state…² state…³ state…⁴ office candi…⁵ party…⁶ writein\n   <dbl> <chr>    <chr>     <dbl>   <dbl>   <dbl> <chr>  <chr>   <chr>   <lgl>  \n 1  1976 ALABAMA  AL            1      63      41 US PR… \"CARTE… DEMOCR… FALSE  \n 2  1976 ALABAMA  AL            1      63      41 US PR… \"FORD,… REPUBL… FALSE  \n 3  1976 ALABAMA  AL            1      63      41 US PR… \"MADDO… AMERIC… FALSE  \n 4  1976 ALABAMA  AL            1      63      41 US PR… \"BUBAR… PROHIB… FALSE  \n 5  1976 ALABAMA  AL            1      63      41 US PR… \"HALL,… COMMUN… FALSE  \n 6  1976 ALABAMA  AL            1      63      41 US PR… \"MACBR… LIBERT… FALSE  \n 7  1976 ALABAMA  AL            1      63      41 US PR…  <NA>   <NA>    TRUE   \n 8  1976 ALASKA   AK            2      94      81 US PR… \"FORD,… REPUBL… FALSE  \n 9  1976 ALASKA   AK            2      94      81 US PR… \"CARTE… DEMOCR… FALSE  \n10  1976 ALASKA   AK            2      94      81 US PR… \"MACBR… LIBERT… FALSE  \n11  1976 ALASKA   AK            2      94      81 US PR…  <NA>   <NA>    TRUE   \n12  1976 ARIZONA  AZ            4      86      61 US PR… \"FORD,… REPUBL… FALSE  \n13  1976 ARIZONA  AZ            4      86      61 US PR… \"CARTE… DEMOCR… FALSE  \n14  1976 ARIZONA  AZ            4      86      61 US PR… \"MCCAR… INDEPE… FALSE  \n15  1976 ARIZONA  AZ            4      86      61 US PR… \"MACBR… LIBERT… FALSE  \n16  1976 ARIZONA  AZ            4      86      61 US PR… \"CAMEJ… SOCIAL… FALSE  \n17  1976 ARIZONA  AZ            4      86      61 US PR… \"ANDER… AMERIC… FALSE  \n18  1976 ARIZONA  AZ            4      86      61 US PR… \"MADDO… AMERIC… FALSE  \n19  1976 ARIZONA  AZ            4      86      61 US PR…  <NA>   <NA>    TRUE   \n20  1976 ARKANSAS AR            5      71      42 US PR… \"CARTE… DEMOCR… FALSE  \n# … with 7 more variables: candidatevotes <dbl>, totalvotes <dbl>,\n#   version <dbl>, notes <lgl>, party_simplified <chr>,\n#   party_simplified2 <chr>, party_dem <dbl>, and abbreviated variable names\n#   ¹​state_po, ²​state_fips, ³​state_cen, ⁴​state_ic, ⁵​candidate, ⁶​party_detailed\n\n\nCode\ncolnames(election_full)\n\n\n [1] \"year\"              \"state\"             \"state_po\"         \n [4] \"state_fips\"        \"state_cen\"         \"state_ic\"         \n [7] \"office\"            \"candidate\"         \"party_detailed\"   \n[10] \"writein\"           \"candidatevotes\"    \"totalvotes\"       \n[13] \"version\"           \"notes\"             \"party_simplified\" \n[16] \"party_simplified2\" \"party_dem\"        \n\n\nCode\nsummary(election_full)\n\n\n      year         state             state_po           state_fips   \n Min.   :1976   Length:4287        Length:4287        Min.   : 1.00  \n 1st Qu.:1988   Class :character   Class :character   1st Qu.:16.00  \n Median :2000   Mode  :character   Mode  :character   Median :28.00  \n Mean   :1999                                         Mean   :28.62  \n 3rd Qu.:2012                                         3rd Qu.:41.00  \n Max.   :2020                                         Max.   :56.00  \n   state_cen        state_ic        office           candidate        \n Min.   :11.00   Min.   : 1.00   Length:4287        Length:4287       \n 1st Qu.:33.00   1st Qu.:22.00   Class :character   Class :character  \n Median :53.00   Median :42.00   Mode  :character   Mode  :character  \n Mean   :53.67   Mean   :39.75                                        \n 3rd Qu.:81.00   3rd Qu.:61.00                                        \n Max.   :95.00   Max.   :82.00                                        \n party_detailed      writein        candidatevotes       totalvotes      \n Length:4287        Mode :logical   Min.   :       0   Min.   :  123574  \n Class :character   FALSE:3807      1st Qu.:    1177   1st Qu.:  652274  \n Mode  :character   TRUE :477       Median :    7499   Median : 1569180  \n                    NA's :3         Mean   :  311908   Mean   : 2366924  \n                                    3rd Qu.:  199242   3rd Qu.: 3033118  \n                                    Max.   :11110250   Max.   :17500881  \n    version          notes         party_simplified   party_simplified2 \n Min.   :20210113   Mode:logical   Length:4287        Length:4287       \n 1st Qu.:20210113   NA's:4287      Class :character   Class :character  \n Median :20210113                  Mode  :character   Mode  :character  \n Mean   :20210113                                                       \n 3rd Qu.:20210113                                                       \n Max.   :20210113                                                       \n   party_dem     \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.1428  \n 3rd Qu.:0.0000  \n Max.   :1.0000  \n\n\nThis dataframe contains state-level election results for all 50 states and the District of Columbia for the six Presidential elections from 1976 to 2020. (I am currently not sure that I will use that entire date range, particularly because it does not exactly coincide with the turnout data available, but for now I am including the full data set.) Included in the dataframe are candidate vote totals and party affiliations, which I have used to add an extra column, party_dem, which is a dummy variable recording whether or not a given candidate is a Democrat. The data already come in tidy, which is a nice touch; a “case” or row is a given candidate’s performance in a given state’s Presidential election in a given year."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "href": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Turnout data, 1980-2014",
    "text": "Turnout data, 1980-2014\nObtained from the US Elections Project on 10/11/2022.\n\n\nCode\nturnout <- read_excel(\"./_data/1980-2014 November General Election.xlsx\",\n                      skip=2,\n                      col_types=c(\n                        \"numeric\",\"skip\",\"skip\",\"text\",\n                        \"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\"\n                      ),\n                      col_names=c(\n                        \"year\",\"state\",\n                        \"totballots_vep_rate\",\"highestoff_vep_rate\",\"highestoff_vap_rate\",\n                        \"totalballots_count\",\"highestoff_count\",\"vep_count\",\"vap_count\",\n                        \"noncitizen_percent\",\"prison_count\",\"probation_count\",\n                        \"parole_count\",\"totineligible_count\",\"overseas_count\"\n                      ))\n\nhead(turnout,n=20)\n\n\n# A tibble: 20 × 15\n    year state   totba…¹ highe…² highe…³ total…⁴ highe…⁵ vep_c…⁶ vap_c…⁷ nonci…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  2014 United…   0.367   0.36    0.332  8.33e7  8.17e7  2.27e8  2.46e8   0.084\n 2  2014 Alabama   0.332   0.329   0.315  1.19e6  1.18e6  3.59e6  3.75e6   0.025\n 3  2014 Alaska    0.548   0.542   0.51   2.85e5  2.82e5  5.21e5  5.53e5   0.039\n 4  2014 Arizona   0.341   0.334   0.295  1.54e6  1.51e6  4.51e6  5.11e6   0.101\n 5  2014 Arkans…   0.403   0.401   0.375  8.53e5  8.49e5  2.12e6  2.26e6   0.04 \n 6  2014 Califo…   0.307   0.299   0.247  7.51e6  7.32e6  2.44e7  2.96e7   0.168\n 7  2014 Colora…   0.547   0.537   0.494  2.08e6  2.04e6  3.80e6  4.13e6   0.072\n 8  2014 Connec…   0.425   0.423   0.385  1.10e6  1.09e6  2.58e6  2.83e6   0.082\n 9  2014 Delawa…   0.349   0.343   0.318  2.38e5  2.34e5  6.82e5  7.35e5   0.051\n10  2014 Distri…   0.357   0.353   0.32   1.77e5  1.75e5  4.96e5  5.47e5   0.094\n11  2014 Florida   0.433   0.428   0.376  6.03e6  5.95e6  1.39e7  1.58e7   0.106\n12  2014 Georgia   0.386   0.382   0.338  2.60e6  2.57e6  6.73e6  7.60e6   0.071\n13  2014 Hawaii    0.365   0.362   0.329  3.70e5  3.66e5  1.01e6  1.11e6   0.086\n14  2014 Idaho     0.398   0.393   0.365  4.45e5  4.40e5  1.12e6  1.21e6   0.046\n15  2014 Illino…   0.408   0.402   0.366  3.68e6  3.63e6  9.03e6  9.92e6   0.085\n16  2014 Indiana   0.287   0.278   0.267  1.39e6  1.34e6  4.83e6  5.03e6   0.035\n17  2014 Iowa      0.503   0.498   0.473  1.14e6  1.13e6  2.27e6  2.39e6   0.036\n18  2014 Kansas    0.433   0.425   0.398  8.87e5  8.70e5  2.05e6  2.18e6   0.052\n19  2014 Kentuc…   0.449   0.442   0.422  1.46e6  1.44e6  3.25e6  3.41e6   0.027\n20  2014 Louisi…   0.449   0.439   0.415  1.50e6  1.47e6  3.35e6  3.55e6   0.03 \n# … with 5 more variables: prison_count <dbl>, probation_count <dbl>,\n#   parole_count <dbl>, totineligible_count <dbl>, overseas_count <dbl>, and\n#   abbreviated variable names ¹​totballots_vep_rate, ²​highestoff_vep_rate,\n#   ³​highestoff_vap_rate, ⁴​totalballots_count, ⁵​highestoff_count, ⁶​vep_count,\n#   ⁷​vap_count, ⁸​noncitizen_percent\n\n\nCode\ncolnames(turnout)\n\n\n [1] \"year\"                \"state\"               \"totballots_vep_rate\"\n [4] \"highestoff_vep_rate\" \"highestoff_vap_rate\" \"totalballots_count\" \n [7] \"highestoff_count\"    \"vep_count\"           \"vap_count\"          \n[10] \"noncitizen_percent\"  \"prison_count\"        \"probation_count\"    \n[13] \"parole_count\"        \"totineligible_count\" \"overseas_count\"     \n\n\nCode\nsummary(turnout)\n\n\n      year         state           totballots_vep_rate highestoff_vep_rate\n Min.   :1980   Length:936         Min.   :0.0000      Min.   :0.2020     \n 1st Qu.:1988   Class :character   1st Qu.:0.4310      1st Qu.:0.4140     \n Median :1997   Mode  :character   Median :0.5200      Median :0.5010     \n Mean   :1997                      Mean   :0.5125      Mean   :0.4993     \n 3rd Qu.:2006                      3rd Qu.:0.6040      3rd Qu.:0.5840     \n Max.   :2014                      Max.   :0.7880      Max.   :0.7840     \n                                   NA's   :215         NA's   :1          \n highestoff_vap_rate totalballots_count  highestoff_count   \n Min.   :0.1990      Min.   :   122356   Min.   :   117623  \n 1st Qu.:0.3895      1st Qu.:   422851   1st Qu.:   488820  \n Median :0.4770      Median :  1170867   Median :  1236230  \n Mean   :0.4733      Mean   :  3074280   Mean   :  3509231  \n 3rd Qu.:0.5560      3rd Qu.:  2395791   3rd Qu.:  2336586  \n Max.   :0.7390      Max.   :132609063   Max.   :131304731  \n NA's   :1           NA's   :223         NA's   :1          \n   vep_count           vap_count         noncitizen_percent  prison_count    \n Min.   :   270122   Min.   :   277261   Min.   :0.00400    Min.   :      0  \n 1st Qu.:   999644   1st Qu.:  1044366   1st Qu.:0.01500    1st Qu.:   3464  \n Median :  2662524   Median :  2778086   Median :0.03100    Median :  10018  \n Mean   :  7277622   Mean   :  7840064   Mean   :0.04344    Mean   :  39257  \n 3rd Qu.:  4569632   3rd Qu.:  4898253   3rd Qu.:0.06600    3rd Qu.:  24819  \n Max.   :227157964   Max.   :245712915   Max.   :0.18900    Max.   :1605448  \n                                                                             \n probation_count    parole_count    totineligible_count overseas_count   \n Min.   :      0   Min.   :     0   Min.   :      0     Min.   :   6916  \n 1st Qu.:      0   1st Qu.:     0   1st Qu.:   6210     1st Qu.:  43108  \n Median :   7982   Median :  1870   Median :  21329     Median :  89605  \n Mean   :  67542   Mean   : 16227   Mean   :  90039     Mean   : 920963  \n 3rd Qu.:  38902   3rd Qu.:  6592   3rd Qu.:  52525     3rd Qu.:1803021  \n Max.   :2451708   Max.   :637410   Max.   :3363118     Max.   :5345814  \n                                                        NA's   :867      \n\n\nAdditional turnout data are available from the USEP by election from 2000-2020, albeit in their own individual spreadsheets; I may end up merging the 2016 and 2020 spreadsheets into this 1980-2014 set. It is important to note that this dataset includes observations for both Presidential and midterm election years, while I only intend to analyze Presidential elections.\nThis dataset makes distinctions between turnout based on voting-age population (VAP) and voting-eligible population (VEP). The literature generally agrees that VEP is the most reliable and consistent measure. However, given that one of the main differences between the two is the barrier of felony disenfranchisement, a barrier that is often inequitably distributed by race, I may end up using VAP turnout in my analysis; I have not yet decided as of the time of this submission."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "href": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Voter ID data, 2000-2020",
    "text": "Voter ID data, 2000-2020\nObtained from the National Conference of State Legislatures, who kindly provided via email a spreadsheet version of the data on this webpage on 10/11/2022.\n\n\nCode\nvoter_id <- read_excel(\"./_data/voter_id_chronology.xlsx\",\n                      skip = 2,\n                      col_types = c(\"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"skip\"))\n\nvoter_id <- voter_id %>% \n  pivot_longer(cols=c(2:7),\n               names_to=\"year\",\n               values_to=\"id_text\") %>% \n  mutate(id_req = case_when(\n    grepl(\"no id\", id_text, ignore.case = TRUE) ~ 0,\n    TRUE ~ 1\n  )) %>% \n  mutate(id_strict = case_when(\n    grepl(\"Strict\", id_text) ~ 1,\n    TRUE ~ 0\n  )) %>% \n  mutate(id_photo = case_when(\n    grepl(\" photo\", id_text, ignore.case = TRUE) ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(voter_id,n=20)\n\n\n# A tibble: 20 × 6\n   State    year  id_text                 id_req id_strict id_photo\n   <chr>    <chr> <chr>                    <dbl>     <dbl>    <dbl>\n 1 Alabama  2000  No ID required at polls      0         0        0\n 2 Alabama  2004  Non-strict, non-photo        1         0        0\n 3 Alabama  2008  Non-strict, non-photo        1         0        0\n 4 Alabama  2012  Non-strict, non-photo        1         0        0\n 5 Alabama  2016  Non-strict, photo            1         0        1\n 6 Alabama  2020  Non-strict, photo            1         0        1\n 7 Alaska   2000  Non-strict, non-photo        1         0        0\n 8 Alaska   2004  Non-strict, non-photo        1         0        0\n 9 Alaska   2008  Non-strict, non-photo        1         0        0\n10 Alaska   2012  Non-strict, non-photo        1         0        0\n11 Alaska   2016  Non-strict, non-photo        1         0        0\n12 Alaska   2020  Non-strict, non-photo        1         0        0\n13 Arizona  2000  No ID required at polls      0         0        0\n14 Arizona  2004  No ID required at polls      0         0        0\n15 Arizona  2008  Strict non-photo             1         1        0\n16 Arizona  2012  Strict non-photo             1         1        0\n17 Arizona  2016  Strict non-photo             1         1        0\n18 Arizona  2020  Strict non-photo             1         1        0\n19 Arkansas 2000  Non-strict, non-photo        1         0        0\n20 Arkansas 2004  Non-strict, non-photo        1         0        0\n\n\nCode\ncolnames(voter_id)\n\n\n[1] \"State\"     \"year\"      \"id_text\"   \"id_req\"    \"id_strict\" \"id_photo\" \n\n\nCode\nsummary(voter_id)\n\n\n    State               year             id_text              id_req      \n Length:306         Length:306         Length:306         Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :1.0000  \n                                                          Mean   :0.5033  \n                                                          3rd Qu.:1.0000  \n                                                          Max.   :1.0000  \n   id_strict          id_photo     \n Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000  \n Mean   :0.09804   Mean   :0.1928  \n 3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000  \n\n\nGiven that barriers to voting factor into the argument behind my research, I wanted to include data on voter ID laws in my analysis, as a controlling (or other type of) variable. The data here track voter ID laws across all 50 U.S. states and the District of Columbia from 2000 to 2020.\nThese data are surprisingly well balanced when it comes to the occurrence of voter ID laws; 50.33 percent of elections were held under voter-ID laws of some sort. Cases are also specified by whether or not a voter ID law was strict (i.e. required the voter to cast a provisional ballot and verify their identity after Election Day), and whether or not the state required a photo on the identification. Strict voter ID laws are the most rare, occurring in only 9.8 percent of elections in the data set; photo requirements are slightly more common, occurring in 19.28 percent of elections."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html",
    "href": "posts/NiyatiSharma_blog1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#introduction",
    "href": "posts/NiyatiSharma_blog1.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nCredit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the leader.So the purpose of credit analysis is to determine the creditworthiness of borrowers by measuring the risk of loss that the lender is exposed to.When calculating the credit risk of a particular borrower, lenders consider various factors like analyze different documents, such as the borrower’s income statement, balance sheet, credit reports, and other documents that reveal the financial situation of the borrower. to evaluate the characteristics of the borrower and conditions of the loan to estimate the probability of default and the subsequent risk of financial loss."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#research-question",
    "href": "posts/NiyatiSharma_blog1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nQ1. How credit risk depends on the age of the person. Q2. Dominating factor on which credit risk depends. Q3. Is credit risk depends on loan_intent?"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#hypothesis",
    "href": "posts/NiyatiSharma_blog1.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nAccording to research credit risk of a particular borrower, lenders consider various factors include the borrower’s capacity to repay are income, character, house ownership, and credit history. Check the relationship between the age, income with credit risk with new dataset."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#dataset",
    "href": "posts/NiyatiSharma_blog1.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThis dataset contains columns simulating credit bureau data, factors on which credit risk depends. The variables of interest for me are income, age, employment length and home ownership.\n\n\nCode\nlibrary(readr)\ndf <- read_csv(\"C:/Users/Lenovo/Downloads/credit_risk_dataset_1.csv\")\n\n\nError: 'C:/Users/Lenovo/Downloads/credit_risk_dataset_1.csv' does not exist.\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html",
    "href": "posts/HW2_ShoshanaBuck.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Tail area and standard error for Bypass",
    "text": "Tail area and standard error for Bypass\n\n\nCode\ntail_area<- (1-.90)/2\ntail_area\n\n\n[1] 0.05\n\n\nCode\nstandard_error<- 10/sqrt(539)\nstandard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "title": "Homework 2",
    "section": "t-value for Bypass",
    "text": "t-value for Bypass\n\n\nCode\nt_score<- qt(p= 1-tail_area, df= 538)\nt_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for Bypass",
    "text": "Confidence interval and margin of error for Bypass\n\n\nCode\nCI<- c(19 - t_score * standard_error, 19 + t_score * standard_error)\nCI\n\n\n[1] 18.29029 19.70971\n\n\nCode\nMOE<- t_score *standard_error\nMOE *1.41\n\n\n[1] 1.000692"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "title": "Homework 2",
    "section": "Standard error of angiography",
    "text": "Standard error of angiography\n\n\nCode\nstandard_error2<- 9/sqrt(847)\nstandard_error2\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "title": "Homework 2",
    "section": "t-score for angiography",
    "text": "t-score for angiography\n\n\nCode\nt_score2<- qt(p= 1-.05, df= 846)\nt_score2\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for angiography",
    "text": "Confidence interval and margin of error for angiography\n\n\nCode\nCI<- c(18 - t_score2 * standard_error2, 18 + t_score2 * standard_error2)\nCI\n\n\n[1] 17.49078 18.50922\n\n\nCode\nMOE2<- t_score2 *standard_error2\nMOE2 *1.01\n\n\n[1] 0.5143103\n\n\nThe Bypass points are [18.29029 & 19.70971] days and has a margin of error of +/- 0.7. Whereas the angiography is [17.49 & 18.50] days with a margin of error of +/- 0.5. Angigography is more narrower because it has a larger sample size and the range between the high and low end of the confidence interval is smaller."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "href": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "title": "Homework 2",
    "section": "Point estimate P",
    "text": "Point estimate P\n\n\nCode\ns_size<- 1031\nb<- 567\n\npoint_estimate<- b/s_size\npoint_estimate\n\n\n[1] 0.5499515"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "title": "Homework 2",
    "section": "95% confidence interval for P",
    "text": "95% confidence interval for P\n\n\nCode\nprop.test(b,s_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  b out of s_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nBased off the point estimate, 54% of the adult Americans that were surveyed by the National Center for Public Policy believe that college education is essential for success. 95% confidence interval of adult Americans who believe that college education is essential for success is [0.5189682 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "href": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "title": "Homework 2",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nCode\nsd<- (200-30)/4\nsd\n\n\n[1] 42.5"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "href": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "title": "Homework 2",
    "section": "Solving for n",
    "text": "Solving for n\n\n\nCode\n#steps for the equation\n#1. 5 = 1.96 * (42.5/sqrt(n))\n\n#2. 5 = 8.3/sqrt(n)\n\n#3. 5*sqrt(n)= 83.3\n\n#4. sqrt(n) = 83.3/5\n\n#5. n= (83.3/5)^2\n\n#6. n= 278.89\n\n\nThe standard deviation from the data is 42.5. Since we have solved for the standard deviation we can plug it into the CI equation and solve for n."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-4",
    "href": "posts/HW2_ShoshanaBuck.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nA\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. ## Assumptions\nWe are assuming there is normal distribution, the null hypothesis is: μ= 500 and the alternative hypothesis is 500> μ <500."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error",
    "title": "Homework 2",
    "section": "Standard error",
    "text": "Standard error\n\n\nCode\ns_sizef<- 9\nsd<-90\ns_meanf<- 410\nnull_hypo_mean<- 500\n\nstandard_errorf<- sd/sqrt(s_sizef)\nstandard_errorf\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score",
    "href": "posts/HW2_ShoshanaBuck.html#t-score",
    "title": "Homework 2",
    "section": "t-score",
    "text": "t-score\n\n\nCode\nt_stat<- (s_meanf-null_hypo_mean)/standard_errorf\nt_stat\n\n\n[1] -3\n\n\nI took the sample mean of 410 subtracted that from the mu = 500 and then divided it by the standard error = 30."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#p-value",
    "href": "posts/HW2_ShoshanaBuck.html#p-value",
    "title": "Homework 2",
    "section": "p-value",
    "text": "p-value\n\n\nCode\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nThe p-value than the 5% significance level so we can reject the null hypothesis in favor of the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#b-c",
    "href": "posts/HW2_ShoshanaBuck.html#b-c",
    "title": "Homework 2",
    "section": "B +C",
    "text": "B +C\nReport the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500.\n\n\nCode\nupper_p_value<- (pt(t_stat, df=8, lower.tail = FALSE))\nupper_p_value\n\n\n[1] 0.9914642\n\n\nCode\nlower_p_value<- (pt(t_stat, df=8, lower.tail = TRUE))\nlower_p_value\n\n\n[1] 0.008535841\n\n\nThe upper-tailed p-value is 0.99 and the lower-tailed p-value is 0.008. If you add the two tails together they will equal 1."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-5",
    "href": "posts/HW2_ShoshanaBuck.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\njones_sample_mean<- 519.5\nsmith_sample_mean<-519.7\nnull_hyp<- 500\njones_se<- 10\nsmith_se<- 10\n\n\n\nA: Jones t-score and p-value\n\n\nCode\njones_t_stat<- (jones_sample_mean-null_hyp)/jones_se\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value<- pt(jones_t_stat, df=999, lower.tail = FALSE) *2\njones_p_value\n\n\n[1] 0.05145555\n\n\n\n\nA: Smith t-score and p-value\n\n\nCode\nsmith_t_stat<-(smith_sample_mean-null_hyp)/smith_se\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value<- pt(smith_t_stat, df=999, lower.tail = FALSE)*2\nsmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\nB\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe results are “statistically significant when the p-value is smaller than the 0.05. Jones p-value is 0.051 which is greater than the 0.05 significance level which means it is not statistically significant and we cannot reject the null hypothesis. Smith’s p-value is 0.49 which is smaller than the significance level which means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\n\nC\n“P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” is a misleading statement without providing the p-values because it makes it seem that there is a drastic difference between Jones and Smith that caused one hypothesis to be statistically significant and the other one not to be. However, when looking at the actual p-value it can be noted that there is a very small difference between the values."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-6",
    "href": "posts/HW2_ShoshanaBuck.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents?\nAt the 95% confidence level the p-value is 0.03 which is less than the 5% significance level. This proves that we can reject the null hypothesis and that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/KenDocekal_finalproject1.html",
    "href": "posts/KenDocekal_finalproject1.html",
    "title": "Final Project 1",
    "section": "",
    "text": "Research Question\nHow much does state policy intervention impact future social and economic value preferences in residents?\nWhile political values often explicitly inform social and economic policy actions taken by governments, policy actions themselves can also affect the development of the values of both program recipients and the greater public. Low-income recipients are assumed to benefit from, and therefore favor, state intervention and redistributive policies while upper income groups are assumed to be against but this is not always true, especially at the program level (Bueno et al.). Authors like Holland note that “the poor only have an economic interest in supporting social expenditures in contexts where they expect policies to redistribute resources or risks in their favor”.\nThis study seeks to better understand the relationship between policy action and value formation at the sub-national level by looking at the effect of US state policy interventions on residents’ subsequent policy preferences. By looking at how differences in US states’ social and economic policy intervention from 1936 to 2000 we can see how these factors may shape the subsequent policy values of residents. The dataset “Correlates of State Policy” includes variables which also allow us to better understand the role of differences in policy design and implementation by controlling for variables that may moderate impact, such as the length of policy implementation (Soss) and differences in economic interest (Ansell).\nSources:\nAnsell, Ben. 2014. “The Political Economy of Ownership.” American Political Science Review 108(02):383{402.\nBoehmke, Frederick J., and Paul Skinner. 2012. “State Policy Innovativeness Revisited.” State Politics and Policy Quarterly, 12(3):303-29.\nBueno, Natalia and Nunes, Felipe and Zucco, Cesar, Making the bourgeoisie? Values, voice, and state-provided homeownership (January 7, 2022). SSRN.\nCaughey, Devin, and Christopher Warshaw. 2015. “The Dynamics of State Policy Liberalism, 1936–2014.” American Journal of Political Science, September. doi: 10.1111/ajps.12219.\nHolland, Alisha C. 2018. “Diminished Expectations: Redistributive preferences in truncated welfare states.” World Politics 70(4):555{594\nJacoby, William G., and Saundra K. Schneider. 2008. “A New Measure of Policy Spending Priorities in the American States.”\nJordan, Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing, MI: Institute for Public Policy and Social Research (IPPSR).\nRigby, Elizabeth and Gerald C. Wright. 2013. “Political Parties and Representation of the Poor in the American States.” American Journal of Political Science 57(3): 552-565.\nSoss, Joe. 1999. “Lessons of Welfare: Policy Design, Political Learning, and Political Action.” The American Political Science Review 93(2):363{380.\n\n\nHypothesis\nIncreased state intervention increases US state residents’ preference for future interventions in social and economic policy.\nThis study proposes to build on Bueno et al.’s exploration of the effects of state-provided home ownership on political values and policy preferences by exploring that relationship at the level of US states. Additionally, instead of focusing on a single social program, we will examine the cumulative effects of multiple policy interventions across 65 years in 50 US states. This will provide insights into the effect of public policy on value differences at the sub-national level and on different subgroups including program non-participants. We will be able to see how this relationship may vary according to state and population characteristics despite differences in policy design and implementation.\n\n\nDescriptive Statistics\nThis dataset is from the Correlates of State Policy Project by the Institute for Public Policy and Social Research at Michigan State University. The full dataset, which contains 928 variables and covers data from 1900 to 2016, draws from multiple sources including government agencies and peer-reviewed articles listed in the Sources section. Due to limited data coverage across all years however, this study will focus on the period from 1935 to 2000. We will examining the following 25 variables (listed with description and years available):\nIndependent-\nYear 1935 - 2000\nState 1935 - 2000\nEcondev - Did State adopt Strategic Planning for Economic Development? 1981 – 1992\nPldvpag - Did State adopt Planning/Development Agency? 1935 – 1978\nUrbrenen - Did State adopt Urban Renewal ? 1941 – 1952\nPollib_median - State Policy Liberalism Score – Median 1936 – 2014\nPolicypriorityscore - State Policy Priority Score - collective goods (e.g., education and highways) v particularized benefits (e.g., health care and welfare) 1982-2005\nPoptotal - Population Total 1900 – 2008\nPopfemale - Female Population 1994 – 2010\nNonwhite - Proportion of the population that is nonwhite 1974 - 2011\nSoc_capital_ma - Hawes et al. Weighted Moving Average Measure of Social Capital 1984 - 2011\nEvangelical_pop - Evangelical Population 1975 - 2013\nNewimmig - New Immigrant Green Card Holders 1988 – 2011\nPopdensity - Population Density 1975 – 1999\nGsp_q - Gross State Product Combined in Millions of 2016 Dollars 1963 – 2010\nGini_coef - Gini Coefficient 1917 - 2013\nHsdiploma - High School Diploma 1975 – 2006\nEducspend - State Education Spending 1975 – 2001\nNofelons - Number of Felons Ineligible to Vote 1980 – 2010\nCo2emissions - Total CO2 emissions from fossil-fuels (metric tons) 1960 – 2001\nIdeo - State Ideology Score 1976 – 2011\nDependent-\nVst_ec - Mean Economic Liberalism- All Voters 2000\nVst_soc - Mean Social Liberalism- All Voters 2000\nVavgec_low - Mean Economic Liberalism Score for Low Income Voting Citizens 2000\nVavgsoc_low - Mean Social Liberalism Score for Low Income Voting Citizens 2000\nReading in dataset\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\n\nstatedata <- read.csv(\"_data/correlatesofstatepolicyprojectv1_10.csv\")\n\n\nSpecifying variables\n\n\nCode\nstatedata1 = subset(statedata, select = c(policypriorityscore, econdev, pldvpag, urbrenen, year, state, poptotal, popfemale, nonwhite, soc_capital_ma, evangelical_pop, newimmig, popdensity, gsp_q, gini_coef, hsdiploma, educspend, nofelons, co2emissions, ideo, pollib_median,vst_ec, vst_soc, vavgec_low, vavgsoc_low))\n\n\nSpecifying date range\n\n\nCode\nsd <- subset(statedata1, year>1934 & year<2001, na.rm = TRUE ) \n\n\nDescriptive statistics\n\n\nCode\nstr(sd)\n\n\n'data.frame':   3366 obs. of  25 variables:\n $ policypriorityscore: num  NA NA NA NA NA NA NA NA NA NA ...\n $ econdev            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pldvpag            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urbrenen           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ year               : int  1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 ...\n $ state              : chr  \"Alaska\" \"Alaska\" \"Alaska\" \"Alaska\" ...\n $ poptotal           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popfemale          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nonwhite           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ soc_capital_ma     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ evangelical_pop    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ newimmig           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popdensity         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ gsp_q              : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gini_coef          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ hsdiploma          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ educspend          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nofelons           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ co2emissions       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ideo               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ pollib_median      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_ec             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_soc            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgec_low         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgsoc_low        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nCode\nglimpse(sd)\n\n\nRows: 3,366\nColumns: 25\n$ policypriorityscore <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ econdev             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pldvpag             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ urbrenen            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ year                <int> 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 19…\n$ state               <chr> \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", …\n$ poptotal            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popfemale           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nonwhite            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ soc_capital_ma      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ evangelical_pop     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ newimmig            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popdensity          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gsp_q               <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gini_coef           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hsdiploma           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ educspend           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nofelons            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ co2emissions        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ideo                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pollib_median       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_ec              <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_soc             <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgec_low          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgsoc_low         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nsummary(sd)\n\n\n policypriorityscore    econdev           pldvpag          urbrenen     \n Min.   :-0.2296     Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.0372     1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median : 0.0144     Median :0.00000   Median :1.0000   Median :1.0000  \n Mean   : 0.0093     Mean   :0.09364   Mean   :0.7703   Mean   :0.5327  \n 3rd Qu.: 0.0638     3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   : 0.1987     Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :2416        NA's   :66        NA's   :66       NA's   :66      \n      year         state              poptotal          popfemale       \n Min.   :1935   Length:3366        Min.   :  100000   Min.   :  236763  \n 1st Qu.:1951   Class :character   1st Qu.:  960954   1st Qu.:  645293  \n Median :1968   Mode  :character   Median : 2600000   Median : 1900000  \n Mean   :1968                      Mean   : 3892303   Mean   : 2692111  \n 3rd Qu.:1984                      3rd Qu.: 4700000   3rd Qu.: 3100000  \n Max.   :2000                      Max.   :34000000   Max.   :17000000  \n                                   NA's   :30         NA's   :3009      \n    nonwhite      soc_capital_ma    evangelical_pop    newimmig     \n Min.   :0.0048   Min.   :-2.9133   Min.   : 1.10   Min.   :   159  \n 1st Qu.:0.0785   1st Qu.:-0.4193   1st Qu.: 9.60   1st Qu.:  1518  \n Median :0.1360   Median : 0.2357   Median :14.10   Median :  3973  \n Mean   :0.1752   Mean   : 0.3108   Mean   :18.83   Mean   : 18447  \n 3rd Qu.:0.2586   3rd Qu.: 1.0615   3rd Qu.:26.00   3rd Qu.: 11424  \n Max.   :0.7130   Max.   : 3.0868   Max.   :74.00   Max.   :732735  \n NA's   :2016     NA's   :2550      NA's   :2066    NA's   :2703    \n   popdensity            gsp_q           gini_coef        hsdiploma    \n Min.   :   0.6496   Min.   :    993   Min.   :0.3215   Min.   : 0.00  \n 1st Qu.:  31.2611   1st Qu.:  12325   1st Qu.:0.4324   1st Qu.:73.90  \n Median :  85.3188   Median :  31568   Median :0.4667   Median :76.80  \n Mean   : 163.7982   Mean   :  74118   Mean   :0.4766   Mean   :75.94  \n 3rd Qu.: 165.7868   3rd Qu.:  83769   3rd Qu.:0.5147   3rd Qu.:80.80  \n Max.   :1082.7000   Max.   :1300000   Max.   :0.7172   Max.   :91.80  \n NA's   :2116        NA's   :1428      NA's   :48       NA's   :2054   \n   educspend          nofelons       co2emissions         ideo        \n Min.   :    0.0   Min.   :     0   Min.   :  4.00   Min.   :-0.5806  \n 1st Qu.:  816.2   1st Qu.:  4668   1st Qu.: 24.00   1st Qu.:-0.2157  \n Median : 1809.5   Median : 15733   Median : 60.00   Median :-0.1392  \n Mean   : 3421.9   Mean   : 34844   Mean   : 88.28   Mean   :-0.1364  \n 3rd Qu.: 4058.0   3rd Qu.: 41280   3rd Qu.:107.50   3rd Qu.:-0.0625  \n Max.   :35482.0   Max.   :499362   Max.   :669.00   Max.   : 0.4545  \n NA's   :2054      NA's   :2805     NA's   :1275     NA's   :2129     \n pollib_median          vst_ec          vst_soc         vavgec_low    \n Min.   :-2.32065   Min.   :-0.367   Min.   :-0.379   Min.   :-0.387  \n 1st Qu.:-0.66509   1st Qu.:-0.171   1st Qu.:-0.163   1st Qu.: 0.021  \n Median :-0.07600   Median :-0.094   Median :-0.001   Median : 0.108  \n Mean   :-0.01096   Mean   :-0.094   Mean   :-0.024   Mean   : 0.079  \n 3rd Qu.: 0.68865   3rd Qu.:-0.047   3rd Qu.: 0.106   3rd Qu.: 0.174  \n Max.   : 2.57199   Max.   : 0.147   Max.   : 0.357   Max.   : 0.290  \n NA's   :114        NA's   :3319     NA's   :3319     NA's   :3319    \n  vavgsoc_low    \n Min.   :-0.466  \n 1st Qu.:-0.200  \n Median :-0.093  \n Mean   :-0.073  \n 3rd Qu.: 0.052  \n Max.   : 0.377  \n NA's   :3319"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html",
    "href": "posts/HW2_PrahithaMovva.html",
    "title": "Homework 2 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(stats)\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#angiography",
    "href": "posts/HW2_PrahithaMovva.html#angiography",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Angiography",
    "text": "Angiography\n\n\nCode\nsample.mean <- 18\nsample.n <- 847\nsample.sd <- 9\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 17.49078 18.50922\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.018436"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#bypass",
    "href": "posts/HW2_PrahithaMovva.html#bypass",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Bypass",
    "text": "Bypass\n\n\nCode\nsample.mean <- 19\nsample.n <- 539\nsample.sd <- 10\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 18.29029 19.70971\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.419421\n\n\nThe 90% confidence interval for angiography is [17.49, 18.51] wait days (1.02) and for bypass is [18.29, 19.71] wait days (1.42). The confidence interval for angiography is slightly narrower (by 0.4)."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a",
    "href": "posts/HW2_PrahithaMovva.html#a",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\nAssumptions:\n\nThe data is normally distributed\nHo is true\n95% CI\n\n\n\nCode\nt.numerator <- sample.mean - population.mean\nt.denominator <- sample.s/sqrt(sample.n)\nt.statistic <- t.numerator/t.denominator\n\np.value <- pt(q=abs(t.statistic), df=sample.n-1, lower.tail=F)*2\nprint(t.statistic)\n\n\n[1] -3\n\n\nCode\nprint(p.value)\n\n\n[1] 0.01707168\n\n\nThe t statistic is -3 and the p-value at 5% significance level is 0.017. Since the p-value is less than 0.05, it is evidence against Ho, i.e., the mean income of female employees differ significantly from $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b",
    "href": "posts/HW2_PrahithaMovva.html#b",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\np.value_less <- pt(q=t.statistic, df=sample.n-1, lower.tail=T)\nprint(p.value_less)\n\n\n[1] 0.008535841\n\n\nHere too, the p-value at 5% significance level is less than 0.05. So we reject Ho and can say that the mean income of female employees is significantly less than $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c",
    "href": "posts/HW2_PrahithaMovva.html#c",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\np.value_greater <- pt(q=t.statistic, df=sample.n-1, lower.tail=F)\nprint(p.value_greater)\n\n\n[1] 0.9914642\n\n\nHere, the p-value at 5% significance level is higher than 0.05 and we fail to reject Ho. This means, we do not have evidence that the mean income of female employees is more than $500 per week.\n\n\nCode\np.value_greater + p.value_less\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a-1",
    "href": "posts/HW2_PrahithaMovva.html#a-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\njones.t <- ((jones.mean-population.mean)/jones.se)\njones.t\n\n\n[1] 1.95\n\n\nCode\njones.p <- pt(q=abs(jones.t), df=sample.n-1, lower.tail=F)*2\njones.p\n\n\n[1] 0.05145555\n\n\nCode\nsmith.t <- ((smith.mean-population.mean)/smith.se)\nsmith.t\n\n\n[1] 1.97\n\n\nCode\nsmith.p <- pt(q=abs(smith.t), df=sample.n-1, lower.tail=F)*2\nsmith.p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b-1",
    "href": "posts/HW2_PrahithaMovva.html#b-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\nAt 5% significance level, the result for Smith is statistically significant but that of Jones is not"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c-1",
    "href": "posts/HW2_PrahithaMovva.html#c-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\nThis example shows using P > 0.05 or P <= 0.05 to see if we can the reject the null hypothesis or not is misleading, if the actual p-value is not reported. Both the p-values are only 0.1 significance level away from 0.05 but only one is significant, so the experiment might not be meaningful without the actual p-values."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html",
    "href": "posts/Final Project - Working Draft.html",
    "title": "",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\n\nError in library(markdown): there is no package called 'markdown'\n\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ readr   2.1.3      ✔ stringr 1.4.1 \n✔ purrr   0.3.5      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#research-question",
    "href": "posts/Final Project - Working Draft.html#research-question",
    "title": "",
    "section": "Research Question",
    "text": "Research Question\nI am interested in examining the relationship between exports from China to the US, and the increase in Co2 emissions over the years. China is marked to be the highest Co2 emitting country followed by the US. However, it is clear that China pulls some of the carbon weight for the United States by manufacturing a wealth of goods. I would like to explore the connection between exports to the US and increasing carbon emissions. This could be helpful in guiding policy in international trade and climate change moving forward. I would also like to specify the industries and goods more closely related to these emissions, if possible."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#hypothesis",
    "href": "posts/Final Project - Working Draft.html#hypothesis",
    "title": "",
    "section": "Hypothesis",
    "text": "Hypothesis\nI am starting with the hypothesis that there exports from China to the US is positively related with its Co2 emissions. However, taken together, its also significant to ask whether or not export process alone accounts for the greater sum of Co2 emission increases. The manufacturing process likely plays a role here, and may be considered for further analyses. Exports may be used in an inferential manner, suggesting that increased exports indicate higher rates of manufacturing that could thereby increase Co2 emissions. As such, exports would be an indirect measure of domestic activity, the correlation of which could lead to further insights. A search of datasets with more direct measures of carbon emissions in China relating to trade and supply of goods to the US may also be considered."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#export-data",
    "href": "posts/Final Project - Working Draft.html#export-data",
    "title": "",
    "section": "Export Data",
    "text": "Export Data\nTwo initial datasets have been pulled for the purpose of this study. The first set includes data on exports from China to the US. The set has 2 columns of interest representing the date, value (in US dollars”). The overall dataset has 3 columns, each containing 30 rows. This dataset was chosen because it covers a relatively adequate sample range from 1992-2020. This data was pulled from the United Nations COMTRADE database on comerce and trade.\n\ncomtrade_historical_CHNUSA00002 <- read.csv(\"~/Downloads/comtrade_historical_CHNUSA00002.csv\")\n\nWarning in file(file, \"rt\"): cannot open file '/home/runner/Downloads/\ncomtrade_historical_CHNUSA00002.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nexports<-comtrade_historical_CHNUSA00002\n\nError in eval(expr, envir, enclos): object 'comtrade_historical_CHNUSA00002' not found\n\nglimpse(exports)\n\nError in glimpse(exports): object 'exports' not found\n\nsummary(exports)\n\nError in summary(exports): object 'exports' not found"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#carbon-data",
    "href": "posts/Final Project - Working Draft.html#carbon-data",
    "title": "",
    "section": "Carbon Data",
    "text": "Carbon Data\nCarbon data was pulled to show the difference in carbon emissions from china between the years 2010-2020. It is unclear, yet, if this dataset will be used for final drafts, as it exludes a number of years reviewed in the export data. As a result, the gaps in years may lead to weaker analyses. For now, this data will be considered.\n\nstatistic_id270499_global_co2_emissions_by_select_country_2010_2020<-read_excel(\"Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx\")\n\nError: `path` does not exist: 'Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx'\n\n\n\ncarbon<-statistic_id270499_global_co2_emissions_by_select_country_2010_2020\n\nError in eval(expr, envir, enclos): object 'statistic_id270499_global_co2_emissions_by_select_country_2010_2020' not found\n\nsummarize(carbon)\n\nError in summarize(carbon): object 'carbon' not found\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html",
    "href": "posts/NiyatiSharma_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\n#| label: setup\n#| warning: false\n#| message: false\n \nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section",
    "href": "posts/NiyatiSharma_HW2.html#section",
    "title": "Homework 2",
    "section": "1",
    "text": "1\nCreating the table with the given data.\n\n\nCode\nSP <- c('Bypass', 'Angiography')\nSS <- c(539, 847)\nMW <- c(19, 18)\nSD <- c(10, 9)\n\nServeyData <- data.frame(SP, SS, MW, SD)\nServeyData\n\n\n           SP  SS MW SD\n1      Bypass 539 19 10\n2 Angiography 847 18  9\n\n\nCalculate Standard error\n\n\nCode\nSE <- SD / sqrt(SS)\nSE\n\n\n[1] 0.4307305 0.3092437\n\n\ncalculate the area of the two tails\n\n\nCode\nCL <- 0.90  \n#area in each tail of the distribution for 90%\ntail_area <- (1-CL)/2\ntail_area\n\n\n[1] 0.05\n\n\ncalculate t-values by using the qt() function\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = SS-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\ncalculate the confidence interval\n\n\nCode\nCI <- c(MW - t_score * SE,\n        MW + t_score * SE)\nprint(CI)\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days and for angiography it is [17.49, 18.51] days. The confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-1",
    "href": "posts/NiyatiSharma_HW2.html#section-1",
    "title": "Homework 2",
    "section": "2",
    "text": "2\nUsing prop.test() to calculate p and the 95% confidence interval.\n\n\nCode\nset.seed(0)\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the point estimate is 0.5195839 - 0.5803191.The point estimate for the proportion of all adult Americans who believe that a college education is essential for success is 0.55."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-2",
    "href": "posts/NiyatiSharma_HW2.html#section-2",
    "title": "Homework 2",
    "section": "3",
    "text": "3\nCalculate the min sample size\n\n\nCode\n# calculate population SD.\nSD <- (200-30)/4\n#margin of error\nME <- (10/2)\n# calculate sample size.\nsamplesize <- ((1.96*SD)/ME)^2\nsamplesize\n\n\n[1] 277.5556\n\n\nthe size of the sample should be 278"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-3",
    "href": "posts/NiyatiSharma_HW2.html#section-3",
    "title": "Homework 2",
    "section": "4",
    "text": "4"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a",
    "href": "posts/NiyatiSharma_HW2.html#a",
    "title": "Homework 2",
    "section": "a",
    "text": "a\ncalculate t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\nt_stats <- (410-500)/(90/sqrt(9))\nt_stats\n\n\n[1] -3\n\n\nCalculate P value\n\n\nCode\np_value <- 2* pt(t_stats, df=8)\np_value\n\n\n[1] 0.01707168\n\n\nThe test statistic is -3 and the p-value is 0.01707168.The p-value is substantially less than .05 is the evidence that we can reject the null hypothesis. There is strong evidence that the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b",
    "href": "posts/NiyatiSharma_HW2.html#b",
    "title": "Homework 2",
    "section": "b",
    "text": "b\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = TRUE)\nPL\n\n\n[1] 0.008535841\n\n\nSince p-value is 0.0085 is less than the alpha level of 0.05, we can reject the null hypothesis. There is evidence that the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c",
    "href": "posts/NiyatiSharma_HW2.html#c",
    "title": "Homework 2",
    "section": "c",
    "text": "c\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = FALSE)\nPL\n\n\n[1] 0.9914642\n\n\nSince p-value is 0.991 is more than the alpha level of 0.05, we cannot reject the null hypothesis. There is evidence that the mean income of female employees is more than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-4",
    "href": "posts/NiyatiSharma_HW2.html#section-4",
    "title": "Homework 2",
    "section": "5",
    "text": "5"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a-1",
    "href": "posts/NiyatiSharma_HW2.html#a-1",
    "title": "Homework 2",
    "section": "a",
    "text": "a\n\n\nCode\n# calculate standard deviation \nStd_Dev <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_jones <- ((519.5-500)/Std_Dev) * sqrt(1000)\nt_jones\n\n\n[1] 1.95\n\n\nCode\n# calculate p-value for Jones.\np_jones <- 2*(pt(q=t_jones, df=999, lower.tail=FALSE))\np_jones\n\n\n[1] 0.05145555\n\n\nCode\n# calculate t for Smith.\nt_smith <- ((519.7-500)/Std_Dev) * sqrt(1000)\nt_smith\n\n\n[1] 1.97\n\n\nCode\n# calculate p-value for Smith.\np_smith <- 2*(pt(q=t_smith, df=999, lower.tail=FALSE))\np_smith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b-1",
    "href": "posts/NiyatiSharma_HW2.html#b-1",
    "title": "Homework 2",
    "section": "b",
    "text": "b\nAt the .05 significance level, we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c-1",
    "href": "posts/NiyatiSharma_HW2.html#c-1",
    "title": "Homework 2",
    "section": "c",
    "text": "c\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-5",
    "href": "posts/NiyatiSharma_HW2.html#section-5",
    "title": "Homework 2",
    "section": "6",
    "text": "6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nHere we can see that the p value for this is .038 which means we can reject the null hypothesis that gas prices are equal to or greater than 45 cents. The mean sample that came up was also within the range of the confidence interval."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html",
    "href": "posts/FinalProjectPart2_DonnySnyder.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(pollster)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(knitr)\nlibrary(foreign)\nlibrary(scales)\nlibrary(questionr)\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ purrr   0.3.5\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n\n\nCode\nlibrary(tibble)\nlibrary(extrafont)\n\n\nRegistering fonts with R\n\n\nCode\nlibrary(tidyr)\nlibrary(readr)\nlibrary(irr)\n\n\nLoading required package: lpSolve\n#I have added my analyses on to my initial template from Part 1 - I hope this is what you had envisioned for this assignment"
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#research-question",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#research-question",
    "title": "Final Project Part 2",
    "section": "Research Question",
    "text": "Research Question\nAffective polarization describes a heightened state of animosity between partisans that has steadily grown from the 1970s to today (Iyengar et al., 2019). Identifying antecedents of affective polarization is essential to creating intervention strategies into this negative state of politics. Levendusky (2009) proposes a social model where individuals making sense of simplified elite cues enables people to understand the relevant identities of the political landscape, which may lead to downstream affective polarization. I intend to expand on this model, testing a construct of construal level, or the level of abstraction to concreteness (Trope & Liberman, 2010) with which partisans perceive partisan groups and group cues. This levels varies in how individuals describe different constructs as having more concrete or abstract characteristics, such as mentioning specific groups as opposed to vague ideological concepts (view the Appendix at the bottom for more information on how this was qualitatively coded). Prior studies suggest that lower construal may serve as an antecedent to affective polarization when partisans view issues in more concrete, group terms (Snyder, Unpublished). This study will expand these models into extant, large scale, political science datasets. Additionally, this project will employ supervised machine learning models to qualitatively code a large-n sample of free response questions."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#hypotheses",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#hypotheses",
    "title": "Final Project Part 2",
    "section": "Hypotheses",
    "text": "Hypotheses\nI hypothesize that partisans who are qualitatively coded as having a lower construal level will demonstrate higher levels of group/affective polarization, as measured on a feeling thermometer or measures of feelings about political groups - whichever is available in the datasets.\nI hypothesize that using a sentiment analysis, these tendencies may be moderated by valence of their free response, with stronger valence enhancing the effect of construal level on affective polarization - valence being the psychological term for positive vs negative sentiment."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#datasets",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#datasets",
    "title": "Final Project Part 2",
    "section": "Datasets",
    "text": "Datasets\nI intend to use UMass Poll, ANES, and Polarization Research Lab datasets for my studies. UMass Poll data will be used for Study 1, which is shown in the initial analyses here. Studies 2 and 3 will include data (that will be collected later in November) obtained from an accepted application for survey space from Dartmouth’s Polarization Research Lab. ANES data, specifically the free response questions in 1992, will be used to compare the qualitative coding results of the free responses to Mason’s social sorting measures. I hope you will forgive me not having all of these results right now, as I will need to hand code 2000+ cases before these studies are over, and some of the data has not been collected yet, but should be before the Final Project is due.\n#Analyses As mentioned in the previous section, initial analyses have been performed to test the first hypothesis. UMass Poll data has been qualitatively coded and analyzed for this purpose. In order to measure the effectiveness of the qualitative coding, multiple models and portions of variables were compared.\n\n\nCode\n#Clean Data\ndoto <- read_sav(\"12_21_Data.sav\")\ncaitCode <- read_csv(\"CaitlynQualCodingCP.csv\")\n\n\nRows: 815 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Response\ndbl (4): Resp#, Who, How, model1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndCode <- read_csv(\"qualCodingCRTno99.csv\")\n\n\nNew names:\nRows: 815 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Who, shows a concrete understanding of group information dbl (4):\nrespondent, Who, How, model1 lgl (1): ...5\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...5`\n\n\nCode\ndoto <- subset(doto, doto$Q33_open != \"__NA__\")\nRdoto <- subset(doto, doto$pid3lean == \"Republicans\")\nDdoto <- subset(doto, doto$pid3lean == \"Democrats\")\nIdoto <- subset(doto, doto$pid3lean == \"Independents\")\n\ndoto$affPol <- doto$Q10_democrats - doto$Q10_republicans\ndoto$CCModel2 <- caitCode$model1\ncaitCode$whoOrHow <- recode(caitCode$model1, \"0\" = \"0\", .default = \"1\")\ndoto$CCModel <- caitCode$whoOrHow\ndoto$absAffPol <- abs(doto$affPol)\ndoto$dummy <- rep(\"Dummy\", 815)\n\ndata <- aggregate(absAffPol ~ CCModel2, doto, mean)\npartyData <- aggregate(absAffPol ~ CCModel2 + pid3lean, doto, mean)\npartyData <- partyData[-(1:3),]\ndata$CCModel2 <- recode(data$CCModel2, \"2\" = \"Low Construal\", \"1\" = \"Medium Construal\", \"0\" = \"High Construal\")\ndata$CCModel2 <- factor(data$CCModel2, levels = c(\"High Construal\", \"Medium Construal\", \"Low Construal\"))\n\npartyData$CCModel2 <- recode(partyData$CCModel2, \"2\" = \"Low Construal\", \"1\" = \"Medium Construal\", \"0\" = \"High Construal\")\npartyData$CCModel2 <- factor(partyData$CCModel2, levels = c(\"High Construal\", \"Medium Construal\", \"Low Construal\"))\n\n#Plot Data\nggplot(data = data, aes(x = CCModel2, y = absAffPol)) + geom_bar(stat = \"identity\")\n\n\n\n\n\nCode\nggplot(data = partyData, aes(x = CCModel2, y = absAffPol, fill = pid3lean)) + geom_bar(stat = \"identity\", position = \"dodge\") + scale_fill_manual(values = c(\"Democrats\" = \"#00405b\", \"Independents\" = \"#4F7942\", \"Republicans\" = \"#7d0000\")) \n\n\n\n\n\nCode\n#comparison of regression models (1 is the best model)\nmodel1 <- lm(formula = absAffPol ~ CCModel2 + educ + newsint, data = doto)\nsummary(model1)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2 + educ + newsint, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.961 -22.197   2.562  21.240  70.032 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  56.3856     3.6274  15.544  < 2e-16 ***\nCCModel2      5.6838     1.4298   3.975 7.67e-05 ***\neduc          0.7301     0.6821   1.070    0.285    \nnewsint      -6.9825     0.9325  -7.488 1.84e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.24 on 803 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.1049,    Adjusted R-squared:  0.1015 \nF-statistic: 31.36 on 3 and 803 DF,  p-value: < 2.2e-16\n\n\nCode\nmodel2 <- lm(formula = absAffPol ~ CCModel2, data = doto)\nsummary(model2)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.184 -24.443   3.299  23.299  54.299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   45.701      1.473  31.035  < 2e-16 ***\nCCModel2       7.742      1.455   5.322 1.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.3 on 805 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.03399,   Adjusted R-squared:  0.03279 \nF-statistic: 28.33 on 1 and 805 DF,  p-value: 1.331e-07\n\n\nCode\nmodel3 <- lm(formula = absAffPol ~ CCModel, data = doto)\nsummary(model3)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.089 -25.089   3.911  22.911  53.763 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   46.237      1.585   29.16  < 2e-16 ***\nCCModel1       8.852      2.098    4.22 2.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.49 on 805 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.02164,   Adjusted R-squared:  0.02043 \nF-statistic: 17.81 on 1 and 805 DF,  p-value: 2.721e-05\n\n\nCode\nmodel4 <- lm(formula = absAffPol ~ CCModel2 + CCModel, data = doto)\nsummary(model4)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2 + CCModel, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.705 -23.971   2.763  23.652  53.763 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   46.237      1.575  29.348  < 2e-16 ***\nCCModel2      10.357      3.094   3.347 0.000854 ***\nCCModel1      -4.246      4.433  -0.958 0.338511    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.31 on 804 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.03509,   Adjusted R-squared:  0.03269 \nF-statistic: 14.62 on 2 and 804 DF,  p-value: 5.801e-07\n\n\nCode\n#checking for interrater reliability of qualitative coding\nirrCheck <- cbind(caitCode$Who, caitCode$How, dCode$Who, dCode$How)\nirrCheck <- as.data.frame(irrCheck)\n\nkappa2(irrCheck[,c(1,3)], \"unweighted\")\n\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 815 \n   Raters = 2 \n    Kappa = 0.792 \n\n        z = 22.9 \n  p-value = 0 \n\n\nCode\nkappa2(irrCheck[,c(2,4)], \"unweighted\")\n\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 815 \n   Raters = 2 \n    Kappa = 0.7 \n\n        z = 20.3 \n  p-value = 0 \n\n\nCode\n#Represents very high interrater reliability for both\n\n\n#Choosing the Final Model The Final model (model 1), was chosen because it represents the highest amount of explanatory power. It is the most significant of any of the variables, and this significance does not disappear when put into a multiple regression model with other explanatory variables, both of Affective Polarization and of other qualitative coding models/combinations. Both the “Who” and “How” aspects of the qualitative coding are shown to independently capture portions of what we operationalize as construal level, which is shown to relate to downstream affective polarization. In future studies, I will also test additional qualitative coding models, as this model is most applicable to the vague UMass Poll question of issue-oriented responses to critical race theory. In this first model, valence is partially coded for in the “how” term, which is why a sentiment analysis has not been performed yet for this Study. In subsequent models in Studies 2 and 3, this will be examined in tandem with an alternative potential coding scheme."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#references",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#references",
    "title": "Final Project Part 2",
    "section": "References",
    "text": "References\nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). The origins and consequences of affective polarization in the United States. Annual Review of Political Science, 22(1), 129-146. Levendusky, M. (2009). The partisan sort: How liberals became Democrats and conservatives became Republicans. University of Chicago Press. Snyder, D. (2022). Keep It Simple Stupid: How Individual Differences in Cue Construal Explain Variations in Affective Polarization. Unpublished Manuscript Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. Psychological review, 117(2), 440.\n#Appendix: Qualitative Coding Instructions for Raters Construal Level Theory Qualitative Coding Key\nThe purpose of this task is to get an understanding of how clear respondents’ perceptions of the political landscape are, thorough the lens of how they interpret abstract issues. To operationalize this for the current task, you will be qualitatively coding along two dimensions – “Who” and “How”. Each of these dimensions will be coded as either a 1 or a 0. This is taken from the Critical Race Theory data, although the measurement does not apply to CRT in particular, it is somewhat unrelated.\n“Who” Overview: The purpose of coding this “Who” construct is to understand how clearly people perceive the groups involved in political issues. A “1” for this will involve a specific group mentioned. A specific group involves a group name that cannot be interpreted in multiple ways. This includes any concrete demographic.\nWho “1” examples: Mention of… “Democrats”, “Republicans”, “Marxists”, “Kids”, “Blacks”, “Whites”, “Teachers”… any other instances where it is expressly clear that they are referring to a specific group.\nIf there is even one occurrence of any of these in the free response, it should be coded as “1.”\nWho “0” examples: No groups mentioned, or groups are only mentioned in the abstract. Examples of groups mentioned in the abstract: “People of Color”, “Certain groups of people.”\nIf there is no specific group information, or only abstract groups, or they respond “I don’t know”, “Who” should be coded as a “0.”\n“How” Overview: The purpose of coding this “How” construct is to understand whether respondents will try to provide information about their own perspectives to the reader, despite this not being the purpose of the task, which was to “Define critical race theory.”\nHow “1” coding: If you code a response as a “1” for “How”, it should be from a “yes” answer to one or both of these two questions; 1. Does the respondent mention how they personally feel about the merit of Critical Race Theory? 2. Is the respondent trying to suggest to you how you should feel about the merit of Critical Race Theory? This includes both positive and negative perspectives about Critical Race Theory.\nHow “0” Coding: A “How” “0” should be a response that merely attempts to define, and not provide a description of how the respondent feels about critical race theory."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html",
    "href": "posts/FinalPt1_KarenKimble.html",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"_data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\nNYC_2021 <- read_csv(\"_data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nWarning: NAs introduced by coercion"
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#research-question",
    "href": "posts/FinalPt1_KarenKimble.html#research-question",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "href": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "href": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nRows: 18,142\nColumns: 36\n$ DBN                            <chr> \"01M015\", \"01M015\", \"01M015\", \"01M015\",…\n$ `School Name`                  <chr> \"P.S. 015 Roberto Clemente\", \"P.S. 015 …\n$ Year                           <chr> \"2014-15\", \"2015-16\", \"2016-17\", \"2017-…\n$ `Total Enrollment`             <dbl> 183, 176, 178, 190, 174, 270, 270, 271,…\n$ `Grade K`                      <dbl> 27, 32, 28, 28, 20, 44, 47, 37, 34, 30,…\n$ `Grade 1`                      <dbl> 47, 33, 33, 32, 33, 40, 43, 46, 38, 39,…\n$ `Grade 2`                      <dbl> 31, 39, 27, 33, 30, 39, 41, 47, 42, 43,…\n$ `Grade 3`                      <dbl> 19, 23, 31, 23, 30, 35, 43, 40, 46, 41,…\n$ `Grade 4`                      <dbl> 17, 17, 24, 31, 20, 40, 35, 43, 42, 44,…\n$ `Grade 5`                      <dbl> 24, 18, 18, 26, 28, 42, 40, 34, 42, 42,…\n$ `Grade 6`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 7`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 8`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 9`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 10`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 11`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 12`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `# Female`                     <dbl> 84, 83, 83, 99, 85, 132, 125, 127, 114,…\n$ `% Female`                     <dbl> 0.459, 0.472, 0.466, 0.521, 0.489, 0.48…\n$ `# Male`                       <dbl> 99, 93, 95, 91, 89, 138, 145, 144, 143,…\n$ `% Male`                       <dbl> 0.541, 0.528, 0.534, 0.479, 0.511, 0.51…\n$ `# Asian`                      <dbl> 8, 9, 14, 20, 24, 30, 27, 24, 23, 14, 2…\n$ `% Asian`                      <dbl> 0.044, 0.051, 0.079, 0.105, 0.138, 0.11…\n$ `# Black`                      <dbl> 65, 57, 51, 52, 48, 47, 55, 51, 49, 52,…\n$ `% Black`                      <dbl> 0.355, 0.324, 0.287, 0.274, 0.276, 0.17…\n$ `# Hispanic`                   <dbl> 107, 105, 105, 110, 95, 158, 169, 180, …\n$ `% Hispanic`                   <dbl> 0.585, 0.597, 0.590, 0.579, 0.546, 0.58…\n$ `# White`                      <dbl> 2, 2, 4, 6, 6, 27, 16, 15, 16, 18, 25, …\n$ `% White`                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ `# Students with Disabilities` <dbl> 64, 60, 51, 49, 38, 82, 82, 88, 90, 92,…\n$ `% Students with Disabilities` <dbl> 0.350, 0.341, 0.287, 0.258, 0.218, 0.30…\n$ `# English Language Learners`  <dbl> 17, 16, 12, 8, 8, 18, 13, 9, 8, 8, 120,…\n$ `% English Language Learners`  <dbl> 0.093, 0.091, 0.067, 0.042, 0.046, 0.06…\n$ `# Poverty`                    <chr> \"169\", \"149\", \"152\", \"161\", \"145\", \"200…\n$ `% Poverty`                    <dbl> 92.3, 84.7, 85.4, 84.7, 83.3, 74.1, 80.…\n$ `Economic Need Index`          <dbl> 0.930, 0.889, 0.882, 0.890, 0.880, 0.60…\n\n\n\n\nCode\nsummary(school_data)\n\n\n     DBN            School Name            Year           Total Enrollment\n Length:18142       Length:18142       Length:18142       Min.   :   7.0  \n Class :character   Class :character   Class :character   1st Qu.: 323.0  \n Mode  :character   Mode  :character   Mode  :character   Median : 477.0  \n                                                          Mean   : 592.3  \n                                                          3rd Qu.: 695.0  \n                                                          Max.   :6040.0  \n                                                                          \n    Grade K          Grade 1          Grade 2          Grade 3      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 32.00   Median : 33.00   Median : 32.00   Median : 28.00  \n Mean   : 44.25   Mean   : 45.79   Mean   : 45.73   Mean   : 45.33  \n 3rd Qu.: 78.00   3rd Qu.: 81.00   3rd Qu.: 82.00   3rd Qu.: 81.00  \n Max.   :393.00   Max.   :383.00   Max.   :349.00   Max.   :369.00  \n                                                                    \n    Grade 4         Grade 5          Grade 6          Grade 7      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.0   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 22.0   Median : 19.00   Median :  0.00   Median :  0.00  \n Mean   : 44.8   Mean   : 44.18   Mean   : 43.15   Mean   : 42.37  \n 3rd Qu.: 80.0   3rd Qu.: 80.00   3rd Qu.: 64.00   3rd Qu.: 62.00  \n Max.   :376.0   Max.   :351.00   Max.   :771.00   Max.   :796.00  \n                                                                   \n    Grade 8          Grade 9           Grade 10         Grade 11      \n Min.   :  0.00   Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n 1st Qu.:  0.00   1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00  \n Median :  0.00   Median :   0.00   Median :   0.0   Median :   0.00  \n Mean   : 41.88   Mean   :  49.34   Mean   :  48.7   Mean   :  39.85  \n 3rd Qu.: 60.00   3rd Qu.:  68.00   3rd Qu.:  69.0   3rd Qu.:  54.00  \n Max.   :784.00   Max.   :1555.00   Max.   :3832.0   Max.   :1529.00  \n                                                                      \n    Grade 12          # Female         % Female          # Male      \n Min.   :   0.00   Min.   :   0.0   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:   0.00   1st Qu.: 146.0   1st Qu.:0.4620   1st Qu.: 163.0  \n Median :   0.00   Median : 232.0   Median :0.4880   Median : 248.0  \n Mean   :  39.58   Mean   : 287.4   Mean   :0.4827   Mean   : 304.9  \n 3rd Qu.:  53.00   3rd Qu.: 347.0   3rd Qu.:0.5130   3rd Qu.: 364.0  \n Max.   :1566.00   Max.   :2405.0   Max.   :1.0000   Max.   :3635.0  \n                                                                     \n     % Male          # Asian           % Asian          # Black      \n Min.   :0.0000   Min.   :   0.00   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:0.4870   1st Qu.:   5.00   1st Qu.:0.0130   1st Qu.:  42.0  \n Median :0.5120   Median :  17.00   Median :0.0400   Median : 105.0  \n Mean   :0.5173   Mean   :  95.38   Mean   :0.1136   Mean   : 154.1  \n 3rd Qu.:0.5380   3rd Qu.:  79.00   3rd Qu.:0.1400   3rd Qu.: 198.0  \n Max.   :1.0000   Max.   :3671.00   Max.   :0.9470   Max.   :1493.0  \n                                                                     \n    % Black        # Hispanic     % Hispanic        # White       \n Min.   :0.000   Min.   :   1   Min.   :0.0060   Min.   :   0.00  \n 1st Qu.:0.083   1st Qu.:  89   1st Qu.:0.1980   1st Qu.:   6.00  \n Median :0.251   Median : 180   Median :0.3990   Median :  15.00  \n Mean   :0.316   Mean   : 241   Mean   :0.4251   Mean   :  87.24  \n 3rd Qu.:0.502   3rd Qu.: 313   3rd Qu.:0.6323   3rd Qu.:  78.00  \n Max.   :0.987   Max.   :2056   Max.   :1.0000   Max.   :3190.00  \n                                                                  \n    % White       # Students with Disabilities % Students with Disabilities\n Min.   :0.0000   Min.   :  0.0                Min.   :0.0000              \n 1st Qu.:0.0140   1st Qu.: 66.0                1st Qu.:0.1570              \n Median :0.0330   Median : 98.0                Median :0.2030              \n Mean   :0.1205   Mean   :121.6                Mean   :0.2295              \n 3rd Qu.:0.1440   3rd Qu.:146.0                3rd Qu.:0.2540              \n Max.   :0.9450   Max.   :925.0                Max.   :1.0000              \n                                                                           \n # English Language Learners % English Language Learners  # Poverty        \n Min.   :   0.0              Min.   :0.0000              Length:18142      \n 1st Qu.:  18.0              1st Qu.:0.0430              Class :character  \n Median :  43.0              Median :0.0950              Mode  :character  \n Mean   :  81.1              Mean   :0.1363                                \n 3rd Qu.: 100.0              3rd Qu.:0.1800                                \n Max.   :1219.0              Max.   :1.0000                                \n                                                                           \n   % Poverty      Economic Need Index\n Min.   :  2.90   Min.   :0.030      \n 1st Qu.: 69.30   1st Qu.:0.579      \n Median : 81.40   Median :0.743      \n Mean   : 75.89   Mean   :0.691      \n 3rd Qu.: 89.90   3rd Qu.:0.846      \n Max.   :100.00   Max.   :0.998      \n                  NA's   :9169       \n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#references",
    "href": "posts/FinalPt1_KarenKimble.html#references",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html",
    "href": "posts/KalimahMuhammad_hw1.html",
    "title": "Resubmission: Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot2)\nlungcap<- read_excel(\"_data/LungCapData.xls\")\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "href": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "title": "Resubmission: Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n\n\n1b. Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Gender `mean(LungCap)`\n  <chr>            <dbl>\n1 female            7.41\n2 male              8.31\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Lung capacity for those age 13 and under\nlungcap %>%\nfilter(Age <= 13)%>%\ngroup_by(Smoke)%>%\nsummarise(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\nCode\n#Lung capacity for those between the age of 14 to 15\nlungcap%>%\nfilter(Age== 14 | Age ==15)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               9.14\n2 yes              8.39\n\n\nCode\n#Lung capacity for those between the age of 16 to 17\nlungcap%>%\nfilter(Age==16 |Age==16)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no              10.1 \n2 yes              8.90\n\n\nCode\n#Lung capacity for those 18 and older\nlungcap%>%\nfilter(Age>=18)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               11.1\n2 yes              10.5\n\n\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nWith the exception of those age 13 years old and under, all non-smokers had a greater lung capacity than smokers. For those over the age of 18, the difference of the average in lung capacity for non-smokers to smokers was 0.55. For 16-17 year olds, the difference was the greatest at 1.16. The difference for 14-15 year olds was 0.74 and for those 13 years old and under, the difference was -0.843.\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\nHere the average lung capacity for non-smokers is higher than for smokers. This differs from the results of question 1c. Overall, we see lung capacity increase with age irrespective of smoking so this may contribute to the change in results.\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "href": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "title": "Resubmission: Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\npriors<- c(0, 1, 2, 3, 4)\nfrequency<- c(128, 434, 160, 64, 24)\nprison <-data.frame(priors,frequency)\nView(prison)\n\n\nWarning in View(prison): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20%\n2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69%\n2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89%\n2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11%\n2e. What is the expected value for the number of prior convictions?\n\n\nCode\n((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/sum(frequency)\n\n\n[1] 1.28642\n\n\n2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison$priors)*((810-1)/810)#calculate population variance\n\n\n[1] 2.496914\n\n\nCode\nsd(prison$priors)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/KarenDetter_FinalPt1.html",
    "href": "posts/KarenDetter_FinalPt1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Background / Research Question\nWhat predicts support for government regulation of ‘Big Tech’?\nIn 2001, Google piloted a program to boost profits, which were sinking as the “dot-com bubble” burst, by collecting data generated from users’ search queries and using it to sell precisely targeted advertising. The company’s ad revenues grew so quickly that they expanded their data collection tools with tracking “cookies” and predictive algorithms. Other technology firms took notice of Google’s soaring profits, and the sale of passively-collected data from people’s online activities soon became the predominant business model of the internet economy (Zuboff, 2015).\nAs the data-collection practices of ‘Big Tech’ firms, including Google, Amazon, Facebook (Meta), Apple, and Microsoft, have gradually been exposed, the public is now aware that the ‘free’ platforms that have become essential to daily life are actually harvesting personal information as payment. Despite consumers being essentially extorted into accepting this arrangement, regulatory intervention of ‘surveillance capitalism’ has remained limited.\nOver the two decades since passive data collection began commercializing the internet, survey research has shown the American public’s increasing concern about the dominance Big Tech has been allowed to exert. A 2019 study conducted by Pew Research Center found that 81% of Democrats and 70% of Republicans think there should be more government regulation of corporate data-use practices (Pew Research Center, 2019). It is very unusual to find majorities of both Republicans and Democrats agreeing on any policy position, since party affiliation is known to be a main predictor of any political stance, especially in the current polarized climate. The natural question that arises, then, is what other factors predict support for increased regulation of data-collection practices?\n\n\nHypothesis\nAlthough few studies have directly examined the mechanisms behind public support for regulation of passive data collection, a good amount of research has been done on factors influencing individual adoption of privacy protection measures (Barth et al., 2019; Boerman et al., 2021; Turow et al., 2015). It seems a reasonable extrapolation that these factors would similarly influence support for additional data privacy regulation, leading to these hypotheses:\n\nA higher level of awareness of data collection issues predicts support for increased ‘Big Tech’ regulation.\nGreater understanding of how companies use passively collected data predicts support for increased regulation.\nThe feeling of having no personal control over online tracking ‘digital resignation’ predicts support for increased regulation.\nCertain demographic traits (age group, education level, and political ideology) have some kind of effect on attitudes toward ‘Big Tech’ regulation.\n\nSince there are currently dozens of data privacy bills pending in Congress, pinpointing the forces driving support for this type of legislation can help with both shaping the regulatory framework needed and appealing for broader support from voters.\n\n\nDescriptive Statistics\nPew Research Center’s American Trends Panel (Wave 49) data set can provide insight into which of these factors are predictive of support for greater regulation of technology company data practices. In June 2019, an online survey covering a wide variety of topics was conducted and 4,272 separate observations for 144 variables were collected from adults age 18 and over. The margin of error (at the 95% confidence level) is given as +/- 1.87 percentage points.\nThe data set was compiled in SPSS and all pertinent variables are categorical.\n\n\nCode\n#read in data from SPSS file\nwav49 <- read_sav(\"_data/ATPW49.sav\")\nwav49\n\n\n# A tibble: 4,272 × 144\n     QKEY DEVICE_TYPE_…¹ LANG_…² FORM_…³ SOCME…⁴ SOCME…⁵ SOCME…⁶ SOCME…⁷ SNSUS…⁸\n    <dbl> <dbl+lbl>      <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l>\n 1 100260 2 [Tablet]     9 [Eng… 2 [For… 2 [No,… 2 [No,… 2 [No,… 2 [No,… 0 [Doe…\n 2 100588 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 2 [No,… 1 [Soc…\n 3 100637 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 4 101224 1 [Mobile pho… 9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 5 101322 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 6 101437 3 [Desktop]    9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 7 101472 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 1 [Yes… 2 [No,… 1 [Soc…\n 8 101493 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 1 [Yes… 1 [Soc…\n 9 102198 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 2 [No,… 1 [Yes… 1 [Soc…\n10 103094 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Soc…\n# … with 4,262 more rows, 135 more variables: ELECTFTGSNSINT_W49 <dbl+lbl>,\n#   TALKDISASNSINT_W49 <dbl+lbl>, TALKCMNSNSINT_W49 <dbl+lbl>,\n#   SECUR1_W49 <dbl+lbl>, PRIVACYNEWS1_W49 <dbl+lbl>,\n#   HOMEASSIST1_W49 <dbl+lbl>, HOMEASSIST2_W49 <dbl+lbl>,\n#   HOMEASSIST3_W49 <dbl+lbl>, HOMEASSIST4_W49 <dbl+lbl>,\n#   HOMEASSIST5a_W49 <dbl+lbl>, HOMEASSIST5b_W49 <dbl+lbl>,\n#   HOMEIOT_W49 <dbl+lbl>, FITTRACK_W49 <dbl+lbl>, LOYALTY_W49 <dbl+lbl>, …\n\n\nSince there are so many variables in the data set, selecting the variables of interest into a new data frame will make it easier to manage:\n\n\nCode\nsel_vars <- c('PRIVACYNEWS1_W49', 'TRACKCO1a_W49', 'CONTROLCO_W49', 'UNDERSTANDCO_W49', 'ANONYMOUS1CO_W49', 'PP4_W49', 'PRIVACYREG_W49', 'GOVREGV1_W49', 'PROFILE4_W49', 'F_AGECAT', 'F_EDUCCAT', 'F_PARTYSUM_FINAL', 'F_IDEO')\nwav49_selected <- wav49[sel_vars]\nwav49_selected\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS1_…¹ TRACKC…² CONTRO…³ UNDERS…⁴ ANONYM…⁵ PP4_W49  PRIVA…⁶ GOVREG…⁷\n   <dbl+lbl>       <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+l> <dbl+lb>\n 1 4 [Not at all … NA       NA       NA       NA       NA       3 [Ver… NA      \n 2 3 [Not too clo…  3 [Som…  2 [Som…  3 [Ver…  1 [Yes…  3 [Ver… 3 [Ver…  1 [Mor…\n 3 3 [Not too clo…  3 [Som…  3 [Ver…  3 [Ver…  1 [Yes…  2 [Som… 3 [Ver…  1 [Mor…\n 4 4 [Not at all … NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 5 4 [Not at all …  1 [All…  4 [No …  4 [Not…  2 [No,… NA       4 [Not…  1 [Mor…\n 6 2 [Somewhat cl… NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 7 2 [Somewhat cl…  2 [Mos…  3 [Ver…  3 [Ver…  2 [No,…  2 [Som… 2 [Som…  3 [Abo…\n 8 1 [Very closel…  1 [All…  4 [No …  4 [Not…  2 [No,… NA       3 [Ver…  3 [Abo…\n 9 3 [Not too clo…  1 [All…  3 [Ver…  2 [Som…  2 [No,…  3 [Ver… 3 [Ver…  1 [Mor…\n10 3 [Not too clo…  3 [Som…  2 [Som…  1 [A g…  1 [Yes…  2 [Som… 2 [Som…  2 [Les…\n# … with 4,262 more rows, 5 more variables: PROFILE4_W49 <dbl+lbl>,\n#   F_AGECAT <dbl+lbl>, F_EDUCCAT <dbl+lbl>, F_PARTYSUM_FINAL <dbl+lbl>,\n#   F_IDEO <dbl+lbl>, and abbreviated variable names ¹​PRIVACYNEWS1_W49,\n#   ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49, ⁵​ANONYMOUS1CO_W49,\n#   ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49\n\n\nThe variable labels contain the survey questions asked:\n\n\nCode\n#summary of $variable names and their [labels]\nvar_label(wav49_selected)\n\n\n$PRIVACYNEWS1_W49\n[1] \"PRIVACYNEWS1. How closely, if at all, do you follow news about privacy issues?\"\n\n$TRACKCO1a_W49\n[1] \"TRACKCO1a. As far as you know, how much of what you do ONLINE or on your cellphone is being tracked by advertisers, technology firms or other companies?\"\n\n$CONTROLCO_W49\n[1] \"CONTROLCO. How much control do you think you have over the data that companies collect about you?\"\n\n$UNDERSTANDCO_W49\n[1] \"UNDERSTANDCO. How much do you feel you understand what companies are doing with the data they collect about you?\"\n\n$ANONYMOUS1CO_W49\n[1] \"ANONYMOUS1CO. Do you think it is possible to go about daily life today without having companies collect data about you?\"\n\n$PP4_W49\n[1] \"PP4. How much do you typically understand the privacy policies you read?\"\n\n$PRIVACYREG_W49\n[1] \"PRIVACYREG. How much do you feel you understand the laws and regulations that are currently in place to protect your data privacy?\"\n\n$GOVREGV1_W49\n[1] \"GOVREGV1. How much government regulation of what companies can do with their customers’ personal information do you think there should be?\"\n\n$PROFILE4_W49\n[1] \"PROFILE4. How much, if at all, do you understand what data about you is being used to create these advertisements?\"\n\n$F_AGECAT\n[1] \"Age category\"\n\n$F_EDUCCAT\n[1] \"Education level category\"\n\n$F_PARTYSUM_FINAL\n[1] \"Party summary\"\n\n$F_IDEO\n[1] \"Ideology\"\n\n\nBecause the data set is made up of categorical variables, transformation is required before computing any statistics:\n\n\nCode\n#convert all variables to factors\nwav49_factored <- wav49_selected %>%\n  mutate_all(as_factor)\n#convert user-defined missing values to regular missing values\nzap_missing(wav49_factored)\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS…¹ TRACK…² CONTR…³ UNDER…⁴ ANONY…⁵ PP4_W49 PRIVA…⁶ GOVRE…⁷ PROFI…⁸\n   <fct>         <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>  \n 1 Not at all c… <NA>    <NA>    <NA>    <NA>    <NA>    Very l… <NA>    <NA>   \n 2 Not too clos… Some o… Some c… Very l… Yes, i… Very l… Very l… More r… <NA>   \n 3 Not too clos… Some o… Very l… Very l… Yes, i… Some    Very l… More r… Somewh…\n 4 Not at all c… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    <NA>   \n 5 Not at all c… All or… No con… Nothing No, it… <NA>    Not at… More r… Not to…\n 6 Somewhat clo… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    Not to…\n 7 Somewhat clo… Most o… Very l… Very l… No, it… Some    Some    About … Somewh…\n 8 Very closely  All or… No con… Nothing No, it… <NA>    Very l… About … Somewh…\n 9 Not too clos… All or… Very l… Some    No, it… Very l… Very l… More r… Somewh…\n10 Not too clos… Some o… Some c… A grea… Yes, i… Some    Some    Less r… Somewh…\n# … with 4,262 more rows, 4 more variables: F_AGECAT <fct>, F_EDUCCAT <fct>,\n#   F_PARTYSUM_FINAL <fct>, F_IDEO <fct>, and abbreviated variable names\n#   ¹​PRIVACYNEWS1_W49, ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49,\n#   ⁵​ANONYMOUS1CO_W49, ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49, ⁸​PROFILE4_W49\n\n\nAfter the variables are converted to meaningful factors, a summary of response frequencies can be generated:\n\n\nCode\nsummary(wav49_factored)\n\n\n           PRIVACYNEWS1_W49                 TRACKCO1a_W49 \n Very closely      : 461    All or almost all of it: 881  \n Somewhat closely  :2046    Most of it             : 703  \n Not too closely   :1397    Some of it             : 381  \n Not at all closely: 359    Very little of it      :  88  \n Refused           :   9    None of it             :  76  \n                            Refused                :  11  \n                            NA's                   :2132  \n                 CONTROLCO_W49      UNDERSTANDCO_W49\n A great deal of control:  68   A great deal: 132   \n Some control           : 313   Some        : 716   \n Very little control    :1134   Very little :1040   \n No control             : 621   Nothing     : 242   \n Refused                :   4   Refused     :  10   \n NA's                   :2132   NA's        :2132   \n                                                    \n               ANONYMOUS1CO_W49         PP4_W49          PRIVACYREG_W49\n Yes, it is possible   : 772    A great deal: 328   A great deal: 136  \n No, it is not possible:1357    Some        :1405   Some        :1380  \n Refused               :  11    Very little : 751   Very little :2153  \n NA's                  :2132    Not at all  :  82   Not at all  : 593  \n                                Refused     :   5   Refused     :  10  \n                                NA's        :1701                      \n                                                                       \n                GOVREGV1_W49        PROFILE4_W49    F_AGECAT   \n More regulation      :1631   A great deal: 384   18-29 : 671  \n Less regulation      : 145   Somewhat    :1410   30-49 :1314  \n About the same amount: 331   Not too much: 900   50-64 :1308  \n Refused              :  33   Not at all  : 113   65+   : 977  \n NA's                 :2132   Refused     :   9   DK/REF:   2  \n                              NA's        :1456                \n                                                               \n                 F_EDUCCAT              F_PARTYSUM_FINAL\n College graduate+    :1600   Rep/Lean Rep      :1823   \n Some College         :1182   Dem/Lean Dem      :2296   \n H.S. graduate or less:1483   DK/Refused/No lean: 153   \n Don't know/Refused   :   7                             \n                                                        \n                                                        \n                                                        \n               F_IDEO    \n Very conservative: 353  \n Conservative     : 977  \n Moderate         :1615  \n Liberal          : 828  \n Very liberal     : 386  \n Refused          : 113  \n                         \n\n\n*High NA value indicates that the question was not presented to all respondents\nThe data set is now primed for examining correlations and testing hypotheses.\n\n\nReferences\nBarth, S., de Jong, M. D. T., Junger, M., Hartel, P. H. & Roppelt, J. C. (2019). Putting the privacy paradox to the test: Online privacy and security behaviors among users with technical knowledge, privacy awareness, and financial resources. Telematics and Informatics, 41, 55–69. doi:10.1016/j.tele.2019.03.003\nBoerman, S. C., Kruikemeier, S., & Zuiderveen Borgesius, F. J. (2021). Exploring Motivations for Online Privacy Protection Behavior: Insights From Panel Data. Communication Research, 48(7), 953–977. https://doi.org/10.1177/0093650218800915\nPew Research Center. (2019). Americans and privacy: Concerned, confused and feeling lack of control over their personal information. https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-concerned-confused-and- feeling-lack-of-control-over-their-personal-information/\nPew Research Center. (2020). Wave 49 American trends panel [Data set]. https://www.pewresearch.org/internet/dataset/american-trends-panel-wave-49/\nTurow, J., Hennessy, M. & Draper, N. (2015). The tradeoff fallacy – How marketers are misrepresenting American consumers and opening them up to exploitation. Annenberg School for Communication.\nZuboff, S. (2015). Big other: Surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1), 75–89. doi:10.1057/jit.2015.5"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html",
    "href": "posts/MeghaJoseph_HW3.html",
    "title": "Home Work 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-1",
    "href": "posts/MeghaJoseph_HW3.html#question-1",
    "title": "Home Work 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11) \nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section",
    "href": "posts/MeghaJoseph_HW3.html#section",
    "title": "Home Work 3",
    "section": "1.1.1",
    "text": "1.1.1\nThe predictor variable is ppgdp. The response variable is fertility."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section-1",
    "href": "posts/MeghaJoseph_HW3.html#section-1",
    "title": "Home Work 3",
    "section": "1.1.2",
    "text": "1.1.2"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section-2",
    "href": "posts/MeghaJoseph_HW3.html#section-2",
    "title": "Home Work 3",
    "section": "1.1.3",
    "text": "1.1.3"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-2",
    "href": "posts/MeghaJoseph_HW3.html#question-2",
    "title": "Home Work 3",
    "section": "Question 2",
    "text": "Question 2\n\nThe slope of the prediction equation would change. It would be the initial version’s slope divided by 1.33 to account for the change in unit to pounds.\n\nb.The correlation does not change, because it standardizes the slope (thus is not impacted by unit of measure)."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-3",
    "href": "posts/MeghaJoseph_HW3.html#question-3",
    "title": "Home Work 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\n\n\nCode\n#load dataset \ndata(water)\n\n#scatterplot matrix\npairs(water)\n\n\n\n\n\nCode\n#calculate the summary\nsummary(water)\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\n\nIn this scatterplot matrix, the precipitation levels for the ‘A’ named lakes seem to have a positive (relatively linear) correlation (although unsure how strong) with each other and the ‘O’ named lakes seem to have one as well with each other. The year variable does not appear to have a relationship to any of the variables. Also, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-4",
    "href": "posts/MeghaJoseph_HW3.html#question-4",
    "title": "Home Work 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\n# load dataset, select variables, preview dataset\ndata(Rateprof)\n\nRateprof <- Rateprof %>%\n  select(c(quality, clarity, helpfulness, easiness, raterInterest))  \n\nhead(Rateprof)\n\n\n   quality  clarity helpfulness easiness raterInterest\n1 4.636364 4.636364    4.636364 4.818182      3.545455\n2 4.318182 4.090909    4.545455 4.363636      4.000000\n3 4.790698 4.860465    4.720930 4.604651      3.432432\n4 4.250000 4.041667    4.458333 2.791667      3.181818\n5 4.684211 4.684211    4.684211 4.473684      4.214286\n6 4.233333 4.200000    4.266667 4.533333      3.916667\n\n\n\n\nCode\npairs(Rateprof)\n\n\n\n\n\nReferring to the scatterplot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality.\n##Question 5"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html",
    "href": "posts/EmmaRasmussenFinalPart2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(googlesheets4)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#research-question",
    "href": "posts/EmmaRasmussenFinalPart2.html#research-question",
    "title": "Final Project Part 2",
    "section": "Research Question",
    "text": "Research Question\nDoes political partisanship correlate with COVID-19 death rates?"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#introduction",
    "href": "posts/EmmaRasmussenFinalPart2.html#introduction",
    "title": "Final Project Part 2",
    "section": "Introduction",
    "text": "Introduction\nThe COVID-19 pandemic became a political matter. Behaviors associated with COVID-19 prevention were adopted on partisan lines (masking, social distancing, and vaccine uptake). Early in the pandemic, mask mandates were protested in some communities. My research question is have these behaviors affected COVID-19 death rates along partisan lines? If so, public health interventions could target communities that may be higher risk for COVID-19 deaths based on political partisanship.\nFor this analysis I used cumulative COVID-19 death toll as opposed to infection rates as infection rates are constantly changing over time. Other studies have looked at infection rates on partisan lines over waves of the pandemic, see this study from the Pew Research Center (Jones 2022)). I measured partisanship using 2016 county-level election results (% voting for Trump). My research is looking to see if (county-level) Trump support correlates with COVID-19 death rates. The unit of analysis for this study was U.S. counties.\nI also control for age (percent population over 65), income (median household income in 2020), urbanization (Urban-Rural Continuum Code), and policy (2020 governor dummy variable)."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#hypothesis",
    "href": "posts/EmmaRasmussenFinalPart2.html#hypothesis",
    "title": "Final Project Part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nWhile I came up with this research idea on my own, other organizations such as NPR (Wood and Brumfiel 2021) and the Pew Research Center (Jones 2022) have already tested this and found a significant correlation in Trump support and COVID-19 death and infection rates. For this project, I will use more recent data and include additional control variables that were not accounted for in these previous studies.\nH0: B1 is zero. There is no correlation between political partisanship and COVID-19 death rates. Ha: B1 is not zero. There is a correlation between partisanship and COVID-19 death rates."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#variables",
    "href": "posts/EmmaRasmussenFinalPart2.html#variables",
    "title": "Final Project Part 2",
    "section": "Variables",
    "text": "Variables\n\nPolitical Partisanship\nFor this project, I measured partisanship as percentage of the county that voted for Trump in the 2016 election against Clinton. I did not use 2020 election results in case counties “flipped” support as a result of COVID-19. Below I tidied the data, filtering out only 2016 election results, created a variable: percent voting for Trump in 2016, and selected the percent_trump and FIPS Code variables to use in the analysis.\nThe data used for this variable came from the MIT Election Data and Science Lab (2021).\n\ngs4_deauth()\n\n#2016 election df\n\nvotedf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1fmxoA_bibvsxsvgRdVPCgMA7DkmJNZfxiWgLgCLcsOY/edit#gid=937778872\")\n\nvotedf$county_fips <- as.character(votedf$county_fips)\nvotedf<-mutate(votedf, county_fipsNEW=str_pad(votedf$county_fips, 5, pad = \"0\"))\nvotedf<-votedf %>% \n  filter(year==2016, candidate==\"DONALD TRUMP\")\n  votedf$percent_trump <-votedf$candidatevotes/votedf$totalvotes\n  \nvotedf<- select(votedf, county_fipsNEW, percent_trump)\nhead(votedf)\n\n# A tibble: 6 × 2\n  county_fipsNEW percent_trump\n  <chr>                  <dbl>\n1 01001                  0.728\n2 01003                  0.765\n3 01005                  0.521\n4 01007                  0.764\n5 01009                  0.893\n6 01011                  0.242\n\n\n\n\nCOVID-19 Cumulative Death Rate\nThis data comes from USAFacts.org. Their collection methods are thoroughly described on their website (see USA Facts 2022 in References). Originally I planned on using CDC/NIH data, however they only had data for a little over 1,000 counties, compared to USAFacts which had data for over 3,000. USAFacts compiles their data from CDC, but also town, county, and state leading to a larger number of observations in their dataset.\nCumulative county-level death toll is taken as of March 19, 2022, when many states stopped regularly reporting COVID-19 deaths (and beginning late in January 2020) (USA Facts 2022).\n\ncoviddf_2<-read_sheet(\"https://docs.google.com/spreadsheets/d/1ZKa3sg_UdtyX5z0OGGVZN6nuqcC_OKWcsOjm1ZHNjQY/edit#gid=716391091\")\n\n#adding leading 0 back to fips\ncoviddf_2<-mutate(coviddf_2, county_fipsNEW=str_pad(coviddf_2$\"countyFIPS\", 5, pad = \"0\"))\n\n#selecting march 19, 2022, the day before the first day of spring in 2022, when many states stopped/slowed reporting (USA Facts)\ncoviddf_2<-select(coviddf_2, county_fipsNEW, \"County Name\", State, \"2022-03-19\")\ncoviddf_2<-rename(coviddf_2, \"covid_deaths\" = \"2022-03-19\")\n\n\n\nControl: Age\nBecause age correlates with political party and older Americans (over 65) were disproportionately more likely to die as a result of COVID-19, this is included in the analysis as a control. According to an article from the Mayo Clinic Website, 81% of COVID-19 deaths occured in individuals 65 or over (Mayo Clinic Staff 2022). Previous studies have controlled for age as well (Brumfiel and Wood 2021). I controlled for age by creating the variable: percent of the county’s population over 65 years of age.\n\n#age df\n\n#Please see pdf of code tidying I did to make df into a manageable size for google sheets\n\nage<- read_sheet(\"https://docs.google.com/spreadsheets/d/1EysREWJ61NCSYyiYH8-2pmZC_TnuLTgqa8Lz5ZpsPZ4/edit#gid=271068707\")\nhead(age)\n\n# A tibble: 6 × 6\n  STNAME  CTYNAME        county_fipsNEW tot_pop over65 over65_pct         \n  <chr>   <chr>          <chr>          <chr>   <chr>  <chr>              \n1 Alabama Autauga County 01001          55869   8924   0.1597307988329843 \n2 Alabama Baldwin County 01003          223234  46830  0.20977987224168362\n3 Alabama Barbour County 01005          24686   4861   0.19691323017094708\n4 Alabama Bibb County    01007          22394   3733   0.16669643654550326\n5 Alabama Blount County  01009          57826   10814  0.18700930377338915\n6 Alabama Bullock County 01011          10101   1711   0.1693891693891694 \n\n\n\n\nControl: Policy Dummy Variable\nThis variable (2020 governor party) attempts to control for local and state policy such as mask mandates, stay-at-home orders, and vaccine requirements to attend large events that contribute to COVID-19 death and infection rates. This data set was created from a table of current U.S. governors and their political party on Ballotpedia.org (2022). I adjusted a couple observations when governors had been elected more recently than March 11, when the WHO declared COVID-19 a global pandemic. This data represents the political party of state governors as of March 11, 2020.\n\n#Governor/policy df\n\n#reading in gov dummy variable: whoever was in office March 11, 2020 (when WHO declared covid-19 a global pandemic)\n\ngov2020<-read_sheet(\"https://docs.google.com/spreadsheets/d/1-pToTikvnXl1-lT-xazjoxX0sOCwOH5-7Bl8vJqu9Tk/edit#gid=0\")\n\ngov2020$Office<-str_remove(gov2020$Office, \"Governor of \")\ngov2020<-rename(gov2020, \"STNAME\" = Office)\ngov2020<-select(gov2020, STNAME, Name, Party)\nhead(gov2020)\n\n# A tibble: 6 × 3\n  STNAME         Name                 Party     \n  <chr>          <chr>                <chr>     \n1 Alabama        Kay Ivey             Republican\n2 Alaska         Mike Dunleavy        Republican\n3 American Samoa Lemanu Palepoi Mauga Democratic\n4 Arizona        Doug Ducey           Republican\n5 Arkansas       Asa Hutchinson       Republican\n6 California     Gavin Newsom         Democratic\n\n\n\n\nControl: Income\nMedian household income by county in 2020 is taken from this dataset, from the Economic Research Service at United States Department of Agriculture (2022). Income likely correlates with both political affiliation and COVID-19 death rates (access to medical care, preventative treatment etc), which is why it is included in the analysis.\n\n\nControl: Rural-Urban Continuum Code\nThis is also taken from the Economic Research Service/USDA dataset. According to the USDA, the 2013 rural-urban continuum code is a “classification scheme that distinguishes metropolitan counties by the population size of their metro area, and nonmetropolitan counties by degree of urbanization and adjacency to a metro area”. In my opinion, this makes more sense to include than a simple calculation of population density, because it takes into account cities/ how close people settle to cities rather than just population divided by land area. (For instance, a large county, land-wise, may have a majority of the population in a large city, however this could still result in a fairly small population density depending on land area). The Rural Urban Continuum code is on an integer scale from 1 to 9, where 1 represents the most urban/metro counties, and 9 represents the most rural counties.\nGenerally speaking, more rural counties tend to favor Trump while more metropolitan areas tend to be more democratic. At the same time, one would expect that more densely populated areas/ people living closer together in cities would experience higher infection (and therefore death rates) of COVID-19.\n\n#Income df\n\n#reading in income variable sheet, renaming variables, and selecting only relevant columns, then renaming fips code variable to join to other df's\n\nincome<-read_sheet(\"https://docs.google.com/spreadsheets/d/1ntReIIrpzjRvGabr64-91xEpSJk10r6Er1CX3pG5zBg/edit#gid=1233692484\", skip=4) %>% \n  rename(\"med_income_2020\" = Median_Household_Income_2020, \"county_fipsNEW\" = FIPS_code) %>% \n  select(county_fipsNEW, State, Area_name, med_income_2020, Rural_urban_continuum_code_2013)\nhead(income)\n\n# A tibble: 6 × 5\n  county_fipsNEW State Area_name          med_income_2020 Rural_urban_continuu…¹\n  <chr>          <chr> <chr>                        <dbl>                  <dbl>\n1 00000          US    United States                67340                     NA\n2 01000          AL    Alabama                      53958                     NA\n3 01001          AL    Autauga County, AL           67565                      2\n4 01003          AL    Baldwin County, AL           71135                      3\n5 01005          AL    Barbour County, AL           38866                      6\n6 01007          AL    Bibb County, AL              50907                      1\n# … with abbreviated variable name ¹​Rural_urban_continuum_code_2013"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#analysis",
    "href": "posts/EmmaRasmussenFinalPart2.html#analysis",
    "title": "Final Project Part 2",
    "section": "Analysis",
    "text": "Analysis\n\nJoining the Dataframes\nBelow I join the data based on FIPS code (except for the governor policy dummy variable, which is joined by state).\n\ncovidvote_2<-votedf %>% \n  left_join(coviddf_2, by=\"county_fipsNEW\")\nhead(covidvote_2)\n\n# A tibble: 6 × 5\n  county_fipsNEW percent_trump `County Name`  State covid_deaths\n  <chr>                  <dbl> <chr>          <chr> <chr>       \n1 01001                  0.728 Autauga County AL    210         \n2 01003                  0.765 Baldwin County AL    669         \n3 01005                  0.521 Barbour County AL    94          \n4 01007                  0.764 Bibb County    AL    100         \n5 01009                  0.893 Blount County  AL    230         \n6 01011                  0.242 Bullock County AL    52          \n\ncovid_vote_3 <- covidvote_2 %>% \n  left_join(age, by= \"county_fipsNEW\")\nhead(covid_vote_3)\n\n# A tibble: 6 × 10\n  county_f…¹ perce…² Count…³ State covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>        <dbl> <chr>   <chr> <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001        0.728 Autaug… AL    210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003        0.765 Baldwi… AL    669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005        0.521 Barbou… AL    94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007        0.764 Bibb C… AL    100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009        0.893 Blount… AL    230     Alaba… Blount… 57826   10814  0.1870…\n6 01011        0.242 Bulloc… AL    52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with abbreviated variable names ¹​county_fipsNEW, ²​percent_trump,\n#   ³​`County Name`, ⁴​covid_deaths, ⁵​over65_pct\n\ncovid_vote_4<- covid_vote_3 %>% \n  left_join(gov2020, by=\"STNAME\")\nhead(covid_vote_4)\n\n# A tibble: 6 × 12\n  county_f…¹ perce…² Count…³ State covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>        <dbl> <chr>   <chr> <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001        0.728 Autaug… AL    210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003        0.765 Baldwi… AL    669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005        0.521 Barbou… AL    94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007        0.764 Bibb C… AL    100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009        0.893 Blount… AL    230     Alaba… Blount… 57826   10814  0.1870…\n6 01011        0.242 Bulloc… AL    52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with 2 more variables: Name <chr>, Party <chr>, and abbreviated variable\n#   names ¹​county_fipsNEW, ²​percent_trump, ³​`County Name`, ⁴​covid_deaths,\n#   ⁵​over65_pct\n\ncovid_vote_5<- covid_vote_4 %>% \n  left_join(income, by= \"county_fipsNEW\")\nhead(covid_vote_5)\n\n# A tibble: 6 × 16\n  county…¹ perce…² Count…³ State.x covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>      <dbl> <chr>   <chr>   <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001      0.728 Autaug… AL      210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003      0.765 Baldwi… AL      669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005      0.521 Barbou… AL      94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007      0.764 Bibb C… AL      100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009      0.893 Blount… AL      230     Alaba… Blount… 57826   10814  0.1870…\n6 01011      0.242 Bulloc… AL      52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with 6 more variables: Name <chr>, Party <chr>, State.y <chr>,\n#   Area_name <chr>, med_income_2020 <dbl>,\n#   Rural_urban_continuum_code_2013 <dbl>, and abbreviated variable names\n#   ¹​county_fipsNEW, ²​percent_trump, ³​`County Name`, ⁴​covid_deaths, ⁵​over65_pct\n\n#covid_vote_5 has all 5 dataframes joined together\n\nMore tidying:\n\n#converting covid deaths and total population to numeric variables\n\ncovid_vote_5$covid_deaths<-as.numeric(covid_vote_5$covid_deaths)\ncovid_vote_5$tot_pop<-as.numeric(covid_vote_5$tot_pop)\ncovid_vote_5$over65_pct<-as.numeric(covid_vote_5$over65_pct)\n\n#creating a death rate variable (COVID-19 deaths per 100,000 by county from January 2020-March 2022)\ncovid_vote_5$\"covid_death_rate\" = (covid_vote_5$covid_deaths / covid_vote_5$tot_pop)*100000\n\n#selecting only relevant columns for analysis\ncovid_vote_5<- select(covid_vote_5, county_fipsNEW, STNAME, CTYNAME, covid_death_rate, percent_trump, over65_pct, Party, med_income_2020, tot_pop, Rural_urban_continuum_code_2013)\nhead(covid_vote_5)\n\n# A tibble: 6 × 10\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with abbreviated variable names ¹​county_fipsNEW, ²​covid_death_rate,\n#   ³​percent_trump, ⁴​over65_pct, ⁵​med_income_2020,\n#   ⁶​Rural_urban_continuum_code_2013\n\n\n\n# Creating df where counties with death rates of exactly 0 (no covid deaths) are excluded. This will allow me to take the log  of death rates in later analysis, and exclude counties that potentially did not report COVID-19 deaths at all. Exluding counties where covid_death_pct is greater than zero removes 66 counties from the analysis.\n\ncovid_vote_5_no_zero<-filter(covid_vote_5, covid_death_rate >0)\nhead(covid_vote_5_no_zero)\n\n# A tibble: 6 × 10\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with abbreviated variable names ¹​county_fipsNEW, ²​covid_death_rate,\n#   ³​percent_trump, ⁴​over65_pct, ⁵​med_income_2020,\n#   ⁶​Rural_urban_continuum_code_2013\n\n\n\n\nExplotatory Analysis\n\n#Scatter plot of percent trump and percent population dying of covid. \nvisual1<-ggplot(covid_vote_5, aes(x=percent_trump, y=covid_death_rate))+geom_point()+geom_smooth()+labs(x= \"(%) Votes for Trump in 2016\", y=\"COVID-19 Deaths per 100,000 People\", title= \"COVID-19 Death Rate in U.S. Counties Based on 2016 Trump Support\")\nggplotly(visual1)\n\n\n\n\n\nThere appears to be a slight positive trend, especially among counties where percentage Trump votes was over 50%. Interestingly, on this plot, the death rate appears to decrease slightly from 0 to 40% percent voting for Trump, and then increases. Perhaps this could be due to the fact that highly democratic counties are more likely to be more urban- therefore people live more close together and infection rate is likely higher. From this visual, it appears that the county with the highest COVID-19 death rate (1123 per 100,000) voted heavily for Trump in 2016 (over 90%). The counties with death rates of 0 appear to come from across parties (however these counties might not have reported COVID-19 deaths at all).\n\n#Creating a new variable, whether trump or clinton was the majority vote\ncovid_vote_5<- covid_vote_5%>% \n  mutate(majority= case_when(percent_trump > 0.5 ~ \"Trump_favor\",\n                                             percent_trump < 0.5 ~ \"Clinton_favor\"))\nhead(covid_vote_5)\n\n# A tibble: 6 × 11\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with 1 more variable: majority <chr>, and abbreviated variable names\n#   ¹​county_fipsNEW, ²​covid_death_rate, ³​percent_trump, ⁴​over65_pct,\n#   ⁵​med_income_2020, ⁶​Rural_urban_continuum_code_2013\n\n#creating a boxplot to compare means of two groups\nggplot(na.omit(covid_vote_5), aes(x=majority, y=covid_death_rate))+geom_boxplot()+labs(x=\"Majority\", y=\"COVID-19 Deaths per 100,000 People\", title= \"COVID-19 Death Rates in Clinton and Trump Majority Counties\")\n\n\n\n\nAccording to this boxplot, there appears to be a difference in median COVID-19 death rates based on if a county voted majority Trump of Clinton.\n\n\nModeling this Relationship\nBelow I try two different models including all of the control variables identified above. In the second model I omit counties where covid_death_rate is zero so I can log transform covid_death_rate. I think a log transformation could make sense in this case- perhaps only counties that had higher percentage Trump votes also had higher corresponding COVID-19 death rates (see scatterplot above).\n\n#Model with covid_death_pct with 0 values included\nsummary(lm(covid_death_rate ~ percent_trump + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5))\n\n\nCall:\nlm(formula = covid_death_rate ~ percent_trump + over65_pct + \n    med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-447.34  -82.23   -3.53   79.19  765.10 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      5.406e+02  2.043e+01  26.463  < 2e-16 ***\npercent_trump                    1.635e+02  1.778e+01   9.196  < 2e-16 ***\nover65_pct                      -6.920e+01  5.964e+01  -1.160    0.246    \nmed_income_2020                 -4.955e-03  1.968e-04 -25.182  < 2e-16 ***\nPartyRepublican                  2.536e+01  5.252e+00   4.829 1.44e-06 ***\nRural_urban_continuum_code_2013 -8.644e-01  1.160e+00  -0.745    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.6 on 3107 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.2668,    Adjusted R-squared:  0.2656 \nF-statistic: 226.1 on 5 and 3107 DF,  p-value: < 2.2e-16\n\nfit1<-lm(covid_death_rate ~ percent_trump + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5)\n\n#Model with covid_death_pct with 0 values excluded and logged covid_death_pct\nsummary(lm(log(covid_death_rate) ~ percent_trump + over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, data= covid_vote_5_no_zero))\n\n\nCall:\nlm(formula = log(covid_death_rate) ~ percent_trump + over65_pct + \n    med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5_no_zero)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15841 -0.19737  0.06346  0.27447  1.16727 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      6.382e+00  6.656e-02  95.877  < 2e-16 ***\npercent_trump                    7.718e-01  5.789e-02  13.331  < 2e-16 ***\nover65_pct                      -2.974e-01  1.941e-01  -1.532  0.12557    \nmed_income_2020                 -1.720e-05  6.410e-07 -26.828  < 2e-16 ***\nPartyRepublican                  5.297e-02  1.705e-02   3.106  0.00192 ** \nRural_urban_continuum_code_2013 -1.826e-02  3.787e-03  -4.821  1.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4484 on 3085 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.2852,    Adjusted R-squared:  0.2841 \nF-statistic: 246.2 on 5 and 3085 DF,  p-value: < 2.2e-16\n\nfit2<-lm(log(covid_death_rate) ~ percent_trump + over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, data= covid_vote_5_no_zero)\n\nThe second model, where covid_death_rate is log transformed and the 66 observations where COVID-19 death rate is zero are omitted has the higher adjusted r-squared.\nIn both models, the coefficient of interest percent_trump, is significant at the 0.001 significance level and has a positive coefficient. This model suggests there is evidence that counties that vote higher for Trump experience higher COVID-19 death rates. Still, the adjusted R-squared is not very high.\nConcerns with this model: - Potential multicollinearity between over65_pct and med_income_2020? and between gov dummy and percent trump? - why is the coefficient for percent over 65 negative?\nBelow I test a polynomial model (where at first death rate decreases and then decreases as the curved line in the scatterplot above), however, the adjusted R squared is still smaller than in the above model.\n\n#polynomial model\nsummary(lm(covid_death_rate ~ poly(percent_trump, 2, raw=TRUE) + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5))\n\n\nCall:\nlm(formula = covid_death_rate ~ poly(percent_trump, 2, raw = TRUE) + \n    over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-470.63  -82.97   -3.05   76.94  770.31 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          6.393e+02  3.181e+01  20.093  < 2e-16 ***\npoly(percent_trump, 2, raw = TRUE)1 -2.056e+02  9.310e+01  -2.209   0.0273 *  \npoly(percent_trump, 2, raw = TRUE)2  3.286e+02  8.136e+01   4.039 5.50e-05 ***\nover65_pct                          -4.899e+01  5.970e+01  -0.821   0.4120    \nmed_income_2020                     -5.004e-03  1.967e-04 -25.443  < 2e-16 ***\nPartyRepublican                      2.273e+01  5.279e+00   4.305 1.72e-05 ***\nRural_urban_continuum_code_2013     -1.759e+00  1.178e+00  -1.492   0.1357    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.2 on 3106 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.2706,    Adjusted R-squared:  0.2692 \nF-statistic:   192 on 6 and 3106 DF,  p-value: < 2.2e-16\n\nfit3<-lm(covid_death_rate ~ poly(percent_trump, 2, raw=TRUE) + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#diagnositics",
    "href": "posts/EmmaRasmussenFinalPart2.html#diagnositics",
    "title": "Final Project Part 2",
    "section": "Diagnositics",
    "text": "Diagnositics\n\npar(mfrow= c(2,3)); plot(fit1, which=1:6)\n\n\n\npar(mfrow= c(2,3)); plot(fit2, which=1:6)\n\n\n\npar(mfrow= c(2,3)); plot(fit3, which=1:6)\n\n\n\n\nAccording to the diagnostic plots, none of the models seem to fit super well. For fit2, (the one where covid_death_rate is log transformed,) the residuals seem to have a trend (higher fitted values have lower residuals). Same with the Q-Q, plot, lower theoretical quantiles gave significantly lower standardized residuals. The scale location graph has a negative trend, suggesting variance may not be constant. There may be a couple outliers significantly affecting the model according 4/n which is approximately 0.0013. Residuals versus leverage looks ok, there does not appear to be any super concerning observations. Finally, Cooks dist to leverage 2898 has a high cooks distance and leverage and likely has a large influence on the model. The other models display similar issues.\nMoving into part three of the project, I may look into other control variables that may improve the model or other transformations to improve R squared. Regardless there does not seem to be a super strong observable trend."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#references",
    "href": "posts/EmmaRasmussenFinalPart2.html#references",
    "title": "Final Project Part 2",
    "section": "References",
    "text": "References\nBallotpedia. (2022). Partisan composition of governors. Accessed [November 10, 2022]. https://ballotpedia.org/Partisan_composition_of_governors\nEconomic Research Service. (2022). Unemployment and median household income for the U.S., States, and counties, 2000-2021. Accessed from the United States Department of Agriculture [November 10, 2022]. https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data/\nJones, B. (2022). The Changing Political Geography of COVID-19 Over the Last Two Years. Pew Research Center. March 3, 2022. https://www.pewresearch.org/politics/2022/03/03/the-changing-political-geography-of-covid-19-over-the-last-two-years/\nMayo Clinic Staff. (2022). “COVID-19: Who’s at Higher Risk of Serious Symptoms?” Accessed from Mayo Clinic, [November 11, 2022]. https://www.mayoclinic.org/diseases-conditions/coronavirus/in-depth/coronavirus-who-is-at-risk/art-20483301\nMIT Election Data and Science Lab. (2021) County Presidential Election Returns 2000-2020. Accessed from the Harvard Dataverse [October 11, 2022]. https://doi.org/10.7910/DVN/VOQCHQ\nNational Center for Health Statistics. (2022). Provisional COVID-19 Deaths by County, and Race and Hispanic Origin. Accessed from the Centers for Disease Control [October 11, 2022]. https://data.cdc.gov/d/k8wy-p9cg\nUSA Facts. (2022).US COVID-19 cases and deaths by state. Accessed [November 10, 2022]. https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/?utm_source=usnews&utm_medium=partnership&utm_campaign=2020&utm_content=healthiestcommunitiescovid\nUnited States Census Bureau. (2019). Annual County Resident Population Estimates by Age, Sex, Race, and Hispanic Origin: April 1, 2010 to July 1, 2019 (CC-EST2019-ALLDATA). Accessed from Census.gov [November 11, 2022].https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html\nWood, D. and Brumfiel, G. (2021). Pro-Trump counties now have far higher COVID death rates. Misinformation is to blame. NPR. December 5, 2021. https://www.npr.org/sections/health-shots/2021/12/05/1059828993/data-vaccine-misinformation-trump-counties-covid-death-rate\n[Need to add italics to references]"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html",
    "href": "posts/KenDocekal_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q1",
    "href": "posts/KenDocekal_HW3.html#q1",
    "title": "HW3",
    "section": "Q1",
    "text": "Q1\nLoading in data\n\n\nCode\nlibrary(alr4) \n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss) \n\n\n\n\nCode\ndata('UN11', package = 'alr4')\n\n\n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q2",
    "href": "posts/KenDocekal_HW3.html#q2",
    "title": "HW3",
    "section": "Q2",
    "text": "Q2"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q3",
    "href": "posts/KenDocekal_HW3.html#q3",
    "title": "HW3",
    "section": "Q3",
    "text": "Q3\nLoading in water data.\n\n\nCode\ndata('water', package = 'alr4')\n\n\nLooking at the scatter plot matrix shows relationships between variables for year, precipitation at six Sierra Nevada mountain sites - APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE, and stream runoff volume near Bishop CA - BSAAM.\nFirst column shows precipitation then runoff (y-axis) by year(x-axis). We can see that for most sites precipitation has a somewhat wide distribution confined towards the lower end of the range with some outliners. OPRC precipitation is has greater spread while OPSLAKE and BSAAM show somewhat of a convex relationship.\nLooking at the last row comparing runoff (y-axis) to precipitation (x-axis); there is minimal correlation between precipitation from APMAM, APSAB, and APSLAKE and runoff levels but a strong positive linear correlation with precipitation from OPBPC, OPRC, and OPSLAKE sites. OPBPC, OPRC, and OPSLAKE sites’ greater correlation with BSAAM implies that these sites may be closer or more influential to stream runoff volume near Bishop CA.\nWhen focusing on the relationships between precipitation across sites there seems to be two groupings with high correlations; APMAM, APSAB, and APSLAKE all show fairly strong positive linear relationships with each other as do OPBPC, OPRC, and OPSLAKE. Across these groups of variables however the relationship is less clear and values are generally clustered among the lower values.This implies that sites based on these two groupings may share closer geographic proximity and are therefore more similarly affected by precipitation levels.\n\n\nCode\npairs(water)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q4",
    "href": "posts/KenDocekal_HW3.html#q4",
    "title": "HW3",
    "section": "Q4",
    "text": "Q4\nLoading in Rateprof data.\n\n\nCode\ndata('Rateprof', package = 'alr4')\n\n\nSpecifying rating variables - quality, helpfulness, clarity, easiness, raterInterest.\n\n\nCode\nRateprof1 = subset(Rateprof, select = c(quality, helpfulness, clarity, easiness, raterInterest))\n\nhead(Rateprof1)\n\n\n   quality helpfulness  clarity easiness raterInterest\n1 4.636364    4.636364 4.636364 4.818182      3.545455\n2 4.318182    4.545455 4.090909 4.363636      4.000000\n3 4.790698    4.720930 4.860465 4.604651      3.432432\n4 4.250000    4.458333 4.041667 2.791667      3.181818\n5 4.684211    4.684211 4.684211 4.473684      4.214286\n6 4.233333    4.266667 4.200000 4.533333      3.916667\n\n\nThe scatter plot matrix shows very strong positive linear relationships between quality, helpfulness, and clarity. Easiness is also positively correlated with these three variables but less strongly. raterInterest is even less strongly correlated with quality, helpfulness, clarity, and easiness (especially with easiness) but there still seems to be a slightly positive linear relationship.\n\n\nCode\npairs(Rateprof1)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q5",
    "href": "posts/KenDocekal_HW3.html#q5",
    "title": "HW3",
    "section": "Q5",
    "text": "Q5\nLoading in student.survey data.\n\n\nCode\ndata('student.survey', package = 'smss')\n\n\nReviewing variables; pi is political ideology, re is religiosity, hi is high school GPA, and tv is average hours of TV watching per week.\n\n\nCode\n?student.survey\n\nview(student.survey)"
  },
  {
    "objectID": "posts/HW1_Yakub Rabiutheen.html",
    "href": "posts/HW1_Yakub Rabiutheen.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap,freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nComparison of the Genders for both Men and Women using a Boxplot.\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\n\n\n\nHere is the capacity of Smokers vs Non-Smokers\n\n\nCode\nboxplot(df$LungCap~df$Smoke,\n        ylab = \"Capacity\", \n        main = \"Lung Capacity of Smokers Vs Non-Smokers\",\n        las = 1)\n\n\n\n\n\n\n\n\nLet’s break it down even further, this is the Lung Capacity by Age Group\n\n\nCode\ndf$Agegroups<-cut(df$Age,breaks=c(-Inf, 13, 15, 17, 20), labels=c(\"0-13 years\", \"14-15 years\", \"16-17 years\", \"18+ years\"))\n\n\nBelow is the overall Lung Capacity of Age Groups without including Smokers.\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Gender)) +\n          geom_bar(stat = \"identity\") +\n          coord_flip() +\n          theme_classic()\n\n\n\n\n\n#e\nHere is a comparision of AgeGroup Lung Capacity in comparison with Smoker vs Non-Smoker.\n\n\nCode\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\n\nBased on the comparison of lung capacities between Smoker and Non-Smoker the results are pretty similar.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nQuestion 2\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\ndf <- data.frame(X, Frequency)\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nAs shown below, the most common Prior Convictions is 1.\n\n\nCode\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nDividing by the total among 810 we can determine the probability for each. 810 is the Sum of the Frequency which I checked manually.\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\n\n\nError in mutate(df, Probability = Frequency/sum(Frequency)): could not find function \"mutate\"\n\n\nCode\ndf2\n\n\nError in eval(expr, envir, enclos): object 'df2' not found\n\n\n\nFilter for Probability of 2 Convictions\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\n\nError in df2 %>% filter(X < 2): could not find function \"%>%\"\n\n\nCode\nsum(b2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'b2' not found\n\n\n\nFilter for Probability of Less than 2 Convictions\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\n\nError in df2 %>% filter(X <= 2): could not find function \"%>%\"\n\n\nCode\nsum(c2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'c2' not found\n\n\n\n\n\nFilter for Probability of greater than 2 convictions.\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\n\nError in df2 %>% filter(X > 2): could not find function \"%>%\"\n\n\nCode\nsum(d2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'd2' not found\n\n\nWhat is the expected value of the number of prior convictions?\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\n\n\nError in weighted.mean(df2$X, df2$Probability): object 'df2' not found\n\n\nCode\ne\n\n\nError in eval(expr, envir, enclos): object 'e' not found\n\n\n\n\n\nVariance and Standard Deviation for Question.\n\n\nCode\nvar(df$X)\n\n\n[1] 2.5\n\n\n\n\nCode\nsd(df$X)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/Final_SteveONeill.html#introduction",
    "href": "posts/Final_SteveONeill.html#introduction",
    "title": "Final Part 1",
    "section": "Introduction",
    "text": "Introduction\nBy analyzing public Paycheck Protection Program data, I have the ability to examine correlations between PPP loan size, forgiveness status, business status, geographical location, and - most optimistically - political affiliation. In the United States, the PPP was introduced as an emergency economic measure in 2020 during the Covid-19 pandemic. As a massive direct-loan program, some have criticized it for lax oversight.\nPPP fraud is an important issue, but finding lawbreakers is probably better left to the Feds. Instead, I think there is room in the academic body of work to study political affiliation for loan recipients.\n\nExisting Work\nThere have already been many studies about the PPP focusing on overall economic impact and the emergence of non-traditional, non-bank lenders.\nSeparately, two interesting studies have been done about racial disparities re: access to PPP loans, with one finding that Black-owned businesses in Florida were 25% less likely to receive PPP funds (Chernenko & Scharfstein, 2022), and another finding even higher of a disparity (50%) but taking note of mitigating factors (e.g. access to “fintech” lending instead of traditional banks) (Atkins, Cook & Seamans)\nI am not so much interested in the efficacy of the PPP or the racial make-up of PPP borrowers. But I am interested in their political affiliation, i.e. if they registered are Democrat or Republican. So, similarly to Chernenko & Scharfstein, I intend to use public-access data to cross-reference federal PPP data with state-level voter registration and corporation search data.\nA relevant paper I found after I started looking at this data - “Buying the Vote? The Economics of Electoral Politics and Small-Business Loans” - does look at data from SBA and PPP loans and pit them against the hypothesis ====that “electoral considerations may have tilted the allocation of PPP funds toward firms in areas or industries that could have a significant impact on the results of the 2020 election”.(Duchin & Hackney, 2021)\nThat particular study measured political ad spending and FEC filings as indicators of electoral considerations and used the Partisan Voting Index (PVI) from the Cook Political National Report to find which states were “battleground” or solid-R states. PPP lending outcomes were compared with electoral considerations to find if the SBA (under Trump) preferred to give PPP loans to swing voters or members of the party “base” in anticipation of an upcoming general election.\nRather than aggregate-level state data, my analysis will compare individuals’ personal voter affiliation - if they are registered, and to which party - with their loan amount, forgiveness status, and other metrics."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#research-question",
    "href": "posts/Final_SteveONeill.html#research-question",
    "title": "Final Part 1",
    "section": "Research Question",
    "text": "Research Question\nI am still developing my research question, but a simple one could be:\nHow does political affiliation affect PPP loan forgiveness status?\nThe data for PPP loans is current available on ProPublica, but not in a queryable way.\nFortunately, the underlying data has been made public at https://data.sba.gov/dataset/ppp-foia.\nMy hypotheses could be that:\nHypothesis 1 (H1): PPP loan forgiveness was given to registered Republicans more often than registered Democrats\nNull Hypothesis (H0): There is no difference in loan forgiveness given to registered Democrats vs registered Republicans.\nIn some ways this is similar to Duchin & Hackney, but this benefits from a narrower look at who actually received PPP stimulus rather than eventual outcomes on the state level.\nClearly a few effects that would need to be controlled for. For example, Republicans could be more likely to be in any kind of business in the first place, confounding my results.\nIdeally, I would have focused on my home state of Massachusetts. However, although voter registration data is personally available in MA, you can’t download it all in one .csv, and scraping the Commonwealth’s website is not allowed. The National Conference of State Legislatures keeps track of which states have full voter registration data for download. For now, I will use Ohio as an example (although later on I will explain why I could have chosen better):\n\nPPP Data Dictionary\nTo help understand which fields mean what, the PPP data comes with a “Data Dictionary” with an explanation of columns [scroll right to see explanation]:\n\n\nCode\nppp_data_dictionary <- read_excel(\"_data/ppp-data-dictionary.xlsx\")\nppp_data_dictionary\n\n\n\n\n  \n\n\n\n\n\nPPP Data (Ohio)\nImporting the data of the actual PPP loans is pretty easy. This .csv contains just the PPP loans below $150k - others are contained in a smaller spreadsheet that covers all 50 states, and will be part of my final project.\n\n\nCode\n#ppp_all <- read_csv(\"_data/public_up_to_150k_9_220930.csv\")\n#ppp_all\n\n\nIt still seems to capture all of Ohio, because Ohio doesn’t come last alphabetically. But to manage it better I need to sample it to 200k rows. The code below is commented out because this is a one-time process. Henceforth, I will be working with the .csv I generated in this step.\n\n\nCode\n#ppp_ohio <- ppp_all %>% filter(BorrowerState == \"OH\")\n#ppp_ohio_sampled_10k <- ppp_ohio %>% sample_n(10000)\n#write_csv(ppp_ohio_sampled_10k, \"_data/ppp_ohio_sampled_10k.csv\")\n\nppp_ohio <- read_csv(\"_data/ppp_ohio_sampled_10k.csv\")\n\n\nRows: 10000 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (35): DateApproved, SBAOfficeCode, ProcessingMethod, BorrowerName, Borro...\ndbl (18): LoanNumber, Term, SBAGuarantyPercentage, InitialApprovalAmount, Cu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nLLCs, Corporations\nVisually, you may notice that businesses and LLCs make up a majority of the loan recipients.\nWhen I was preparing to do this project for Massachusetts - before I knew about the limitation of voter registration data in that state - I contacted the Secretary of the Commonwealth and they sent me a document with the entire “Corporation Search” data in tabular format, with the name of the founder, every board member, business type, year established, etc. I anticipated I would be able to match those individuals with the voter registration data, but that data was unfortunately available.\nThe Corporation Search data would be essential to the successful completion of the project.\nI have not inquired yet, but assume Ohio will provide the same Corporation Search data. And if they do not, I will just pick another state that 1.) has public voter registration information, and 2.) can supply the Corporation Search data in .csv or .xlsx (as Massachusetts did).\n\n\nDeeper Looks\n\n\nCode\nglimpse(ppp_ohio)\n\n\nRows: 10,000\nColumns: 53\n$ LoanNumber                  <dbl> 8316258510, 2043838906, 2695968905, 786696…\n$ DateApproved                <chr> \"03/09/2021\", \"04/26/2021\", \"04/27/2021\", …\n$ SBAOfficeCode               <chr> \"0593\", \"0549\", \"0593\", \"0593\", \"0593\", \"0…\n$ ProcessingMethod            <chr> \"PPS\", \"PPP\", \"PPS\", \"PPP\", \"PPP\", \"PPS\", …\n$ BorrowerName                <chr> \"JASON MILLER\", \"JOHN HOLLY\", \"SHELLESSA D…\n$ BorrowerAddress             <chr> \"1008 Gateway Dr\", \"3977 E 186th St\", \"42 …\n$ BorrowerCity                <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ BorrowerState               <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ BorrowerZip                 <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ LoanStatusDate              <chr> \"11/19/2021\", \"01/31/2022\", \"09/25/2021\", …\n$ LoanStatus                  <chr> \"Paid in Full\", \"Paid in Full\", \"Paid in F…\n$ Term                        <dbl> 60, 60, 60, 24, 60, 60, 24, 60, 24, 60, 60…\n$ SBAGuarantyPercentage       <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 10…\n$ InitialApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ CurrentApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ UndisbursedAmount           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FranchiseName               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ServicingLenderLocationID   <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ ServicingLenderName         <chr> \"Civista Bank\", \"American Lending Center\",…\n$ ServicingLenderAddress      <chr> \"100 E Water St\", \"1 World Trade Center, S…\n$ ServicingLenderCity         <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ ServicingLenderState        <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ ServicingLenderZip          <chr> \"44870-2524\", \"90831\", \"92653\", \"37030-120…\n$ RuralUrbanIndicator         <chr> \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U…\n$ HubzoneIndicator            <chr> \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ LMIIndicator                <chr> \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ BusinessAgeDescription      <chr> \"Existing or more than 2 years old\", \"Exis…\n$ ProjectCity                 <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ ProjectCountyName           <chr> \"MONTGOMERY\", \"CUYAHOGA\", \"MONTGOMERY\", \"H…\n$ ProjectState                <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ ProjectZip                  <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ CD                          <chr> \"OH-10\", \"OH-11\", \"OH-10\", \"OH-01\", \"OH-01…\n$ JobsReported                <dbl> 12, 1, 1, 14, 1, 1, 17, 1, 2, 1, 2, 1, 1, …\n$ NAICSCode                   <dbl> 722410, 722320, 624110, 721110, 531210, 56…\n$ Race                        <chr> \"White\", \"Unanswered\", \"Unanswered\", \"Unan…\n$ Ethnicity                   <chr> \"Not Hispanic or Latino\", \"Unknown/NotStat…\n$ UTILITIES_PROCEED           <dbl> 1, NA, NA, NA, NA, 1, NA, NA, NA, 1, 1, NA…\n$ PAYROLL_PROCEED             <dbl> 61853.00, 20833.33, 11414.00, 57540.00, 20…\n$ MORTGAGE_INTEREST_PROCEED   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RENT_PROCEED                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ REFINANCE_EIDL_PROCEED      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ HEALTH_CARE_PROCEED         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ DEBT_INTEREST_PROCEED       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ BusinessType                <chr> \"Limited  Liability Company(LLC)\", \"Sole P…\n$ OriginatingLenderLocationID <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ OriginatingLender           <chr> \"Civista Bank\", \"American Lending Center\",…\n$ OriginatingLenderCity       <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ OriginatingLenderState      <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ Gender                      <chr> \"Male Owned\", \"Unanswered\", \"Unanswered\", …\n$ Veteran                     <chr> \"Non-Veteran\", \"Unanswered\", \"Unanswered\",…\n$ NonProfit                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ForgivenessAmount           <dbl> 62252.49, 20924.77, 11447.46, 57990.86, 20…\n$ ForgivenessDate             <chr> \"10/26/2021\", \"10/06/2021\", \"08/20/2021\", …\n\n\nForgivenessAmount, ForgivenessDate, and BorrowerName promise to be the most useful variables.\n\n\nCode\nprint(summarytools::dfSummary(ppp_ohio,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\n\n\nData Frame Summary\nppp_ohio\nDimensions: 10000 x 53\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      LoanNumber\n[numeric]\n      Mean (sd) : 5492418880 (2600653757)min ≤ med ≤ max:1000358703 ≤ 5564758604 ≤ 9997128810IQR (CV) : 4509922449 (0.5)\n      10000 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DateApproved\n[character]\n      1. 05/01/20202. 04/28/20203. 04/15/20204. 04/27/20205. 04/14/20206. 05/12/20217. 04/30/20208. 04/29/20209. 04/22/202110. 03/23/2021[ 212 others ]\n      724(7.2%)312(3.1%)265(2.6%)240(2.4%)179(1.8%)164(1.6%)155(1.6%)154(1.5%)150(1.5%)149(1.5%)7508(75.1%)\n      \n      0\n(0.0%)\n    \n    \n      SBAOfficeCode\n[character]\n      1. 01012. 05493. 0593\n      1(0.0%)4830(48.3%)5169(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      ProcessingMethod\n[character]\n      1. PPP2. PPS\n      7835(78.3%)2165(21.6%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerName\n[character]\n      1. MICHAEL JOHNSON2. 16ELEVEN LLC3. ALL SERVICE AERATION4. ANDREW BRUSH5. ANTONIO JONES6. ASHLEY JACKSON7. BLUE FITNESS INC8. BRITTANY JONES9. BRITTNEY HAYWARD10. CAMERON JOHNSON[ 9924 others ]\n      3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9979(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerAddress\n[character]\n      1. 24 E North St2. 24455 Lake Shore Blvd3. 4503 Marburg Ave4. 4910 Tiedeman Dr5. 1 E Campus View Blvd6. 100 E Campus View Blvd St7. 100 E Wilson Bridge Rd8. 1005 W 3rd Ave9. 109 Elm St10. 11271 Reading Rd[ 9911 others ]\n      3(0.0%)3(0.0%)3(0.0%)3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9976(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1299 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerZip\n[character]\n      1. 441452. 441243. 432154. 450115. 430176. 430657. 452388. 440959. 4413910. 45242[ 8358 others ]\n      21(0.2%)20(0.2%)17(0.2%)17(0.2%)16(0.2%)16(0.2%)15(0.1%)14(0.1%)14(0.1%)14(0.1%)9836(98.4%)\n      \n      0\n(0.0%)\n    \n    \n      LoanStatusDate\n[character]\n      1. 03/22/20222. 11/17/20213. 01/06/20224. 08/17/20215. 09/28/20216. 09/25/20217. 10/21/20218. 11/20/20219. 10/20/202110. 10/15/2021[ 380 others ]\n      392(4.5%)258(2.9%)243(2.8%)240(2.7%)166(1.9%)159(1.8%)131(1.5%)130(1.5%)121(1.4%)102(1.2%)6844(77.9%)\n      \n      1214\n(12.1%)\n    \n    \n      LoanStatus\n[character]\n      1. Exemption 42. Paid in Full\n      1214(12.1%)8786(87.9%)\n      \n      0\n(0.0%)\n    \n    \n      Term\n[numeric]\n      Mean (sd) : 47.7 (17.2)min ≤ med ≤ max:0 ≤ 60 ≤ 75IQR (CV) : 36 (0.4)\n      28 distinct values\n      \n      0\n(0.0%)\n    \n    \n      SBAGuarantyPercentage\n[numeric]\n      1 distinct value\n      100:10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      InitialApprovalAmount\n[numeric]\n      Mean (sd) : 26700 (28548)min ≤ med ≤ max:105 ≤ 20207 ≤ 252947IQR (CV) : 16700 (1.1)\n      5789 distinct values\n      \n      0\n(0.0%)\n    \n    \n      CurrentApprovalAmount\n[numeric]\n      Mean (sd) : 26570.5 (28170.1)min ≤ med ≤ max:105 ≤ 20207 ≤ 149700IQR (CV) : 16654.9 (1.1)\n      5803 distinct values\n      \n      0\n(0.0%)\n    \n    \n      UndisbursedAmount\n[numeric]\n      1 distinct value\n      0:9998(100.0%)\n      \n      2\n(0.0%)\n    \n    \n      FranchiseName\n[character]\n      1. Subway2. Comfort Inn by Choice Hot3. H&R Block - Franchise Lic4. Hot Head Burritos5. Vision Source6. Dunkin' Donuts7. Holiday Inn Express (Lice8. Orange Leaf Frozen Yogurt9. Wild Birds Unlimited10. Zoup![ 79 others ]\n      9(8.2%)3(2.7%)3(2.7%)3(2.7%)3(2.7%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)79(71.8%)\n      \n      9890\n(98.9%)\n    \n    \n      ServicingLenderLocationID\n[numeric]\n      Mean (sd) : 214404.3 (203525.1)min ≤ med ≤ max:4282 ≤ 79184 ≤ 538160IQR (CV) : 441813 (0.9)\n      327 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderName\n[character]\n      1. The Huntington National B2. Harvest Small Business Fi3. Prestamos CDFI, LLC4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. Cross River Bank[ 314 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderAddress\n[character]\n      1. 17 S High St.2. 24422 Avenida de la Carlo3. 1024 East Buckeye Road Su4. 2247 Central Drive5. 38 Fountain Sq Plz6. 7000 SW 97th Avenue Suite7. 222 Delaware Ave8. 1111 Polaris Pkwy9. 425 Walnut St10. 885 Teaneck Rd[ 315 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Laguna Hills4. Phoenix5. Bedford6. Miami7. WILMINGTON8. TEANECK9. CLEVELAND10. LAKE MARY[ 255 others ]\n      1275(12.8%)975(9.8%)718(7.2%)624(6.2%)484(4.8%)427(4.3%)382(3.8%)306(3.1%)302(3.0%)248(2.5%)4259(42.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. NJ7. DE8. PA9. NY10. RI[ 29 others ]\n      5178(51.8%)907(9.1%)681(6.8%)625(6.2%)515(5.1%)379(3.8%)378(3.8%)362(3.6%)243(2.4%)216(2.2%)516(5.2%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderZip\n[character]\n      1. 43215-34132. 926533. 850344. 760215. 452636. 331737. 19801-16218. 43240-20319. 45202-395610. 07666-4546[ 316 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      RuralUrbanIndicator\n[character]\n      1. R2. U\n      2381(23.8%)7619(76.2%)\n      \n      0\n(0.0%)\n    \n    \n      HubzoneIndicator\n[character]\n      1. N2. Y\n      6935(69.3%)3065(30.6%)\n      \n      0\n(0.0%)\n    \n    \n      LMIIndicator\n[character]\n      1. N2. Y\n      6823(68.2%)3177(31.8%)\n      \n      0\n(0.0%)\n    \n    \n      BusinessAgeDescription\n[character]\n      1. Existing or more than 2 y2. New Business or 2 years o3. Startup, Loan Funds will 4. Unanswered\n      9005(90.0%)433(4.3%)1(0.0%)561(5.6%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1298 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCountyName\n[character]\n      1. CUYAHOGA2. FRANKLIN3. HAMILTON4. SUMMIT5. LUCAS6. MONTGOMERY7. STARK8. BUTLER9. MAHONING10. LORAIN[ 78 others ]\n      1810(18.1%)1277(12.8%)800(8.0%)491(4.9%)473(4.7%)473(4.7%)273(2.7%)237(2.4%)224(2.2%)197(2.0%)3745(37.5%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectZip\n[character]\n      1. 44145-00012. 43215-00013. 44124-00014. 45011-00015. 43065-00016. 45242-00017. 43017-00018. 44122-00019. 44139-000110. 43015-0001[ 8476 others ]\n      21(0.2%)19(0.2%)19(0.2%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)9847(98.5%)\n      \n      0\n(0.0%)\n    \n    \n      CD\n[character]\n      1. OH-112. OH-033. OH-054. OH-015. OH-096. OH-147. OH-128. OH-089. OH-1010. OH-13[ 7 others ]\n      1352(13.5%)832(8.3%)699(7.0%)679(6.8%)646(6.5%)613(6.1%)610(6.1%)555(5.6%)555(5.6%)541(5.4%)2918(29.2%)\n      \n      0\n(0.0%)\n    \n    \n      JobsReported\n[numeric]\n      Mean (sd) : 4.1 (11.9)min ≤ med ≤ max:1 ≤ 1 ≤ 500IQR (CV) : 3 (2.9)\n      69 distinct values\n      \n      0\n(0.0%)\n    \n    \n      NAICSCode\n[numeric]\n      Mean (sd) : 540071.1 (202394.1)min ≤ med ≤ max:111110 ≤ 541511 ≤ 999990IQR (CV) : 267779.5 (0.4)\n      686 distinct values\n      \n      113\n(1.1%)\n    \n    \n      Race\n[character]\n      1. American Indian or Alaska2. Asian3. Black or African American4. Native Hawaiian or Other 5. Unanswered6. White\n      23(0.2%)129(1.3%)1103(11.0%)7(0.1%)6905(69.0%)1833(18.3%)\n      \n      0\n(0.0%)\n    \n    \n      Ethnicity\n[character]\n      1. Hispanic or Latino2. Not Hispanic or Latino3. Unknown/NotStated\n      119(1.2%)3279(32.8%)6602(66.0%)\n      \n      0\n(0.0%)\n    \n    \n      UTILITIES_PROCEED\n[numeric]\n      Mean (sd) : 513.4 (3183.8)min ≤ med ≤ max:0 ≤ 1 ≤ 134700IQR (CV) : 0 (6.2)\n      345 distinct values\n      \n      6802\n(68.0%)\n    \n    \n      PAYROLL_PROCEED\n[numeric]\n      Mean (sd) : 26102.4 (27604.3)min ≤ med ≤ max:0 ≤ 20052 ≤ 149700IQR (CV) : 16222 (1.1)\n      6362 distinct values\n      \n      10\n(0.1%)\n    \n    \n      MORTGAGE_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 2901 (7033.4)min ≤ med ≤ max:0 ≤ 1210.7 ≤ 86712IQR (CV) : 2737.8 (2.4)\n      140 distinct values\n      \n      9805\n(98.0%)\n    \n    \n      RENT_PROCEED\n[numeric]\n      Mean (sd) : 5396.7 (5788.6)min ≤ med ≤ max:0 ≤ 3700 ≤ 36502IQR (CV) : 5746.7 (1.1)\n      263 distinct values\n      \n      9632\n(96.3%)\n    \n    \n      REFINANCE_EIDL_PROCEED\n[numeric]\n      Mean (sd) : 1550.9 (3835.3)min ≤ med ≤ max:0 ≤ 0 ≤ 25200IQR (CV) : 1000 (2.5)\n      13 distinct values\n      \n      9931\n(99.3%)\n    \n    \n      HEALTH_CARE_PROCEED\n[numeric]\n      Mean (sd) : 4122 (5865.4)min ≤ med ≤ max:0 ≤ 2070 ≤ 32900IQR (CV) : 4600 (1.4)\n      86 distinct values\n      \n      9871\n(98.7%)\n    \n    \n      DEBT_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 1349.6 (4310.4)min ≤ med ≤ max:0 ≤ 150 ≤ 36100IQR (CV) : 1186.5 (3.2)\n      34 distinct values\n      \n      9924\n(99.2%)\n    \n    \n      BusinessType\n[character]\n      1. Sole Proprietorship2. Limited  Liability Compan3. Corporation4. Independent Contractors5. Subchapter S Corporation6. Self-Employed Individuals7. Non-Profit Organization8. Single Member LLC9. Partnership10. Limited Liability Partner[ 7 others ]\n      3250(32.5%)2545(25.5%)1285(12.9%)865(8.7%)790(7.9%)760(7.6%)227(2.3%)89(0.9%)70(0.7%)60(0.6%)52(0.5%)\n      \n      7\n(0.1%)\n    \n    \n      OriginatingLenderLocationID\n[numeric]\n      Mean (sd) : 213535.4 (203054.8)min ≤ med ≤ max:4282 ≤ 78723 ≤ 531105IQR (CV) : 430853.2 (1)\n      331 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLender\n[character]\n      1. The Huntington National B2. Prestamos CDFI, LLC3. Harvest Small Business Fi4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. KeyBank National Associat[ 317 others ]\n      846(8.5%)624(6.2%)497(5.0%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)292(2.9%)5295(52.9%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Phoenix4. Laguna Hills5. Bedford6. Miami7. WILMINGTON8. CLEVELAND9. TEANECK10. LAKE MARY[ 259 others ]\n      1275(12.8%)975(9.8%)624(6.2%)497(5.0%)484(4.8%)427(4.3%)382(3.8%)302(3.0%)268(2.7%)247(2.5%)4519(45.2%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. DE7. NJ8. PA9. RI10. UT[ 29 others ]\n      5183(51.8%)849(8.5%)681(6.8%)625(6.2%)519(5.2%)378(3.8%)349(3.5%)311(3.1%)216(2.2%)209(2.1%)680(6.8%)\n      \n      0\n(0.0%)\n    \n    \n      Gender\n[character]\n      1. Female Owned2. Male Owned3. Unanswered\n      1604(16.0%)2753(27.5%)5643(56.4%)\n      \n      0\n(0.0%)\n    \n    \n      Veteran\n[character]\n      1. Non-Veteran2. Unanswered3. Veteran\n      3701(37.0%)6096(61.0%)203(2.0%)\n      \n      0\n(0.0%)\n    \n    \n      NonProfit\n[character]\n      1. Y\n      251(100.0%)\n      \n      9749\n(97.5%)\n    \n    \n      ForgivenessAmount\n[numeric]\n      Mean (sd) : 27785.1 (29446.5)min ≤ med ≤ max:12.6 ≤ 20307.2 ≤ 150842.8IQR (CV) : 20463.8 (1.1)\n      8422 distinct values\n      \n      1145\n(11.5%)\n    \n    \n      ForgivenessDate\n[character]\n      1. 06/15/20212. 10/06/20213. 01/07/20214. 09/07/20215. 09/29/20216. 09/01/20217. 06/23/20218. 09/22/20219. 11/10/202110. 10/14/2021[ 432 others ]\n      164(1.9%)147(1.7%)132(1.5%)98(1.1%)96(1.1%)92(1.0%)91(1.0%)91(1.0%)82(0.9%)79(0.9%)7783(87.9%)\n      \n      1145\n(11.5%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2022-11-12\n\n\n\n(Data below from unsampled, larger dataset)\nFrom a first look, most applicants reported only one employee needing coverage under the PPP. In fact, “Sole Proprietorship” was the highest category of BusinessType, above even LLCs, with 33.3%.\n19% of applicants were white, leading the race category, and 68% were unreported.\nMedian ForgivenessAmount was $20,438.7, and most loans were forgiven in 2021."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#voter-registration",
    "href": "posts/Final_SteveONeill.html#voter-registration",
    "title": "Final Part 1",
    "section": "Voter Registration",
    "text": "Voter Registration\nThe Ohio voter registration data is easily read-in:\n\n\nCode\n#ohio_voters <- read_csv(\"_data/SWVF_1_22.txt\")\n#ohio_voters_sampled <- ohio_voters %>% sample_n(10000)\n#write_csv(ohio_voters_sampled, \"_data/ohio_voters_sampled.csv\")\n\nohio_voters <- read_csv(\"_data/ohio_voters_sampled.csv\")\n\n\nRows: 10000 Columns: 116\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (91): SOS_VOTERID, COUNTY_NUMBER, LAST_NAME, FIRST_NAME, MIDDLE_NAME, S...\ndbl   (4): COUNTY_ID, RESIDENTIAL_ZIP, MAILING_ZIP, STATE_REPRESENTATIVE_DIS...\nlgl  (19): RESIDENTIAL_COUNTRY, RESIDENTIAL_POSTALCODE, MAILING_SECONDARY_AD...\ndate  (2): DATE_OF_BIRTH, REGISTRATION_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis is a 1gb+ file, which needs to be fully complete in order for the join of sampled PPP data. For the actual project, I will do that in a one-time process on my local computer.\nZip codes will let us know with high confidence that we are dealing with the same business owner even if names are duplicated.\nThe historical information tells us if they have changed their party affiliation from year-to-year, which can be useful in determining if political rent-seeking was successful - we can even see ‘D’ or ‘R’ going back 22 years:\n\n\nCode\nglimpse(ohio_voters)\n\n\nRows: 10,000\nColumns: 116\n$ SOS_VOTERID                   <chr> \"OH0010417183\", \"OH0024878858\", \"OH00159…\n$ COUNTY_NUMBER                 <chr> \"19\", \"18\", \"18\", \"12\", \"18\", \"03\", \"10\"…\n$ COUNTY_ID                     <dbl> 9500876, 2859748, 73683, 122609458, 2158…\n$ LAST_NAME                     <chr> \"COLLINS\", \"SAFO\", \"WHELAN\", \"OVERHOLT\",…\n$ FIRST_NAME                    <chr> \"SHANNON\", \"SHAREEF\", \"PATRICK\", \"ADDISO…\n$ MIDDLE_NAME                   <chr> \"AMY\", \"RESHAUN\", \"HOWARD\", \"ELIZABETH\",…\n$ SUFFIX                        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ DATE_OF_BIRTH                 <date> 1977-02-22, 1998-12-18, 1969-07-21, 200…\n$ REGISTRATION_DATE             <date> 1995-05-19, 2021-09-01, 2022-08-02, 202…\n$ VOTER_STATUS                  <chr> \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", …\n$ PARTY_AFFILIATION             <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ RESIDENTIAL_ADDRESS1          <chr> \"372 S MAIN ST\", \"1629 CARLYON RD\", \"232…\n$ RESIDENTIAL_SECONDARY_ADDR    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_CITY              <chr> \"NEW MADISON\", \"EAST CLEVELAND\", \"NORTH …\n$ RESIDENTIAL_STATE             <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\"…\n$ RESIDENTIAL_ZIP               <dbl> 45346, 44112, 44070, 45502, 44109, 44805…\n$ RESIDENTIAL_ZIP_PLUS4         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_COUNTRY           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_POSTALCODE        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ADDRESS1              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_SECONDARY_ADDRESS     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_CITY                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_STATE                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP_PLUS4             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_COUNTRY               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_POSTAL_CODE           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CAREER_CENTER                 <chr> \"MIAMI VALLEY CAREER TECH\", NA, \"POLARIS…\n$ CITY                          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CITY_SCHOOL_DISTRICT          <chr> NA, \"EAST CLEVELAND CITY SD\", \"NORTH OLM…\n$ COUNTY_COURT_DISTRICT         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CONGRESSIONAL_DISTRICT        <chr> \"08\", \"11\", \"07\", \"15\", \"11\", \"04\", \"06\"…\n$ COURT_OF_APPEALS              <chr> \"02\", \"08\", \"08\", \"02\", \"08\", \"05\", \"07\"…\n$ EDU_SERVICE_CENTER_DISTRICT   <chr> \"DARKE COUNTY ESC\", NA, NA, \"CLARK COUNT…\n$ EXEMPTED_VILL_SCHOOL_DISTRICT <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LIBRARY                       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LOCAL_SCHOOL_DISTRICT         <chr> \"TRI-VILLAGE LOCAL SD (DARKE)\", NA, NA, …\n$ MUNICIPAL_COURT_DISTRICT      <chr> \"DARKE-CO\", \"EAST-CLEVELAND\", \"ROCKY-RIV…\n$ PRECINCT_NAME                 <chr> \"PRECINCT HARRISON EAST & NEW MADISON\", …\n$ PRECINCT_CODE                 <chr> \"19AAZ\", \"18-P-BMR\", \"18-P-CDT\", \"12ADJ\"…\n$ STATE_BOARD_OF_EDUCATION      <chr> \"03\", \"10\", \"11\", \"10\", \"11\", \"05\", \"08\"…\n$ STATE_REPRESENTATIVE_DISTRICT <dbl> 80, 22, 16, 74, 15, 67, 79, 71, 47, 14, …\n$ STATE_SENATE_DISTRICT         <chr> \"05\", \"21\", \"24\", \"10\", \"24\", \"22\", \"33\"…\n$ TOWNSHIP                      <chr> \"HARRISON TWP\", NA, NA, \"BETHEL TOWNSHIP…\n$ VILLAGE                       <chr> \"NEW MADISON VILLAGE\", NA, NA, NA, NA, N…\n$ WARD                          <chr> NA, \"EAST CLEVELAND WARD 2\", \"NORTH OLMS…\n$ `PRIMARY-03/07/2000`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/07/2000`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, NA…\n$ `SPECIAL-05/08/2001`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/06/2001`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-05/07/2002`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/05/2002`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, \"X\", NA…\n$ `SPECIAL-05/06/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/04/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-03/02/2004`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/02/2004`          <chr> \"X\", NA, \"X\", NA, NA, NA, \"X\", NA, \"X\", …\n$ `SPECIAL-02/08/2005`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/03/2005`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/13/2005`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2005`          <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, NA, NA…\n$ `SPECIAL-02/07/2006`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/02/2006`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `GENERAL-11/07/2006`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/11/2007`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-12/11/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-03/04/2008`          <chr> NA, NA, NA, NA, \"D\", NA, \"D\", NA, \"R\", N…\n$ `PRIMARY-10/14/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/04/2008`          <chr> \"X\", NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\",…\n$ `GENERAL-11/18/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/05/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/08/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/15/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/29/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2009`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/04/2010`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `PRIMARY-07/13/2010`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/07/2010`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2010`          <chr> \"X\", NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/03/2011`          <chr> \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `PRIMARY-09/13/2011`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2011`          <chr> \"X\", NA, \"X\", NA, NA, NA, NA, NA, \"X\", N…\n$ `PRIMARY-03/06/2012`          <chr> NA, NA, NA, NA, \"R\", NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/06/2012`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/07/2013`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/10/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-10/01/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2013`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/06/2014`          <chr> \"L\", NA, NA, NA, NA, NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/04/2014`          <chr> NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", N…\n$ `PRIMARY-05/05/2015`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/15/2015`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2015`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-03/15/2016`          <chr> \"D\", NA, NA, NA, NA, \"R\", \"D\", NA, \"R\", …\n$ `GENERAL-06/07/2016`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/13/2016`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2016`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"X…\n$ `PRIMARY-05/02/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/12/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/07/2017`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2018`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, NA, \"R…\n$ `GENERAL-08/07/2018`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/06/2018`          <chr> \"X\", \"X\", NA, NA, NA, \"X\", \"X\", \"X\", \"X\"…\n$ `PRIMARY-05/07/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/10/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2019`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X…\n$ `PRIMARY-03/17/2020`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, \"R\"…\n$ `GENERAL-11/03/2020`          <chr> \"X\", \"X\", \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"…\n$ `PRIMARY-05/04/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"X\",…\n$ `PRIMARY-08/03/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/14/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2021`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", \"X\", NA, …\n$ `PRIMARY-05/03/2022`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ `PRIMARY-08/02/2022`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n\n\n\n\nCode\nprint(summarytools::dfSummary(ohio_voters,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\n\n\nData Frame Summary\nohio_voters\nDimensions: 10000 x 116\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      SOS_VOTERID\n[character]\n      1. OH00100003302. OH00100003443. OH00100008674. OH00100012975. OH00100022616. OH00100028027. OH00100028178. OH00100028989. OH001000297910. OH0010003129[ 9990 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)9990(99.9%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_NUMBER\n[character]\n      1. 182. 093. 214. 135. 126. 157. 028. 049. 2210. 07[ 12 others ]\n      4086(40.9%)1187(11.9%)685(6.9%)683(6.8%)392(3.9%)346(3.5%)326(3.3%)286(2.9%)249(2.5%)208(2.1%)1552(15.5%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_ID\n[numeric]\n      Mean (sd) : 4804609 (14312683)min ≤ med ≤ max:30 ≤ 1939790 ≤ 122614854IQR (CV) : 2624441 (3)\n      9992 distinct values\n      \n      0\n(0.0%)\n    \n    \n      LAST_NAME\n[character]\n      1. SMITH2. JOHNSON3. WILLIAMS4. MILLER5. JONES6. BROWN7. THOMPSON8. DAVIS9. JACKSON10. MOORE[ 6190 others ]\n      91(0.9%)81(0.8%)72(0.7%)63(0.6%)59(0.6%)58(0.6%)38(0.4%)35(0.4%)33(0.3%)33(0.3%)9437(94.4%)\n      \n      0\n(0.0%)\n    \n    \n      FIRST_NAME\n[character]\n      1. MICHAEL2. ROBERT3. JOHN4. DAVID5. WILLIAM6. JAMES7. MARY8. THOMAS9. CHRISTOPHER10. RICHARD[ 2495 others ]\n      160(1.6%)143(1.4%)130(1.3%)126(1.3%)125(1.2%)123(1.2%)94(0.9%)88(0.9%)78(0.8%)74(0.7%)8859(88.6%)\n      \n      0\n(0.0%)\n    \n    \n      MIDDLE_NAME\n[character]\n      1. A2. M3. L4. J5. E6. D7. R8. C9. S10. ANN[ 1186 others ]\n      713(7.8%)696(7.6%)677(7.4%)496(5.4%)322(3.5%)314(3.4%)302(3.3%)227(2.5%)220(2.4%)216(2.4%)4975(54.3%)\n      \n      842\n(8.4%)\n    \n    \n      SUFFIX\n[character]\n      1. I2. II3. III4. IV5. JR6. SR7. V\n      2(0.5%)35(8.4%)57(13.6%)8(1.9%)259(61.8%)56(13.4%)2(0.5%)\n      \n      9581\n(95.8%)\n    \n    \n      DATE_OF_BIRTH\n[Date]\n      min : 1922-07-17med : 1972-01-18max : 2004-10-25range : 82y 3m 8d\n      8207 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REGISTRATION_DATE\n[Date]\n      min : 1900-01-01med : 2015-12-31max : 2022-10-07range : 122y 9m 6d\n      4484 distinct values\n      \n      0\n(0.0%)\n    \n    \n      VOTER_STATUS\n[character]\n      1. ACTIVE2. CONFIRMATION\n      8534(85.3%)1466(14.7%)\n      \n      0\n(0.0%)\n    \n    \n      PARTY_AFFILIATION\n[character]\n      1. D2. L3. R\n      1431(45.7%)2(0.1%)1697(54.2%)\n      \n      6870\n(68.7%)\n    \n    \n      RESIDENTIAL_ADDRESS1\n[character]\n      1. 2201 ACACIA PARK DR2. 1055 OLD RIVER RD3. 11050 FANCHER RD4. 112 S CLINTON ST5. 12900 LAKE AVE6. 16700 LAKE SHORE BLVD7. 18221 EUCLID AVE8. 19101 VAN AKEN BLVD9. 19201 EUCLID AVE10. 2020 TAYLOR RD[ 9834 others ]\n      4(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)9969(99.7%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_SECONDARY_ADDR\n[character]\n      1. APT 12. UPPR3. APT 24. APT 35. LOWR6. APT 47. APT B8. APT 79. APT 20110. APT A[ 565 others ]\n      54(4.4%)43(3.5%)39(3.2%)34(2.8%)32(2.6%)27(2.2%)22(1.8%)17(1.4%)16(1.3%)16(1.3%)923(75.5%)\n      \n      8777\n(87.8%)\n    \n    \n      RESIDENTIAL_CITY\n[character]\n      1. CLEVELAND2. HAMILTON3. SPRINGFIELD4. PARMA5. MIDDLETOWN6. LIMA7. WEST CHESTER8. LAKEWOOD9. DELAWARE10. CLEVELAND HTS[ 324 others ]\n      1166(11.7%)429(4.3%)286(2.9%)244(2.4%)235(2.4%)225(2.2%)191(1.9%)181(1.8%)180(1.8%)177(1.8%)6686(66.9%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_STATE\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP\n[numeric]\n      Mean (sd) : 44459.3 (713.5)min ≤ med ≤ max:43003 ≤ 44138 ≤ 45896IQR (CV) : 949 (0)\n      338 distinct values\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP_PLUS4\n[character]\n      1. 11012. 16383. 96084. 10305. 10526. 11127. 12018. 12419. 132010. 1325[ 531 others ]\n      3(0.5%)3(0.5%)3(0.5%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)568(96.1%)\n      \n      9409\n(94.1%)\n    \n    \n      RESIDENTIAL_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      RESIDENTIAL_POSTALCODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_ADDRESS1\n[character]\n      1. PO BOX 32. PO BOX 410523. 10 CHRISTOPHER WAY APT 44. 100 ELM AV5. 10072A HORIZON ST6. 10115 VANDERBILT CIRCLE7. 1012 PROSPECT AVE APT 1028. 10919 GLENVIEW AVENUE9. 11 N FISHER DR ROOM 22110. 1121 W COLUMBIA AVE APT  [ 201 others ]\n      2(0.9%)2(0.9%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)201(94.4%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_SECONDARY_ADDRESS\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_CITY\n[character]\n      1. OXFORD2. CLEVELAND3. ANDOVER4. LIMA5. BETHESDA6. CLARKSVILLE7. DELAWARE8. FELICITY9. GENEVA10. GETTYSBURG[ 133 others ]\n      15(7.0%)8(3.8%)6(2.8%)4(1.9%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)162(76.1%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_STATE\n[character]\n      1. OH2. CA3. IL4. VA5. AE6. IN7. MA8. WA9. AL10. BC[ 4 others ]\n      189(89.2%)3(1.4%)3(1.4%)3(1.4%)2(0.9%)2(0.9%)2(0.9%)2(0.9%)1(0.5%)1(0.5%)4(1.9%)\n      \n      9788\n(97.9%)\n    \n    \n      MAILING_ZIP\n[numeric]\n      Mean (sd) : 44768.7 (10335.5)min ≤ med ≤ max:1821 ≤ 44731 ≤ 98433IQR (CV) : 1355.2 (0.2)\n      150 distinct values\n      \n      9790\n(97.9%)\n    \n    \n      MAILING_ZIP_PLUS4\n[character]\n      1. 00622. 01093. 01334. 01655. 01736. 01817. 02088. 02709. 028210. 0367[ 15 others ]\n      1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)15(60.0%)\n      \n      9975\n(99.8%)\n    \n    \n      MAILING_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_POSTAL_CODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CAREER_CENTER\n[character]\n      1. GREAT OAKS CAREER CAMPUSE2. POLARIS CAREER CENTER3. DELAWARE AREA CAREER CENT4. SPRINGFIELD-CLARK COUNTY 5. CUYAHOGA VALLEY CAREER CE6. APOLLO CAREER CENTER7. COLUMBIANA COUNTY CAREER 8. BELMONT-HARRISON CAREER C9. EHOVE CAREER CENTER10. ASHLAND COUNTY-WEST HOLME[ 21 others ]\n      691(15.9%)571(13.2%)553(12.7%)392(9.0%)356(8.2%)281(6.5%)197(4.5%)196(4.5%)162(3.7%)147(3.4%)794(18.3%)\n      \n      5660\n(56.6%)\n    \n    \n      CITY\n[character]\n      1. HAMILTON CITY2. SPRINGFIELD CITY3. MIDDLETOWN CITY4. FAIRFIELD CITY5. DELAWARE CITY6. LIMA CITY7. SANDUSKY CITY8. ASHLAND CITY9. ATHENS CITY10. OXFORD CITY[ 32 others ]\n      172(8.7%)171(8.7%)137(6.9%)124(6.3%)120(6.1%)95(4.8%)79(4.0%)67(3.4%)62(3.1%)56(2.8%)892(45.2%)\n      \n      8025\n(80.2%)\n    \n    \n      CITY_SCHOOL_DISTRICT\n[character]\n      1. CLEVELAND MUNICIPAL CITY 2. PARMA CITY SD3. FAIRFIELD CITY SD4. CLEVELAND HTS-UNIV HTS CI5. LAKEWOOD CITY SD6. HAMILTON CITY SD7. SPRINGFIELD CITY SD8. EUCLID CITY SD9. BEREA CITY SD10. STRONGSVILLE CITY SD[ 55 others ]\n      1167(18.4%)343(5.4%)218(3.4%)214(3.4%)181(2.8%)173(2.7%)166(2.6%)153(2.4%)148(2.3%)148(2.3%)3442(54.2%)\n      \n      3647\n(36.5%)\n    \n    \n      COUNTY_COURT_DISTRICT\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CONGRESSIONAL_DISTRICT\n[character]\n      1. 112. 073. 084. 045. 026. 067. 128. 099. 1010. 14[ 2 others ]\n      2522(25.2%)1564(15.6%)1358(13.6%)1274(12.7%)1006(10.1%)641(6.4%)445(4.4%)374(3.7%)294(2.9%)286(2.9%)236(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      COURT_OF_APPEALS\n[character]\n      1. 022. 033. 044. 055. 066. 077. 088. 119. 12\n      683(6.8%)725(7.2%)244(2.4%)961(9.6%)249(2.5%)641(6.4%)4086(40.9%)286(2.9%)2125(21.2%)\n      \n      0\n(0.0%)\n    \n    \n      EDU_SERVICE_CENTER_DISTRICT\n[character]\n      1. ESC OF CENTRAL OHIO2. BUTLER COUNTY ESC3. CLARK COUNTY ESC4. ALLEN COUNTY ESC5. ASHTABULA COUNTY ESC6. BROWN COUNTY ESC7. NORTH POINT ESC8. COLUMBIANA COUNTY ESC9. SOUTHERN OHIO ESC10. DARKE COUNTY ESC[ 23 others ]\n      503(18.8%)496(18.6%)226(8.5%)202(7.6%)124(4.6%)120(4.5%)120(4.5%)116(4.3%)116(4.3%)80(3.0%)569(21.3%)\n      \n      7328\n(73.3%)\n    \n    \n      EXEMPTED_VILL_SCHOOL_DISTRICT\n[character]\n      1. MILFORD EX VILL SD (CLERM2. CARROLLTON EX VILL SD (CA3. NEW RICHMOND EX VILL SD (4. COLUMBIANA EX VILL SD (CO5. BARNESVILLE EX VILL SD (B6. CHAGRIN FALLS EX VILL SD 7. VERSAILLES EX VILL SD (DA8. MECHANICSBURG EX VILL SD 9. BLUFFTON EX VILL SD (ALLE10. HICKSVILLE EX VILL SD (DE[ 7 others ]\n      176(32.8%)57(10.6%)43(8.0%)32(6.0%)28(5.2%)25(4.7%)24(4.5%)21(3.9%)20(3.7%)19(3.5%)91(17.0%)\n      \n      9464\n(94.6%)\n    \n    \n      LIBRARY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      LOCAL_SCHOOL_DISTRICT\n[character]\n      1. LAKOTA LOCAL SD (BUTLER)2. OLENTANGY LOCAL SD (DELAW3. WEST CLERMONT LOCAL SD (C4. ELIDA LOCAL SD (ALLEN)5. ROSS LOCAL SD (BUTLER)6. SHAWNEE LOCAL SD (ALLEN)7. NORTHEASTERN LOCAL SD (CL8. WESTERN BROWN LOCAL SD (B9. BUCKEYE VALLEY LOCAL SD (10. BIG WALNUT LOCAL SD (DELA[ 103 others ]\n      347(11.2%)324(10.4%)220(7.1%)68(2.2%)67(2.2%)66(2.1%)63(2.0%)60(1.9%)58(1.9%)56(1.8%)1782(57.3%)\n      \n      6889\n(68.9%)\n    \n    \n      MUNICIPAL_COURT_DISTRICT\n[character]\n      1. CLEVELAND2. DELAWARE3. PARMA4. ROCKY-RIVER5. BEREA6. LIMA7. BEDFORD8. GARFIELD-HEIGHTS9. COLUMBIANA-CO10. SHAKER-HEIGHTS[ 16 others ]\n      1175(19.1%)685(11.1%)562(9.1%)427(6.9%)364(5.9%)326(5.3%)280(4.5%)276(4.5%)260(4.2%)200(3.2%)1605(26.1%)\n      \n      3840\n(38.4%)\n    \n    \n      PRECINCT_NAME\n[character]\n      1. PRECINCT GOSHEN2. HAM2WD33. PRECINCT TWIN TWP. GORDON4. PRECINCT E LIVERPOOL 2-A5. CLEVELAND-03-I6. FAIRVIEW PARK-05-A7. NORTH ROYALTON-06-D8. PRECINCT UNION CITY VILLA9. BATAVIA TOWNSHIP A10. BAY VILLAGE-02-C[ 2373 others ]\n      16(0.2%)15(0.1%)15(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9864(98.6%)\n      \n      0\n(0.0%)\n    \n    \n      PRECINCT_CODE\n[character]\n      1. 09-P-ACR2. 19ABP3. 11AAW4. 15AAF5. 18-P-AYX6. 18-P-BSM7. 18-P-CGK8. 19ABD9. 09-P-AIT10. 09-P-AJL[ 2378 others ]\n      15(0.1%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9866(98.7%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_BOARD_OF_EDUCATION\n[character]\n      1. 012. 023. 034. 055. 076. 087. 098. 109. 11\n      959(9.6%)249(2.5%)1244(12.4%)1176(11.8%)685(6.9%)803(8.0%)406(4.1%)2119(21.2%)2359(23.6%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_REPRESENTATIVE_DISTRICT\n[numeric]\n      Mean (sd) : 48.2 (28.4)min ≤ med ≤ max:13 ≤ 47 ≤ 99IQR (CV) : 55 (0.6)\n      40 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STATE_SENATE_DISTRICT\n[character]\n      1. 212. 243. 234. 045. 146. 197. 128. 109. 1810. 33[ 8 others ]\n      1259(12.6%)1192(11.9%)1173(11.7%)1134(11.3%)889(8.9%)791(7.9%)696(7.0%)509(5.1%)462(4.6%)433(4.3%)1462(14.6%)\n      \n      0\n(0.0%)\n    \n    \n      TOWNSHIP\n[character]\n      1. WEST CHESTER TWP2. MIAMI TWP3. UNION  TWP4. LIBERTY TOWNSHIP5. ORANGE TWP6. LIBERTY TWP7. GENOA TWP8. FAIRFIELD TOWNSHIP9. PERRY TWP10. BATAVIA TWP[ 232 others ]\n      224(4.9%)164(3.6%)157(3.4%)130(2.8%)121(2.6%)116(2.5%)101(2.2%)94(2.1%)86(1.9%)84(1.8%)3295(72.1%)\n      \n      5428\n(54.3%)\n    \n    \n      VILLAGE\n[character]\n      1. MT. ORAB VILLAGE2. EAST PALESTINE VILLAGE3. CRESTLINE VILLAGE4. UNION CITY VILLAGE5. BETHEL VILLAGE6. BLUFFTON VILLAGE7. NEW RICHMOND VILLAGE8. BARNESVILLE VILLAGE9. BELLAIRE VILLAGE10. CARROLLTON VILLAGE[ 119 others ]\n      15(2.9%)14(2.7%)13(2.5%)13(2.5%)12(2.3%)12(2.3%)12(2.3%)11(2.1%)10(1.9%)10(1.9%)400(76.6%)\n      \n      9478\n(94.8%)\n    \n    \n      WARD\n[character]\n      1. DELAWARE CITY - WARD2. HAMILTON CTY WARD 13. CLEVELAND WARD 34. CLEVELAND WARD 45. CLEVELAND WARD 116. CLEVELAND WARD 97. CLEVELAND WARD 88. CLEVELAND WARD 109. CLEVELAND WARD 1510. CLEVELAND WARD 16[ 262 others ]\n      120(2.4%)91(1.9%)84(1.7%)83(1.7%)76(1.5%)75(1.5%)74(1.5%)71(1.4%)70(1.4%)68(1.4%)4095(83.5%)\n      \n      5093\n(50.9%)\n    \n    \n      PRIMARY-03/07/2000\n[character]\n      1. D2. E3. R4. X\n      598(35.9%)1(0.1%)697(41.8%)371(22.3%)\n      \n      8333\n(83.3%)\n    \n    \n      GENERAL-11/07/2000\n[character]\n      1. X\n      3328(100.0%)\n      \n      6672\n(66.7%)\n    \n    \n      SPECIAL-05/08/2001\n[character]\n      1. D2. R3. X\n      6(1.6%)44(11.9%)320(86.5%)\n      \n      9630\n(96.3%)\n    \n    \n      GENERAL-11/06/2001\n[character]\n      1. X\n      1866(100.0%)\n      \n      8134\n(81.3%)\n    \n    \n      PRIMARY-05/07/2002\n[character]\n      1. D2. R3. X\n      401(36.5%)308(28.0%)390(35.5%)\n      \n      8901\n(89.0%)\n    \n    \n      GENERAL-11/05/2002\n[character]\n      1. X\n      2454(100.0%)\n      \n      7546\n(75.5%)\n    \n    \n      SPECIAL-05/06/2003\n[character]\n      1. D2. R3. X\n      44(12.4%)38(10.7%)272(76.8%)\n      \n      9646\n(96.5%)\n    \n    \n      GENERAL-11/04/2003\n[character]\n      1. X\n      1312(100.0%)\n      \n      8688\n(86.9%)\n    \n    \n      PRIMARY-03/02/2004\n[character]\n      1. D2. R3. X\n      924(45.5%)468(23.1%)637(31.4%)\n      \n      7971\n(79.7%)\n    \n    \n      GENERAL-11/02/2004\n[character]\n      1. X\n      4737(100.0%)\n      \n      5263\n(52.6%)\n    \n    \n      SPECIAL-02/08/2005\n[character]\n      1. X\n      304(100.0%)\n      \n      9696\n(97.0%)\n    \n    \n      PRIMARY-05/03/2005\n[character]\n      1. D2. R3. X\n      19(2.0%)33(3.5%)884(94.4%)\n      \n      9064\n(90.6%)\n    \n    \n      PRIMARY-09/13/2005\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2005\n[character]\n      1. X\n      2617(100.0%)\n      \n      7383\n(73.8%)\n    \n    \n      SPECIAL-02/07/2006\n[character]\n      1. X\n      71(100.0%)\n      \n      9929\n(99.3%)\n    \n    \n      PRIMARY-05/02/2006\n[character]\n      1. D2. R3. X\n      707(44.4%)572(35.9%)314(19.7%)\n      \n      8407\n(84.1%)\n    \n    \n      GENERAL-11/07/2006\n[character]\n      1. R2. X\n      2(0.1%)3559(99.9%)\n      \n      6439\n(64.4%)\n    \n    \n      PRIMARY-05/08/2007\n[character]\n      1. D2. R3. X\n      71(16.4%)73(16.8%)290(66.8%)\n      \n      9566\n(95.7%)\n    \n    \n      PRIMARY-09/11/2007\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/06/2007\n[character]\n      1. X\n      1346(100.0%)\n      \n      8654\n(86.5%)\n    \n    \n      PRIMARY-11/06/2007\n[character]\n      1. D2. R3. X\n      13(19.1%)19(27.9%)36(52.9%)\n      \n      9932\n(99.3%)\n    \n    \n      GENERAL-12/11/2007\n[character]\n      1. X\n      53(100.0%)\n      \n      9947\n(99.5%)\n    \n    \n      PRIMARY-03/04/2008\n[character]\n      1. D2. R3. X\n      2073(64.4%)965(30.0%)182(5.7%)\n      \n      6780\n(67.8%)\n    \n    \n      PRIMARY-10/14/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/04/2008\n[character]\n      1. X\n      5352(100.0%)\n      \n      4648\n(46.5%)\n    \n    \n      GENERAL-11/18/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-05/05/2009\n[character]\n      1. D2. R3. X\n      41(7.3%)5(0.9%)517(91.8%)\n      \n      9437\n(94.4%)\n    \n    \n      PRIMARY-09/08/2009\n[character]\n      1. D2. R3. X\n      5(3.9%)2(1.6%)120(94.5%)\n      \n      9873\n(98.7%)\n    \n    \n      PRIMARY-09/15/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/29/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2009\n[character]\n      1. D2. X\n      1(0.0%)3175(100.0%)\n      \n      6824\n(68.2%)\n    \n    \n      PRIMARY-05/04/2010\n[character]\n      1. C2. D3. G4. L5. R6. X\n      2(0.1%)756(43.3%)1(0.1%)3(0.2%)812(46.5%)172(9.9%)\n      \n      8254\n(82.5%)\n    \n    \n      PRIMARY-07/13/2010\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/07/2010\n[character]\n      1. D2. R3. X\n      319(65.5%)153(31.4%)15(3.1%)\n      \n      9513\n(95.1%)\n    \n    \n      GENERAL-11/02/2010\n[character]\n      1. X\n      3868(100.0%)\n      \n      6132\n(61.3%)\n    \n    \n      PRIMARY-05/03/2011\n[character]\n      1. D2. R3. X\n      89(12.9%)61(8.9%)539(78.2%)\n      \n      9311\n(93.1%)\n    \n    \n      PRIMARY-09/13/2011\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2011\n[character]\n      1. X\n      3639(100.0%)\n      \n      6361\n(63.6%)\n    \n    \n      PRIMARY-03/06/2012\n[character]\n      1. D2. G3. L4. R5. X\n      790(38.3%)1(0.0%)4(0.2%)1195(57.9%)73(3.5%)\n      \n      7937\n(79.4%)\n    \n    \n      GENERAL-11/06/2012\n[character]\n      1. X\n      5794(100.0%)\n      \n      4206\n(42.1%)\n    \n    \n      PRIMARY-05/07/2013\n[character]\n      1. D2. R3. X\n      24(5.2%)15(3.2%)423(91.6%)\n      \n      9538\n(95.4%)\n    \n    \n      PRIMARY-09/10/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-10/01/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/05/2013\n[character]\n      1. X\n      2347(100.0%)\n      \n      7653\n(76.5%)\n    \n    \n      PRIMARY-05/06/2014\n[character]\n      1. D2. G3. L4. R5. X\n      668(43.8%)2(0.1%)9(0.6%)703(46.1%)144(9.4%)\n      \n      8474\n(84.7%)\n    \n    \n      GENERAL-11/04/2014\n[character]\n      1. X\n      3433(100.0%)\n      \n      6567\n(65.7%)\n    \n    \n      PRIMARY-05/05/2015\n[character]\n      1. D2. R3. X\n      15(4.4%)43(12.6%)282(82.9%)\n      \n      9660\n(96.6%)\n    \n    \n      PRIMARY-09/15/2015\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2015\n[character]\n      1. X\n      3543(100.0%)\n      \n      6457\n(64.6%)\n    \n    \n      PRIMARY-03/15/2016\n[character]\n      1. D2. G3. R4. X\n      1497(40.0%)2(0.1%)2199(58.7%)46(1.2%)\n      \n      6256\n(62.6%)\n    \n    \n      GENERAL-06/07/2016\n[character]\n      1. X\n      80(100.0%)\n      \n      9920\n(99.2%)\n    \n    \n      PRIMARY-09/13/2016\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2016\n[character]\n      1. X\n      6412(100.0%)\n      \n      3588\n(35.9%)\n    \n    \n      PRIMARY-05/02/2017\n[character]\n      1. D2. R3. X\n      1(0.2%)13(3.2%)388(96.5%)\n      \n      9598\n(96.0%)\n    \n    \n      PRIMARY-09/12/2017\n[character]\n      1. D2. X\n      4(2.4%)163(97.6%)\n      \n      9833\n(98.3%)\n    \n    \n      GENERAL-11/07/2017\n[character]\n      1. X\n      2874(100.0%)\n      \n      7126\n(71.3%)\n    \n    \n      PRIMARY-05/08/2018\n[character]\n      1. D2. G3. R4. X\n      954(46.1%)6(0.3%)1006(48.6%)105(5.1%)\n      \n      7929\n(79.3%)\n    \n    \n      GENERAL-08/07/2018\n[character]\n      1. X\n      246(100.0%)\n      \n      9754\n(97.5%)\n    \n    \n      GENERAL-11/06/2018\n[character]\n      1. X\n      5287(100.0%)\n      \n      4713\n(47.1%)\n    \n    \n      PRIMARY-05/07/2019\n[character]\n      1. D2. R3. X\n      12(2.4%)105(21.0%)383(76.6%)\n      \n      9500\n(95.0%)\n    \n    \n      PRIMARY-09/10/2019\n[character]\n      1. D2. X\n      7(35.0%)13(65.0%)\n      \n      9980\n(99.8%)\n    \n    \n      GENERAL-11/05/2019\n[character]\n      1. X\n      2364(100.0%)\n      \n      7636\n(76.4%)\n    \n    \n      PRIMARY-03/17/2020\n[character]\n      1. D2. L3. R4. X\n      1152(52.2%)2(0.1%)937(42.5%)116(5.3%)\n      \n      7793\n(77.9%)\n    \n    \n      GENERAL-11/03/2020\n[character]\n      1. X\n      7230(100.0%)\n      \n      2770\n(27.7%)\n    \n    \n      PRIMARY-05/04/2021\n[character]\n      1. D2. R3. X\n      2(0.6%)25(7.9%)289(91.5%)\n      \n      9684\n(96.8%)\n    \n    \n      PRIMARY-08/03/2021\n[character]\n      1. D2. R\n      355(91.3%)34(8.7%)\n      \n      9611\n(96.1%)\n    \n    \n      PRIMARY-09/14/2021\n[character]\n      1. D2. X\n      7(2.4%)285(97.6%)\n      \n      9708\n(97.1%)\n    \n    \n      GENERAL-11/02/2021\n[character]\n      1. X\n      2546(100.0%)\n      \n      7454\n(74.5%)\n    \n    \n      PRIMARY-05/03/2022\n[character]\n      1. D2. R3. X\n      732(35.8%)1291(63.1%)23(1.1%)\n      \n      7954\n(79.5%)\n    \n    \n      PRIMARY-08/02/2022\n[character]\n      1. D2. R3. X\n      474(47.3%)525(52.4%)3(0.3%)\n      \n      8998\n(90.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.2)2022-11-12\n\n\n\nThis sample of 10k is 46.3% and 53.7% Republican.\nOhio actually has open primaries which is a good reason to consider another state for this dataset. I’ll be looking for others."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#next-steps",
    "href": "posts/Final_SteveONeill.html#next-steps",
    "title": "Final Part 1",
    "section": "Next Steps",
    "text": "Next Steps\nNext, I will find the most advantageous state with a closed primary. Ideally, it will be a battleground or solid-Republican state with freely available voter registration data and a downloadable Corporation Search database (or available upon request).\nI look forward to any feedback or refinements to the hypotheses above."
  },
  {
    "objectID": "posts/KarenDetter_FinalPt2.html",
    "href": "posts/KarenDetter_FinalPt2.html",
    "title": "Final Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(crosstable)\nlibrary(MASS)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_FinalPt2.html#what-predicts-opinions-on-government-regulation-of-big-tech",
    "href": "posts/KarenDetter_FinalPt2.html#what-predicts-opinions-on-government-regulation-of-big-tech",
    "title": "Final Project",
    "section": "What predicts opinions on government regulation of ‘Big Tech’?",
    "text": "What predicts opinions on government regulation of ‘Big Tech’?\nIn 2001, Google piloted a program to boost profits, which were sinking as the “dot-com bubble” burst, by collecting data generated from users’ search queries and using it to sell precisely targeted advertising. The company’s ad revenues grew so quickly that they expanded their data collection tools with tracking “cookies” and predictive algorithms. Other technology firms took notice of Google’s soaring profits, and the sale of passively-collected data from people’s online activities soon became the predominant business model of the internet economy (Zuboff, 2015).\nAs the data-collection practices of “Big Tech” firms, including Google, Amazon, Facebook (Meta), Apple, and Microsoft, have gradually been exposed, the public is now aware that the “free” platforms that have become essential to daily life are actually harvesting personal information as payment. Despite consumers being essentially extorted into accepting this arrangement, regulatory intervention into “surveillance capitalism” has remained limited.\nOver the two decades since passive data collection began commercializing the internet, survey research has shown the American public’s increasing concern over the dominance Big Tech has been allowed to exert. A 2019 study conducted by Pew Research Center found that 81% of Democrats and 70% of Republicans think there should be more government regulation of corporate data-use practices (Pew Research Center, 2019). It is very unusual to find majorities of both Republicans and Democrats agreeing on any policy position, since party affiliation is known to be a main predictor of any political stance, especially in the current polarized climate. The natural question that arises, then, is what other factors might predict support for increased regulation of data-collection practices?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html",
    "href": "posts/Buck_Yoon_finalpart1.html",
    "title": "finalpart1",
    "section": "",
    "text": "we are going to be using the National Longitudinal Study of Adolescent to Adult Health, 1994-2018 we are interested in exploring the relation between education levels and health.\n#Some of our research questions are:\nWhat is the correlation and relationship between someone’s education and health? Does the type and duration of education matter? Are there fields that may be “more healthy”? How does the relationship between education and health differ among the education levels/ is there a difference?\nWhat does this data set have to say to a possible causal link between education and health? Does the data set provide apt data to establish a causal link?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "href": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "title": "finalpart1",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are going to be using the hypothesis from researchers Eric R. Ride and Mark H. Showalter, but using the data from the National Longitudinal Study\nThere hypothesis was: ’The empirical link between education and health is firmly established. Numerous studies document that higher levels of education are positively associated with longer life and better health throughout the lifespan…But measuring the causal links between education and health is a more challenging task.” Estimating the relation between health and education: what do we know and what do we need to know?\nWe are hypothesizing that a positive correlation exists between education and health; the more education an individual receives, the better health the individual may have.\nWe want to look at the National Longitudinal Study of Adolescent to Adult Health 1992-2018 and observe what other factors beyond education there is that can affect the correlation to health. What are the potential moderating or mediating variables?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "href": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "title": "finalpart1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis is an overview of the entire data set we are still determining which specific sections we want to analyze for our final project.\nAccording to ICPSR:\nStudy Purpose: Add Health was developed in response to a mandate from the U.S. Congress to fund a study of adolescent health. Waves I and II focused on the forces that may influence adolescents’ health and risk behaviors, including personal traits, families, friendships, romantic relationships, peer groups, schools, neighborhoods, and communities. As participants aged into adulthood, the scientific goals of the study expanded and evolved. Wave III explored adolescent experiences and behaviors related to decisions, behavior, and health outcomes in the transition to adulthood. Wave IV expanded to examine developmental and health trajectories across the life course of adolescence into young adulthood, using an integrative study design which combined social, behavioral, and biomedical measures data collection. Wave V aimed to track the emergence of chronic disease as the cohort aged into their 30s and early 40s.\nStudy Design: Add health is a school-based longitudinal study of a nationally-representative sample of adolescents in grates 7-12 in the United States in 1945-45. Over more than 20 years of data collection, data have been collected from adolescents, their fellow students, school administrators, parents, siblings, friends, and romantic partners through multiple data collection components. In addition, existing databases with information about respondents’ neighborhoods and communities have been merged with Add Health data, including variables on income poverty, unemployment, availability and utilization of health services, crime, church membership, and social programs and policies.\nSample:\n\nWave I: The Stage 1 in-school sample was a stratified, random sample of all high schools in the United States. A school was eligible for the sample if it included an 11th grade and had a minimum enrollment of 30 students. A feeder school – a school that sent graduates to the high school and that included a 7th grade – was also recruited from the community. The in-school questionnaire was administered to more than 90,000 students in grades 7 through 12. The Stage 2 in-home sample of 27,000 adolescents consisted of a core sample from each community, plus selected special over samples. Eligibility for over samples was determined by an adolescent’s responses on the in-school questionnaire. Adolescents could qualify for more than one sample.\nWave II: The Wave II in-home interview surveyed almost 15,000 of the same students one year after Wave I.\nWave III: The in-home Wave III sample consists of over 15,000 Wave I respondents who could be located and re-interviewed six years later.\nWave IV: All original Wave I in-home respondents were eligible for in-home interviews at Wave IV. At Wave IV, the Add Health sample was dispersed across the nation with respondents living in all 50 states. Administrators were able to locate 92.5% of the Wave IV sample and interviewed 80.3% of eligible sample members.\nWave V: All Wave I respondents who were still living were eligible at Wave V, yielding a pool of 19,828 persons. This pool was split into three stratified random samples for the purposes of survey design testing.\nTime Method: Longitudinal:Panel\nUniverse: Adolescents in grades 7 through 12 during the 1994-1995 school year. Respondents were geographically located in the United States.\nUnits of Observation: Individual\nData Types: Survey Data\nTime periods: 1994 - 2018\nDate of Collections: Wave 1(1994-01 - 1995-12), Wave II(1996-04 - 1996-09), Wave III(2001-04 - 2002 -04), Wave IV(2007-04 - 2009-01), Wave V(2016-03 - 2018-11)\nResponse Rates: Wave 1(79%), Wave 2(88.6%), Wave III(77.4%), Wave IV(80.3%), Wave V(71.8%)."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html",
    "href": "posts/HW1_Solutions_OmerYalcin.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males.\n\n\n\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity.\n\n\n\n\nLess than or equal to 13\n\n\n\nCode\ndf %>%\n  filter(Age <= 13) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %>%\n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %>%\n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %>%\n  filter(Age >= 18) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       11.1\n2 yes      10.5\n\n\n\n\n\nFor three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is.\n\n\n\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nBoth the correlation and the covariance are positive (when one of them is the other has to). Positive values suggest that people who are older tend to have higher lung capacity, confirming what we found. Since correlation is standardized (needs to be between -1 and 1), its absolute value tells us about the strength of the relationship. 0.82 suggests a pretty strong relationship."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\n\n\nCode\ntb %>%\n  filter(X == 2) %>%\n  pull(Frequency) %>%\n  divide_by(n)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\n\n\nCode\ntb %>%\n  filter(X < 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\ntb %>%\n  filter(X <= 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\n\n\nCode\ntb %>%\n  filter(X > 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nExpected number of prior convictions is just a weighted average of the number of prior convictions.\n\nMethod 1: Multiply every value with their frequency, then divide by total frequency i.e. (0 * 128 + 1 * 434 + 2 * 160 ……) / 810.\n\n\n\nCode\nsum(tb$X * tb$Frequency) / n\n\n\n[1] 1.28642\n\n\n\nMethod 2: Multiply every value with their probility, sum them up.\n\n\n\nCode\ntb %>%\n  mutate(probability = Frequency / n) -> tb\n\nprint(tb)\n\n\n# A tibble: 5 × 3\n      X Frequency probability\n  <dbl>     <dbl>       <dbl>\n1     0       128      0.158 \n2     1       434      0.536 \n3     2       160      0.198 \n4     3        64      0.0790\n5     4        24      0.0296\n\n\n\n\nCode\nsum(tb$X * tb$probability)\n\n\n[1] 1.28642\n\n\n\nMethod 3: Recreate the whole sample (a vector that has 128 zeroes, 434 ones, 160 twos, ….) with a total length/size of 810. Take the mean.\n\n\n\nCode\nsample <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\nmean(sample)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\nMethod 1: Let’s start from the end: we have the sample, just call var() and sd()\n\n\n\nCode\ncat('Variance:', var(sample))\n\n\nVariance: 0.8572937\n\n\nCode\ncat('\\nStandard Deviation:', sd(sample))\n\n\n\nStandard Deviation: 0.9259016\n\n\nMethod 2: Manually apply the formula using weights.\nStandard deviation is square root of variance. So let’s calculate variance first. For that we need the mean. Let’s pull the expected value from the previous section:\n\n\nCode\nm <- sum(tb$X * tb$Frequency) / n\n\n\nFor every observation, we’ll need the squared difference from mean (squared deviation from mean).\n\n\nCode\ntb %>%\n  mutate(sq_deviation = (X - m)^2) -> tb \nprint(tb)\n\n\n# A tibble: 5 × 4\n      X Frequency probability sq_deviation\n  <dbl>     <dbl>       <dbl>        <dbl>\n1     0       128      0.158        1.65  \n2     1       434      0.536        0.0820\n3     2       160      0.198        0.509 \n4     3        64      0.0790       2.94  \n5     4        24      0.0296       7.36  \n\n\nThen, we can now multiply them with probability.\n\n\nCode\nsum(tb$sq_deviation * tb$probability)\n\n\n[1] 0.8562353\n\n\nThis gives us the ‘population’ variance. If we wanted the ‘sample’ variance, what the var() function does, we could manually apply the Bessel’s correction:\n\n\nCode\nvariance <- sum(tb$sq_deviation * tb$probability) * (n / (n-1))\nprint(variance)\n\n\n[1] 0.8572937\n\n\nStandard deviation is then just the square root:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9259016\n\n\nThis replicated what we found directly using the sample."
  },
  {
    "objectID": "posts/Project_Yakub Rabiutheen.html",
    "href": "posts/Project_Yakub Rabiutheen.html",
    "title": "Project Rough Draft Proposal",
    "section": "",
    "text": "Hypopthesis\nThis research project will be testing two hypothesis regarding Britain and France.\n#Colonial Powers Hypopthesis. 1. The Years that France and Britain had more Exports is when the rate of colonization increased. 2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\nCode\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\n\n\nError: `path` does not exist: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls'\n\n\nCode\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv' does not exist.\n\n\nCode\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv' does not exist.\n\n\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\n\n\nCode\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\n\n\nError in colnames(Colonial_Years)[3] <- \"Colonizing Country\": object 'Colonial_Years' not found\n\n\nCode\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\n\n\nError in colnames(Colonial_Years)[4] <- \"Year_Colonization_Began\": object 'Colonial_Years' not found\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\n\n\nError in select(Colonial_Years, \"Colonizing Country\", \"Year_Colonization_Began\"): object 'Colonial_Years' not found\n\n\nCode\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\n\n\nError in filter(Colonial_Years, `Colonizing Country` %in% c(\"F\", \"UK\")): object 'Colonial_Years' not found\n\n\nCode\ntable(Colonial_Years)\n\n\nError in table(Colonial_Years): object 'Colonial_Years' not found\n\n\n\n\nCode\nImports_Exports%>% filter(year < '1960') \n\n\nError in filter(., year < \"1960\"): object 'Imports_Exports' not found\n\n\n\n\nCode\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\n\n\nError in filter(Imports_Exports, stateabb %in% c(\"FRN\", \"UKG\")): object 'Imports_Exports' not found\n\n\n\n\nCode\noptions(scipen = 999)    \n\n\nCreated a Forumula to calculate Trade Surplus and Deficits.\n\n\nCode\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\n\n\nError in eval(expr, envir, enclos): object 'colonial_trade' not found\n\n\nFound a better way to find years that France and Britain were running Trade Deficits.\n\n\nCode\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports < colonial_trade$imports, : object 'colonial_trade' not found\n\n\nI did the inverse to find that the UK has always had a Trade Deficit\n\n\nCode\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports > colonial_trade$imports, : object 'colonial_trade' not found\n\n\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-sidebar-title\"}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-navbar-title\"}\n[Fall 2022 Posts]{.hidden render-id=\"quarto-int-navbar:Fall 2022 Posts\"}\n[Contributors]{.hidden render-id=\"quarto-int-navbar:Contributors\"}\n[DACSS]{.hidden render-id=\"quarto-int-navbar:DACSS\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-metatitle\"}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-twittercardtitle\"}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-ogcardtitle\"}\n[International Trade's influence on War]{.hidden render-id=\"quarto-metadesc\"}\n:::\n\n\n\n\n<!-- -->\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Project Rough Draft Proposal\"\nauthor: \"Yakub Rabiutheen\"\ndescription: \"International Trade's influence on War\"\ndate: \"10/11/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - finalproject1\n  - desriptive statistics \n  - probability\n---\n\n# Research Question\n\nHow has international trade influenced how countries interact with each other? This research project looks specifically at  France and Britain which  are grouped together as Colonial Powers to explore the relationship of Colonialism and international trade. This research project will be looking at data from the Correlates Of War Project, which has international trade data from 1870 to 2015. The cut-off year for this research project will be 1960, as  on December 14,1960, the UN declared Colonialism was a human's right's violation and legally declared Colonialism was over(McWhinney,1960).   \n\n\n# Hypopthesis\n\nThis research project will be testing two hypothesis regarding Britain and France.\n\n#Colonial Powers Hypopthesis.\n1. The Years that France and Britain had more Exports is when the rate of colonization increased.\n2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\n\nquarto-executable-code-5450563D\n\n```r\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\nquarto-executable-code-5450563D\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\nquarto-executable-code-5450563D\nlibrary(dplyr)\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\ntable(Colonial_Years)\nquarto-executable-code-5450563D\nImports_Exports%>% filter(year < '1960') \nquarto-executable-code-5450563D\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\nquarto-executable-code-5450563D\noptions(scipen = 999)    \nCreated a Forumula to calculate Trade Surplus and Deficits.\nquarto-executable-code-5450563D\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\nFound a better way to find years that France and Britain were running Trade Deficits. quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\nI did the inverse to find that the UK has always had a Trade Deficit quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/HW3answers-DonnySnyder.html",
    "href": "posts/HW3answers-DonnySnyder.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 1\n\n\nCode\ndata <- UN11\n\nggplot(data, aes(x = ppgdp, y = fertility)) + geom_point()\n\n\n\n\n\nCode\nggplot(data, aes(x = log(ppgdp), y = log(fertility))) + geom_point()\n\n\n\n\n\n#Question 1.1 The predictor is ppgdp and the response is fertility.\n#Question 1.2 A straight-line mean function does not seem to be plausible for this graph.\n#Question 1.3 A simple linear regression model does seem plausible for a summary of the log log graph.\n\n\nCode\ndata$ppgdp2 <- data$ppgdp*0.75\n\nmodel1 <- lm(fertility ~ ppgdp, data)\nsummary(model1)\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nmodel2 <- lm(fertility ~ ppgdp2, data)\nsummary(model2)\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp2      -4.268e-05  6.206e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\ncor(data$fertility, data$ppgdp)\n\n\n[1] -0.4399891\n\n\nCode\ncor(data$fertility, data$ppgdp2)\n\n\n[1] -0.4399891\n\n\n\n\nQuestion 2(a)\nThe slope of the prediction equation will increase, as the units of the explanatory variable are decreased.\n\n\nQuestion 2(b)\nThe correlation will stay the same.\n\n\nQuestion 3\n\n\nCode\nwatData <- water\npairs(watData)\n\n\n\n\n\nCode\nggpairs(watData)\n\n\nError in ggpairs(watData): could not find function \"ggpairs\"\n\n\nOPBPC, OPRC, and OPSLAKE all seem to be highly correlated with BSAAM.\n\n\nQuestion 4\n\n\nCode\nprofData <- Rateprof\nprofData <- data.frame(profData$quality, profData$helpfulness, profData$clarity, profData$easiness, profData$raterInterest)\npairs(profData)\n\n\n\n\n\nIt seems as if quality, helpfulness and clarity are all highly interrelated. easiness and raterInterest are not as highly correlated.\n#Question 5\n\n\nCode\nstud <- as.data.frame(student.survey)\n\n\nError in as.data.frame(student.survey): object 'student.survey' not found\n\n\nCode\nstud$piNum <- NA\n\n\nError in stud$piNum <- NA: object 'stud' not found\n\n\nCode\nstud$reNum <- NA\n\n\nError in stud$reNum <- NA: object 'stud' not found\n\n\nCode\nx = 1\nwhile(x <= 60){\n  if(stud$pi[x] == \"very liberal\"){\n    stud$piNum[x] = -3\n  }\n  if(stud$pi[x] == \"liberal\"){\n    stud$piNum[x] = -2\n  }\n  if(stud$pi[x] == \"slightly liberal\"){\n    stud$piNum[x] = -1\n  }\n  if(stud$pi[x] == \"moderate\"){\n    stud$piNum[x] = 0\n  }\n  if(stud$pi[x] == \"very conservative\"){\n    stud$piNum[x] = 3\n  }\n  if(stud$pi[x] == \"conservative\"){\n    stud$piNum[x] = 2\n  }\n  if(stud$pi[x] == \"slightly liberal\"){\n    stud$piNum[x] = 1\n  }\n  \n  \n  if(stud$re[x] == \"never\"){\n    stud$reNum[x] = 0\n  }\n  if(stud$re[x] == \"occasionally\"){\n    stud$reNum[x] = 1\n  }\n  if(stud$re[x] == \"most weeks\"){\n    stud$reNum[x] = 2\n  }\n  if(stud$re[x] == \"every week\"){\n    stud$reNum[x] = 3\n  }\n  x = x + 1\n}\n\n\nError in eval(expr, envir, enclos): object 'stud' not found\n\n\nCode\nmodel3 <- lm(piNum~reNum, stud)\n\n\nError in is.data.frame(data): object 'stud' not found\n\n\nCode\nsummary(model3)\n\n\nError in summary(model3): object 'model3' not found\n\n\nCode\nmodel4 <- lm(hi~tv, stud)\n\n\nError in is.data.frame(data): object 'stud' not found\n\n\nCode\nsummary(model4)\n\n\nError in summary(model4): object 'model4' not found\n\n\nCode\nggplot(stud, aes(x = reNum, y = piNum)) + geom_jitter()\n\n\nError in ggplot(stud, aes(x = reNum, y = piNum)): object 'stud' not found\n\n\nCode\nggplot(stud, aes(x = tv, y = hi)) + geom_jitter()\n\n\nError in ggplot(stud, aes(x = tv, y = hi)): object 'stud' not found\n\n\nIt seems like the results are that political ideology tends to be more right-leaning as religiosity increases. As hours of tv watching tends to go down, high school GPA tends to go up. These relationships are both statistically significant."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html",
    "href": "posts/KarenDetter_HW2.html",
    "title": "HW 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.1",
    "href": "posts/KarenDetter_HW2.html#q.1",
    "title": "HW 2",
    "section": "Q.1",
    "text": "Q.1\n- Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures:\nBypass -\n\n\nCode\n#assign values\nBs_sd <- 10\nBs_size <- 539\nBs_mean <- 19\n#calculate standard error\nBstandard_error <- Bs_sd / sqrt(Bs_size)\nBstandard_error\n\n\n[1] 0.4307305\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nBtail_area <- (1-confidence_level)/2\nBtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nBt_score <- qt(p = 1-Btail_area, df = Bs_size-1)\nBt_score\n\n\n[1] 1.647691\n\n\n\n\nCode\n#calculate confidence interval\nBCI <- c(Bs_mean - Bt_score * Bstandard_error, Bs_mean + Bt_score * Bstandard_error)\nprint(BCI)\n\n\n[1] 18.29029 19.70971\n\n\nAngiography-\n\n\nCode\n#assign values\nAs_sd <- 9\nAs_size <- 847\nAs_mean <- 18\n#calculate standard error\nAstandard_error <- As_sd / sqrt(As_size)\nAstandard_error\n\n\n[1] 0.3092437\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nAtail_area <- (1-confidence_level)/2\nAtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nAt_score <- qt(p = 1-Atail_area, df = As_size-1)\nAt_score\n\n\n[1] 1.646657\n\n\n\n\nCode\n#calculate confidence interval\nACI <- c(As_mean - At_score * Astandard_error, As_mean + At_score * Astandard_error)\nprint(ACI)\n\n\n[1] 17.49078 18.50922\n\n\n- Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#calculate differences in upper and lower bounds of both confidence intervals\n(Bs_mean + Bt_score * Bstandard_error) - (Bs_mean - Bt_score * Bstandard_error)\n\n\n[1] 1.419421\n\n\nCode\n(As_mean + At_score * Astandard_error) - (As_mean - At_score * Astandard_error)\n\n\n[1] 1.018436\n\n\nAngiography has a narrower confidence interval."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.2",
    "href": "posts/KarenDetter_HW2.html#q.2",
    "title": "HW 2",
    "section": "Q.2",
    "text": "Q.2\n- Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n#assign values\nk <- 567\nn <- 1031\n#calculate sample proportion\np <- k/n\np\n\n\n[1] 0.5499515\n\n\n- Construct and interpret a 95% confidence interval for p\n\n\nCode\n#calculate margin of error\nmargin <- qnorm(0.975) * sqrt(p*(1-p)/n)\n#calculate lower and upper bounds of confidence interval\nlow <- p - margin\nhigh <- p + margin\nprint(low)\n\n\n[1] 0.5195839\n\n\nCode\nprint(high)\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the population proportion is [.52, .58]. Since 95% of confidence intervals calculated from point estimates of population proportions would contain the true mean population proportion, we can be reasonably confident that the true mean proportion of adult Americans who believe a college education is essential for success lies somewhere between 52 and 58%."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.3",
    "href": "posts/KarenDetter_HW2.html#q.3",
    "title": "HW 2",
    "section": "Q.3",
    "text": "Q.3\n- Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#assign values\nz_score <- qnorm(.975) #assuming normal distribution and 95% confidence level\nmargin_error <- 5 #half of confidence interval\n#calculate population standard deviation (one quarter of the range)\npop_sd <- (200-30) / 4\n\n\n\n\nCode\n#calculate sampling size of population mean\nsamp_size <- z_score^2 * pop_sd^2 / margin_error^2\nsamp_size\n\n\n[1] 277.5454\n\n\nThe sample size should be 278."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.4",
    "href": "posts/KarenDetter_HW2.html#q.4",
    "title": "HW 2",
    "section": "Q.4",
    "text": "Q.4\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nassumptions: random sampling, normally distributed data, adequate sample size; hypotheses: \\(H_{0}\\) : \\(\\bar{y}\\) = \\(\\mu\\) ; \\(H_{\\alpha}\\) : \\(\\bar{y}\\) \\(\\neq\\) \\(\\mu\\) ; test statistic: t-statistic\n\n\nCode\n#calculate t-statistic\nt_stat <- (410 - 500) / (90 / (sqrt(9)))\n#calculate two-tailed p-value\np_val <- 2 * (pt(q = t_stat, df=8))\np_val\n\n\n[1] 0.01707168\n\n\nAssuming \\(\\alpha\\) = .05, we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\).\nB. Report the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) < 500. Interpret.\n\n\nCode\n#calculate lower-tail p-value\np_low <- pt(t_stat, df = 8, lower.tail = TRUE)\np_low\n\n\n[1] 0.008535841\n\n\nThis p-value is significantly lower than the .05 significance level, which means that we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\) : \\(\\mu\\) < 500.\nC. Report and interpret the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) > 500.\n\n\nCode\n#calculate lower-tail p-value\np_high <- pt(t_stat, df = 8, lower.tail = FALSE)\np_high\n\n\n[1] 0.9914642\n\n\n\n\nCode\n#double-check p-values\ncheck <- p_high + p_low\ncheck\n\n\n[1] 1\n\n\nThis p-value is significantly higher than the .05 significance level, so in this case we fail to reject \\(H_{0}\\) in favor of \\(H_{\\alpha}\\) : \\(\\mu\\) > 500."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.5",
    "href": "posts/KarenDetter_HW2.html#q.5",
    "title": "HW 2",
    "section": "Q.5",
    "text": "Q.5\nA. Show that t = 1.95 and P-value = 0.051 for Jones Show that t = 1.97 and P-value = 0.049 for Smith\n\n\nCode\n#calculate t-statistics\nJones_t <- (519.5 - 500) / 10\nJones_t\n\n\n[1] 1.95\n\n\nCode\nSmith_t <- (519.7 - 500) / 10\nSmith_t\n\n\n[1] 1.97\n\n\n\n\nCode\n#calculate p-values\nJones_p <- 2 * (pt(q = Jones_t, df=999, lower.tail = FALSE))\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p <- 2 * (pt(q = Smith_t, df=999, lower.tail = FALSE))\nSmith_p\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAt this significance level, Smith’s study would be considered significant and allow for rejection of the null hypothesis. Jones’ study, however, would fail to reject the null.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05”, or as “reject H0” versus “Do not reject H0”, without reporting the actual P-value.\nThis example shows the importance of being specific and thorough in reporting the “significance” of study findings. Both Smith and Jones produced results very near the cutoff point for statistical significance, so it would be critical to know both the actual p-value AND the exact standard, \\(\\leq\\) or <, being used to interpret the results in order to assess the actual impact of the findings. Reporting only “reject” or “do not reject” the null hypothesis would also not provide the information needed to make a judgment of the meaning of the findings, as it would not provide any evidence in support of the claim."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.6",
    "href": "posts/KarenDetter_HW2.html#q.6",
    "title": "HW 2",
    "section": "Q.6",
    "text": "Q.6\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\n#assign values\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n#run one sample t-test\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nAt the 95% confidence level, the p-value of \\(H_{\\alpha}\\) : \\(\\mu\\) < 45 is .04, indicating that we can reject \\(H_{0}\\). Additionally, 45 is above the upper bound of the confidence interval, which also supports the alternative hypothesis."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html",
    "href": "posts/HW3_StephRoberts.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(alr4)\nlibrary(smss)\nlibrary(fastDummies)\n\n\nError in library(fastDummies): there is no package called 'fastDummies'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-1",
    "href": "posts/HW3_StephRoberts.html#question-1",
    "title": "HW3",
    "section": "Question 1",
    "text": "Question 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nattach(UN11)\nun <- UN11\nhead(un)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\n\nCode\n#Check correlation\ncor.test(UN11$ppgdp,UN11$fertility)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  UN11$ppgdp and UN11$fertility\nt = -6.877, df = 197, p-value = 7.903e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5456842 -0.3205140\nsample estimates:\n       cor \n-0.4399891 \n\n\nThere is a weak, negative correlation between fertility and ppGDP.\n\n\nCode\n#Check matrix plot\npairs(UN11)\n\n\n\n\n\nThe relationship of per person GDP to fertility looks somewhat curvy, which indicates diminishing returns on fertility from increasing GDP.\n\n\nCode\n#Linear regression\nsummary(lm(fertility ~ ppgdp, data = UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe R squared in our linear model is very low 0.19, suggesting a non-linear relationship.\nLet’s have a closer look of the dependence of fertility on ppGDP\n\n\nCode\n#Plot variables\nplot( x= UN11$ppgdp, y= UN11$fertility)\n\n\n\n\n\nThis graph is a nice visual representation of our negative correlation, because it shows as ppGPD rises, there are fewer and fewer births per 1000 females\nBut the relationship does not appear linear.\n\n\nCode\n#Plot variables with linear regrssion\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAs we could have predicted, a linear explanation does not exist here. There are very large residuals and the predicted value of fertility after about 60,000 ppgdp enter negative values. Since we can’t have negative births and we do have ppgdp values over 60,000, we should explore a better model to explain this relationship.\n\n\nCode\n#Plot regression of variable logarithms\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n    geom_point(color=2) + \n    geom_smooth(method = \"lm\") +\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 women\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA linear regression for the logarithms of each variable appears to be much more appropriate."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-2",
    "href": "posts/HW3_StephRoberts.html#question-2",
    "title": "HW3",
    "section": "Question 2",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\n\nIf all explanatory values are converted (multiplied) to another number - British pounds - the slope of the prediction equation will change. Since the rate is x 1.33, multiplying by a positive number should make the slope steeper.\n\nHow, if at all, does the correlation change?\n\nThe correlation would not change if only the units of measurement are different. The values in relation to the predictor variable would remain the same."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-3",
    "href": "posts/HW3_StephRoberts.html#question-3",
    "title": "HW3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\nCode\n#Load data\ndata(\"water\")\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\n\n\nCode\n#Check matrix plot\npairs(water, col = 4,main = \"Water Runoff in Sierras\")\n\n\n\n\n\n\n\nCode\n#Multiple regression\nwat <- (lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water))\nsummary(wat)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nAnalysis: The P-values are not very small across the board. Only OPRC and OPSLAKE have p-values of significance, under 0.05. This shows us that individually, the location precipitation do would not make good explanatory variables.\nHowever, the model as a whole has a p-value of < 2.2e-16, indicating it is statistically significant. Also, while the residuals have a wide range (-12690 to 18542), the 1Q (-4936) and 3Q (4173) are fairly close in absolute value. Therefore, the range may be due to some outliars. The adjusted R-squared (0.9123) tells us that 91% of the variation of runoff can be explained by combined location precipitation.\nWith a strong p-value, decent residuals, and a high adjusted R-squared tells us this model could appropriately be used to predict runoff volume near Bishop, California."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-4",
    "href": "posts/HW3_StephRoberts.html#question-4",
    "title": "HW3",
    "section": "Question 4",
    "text": "Question 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\n#Load data\ndata(\"Rateprof\")\n\n#Select 5 variables in question\nrateprof <- Rateprof %>%\n  select(\"quality\",\"helpfulness\",\"clarity\",\"easiness\",\"raterInterest\")\n\n#Create table of ratings\nkable(head(rateprof), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Professor Ratings**\")\n\n\n\nProfessor Ratings\n\n\nQuality\nHelpfulness\nClarity\nEasiness\nRater Interest\n\n\n\n\n4.636364\n4.636364\n4.636364\n4.818182\n3.545455\n\n\n4.318182\n4.545455\n4.090909\n4.363636\n4.000000\n\n\n4.790698\n4.720930\n4.860465\n4.604651\n3.432432\n\n\n4.250000\n4.458333\n4.041667\n2.791667\n3.181818\n\n\n4.684211\n4.684211\n4.684211\n4.473684\n4.214286\n\n\n4.233333\n4.266667\n4.200000\n4.533333\n3.916667\n\n\n\n\n\n\n\nCode\npairs(rateprof, col = 2,main = \"Professor Ratings\")\n\n\n\n\n\nQuality, helpfulness, and clarity all appear to have strong positive linear correlations with one another. There is a moderate positive linear correlation between easiness and clarity, helpfulness, and quality. There is also a moderate positive correlation between rater interest and quality, helpfulness, and clarity. There is a weak positive linear correlation between easiness and rater interest.\n\n\nCode\n#Check the calculated correlations\ncor(rateprof, use = \"complete.obs\",method = c(\"pearson\", \"kendall\", \"spearman\"))\n\n\n                quality helpfulness   clarity  easiness raterInterest\nquality       1.0000000   0.9810314 0.9759608 0.5651154     0.4706688\nhelpfulness   0.9810314   1.0000000 0.9208070 0.5635184     0.4630321\nclarity       0.9759608   0.9208070 1.0000000 0.5358884     0.4611408\neasiness      0.5651154   0.5635184 0.5358884 1.0000000     0.2052237\nraterInterest 0.4706688   0.4630321 0.4611408 0.2052237     1.0000000\n\n\nOur correlation calculations reinforce our interpretation of the relationships based on the scatter plots."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-5",
    "href": "posts/HW3_StephRoberts.html#question-5",
    "title": "HW3",
    "section": "Question 5",
    "text": "Question 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(a)Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases (b) Summarize and interpret results of inferential analyses.\n\n\nCode\ndata(\"student.survey\")\ndim(student.survey)\n\n\n[1] 60 18\n\n\nCode\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\n\n\nCode\n#Select variable in question\nssurvey <- student.survey %>%\n  select(c(pi, re, hi, tv))%>%\n  rename(political_id = pi, religiosity = re, hs_gpa = hi, tv_hours = tv)\n\n#Check for missing data\nis.na(ssurvey) %>% head()\n\n\n     political_id religiosity hs_gpa tv_hours\n[1,]        FALSE       FALSE  FALSE    FALSE\n[2,]        FALSE       FALSE  FALSE    FALSE\n[3,]        FALSE       FALSE  FALSE    FALSE\n[4,]        FALSE       FALSE  FALSE    FALSE\n[5,]        FALSE       FALSE  FALSE    FALSE\n[6,]        FALSE       FALSE  FALSE    FALSE\n\n\nFortunately, we have no missing data.\n\n\nCode\n#Summarize our df to understand responses and value ranges\nsummary(ssurvey)\n\n\n                political_id       religiosity     hs_gpa         tv_hours     \n very liberal         : 8    never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24    occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6    most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10    every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                      3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                      Max.   :4.000   Max.   :37.000  \n very conservative    : 2                                                      \n\n\n\n\nCode\n#Explore variable relationships\npairs(ssurvey)\n\n\n\n\n\nLet’s take a close look at (i) y = political ideology and x = religiosity.\n\n\nCode\n#Visualize religiosity as an explanatory variable for political ideology\nggplot(ssurvey, aes(x = religiosity, y= political_id,fill= political_id)) + \n  geom_bar(stat = \"identity\")+\n   labs(x=\"Religiosity\", y=\"Political Ideology\")+\n    theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n\n\n\n\n\nThis bar graph shows us what we might expect - that those who attend church every week identify with a very conservative political affiliation. On the contrary, people who never or occasionally go to church trend more liberal.\n\n\nCode\n#Transform categorical data to numeric\nssurvey <- ssurvey %>%\n    mutate(pi_n = dplyr::recode(political_id, \n                         \"very liberal\" = 1,\n                         \"liberal\" = 2, \n                         \"slightly liberal\" = 3, \n                         \"moderate\" = 4,\n                         \"slightly conservative\" = 5,\n                         \"conservative\" = 6, \n                         \"very conservative\" = 7\n                         )) %>%\n    mutate(re_n = dplyr::recode(religiosity, \n                         \"never\" = 0,\n                         \"occasionally\" = 1,\n                         \"most weeks\" = 2,\n                         \"every week\" = 3\n                         )) \nglimpse(ssurvey)\n\n\nRows: 60\nColumns: 6\n$ political_id <ord> conservative, liberal, liberal, moderate, very liberal, l…\n$ religiosity  <ord> most weeks, occasionally, most weeks, occasionally, never…\n$ hs_gpa       <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.…\n$ tv_hours     <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, …\n$ pi_n         <dbl> 6, 2, 2, 4, 1, 2, 2, 2, 1, 3, 5, 2, 1, 4, 1, 2, 3, 2, 2, …\n$ re_n         <dbl> 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 3, 3, 0, 0, …\n\n\nCode\n#Visualize linearity\nggplot(data = ssurvey, aes(x = re_n, y = pi_n)) +\n  geom_point() +\n  geom_smooth(method = 'lm')+\n  labs(x=\"Religiosity\", y=\"Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAfter recoding our variable values, political ideology now spans from 1 to 7 with 1 being very liberal and 7 being very conservative. Religiosity spans 0 to 3 with 0 being never attending church and 3 representing attends every week.\n\n\nCode\n#Linear regression summary\n(summary(rel_pol <- lm(formula = pi_n ~ re_n, data = ssurvey)))\n\n\n\nCall:\nlm(formula = pi_n ~ re_n, data = ssurvey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.9012     0.2717   6.997 2.97e-09 ***\nre_n          0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nInterpretation: The residuals of this regression are fairly symmetrical, but they are not very small in relation to our data values. The coefficients tells us that for every 1 unit of increase in political affiliation (ie. 1 category closer to very conservative), there is an estimated 0.9704 increase in religiosity (ie. almost one category closer to every week). The t-values are moderately large, which indicates a relationship exists. With very small p-values, it is unlikely the relationship is due to chance. Therefore, we could reject the null and conclude there is a relationship between religiosity and political affiliation.\nHowever, as the ggplot graph, the high residuals, and the R2 (0.3359) tell us, this model is not a perfect fit for this data. It is likely due to the categorical nature of the explanatory variable that partitions our data into column-like sections in a graph. That is hard to run a straight line through. This model shows about 33% of the variance can be explained by the predictor. There may be other variable that need to be controlled for or added to the analysis to complete a well-fitting model.\nNow, let’s take a close look at (ii) y = high school GPA and x = hours of TV watching.\n\n\nCode\n#Visualize hours per week watching tv as an explanatory variable for high school GPA\nggplot(data = ssurvey, aes(x = tv_hours, y = hs_gpa)) +\n  geom_point() +\n  geom_smooth(method = 'lm')+\n  labs(x=\"TV Hours /Week\", y=\"High School GPA\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(hs_gpa ~ tv_hours, data = ssurvey))\n\n\n\nCall:\nlm(formula = hs_gpa ~ tv_hours, data = ssurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv_hours    -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nInterpretation: This model shows fairly symmetrical residuals, but some are very large. This can be seen on our plot where lower hours of tv has a wide range of associated GPS values. The p-value of 0.0388 allows us to reject the null hypothesis and conclude there is a relationship between tv hours and gpa. However, with a VERY small R-squared, this model may not be best at predicting gpa SOLELY from hours of tv watched per week. The combination of all our calculations lead me to conclude that higher number of hours watched per week may be related to lower GPA. However, the opposite may not be true - fewer hours of tv is not necessarily associated with higher a higher GPA\n\n\nCode\ncor.test(ssurvey$tv_hours, ssurvey$hs_gpa)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  ssurvey$tv_hours and ssurvey$hs_gpa\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\n\nThere is a weak negative correlation between hours of tv watched per week and high school GPA."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html",
    "href": "posts/HW3_RoyYoon.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section",
    "href": "posts/HW3_RoyYoon.html#section",
    "title": "Homework 3",
    "section": "1.1.1",
    "text": "1.1.1\nIdentify the predictor and the response.\nPredictor: gross national product per person in U.S. dollars (ppdgp)\nResponse: fertility\n\n\nCode\nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section-1",
    "href": "posts/HW3_RoyYoon.html#section-1",
    "title": "Homework 3",
    "section": "1.1.2",
    "text": "1.1.2\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nfertiity_ppgddp <- UN11 %>%\n  select(fertility,ppgdp )\n\nfertiity_ppgddp\n\n\n                                 fertility    ppgdp\nAfghanistan                       5.968000    499.0\nAlbania                           1.525000   3677.2\nAlgeria                           2.142000   4473.0\nAngola                            5.135000   4321.9\nAnguilla                          2.000000  13750.1\nArgentina                         2.172000   9162.1\nArmenia                           1.735000   3030.7\nAruba                             1.671000  22851.5\nAustralia                         1.949000  57118.9\nAustria                           1.346000  45158.8\nAzerbaijan                        2.148000   5637.6\nBahamas                           1.877000  22461.6\nBahrain                           2.430000  18184.1\nBangladesh                        2.157000    670.4\nBarbados                          1.575000  14497.3\nBelarus                           1.479000   5702.0\nBelgium                           1.835000  43814.8\nBelize                            2.679000   4495.8\nBenin                             5.078000    741.1\nBermuda                           1.760000  92624.7\nBhutan                            2.258000   2047.2\nBolivia                           3.229000   1977.9\nBosnia and Herzegovina            1.134000   4477.7\nBotswana                          2.617000   7402.9\nBrazil                            1.800000  10715.6\nBrunei Darussalam                 1.984000  32647.6\nBulgaria                          1.546000   6365.1\nBurkina Faso                      5.750000    519.7\nBurundi                           4.051000    176.6\nCambodia                          2.422000    797.2\nCameroon                          4.287000   1206.6\nCanada                            1.691000  46360.9\nCape Verde                        2.279000   3244.0\nCayman Islands                    1.600000  57047.9\nCentral African Republic          4.423000    450.8\nChad                              5.737000    727.4\nChile                             1.832000  11887.7\nChina                             1.559000   4354.0\nColombia                          2.293000   6222.8\nComoros                           4.742000    736.6\nCongo                             4.442000   2665.1\nCook Islands                      2.530806  12212.1\nCosta Rica                        1.812000   7703.8\nCote dIvoire                      4.224000   1154.1\nCroatia                           1.501000  13819.5\nCuba                              1.451000   5704.4\nCyprus                            1.458000  28364.3\nCzech Republic                    1.501000  18838.8\nDemocratic Republic of the Congo  5.485000    200.6\nDenmark                           1.885000  55830.2\nDjibouti                          3.589000   1282.6\nDominica                          3.000000   7020.8\nDominican Republic                2.490000   5195.4\nEast Timor                        5.918000    706.1\nEcuador                           2.393000   4072.6\nEgypt                             2.636000   2653.7\nEl Salvador                       2.171000   3425.6\nEquatorial Guinea                 4.980000  16852.4\nEritrea                           4.243000    429.1\nEstonia                           1.702000  14135.4\nEthiopia                          3.848000    324.6\nFiji                              2.602000   3545.7\nFinland                           1.875000  44501.7\nFrance                            1.987000  39545.9\nFrench Polynesia                  2.033000  24669.0\nGabon                             3.195000  12468.8\nGambia                            4.689000    579.1\nGeorgia                           1.528000   2680.3\nGermany                           1.457000  39857.1\nGhana                             3.988000   1333.2\nGreece                            1.540000  26503.8\nGreenland                         2.217000  35292.7\nGrenada                           2.171000   7429.0\nGuatemala                         3.840000   2882.3\nGuinea                            5.032000    427.5\nGuinea-Bissau                     4.877000    539.4\nGuyana                            2.190000   2996.0\nHaiti                             3.159000    612.7\nHonduras                          2.996000   2026.2\nHong Kong                         1.137000  31823.7\nHungary                           1.430000  12884.0\nIceland                           2.098000  39278.0\nIndia                             2.538000   1406.4\nIndonesia                         2.055000   2949.3\nIran                              1.587000   5227.1\nIraq                              4.535000    888.5\nIreland                           2.097000  46220.3\nIsrael                            2.909000  29311.6\nItaly                             1.476000  33877.1\nJamaica                           2.262000   4899.0\nJapan                             1.418000  43140.9\nJordan                            2.889000   4445.3\nKazakhstan                        2.481000   9166.7\nKenya                             4.623000    801.8\nKiribati                          3.500000   1468.2\nKuwait                            2.251000  45430.4\nKyrgyzstan                        2.621000    865.4\nLaos                              2.543000   1047.6\nLatvia                            1.506000  10663.0\nLebanon                           1.764000   9283.7\nLesotho                           3.051000    980.7\nLiberia                           5.038000    218.6\nLibya                             2.410000  11320.8\nLithuania                         1.495000  10975.5\nLuxembourg                        1.683000 105095.4\nMacao                             1.163000  49990.2\nMadagascar                        4.493000    421.9\nMalawi                            5.968000    357.4\nMalaysia                          2.572000   8372.8\nMaldives                          1.668000   4684.5\nMali                              6.117000    598.8\nMalta                             1.284000  19599.2\nMarshall Islands                  4.384466   3069.4\nMauritania                        4.361000   1131.1\nMauritius                         1.590000   7488.3\nMexico                            2.227000   9100.7\nMicronesia                        3.307000   2678.2\nMoldova                           1.450000   1625.8\nMongolia                          2.446000   2246.7\nMontenegro                        1.630000   6509.8\nMorocco                           2.183000   2865.0\nMozambique                        4.713000    407.5\nMyanmar                           1.939000    876.2\nNamibia                           3.055000   5124.7\nNauru                             3.300000   6190.1\nNepal                             2.587000    534.7\nNeth Antilles                     1.900000  20321.1\nNetherlands                       1.794000  46909.7\nNew Caledonia                     2.091000  35319.5\nNew Zealand                       2.135000  32372.1\nNicaragua                         2.500000   1131.9\nNiger                             6.925000    357.7\nNigeria                           5.431000   1239.8\nNorth Korea                       1.988000    504.0\nNorway                            1.948000  84588.7\nOman                              2.146000  20791.0\nPakistan                          3.201000   1003.2\nPalau                             2.000000  10821.8\nPalestinian Territory             4.270000   1819.5\nPanama                            2.409000   7614.0\nPapua New Guinea                  3.799000   1428.4\nParaguay                          2.858000   2771.1\nPeru                              2.410000   5410.7\nPhilippines                       3.050000   2140.1\nPoland                            1.415000  12263.2\nPortugal                          1.312000  21437.6\nPuerto Rico                       1.757000  26461.0\nQatar                             2.204000  72397.9\nRepublic of Korea                 1.389000  21052.2\nRomania                           1.428000   7522.4\nRussian Federation                1.529000  10351.4\nRwanda                            5.282000    532.3\nSaint Lucia                       1.907000   6677.1\nSamoa                             3.763000   3343.3\nSao Tome and Principe             3.488000   1283.3\nSaudi Arabia                      2.639000  15835.9\nSenegal                           4.605000   1032.7\nSerbia                            1.562000   5123.2\nSeychelles                        2.340000  11450.6\nSierra Leone                      4.728000    351.7\nSingapore                         1.367000  43783.1\nSlovakia                          1.372000  15976.0\nSlovenia                          1.477000  23109.8\nSolomon Islands                   4.041000   1193.5\nSomalia                           6.283000    114.8\nSouth Africa                      2.383000   7254.8\nSpain                             1.504000  30542.8\nSri Lanka                         2.235000   2375.3\nSt Vincent and Grenadines         1.995000   6171.7\nSudan                             4.225000   1824.9\nSuriname                          2.266000   7018.0\nSwaziland                         3.174000   3311.2\nSweden                            1.925000  48906.2\nSwitzerland                       1.536000  68880.2\nSyria                             2.772000   2931.5\nTajikistan                        3.162000    816.0\nTanzania                          5.499000    516.0\nTFYR Macedonia                    1.397000   4434.5\nThailand                          1.528000   4612.8\nTogo                              3.864000    524.6\nTonga                             3.783000   3543.1\nTrinidad and Tobago               1.632000  15205.1\nTunisia                           1.909000   4222.1\nTurkey                            2.022000  10095.1\nTurkmenistan                      2.316000   4587.5\nTuvalu                            3.700000   3187.2\nUganda                            5.901000    509.0\nUkraine                           1.483000   3035.0\nUnited Arab Emirates              1.707000  39624.7\nUnited Kingdom                    1.867000  36326.8\nUnited States                     2.077000  46545.9\nUruguay                           2.043000  11952.4\nUzbekistan                        2.264000   1427.3\nVanuatu                           3.750000   2963.5\nVenezuela                         2.391000  13502.7\nViet Nam                          1.750000   1182.7\nYemen                             4.938000   1437.2\nZambia                            6.300000   1237.8\nZimbabwe                          3.109000    573.1\n\n\n\n\nCode\nfertility_ppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= ppgdp, y = fertility)) + geom_point()\n\nfertility_ppgdp_plot\n\n\n\n\n\nA straight line mean function does not seem plausible for a summary of this graph as a there does not seem to be a plausible predictive straight line that would be able to describe the tendencies in the scatter plot. Rather the shape seems to be asymptotic."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section-2",
    "href": "posts/HW3_RoyYoon.html#section-2",
    "title": "Homework 3",
    "section": "1.1.3",
    "text": "1.1.3\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nfertility_logppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= log(ppgdp), y = fertility)) + geom_point()\n\nfertility_logppgdp_plot\n\n\n\n\n\nA simple linear regression model seems plausible for a summary of this graph as a there seems to be a plausible predictive straight line(negative slope) that would be able to describe the tendencies in the scatter plot of log(ppgdp)\n\n\nCode\nlogfertility_ppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= ppgdp, y = log(fertility))) + geom_point()\n\nlogfertility_ppgdp_plot\n\n\n\n\n\nA simple linear regression model does not seem plausible for a summary of this graph as a there does not seem to be a plausible predictive straight line that would be able to describe the tendencies in the scatter plot of log(fertility)"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#a",
    "href": "posts/HW3_RoyYoon.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nHow, if at all, does the slope of the prediction equation change?\nsuppose there is 10, 20, and 30 British pounds sterling and 13.3, 26.6, and 39.9 USD.\nThe difference between the 10, 20, and 30 British pounds sterling is 10 as the values increment.\nThe difference between 13.3, 26.6, and 39.9 USD is 13.3 as the values increment.\n13.3 is a greater rate of change than 10. Thus looking at the slope as the rate of change, there is a difference in the rate of change."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#b",
    "href": "posts/HW3_RoyYoon.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nHow, if at all, does the correlation change?\nCorrelation is standardized version of the slope. The unit of measurement does not affect the correlation value.\nThere will be no change to the correlation. The correlation observes how close the dad points are to the line of the data. If all points are converted to the same scale, then how close the points remain to the line should remain constant."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#a-1",
    "href": "posts/HW3_RoyYoon.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\n(i) y = political ideology and x = religiosity\n\n\nCode\nstudent.survey\n\n\nError in eval(expr, envir, enclos): object 'student.survey' not found\n\n\nCode\na <- ggplot(data = student.survey, aes(x= as.numeric(pi), y = as.numeric(re))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(pi), y = as.numeric(re))): object 'student.survey' not found\n\n\nCode\na\n\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\n\n\n(ii) y = high school GPA and x = hours of TV watching\n\n\nCode\nb <- ggplot(data = student.survey, aes(x= as.numeric(tv), y = as.numeric(hi))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(tv), y = as.numeric(hi))): object 'student.survey' not found\n\n\nCode\nb\n\n\nError in eval(expr, envir, enclos): object 'b' not found"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#b-1",
    "href": "posts/HW3_RoyYoon.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nSummarize and interpret results of inferential analyses.\n\ny = political ideology and x = religiosity\ny = high school GPA and x = hours of TV watching\n\nthe relationship between the average number of hours per week that one watches tv and the high school gpa on a four point square is negative. The more hours of tvs one watches per week, the lower the gpa is.\n\n\nCode\nsummary(lm(hi ~ tv, data = student.survey))\n\n\nError in is.data.frame(data): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html",
    "href": "posts/HW1_PrahithaMovva.html",
    "title": "Homework 1 - Prahitha Movva",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data = df)\n\n\n\n\n\nFrom the boxplots for gender above, we can see that males seem to have (slightly) higher lung capacity than females.\n\n\n\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity for smokers and nonsmokers seems to be higher for smokers. This does not make sense as we generally expect smokers to have a reduced lung capacity due to the damage from smoking.\n\n\n\n\n\nCode\ndf_ageGroups <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n\nError in mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == : could not find function \"mutate\"\n\n\nCode\nggplot(df_ageGroups, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\nError in ggplot(df_ageGroups, aes(x = LungCap)): could not find function \"ggplot\"\n\n\nWe see non-smokers to have a higher lung capacity than smokers, as expected.\n\n\n\nLung capacity seems to be directly proportional to age and after breaking down the data by age groups, we see that the lung capacities for non-smokers are higher than those of smokers in the same age group (except for less than or equal to 13). This could be because of the total number of observations in each age group. The age group less than or equal to 13 has the highest number of observations - thereby skewing the results (here, mean) for the entire distribution.\n\n\n\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity seems to be positively correlated with age i.e., as age increases, lung capacity increases. Same is the case with covariance."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#a-1",
    "href": "posts/HW1_PrahithaMovva.html#a-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\na <- 160/810\n\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#b-1",
    "href": "posts/HW1_PrahithaMovva.html#b-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\nb <- (128+434)/810\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#c-1",
    "href": "posts/HW1_PrahithaMovva.html#c-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\nc <- (128+434+160)/810\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#d-1",
    "href": "posts/HW1_PrahithaMovva.html#d-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "d",
    "text": "d\n\n\nCode\nd <- (64+24)/810\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#e-1",
    "href": "posts/HW1_PrahithaMovva.html#e-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "e",
    "text": "e\n\n\nCode\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\n\nThe expected value for the number of prior convictions is 1.2864198 or 1, as the number of convictions cannot be a float."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#f-1",
    "href": "posts/HW1_PrahithaMovva.html#f-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "f",
    "text": "f\n\n\nCode\nvar_0 <- ((0-e)^2) * (128/810)\nvar_1 <- ((1-e)^2) * (434/810)\nvar_2 <- ((2-e)^2) * (160/810)\nvar_3 <- ((3-e)^2) * (64/810)\nvar_4 <- ((4-e)^2) * (24/810)\n\nvar <- var_0 + var_1 + var_2 + var_3 + var_4\n\nsd <- sqrt(var)\n\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html",
    "href": "posts/HW2_EmmaRasmussen.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section",
    "href": "posts/HW2_EmmaRasmussen.html#section",
    "title": "Homework 2",
    "section": "1.",
    "text": "1.\n\n\nCode\n#Bypass\n#calculating t-score for 90% confidence interval\ntscoreb<- qt(p=1-.05, df=539-1)\n\n#calculating standard error\nseb<- 10/sqrt(539)\n\nmeanb<- 19\n\nCIb<- c(meanb- (tscoreb*seb), meanb+ (tscoreb*seb))\nCIb\n\n\n[1] 18.29029 19.70971\n\n\nCode\n#Angiography\n#calculating t-score for 90% confidence interval\ntscorea<- qt(p=1-.05, df=847-1)\n\n#calculating standard error\nsea<- 9/sqrt(847)\n\nmeana<- 18\n\nCIa<- c(meana- (tscorea*sea), meana+ (tscorea*sea))\nCIa\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days. The 90% confidence interval for angiography is [17.49, 18.51] days. The confidence interval for angiography is narrower, which makes sense given it has a (slightly) smaller standard deviation and a larger sample size (larger sample size reduces margin of error)."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-1",
    "href": "posts/HW2_EmmaRasmussen.html#section-1",
    "title": "Homework 2",
    "section": "2.",
    "text": "2.\n\n\nCode\n#assigning n= number of trials\nn<- 1031\n#assigning k= number agree\nk<- 567\n\n#calculating point estimate\np<- 567/1031\np\n\n\n[1] 0.5499515\n\n\nCode\n#calculating margin of error for 95% CI. I have no idea how to calculate a confidence interval without a sd. I found this formula online.\nmargin<- qnorm(0.975)*sqrt(p*(1-p)/n)\nmargin\n\n\n[1] 0.03036761\n\n\nCode\nCI<- c(p-margin, p+ margin)\nCI\n\n\n[1] 0.5195839 0.5803191\n\n\nThe 95% confidence interval for American’s agreeing that college education is essential for success is [51.96, 58.03]%. The point estimate for this value based on the survey is 55%."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-2",
    "href": "posts/HW2_EmmaRasmussen.html#section-2",
    "title": "Homework 2",
    "section": "3.",
    "text": "3.\n\n\nCode\n#estimating population standard deviation\n(200-30)/4\n\n\n[1] 42.5\n\n\nCode\n#solving for n in the equation for confidence intervals 5=1.96*(42.5/sqrt(n))\n#n= 277.56 or 278\n\n\nBy plugging in 5 to the formula for confidence intervals (the t-value for 95%, and the standard deviation estimate of 42.5) we get a value of n=277.56 or 278 need to be in the sample to retrieve a confidence interval of width 10."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-3",
    "href": "posts/HW2_EmmaRasmussen.html#section-3",
    "title": "Homework 2",
    "section": "4.",
    "text": "4.\n\na.\n\nAssume data is normally distributed\nHo: μ =500\nHa: μ not equal to 500 μ < 500 μ > 500\nalpha level =0.05\n\n\n\nCode\n#calculating the standard error\nsef<- 90/sqrt(9)\nsef\n\n\n[1] 30\n\n\nCode\n#calculating t-score\ntf<-(410-500)/(sef)\ntf \n\n\n[1] -3\n\n\nCode\n#calculating the p-value from the test statistic (multiply times two because we are doing a two-sided test)\n(pt(q=-3, df=8))*2\n\n\n[1] 0.01707168\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or lower, as the default calculates the lower tail)\npt(-3, 8)\n\n\n[1] 0.008535841\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or higher, as we included lower.tail=FALSE)\npt(-3, 8, lower.tail=FALSE)\n\n\n[1] 0.9914642\n\n\nCode\n# since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than 500 a week\n\n\nTwo-sided: Since our p-value is 0.017 we can reject the null hypothesis at alpha level=0.05. We have evidence that the mean weekly earnings for women at this company is different from $500. ### b. Lower tail: Since our p-value (0.0085) is less than the alpha level of 0.05, we can reject the null. We have evidence that the mean weekly earnings at this company for women is less than $500. ### c.  Upper tail: Since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than $500 a week"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-4",
    "href": "posts/HW2_EmmaRasmussen.html#section-4",
    "title": "Homework 2",
    "section": "5.",
    "text": "5.\n\na.\n\n\nCode\n#Jones\n#calculating t-score\n(519.5-500)/(10)\n\n\n[1] 1.95\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.95, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n##calculating t-score\n(519.7-500)/(10)\n\n\n[1] 1.97\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.97, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.04911426\n\n\n\n\nb.\nAt the .05 significance level, Jones’ findings are not significant but Smith’s findings are.\n\n\nc.\nThis example shows that there is a very find line between rejecting and not rejecting the null hypothesis. Their findings were extremely similar, the means are different by only 0.2. In this way, reporting the p-value retrieved is actually really important to make this distinction. Similarly, Jones’ findings would have been significant at the 0.1 significance level, so rejecting or not rejecting the null hypothesis based on a p-value can be fairly arbitrary."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-5",
    "href": "posts/HW2_EmmaRasmussen.html#section-5",
    "title": "Homework 2",
    "section": "6.",
    "text": "6.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nUsing a 95% confidence level we get a p-value of 0.038. We reject the null hypothesis. We have evidence that the average tax per gallon of gas in the U.S. is less than 45 cents."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html",
    "href": "posts/FinalPart1_CalebHill.html",
    "title": "Final Part 1",
    "section": "",
    "text": "Multiple research reports state that there is a relationship between re-hospitalization rates and social characteristics, such as demographic and economic identifiers, (Barnett, Hsu & McWilliams, 2015; Murray, Allen, Clark, Daly & Jacobs, 2021). Specifically, racial characteristics play a large role in predicting re-hospitalization in a population (Li, Cai & Glance, 2015). While some articles examine economic and health factors contributing to these disparities, very few dig deep into environmental factors that influence this phenomenon, (Spatz, Bernheim, Horwitz & Herrin, 2020). With your zipcode affecting up to 60% of your health outcomes, this research is relevant to better improving one of our most costly health expenditures: hospitalization.\nThis paper aims to explore how different environmental variables impact re-hospitalization rates on a county-by-county level, controlling for racial, ethnic, and sex variables (maybe). These environmental factors will include both common environmental concerns, such as heat index, average temperature, precipitation, and natural disasters, along with the built environment, population density.\nThe data-set chosen for this analysis is taken from the Agency for Healthcare Research and Quality, Social Determinants of Health (SDOH) Database. This data-set has over 300 variables to explore each SDOH domain: social context, economic context, education, healthcare, and the environment. We shall pull data from three of these five domains: social, economic, and environmental.\nTo further reduce data bloat, we shall limit the geographic review to Texas counties – my home state! That should provide us with 200+ observations.\nThe hypothesis for this research report is: *Environmental factors increase rates of re-hospitalization in Texas counties.\nTherefore, the null hypothesis is: *Environmental factors do not increase rates of re-hospitalization in Texas counties.\nMultiple regression analyses shall be employed to determine the relationship – or lack thereof – between these variables.\nFirst I’ll import the relevant libraries.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(corrplot)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\nThen I’ll import the dataset and view the first six rows.\n\n\nCode\ndf <- SDOH_2020_COUNTY_1_0 <- read_excel(\"_data/SDOH_2020_COUNTY_1_0.xlsx\", sheet = \"Data\")\n\n\nWarning: Expecting logical in OA1673 / R1673C391: got '46123'\n\n\nWarning: Expecting logical in OA1765 / R1765C391: got '32510'\n\n\nWarning: Expecting logical in OB1765 / R1765C392: got '41025'\n\n\nWarning: Expecting logical in OC1765 / R1765C393: got '41037'\n\n\nWarning: Expecting logical in OA2799 / R2799C391: got '49017'\n\n\nWarning: Expecting logical in OB2799 / R2799C392: got '49019'\n\n\nWarning: Expecting logical in OC2799 / R2799C393: got '49025'\n\n\nWarning: Expecting logical in OD2799 / R2799C394: got '49055'\n\n\nWarning: Expecting logical in OA2844 / R2844C391: got '51760'\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 685\n   YEAR COUNTYFIPS STATEFIPS STATE COUNTY REGION TERRI…¹ ACS_T…² ACS_T…³ ACS_T…⁴\n  <dbl> <chr>      <chr>     <chr> <chr>  <chr>    <dbl>   <dbl>   <dbl>   <dbl>\n1  2020 01001      01        Alab… Autau… South        0   55639   54929   52404\n2  2020 01003      01        Alab… Baldw… South        0  218289  216518  206329\n3  2020 01005      01        Alab… Barbo… South        0   25026   24792   23694\n4  2020 01007      01        Alab… Bibb … South        0   22374   22073   21121\n5  2020 01009      01        Alab… Bloun… South        0   57755   57164   54250\n6  2020 01011      01        Alab… Bullo… South        0   10173   10143    9579\n# … with 675 more variables: ACS_TOT_POP_ABOVE15 <dbl>,\n#   ACS_TOT_POP_ABOVE16 <dbl>, ACS_TOT_POP_16_19 <dbl>,\n#   ACS_TOT_POP_ABOVE25 <dbl>, ACS_TOT_CIVIL_POP_ABOVE18 <dbl>,\n#   ACS_TOT_CIVIL_VET_POP_ABOVE25 <dbl>, ACS_TOT_OWN_CHILD_BELOW17 <dbl>,\n#   ACS_TOT_WORKER_NWFH <dbl>, ACS_TOT_WORKER_HH <dbl>,\n#   ACS_TOT_CIVILIAN_LABOR <dbl>, ACS_TOT_CIVIL_EMPLOY_POP <dbl>,\n#   ACS_TOT_POP_POV <dbl>, ACS_TOT_CIVIL_NONINST_POP_POV <dbl>, …\n\n\nNext I want to verify the class is a dataframe. Otherwise, I’ll need to transform the data to make it easier to work with.\n\n\nCode\nclass(df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nAll good here.\nNow on to data transformation. We will need to select only the relevant columns for this analysis and filter by Texas, bringing the observations (rows) down to 254.\n\n\nCode\ndf_new <- df %>%\n  select(COUNTYFIPS,\n         STATE,\n         COUNTY,\n         ACS_TOT_POP_WT,\n         ACS_PCT_MALE,\n         ACS_PCT_FEMALE,\n         ACS_PCT_AIAN,\n         ACS_PCT_ASIAN,\n         ACS_PCT_BLACK,\n         ACS_PCT_HISPANIC,\n         ACS_PCT_MULT_RACE,\n         ACS_PCT_NHPI,\n         ACS_PCT_OTHER_RACE,\n         ACS_PCT_WHITE,\n         CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  filter(STATE == \"Texas\")\nhead(df_new)\n\n\n# A tibble: 6 × 22\n  COUNTYF…¹ STATE COUNTY ACS_T…² ACS_P…³ ACS_P…⁴ ACS_P…⁵ ACS_P…⁶ ACS_P…⁷ ACS_P…⁸\n  <chr>     <chr> <chr>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 48001     Texas Ander…   57917    61.2    38.8    0.41    0.6    20.9    17.9 \n2 48003     Texas Andre…   18227    49.6    50.4    0       0.31    2.29   56.9 \n3 48005     Texas Angel…   87119    48.9    51.1    0.31    1.11   15.1    22.3 \n4 48007     Texas Arans…   24220    49.5    50.5    0.9     1.14    0.33   28.0 \n5 48009     Texas Arche…    8754    50.2    49.8    1.46    0.17    1.1     8.21\n6 48011     Texas Armst…    1950    45.7    54.3    0.77    0       0.72    8.46\n# … with 12 more variables: ACS_PCT_MULT_RACE <dbl>, ACS_PCT_NHPI <dbl>,\n#   ACS_PCT_OTHER_RACE <dbl>, ACS_PCT_WHITE <dbl>, CEN_POPDENSITY_COUNTY <dbl>,\n#   NEPHTN_HEATIND_105 <dbl>, NOAAC_AVG_TEMP_YEARLY <dbl>,\n#   NOAAC_PRECIPITATION_AVG_YEARLY <dbl>, NOAAS_TOT_NATURAL_DISASTERS <dbl>,\n#   SAIPE_MEDIAN_HH_INCOME <dbl>, SAIPE_PCT_POV <dbl>,\n#   LTC_AVG_OBS_REHOSP_RATE <dbl>, and abbreviated variable names ¹​COUNTYFIPS,\n#   ²​ACS_TOT_POP_WT, ³​ACS_PCT_MALE, ⁴​ACS_PCT_FEMALE, ⁵​ACS_PCT_AIAN, …\n\n\nOut of 300+ variables, we’ve whittled them down to 22. Of those 22, we have three (3) that are unique identifiers (FIPS, State, and County), 11 that are potential control variables (population, gender, and race / ethnicity), and eight (8) that we can explore (Population Density to Re-hospitalization Rate).\nBefore we launch into exploring these eight variables via descriptive statistics, first we need to determine where the NAs are and see if any of the variables will have a substantial amount of missing data.\n\n\nCode\ncolSums(is.na(df_new))\n\n\n                    COUNTYFIPS                          STATE \n                             0                              0 \n                        COUNTY                 ACS_TOT_POP_WT \n                             0                              0 \n                  ACS_PCT_MALE                 ACS_PCT_FEMALE \n                             0                              0 \n                  ACS_PCT_AIAN                  ACS_PCT_ASIAN \n                             0                              0 \n                 ACS_PCT_BLACK               ACS_PCT_HISPANIC \n                             0                              0 \n             ACS_PCT_MULT_RACE                   ACS_PCT_NHPI \n                             0                              0 \n            ACS_PCT_OTHER_RACE                  ACS_PCT_WHITE \n                             0                              0 \n         CEN_POPDENSITY_COUNTY             NEPHTN_HEATIND_105 \n                             0                              0 \n         NOAAC_AVG_TEMP_YEARLY NOAAC_PRECIPITATION_AVG_YEARLY \n                             0                              0 \n   NOAAS_TOT_NATURAL_DISASTERS         SAIPE_MEDIAN_HH_INCOME \n                             0                              0 \n                 SAIPE_PCT_POV        LTC_AVG_OBS_REHOSP_RATE \n                             0                             44 \n\n\nThis is not ideal, as that’s our dependent variable. However, 44 / 254 is not bad. That still leaves us with plenty of counties to review.\n\n\nCode\ndf_new %>%\n  drop_na() %>%\n  print(nrow(df_new))\n\n\n# A tibble: 210 × 22\n   COUNTYFIPS STATE COUNTY           ACS_TOT_POP_WT ACS_PCT_MALE ACS_PCT_FEMALE\n   <chr>      <chr> <chr>                     <dbl>        <dbl>          <dbl>\n 1 48001      Texas Anderson County           57917         61.2           38.8\n 2 48003      Texas Andrews County            18227         49.6           50.4\n 3 48005      Texas Angelina County           87119         48.9           51.1\n 4 48007      Texas Aransas County            24220         49.5           50.5\n 5 48011      Texas Armstrong County           1950         45.7           54.3\n 6 48013      Texas Atascosa County           50194         50.2           49.8\n 7 48015      Texas Austin County             29892         49.9           50.1\n 8 48017      Texas Bailey County              6916         50.0           50.0\n 9 48019      Texas Bandera County            22770         49.8           50.2\n10 48021      Texas Bastrop County            86839         50.8           49.2\n   ACS_PCT_AIAN ACS_PCT_ASIAN ACS_PCT_BLACK ACS_PCT_HISPANIC ACS_PCT_MULT_RACE\n          <dbl>         <dbl>         <dbl>            <dbl>             <dbl>\n 1         0.41          0.6          20.9             17.9               4.46\n 2         0             0.31          2.29            56.9               5.76\n 3         0.31          1.11         15.1             22.3               3.21\n 4         0.9           1.14          0.33            28.0               6.2 \n 5         0.77          0             0.72             8.46              5.33\n 6         0.08          0.5           1.08            64.7              11.4 \n 7         0.14          0.55          8.77            27.2               2.96\n 8         1             0.68          0.29            65.8               0.49\n 9         1.2           0.34          0.75            19.3               5.97\n10         0.53          0.84          7.83            38.8               6.98\n   ACS_PCT_NHPI ACS_PCT_OTHER_RACE ACS_PCT_WHITE CEN_POPDENSITY_COUNTY\n          <dbl>              <dbl>         <dbl>                 <dbl>\n 1         0.02               2.35          71.2                 54.5 \n 2         0.14              10.2           81.2                 12.2 \n 3         0.01               2.78          77.4                109.  \n 4         0                  3.57          87.9                 96.1 \n 5         0                  1.59          91.6                  2.14\n 6         0                  2.2           84.8                 41.2 \n 7         0                 12.1           75.5                 46.2 \n 8         0                  4.38          93.2                  8.36\n 9         0                  2.7           89.0                 28.8 \n10         0                 18.4           65.4                 97.8 \n   NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_Y…¹ NOAAC…² NOAAS…³ SAIPE…⁴ SAIPE…⁵ LTC_A…⁶\n                <dbl>              <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1                 28               66.5   4.36       46   50879    20.9    0.21\n 2                  0               64.6   0.595      26   76600     9.2    0.2 \n 3                 26               67.6   4.05       15   49943    17      0.18\n 4                  7               73.2   2.24       36   51461    17.1    0.09\n 5                  0               60.6   1.09       90   62256     9.3    0.33\n 6                 47               71.8   2.09       38   60594    14.9    0.16\n 7                 33               70.4   3.41       19   60593    11.4    0.08\n 8                  0               59.6   0.692      38   48259    14.4    0   \n 9                  7               68.2   2.03       29   64389    11      0.08\n10                 44               70.0   2.89       30   74612    10.8    0.14\n# … with 200 more rows, and abbreviated variable names ¹​NOAAC_AVG_TEMP_YEARLY, ²​NOAAC_PRECIPITATION_AVG_YEARLY, ³​NOAAS_TOT_NATURAL_DISASTERS, ⁴​SAIPE_MEDIAN_HH_INCOME, ⁵​SAIPE_PCT_POV, ⁶​LTC_AVG_OBS_REHOSP_RATE\n\n\n210 x 22 is a good place to start. We’ll need to re-do this step for the descriptive statistics section, but we can carry over this object when we fit the linear models."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "href": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "title": "Final Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nFor our preliminary analysis, we’re going to provide summary statistics analyzing the 8 variables relevant to our research question, from Population Density to the end of the data-set, and a visualization for each. Re-hospitalization rates will be the dependent variable in future models, with the 11 demographic variables as potential controls for the regression(s).\n\n\nCode\ndata <- df_new %>%\n  select(CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  drop_na()\nsummary(data)\n\n\n CEN_POPDENSITY_COUNTY NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_YEARLY\n Min.   :   0.78       Min.   : 0.00      Min.   :56.52        \n 1st Qu.:  12.53       1st Qu.: 7.00      1st Qu.:64.89        \n Median :  30.48       Median :24.00      Median :66.53        \n Mean   : 143.53       Mean   :22.14      Mean   :67.06        \n 3rd Qu.:  78.46       3rd Qu.:34.00      3rd Qu.:69.71        \n Max.   :3003.99       Max.   :59.00      Max.   :76.47        \n NOAAC_PRECIPITATION_AVG_YEARLY NOAAS_TOT_NATURAL_DISASTERS\n Min.   :0.4583                 Min.   :  0.00             \n 1st Qu.:1.6135                 1st Qu.: 14.25             \n Median :2.5933                 Median : 28.50             \n Mean   :2.7027                 Mean   : 32.75             \n 3rd Qu.:3.8808                 3rd Qu.: 40.00             \n Max.   :5.4558                 Max.   :186.00             \n SAIPE_MEDIAN_HH_INCOME SAIPE_PCT_POV   LTC_AVG_OBS_REHOSP_RATE\n Min.   : 33513         Min.   : 4.80   Min.   :0.0000         \n 1st Qu.: 48455         1st Qu.:11.45   1st Qu.:0.1100         \n Median : 54536         Median :14.50   Median :0.1500         \n Mean   : 57028         Mean   :14.75   Mean   :0.1528         \n 3rd Qu.: 61901         3rd Qu.:17.40   3rd Qu.:0.2000         \n Max.   :106225         Max.   :28.70   Max.   :1.0000         \n\n\n\nPopulation Density\n\n\nCode\nggplot(data, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\nWe see quite a number of counties have a low population density. This is no surprise, as over 80% of counties in Texas are labeled as “rural” by multiple federal agencies – dependent upon low population density.\nThis is further attested and we see a wide range between this variable’s median (21.8) and mean (119.4). Lots of out-liers. If we had a urban/rural classification code, we could filter on only rural counties to help mitigate this spread. I may need to merge a data-set due to this wide range.\n\n\nHeat Index Over 105F\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_boxplot()\n\n\n\n\n\nTexas is a hot state, and this visualization is evidence of that. The median number of days Texas’ counties experience a heat index of over 105F each year is 20 days per year. One county even reached 59 days!\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe data-set has a very normal distribution, centered around the 25/30 mark – if the number of counties at 0 were removed. Yet because that’s not so, this variable has a sharp bimodal distribution. We may have to separate the data into two bins: those with less than 10 days over 105F and those with more than 10 days over 105F. That’s yet to be determined.\n\n\nAverage Yearly Temperature\n\n\nCode\nggplot(data, aes(NOAAC_AVG_TEMP_YEARLY)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere’s a good distribution. Average temperature each month is between 65 to 67 for most of the counties. The range (20) is also fairly small for a state with such a large area and multiple climates within its borders.\n\n\nAverage Yearly Precipitation\n\n\nCode\nggplot(data, aes(NOAAC_PRECIPITATION_AVG_YEARLY)) +\n  geom_boxplot()\n\n\n\n\n\nAverage precipitation each month is fairly uniform, with the mean at 2.5 inches of rain, on average, each month. This variable will most likely provide less variation in the analysis compared to others, such as population density and heat index. This can be both a good and a bad thing, as variations in precipitation was one of the variables I was most interested in exploring for this project. Oh well.\n\n\nTotal Natural Disasters\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_boxplot()\n\n\n\n\n\nMany high out-liers over 75. Let’s plot a histogram to get a better look at the data’s distribution.\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA right skewed variable, with observations dropping off dramatically once we reach 50 total recorded natural disasters.\n\n\nMedian Household Income\n\n\nCode\nggplot(data, aes(SAIPE_MEDIAN_HH_INCOME)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA couple of high out-liers, hovering around $90,000+ in median household income, but the mean holds at $57,291.\n\n\nPercent in Poverty\n\n\nCode\nggplot(data, aes(SAIPE_PCT_POV)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother close to normal distribution. Most counties have poverty rates ranging from 10% to 20%. There are of course out-liers, especially a good number below 10%, but those are rare.\n\n\nRe-hospitalization Rate\n\n\nCode\nggplot(data, aes(LTC_AVG_OBS_REHOSP_RATE)) +   geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother right skewed variable. Lots of counties with 0.00 rates of re-hospitalization, and few, if any, above 0.25 per 100,000 people. From a health perspective, this is good news! From a research perspective, that’s going to make analysis a little trickier. However, the somewhat normal and/or bimodal distribution should be fairly easy to work with, needing little to no transformation for a linear regression.\n\n\nCorrelation\nFinally, let’s plot a brief correlation matrix to see if there’s any relationships we can explore as a simple linear regression in the next section.\n\n\nCode\ndata %>%\n  cor(data) %>%\n  corrplot(is.corr = FALSE, method=\"number\", tl.cex = .4)\n\n\n\n\n\nThe closer a box is to 1, the higher the correlation. Not particularly exciting news, as it shows there’s not a high correlation between re-hospitalization rates and any of the explanatory variables. This may throw a kink in our analysis – and explain why others haven’t delved deeply into this research!\nPerhaps this step should have been completed first, but nonetheless, we shall continue on with the report. I may pull two more environmental variables, to see if we can find a correlation somewhere. Even so, the sum total of all environmental variables might contribute to re-hospitalization rates as well. I’m just not sure if that – along with control variables – is outside the scope of this report.\nFor Part 2, I’d like to rename the variables to more digestible phrases, and I would like to overhaul the code outputs, to make the tables and visualizations a little easier on the eyes. That’s just polish work, though, and won’t affect the analysis.\nLooking over the Spatz et. al. (2020) article again, the two most significant Built Environment variables (with the highest R2 value) are 1) Long Commute, Driving Alone and 2) Severe Housing Problems. I’m going to scour the SDOH data-set to see what relevant variables match these two and add them into Part 2."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#references",
    "href": "posts/FinalPart1_CalebHill.html#references",
    "title": "Final Part 1",
    "section": "References",
    "text": "References\nBarnett, M., Hsu, J. & McWilliams, M. (2015). “Patient Characteristics and Differences in Hospital Readmission Rates.” JAMA Intern Med., 175(11): 1803-1812.\nLi, Y., Cai, X. & Glance, L. (2015). “Disparities in 30-day rehospitalization rates among Medicare skilled nursing facility residents by race and site of care.” Med Care, 53(12): 1058-1065.\nMurray, F., Allen, M., Clark, C., Daly, C. & Jacobs, D. (2021). “Socio-demographic and -economic factors associated with 30-day readmission for conditions targeted by the hospital readmissions reduction program: a population-based study.” BMC Public Health, 21.\nSpatz, E., Bernheim, S., Horwitz, L. & Herrin, J. (2020). Community factors and hospital wide readmission rates: Does context matter? PLoS One, 15(10)."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html",
    "href": "posts/HW3_ToryBartelloni.html",
    "title": "DACSS 603: Homework 3",
    "section": "",
    "text": "United Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009…We will study the dependence of fertility on ppgdp.\n\n\nCode\nlibrary(alr4)\ndata(UN11)\n\n\n\n\nIdentify the predictor and the response.\nAnswer: The predictor variable in this scenario is ppgdp and the response or dependent variable will be fertility.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nUN11 %>%\n  ggplot(aes(x=ppgdp, y=fertility)) +\n  geom_point() +\n  theme_bw() +\n  labs(title=\"Scatterplot of Fertility and GDP Per Capita\", \n       subtitle=\"Data from United Nations circa 2009\",\n       x=\"GDP Per Capita USD\",\n       y=\"Children per Woman\")\n\n\n\n\n\nThe data do not appear to have a linear relationship. There is a steep decrease in fertility as GDP per capita increases from closer to 0 to about 15,000 and then a much slower rate of decrease to the upper bounds of the data.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nUN11 %>%\n  ggplot(aes(x=log(ppgdp), y=log(fertility))) +\n  geom_point() +\n  theme_bw() +\n  labs(title=\"Scatterplot of Fertility and GDP Per Capita\", \n       subtitle=\"Data from United Nations circa 2009\",\n       x=\"Log of GDP Per Capita USD\",\n       y=\"Log of Children per Woman\")\n\n\n\n\n\nThe relationship with the natural log of both variables appears to have a much closer resemblance to a linear relationship. From appearance only I would be confident a simple linear relationship could be applied to this data."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q2a",
    "href": "posts/HW3_ToryBartelloni.html#q2a",
    "title": "DACSS 603: Homework 3",
    "section": "Q2A",
    "text": "Q2A\nHow, if at all, does the slope of the prediction equation change?\nAnswer: The slope coefficient would decrease because the units used to measure x would increase in the conversion by a factor of ~1.33, but the unit measure of y would stay the same so the same change in y would be observed over “larger” distances of x units."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q2b",
    "href": "posts/HW3_ToryBartelloni.html#q2b",
    "title": "DACSS 603: Homework 3",
    "section": "Q2B",
    "text": "Q2B\nHow, if at all, does the correlation change?\nAnswer: The correlation does not change. The old x and new x are perfectly correlated so would share the same relationship with y."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q5a",
    "href": "posts/HW3_ToryBartelloni.html#q5a",
    "title": "DACSS 603: Homework 3",
    "section": "Q5A",
    "text": "Q5A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\nWe will fist look at the relationship between political ideology and frequency of religious attendance.\n\n\nCode\nstudent.survey.hw %>%\n  ggplot() +\n  geom_point(aes(x=re, y=pi)) +\n  geom_smooth(aes(x=reg2, y=pol2), method=\"lm\") +\n  theme_bw() +\n  labs(title = \"Political Ideology and Religiosity\",\n       subtitle=\"How religious attendance relates to self-identified political ideology\",\n       x=\"Frequency of Religious Attendance\",\n       y=\"Political Ideology\")\n\n\n\n\n\nNow we will look at the relationship between hours of typical number of hours of TV watched (weekly) and high school GPA.\n\n\nCode\nstudent.survey.hw %>%\n  ggplot(aes(x=tv, y=hi)) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  theme_bw() +\n  labs(title = \"High School GPA and TV Watching Habits\",\n       subtitle=\"How typical hours of TV watching relates to high school GPA\",\n       x=\"Hours of TV Watched\\n(Weekly Average)\",\n       y=\"High School GPA\")"
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q5b",
    "href": "posts/HW3_ToryBartelloni.html#q5b",
    "title": "DACSS 603: Homework 3",
    "section": "Q5B",
    "text": "Q5B\nSummarize and interpret results of inferential analyses.\nFirst, let’s examine the political ideology and religious attendance model. Below is the summary of the model.\n\n\nCode\nsummary(pol_reg_mod)\n\n\n\nCall:\nlm(formula = pol2 ~ reg2, data = student.survey.hw)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nreg2          0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThe model is statistically significant, with a < 0.01 p-value for religious attendance. A small amount of the variance in political ideology is explained by the frequency of religious attendance with an R2 of 0.336. The relationship is positive with a one unit increase in religious attendance frequency resulting in an expected increase (toward more conservative) of 0.97 units in political ideology. What we can infer from this model is that people who attend religious services more often are expected to be more conservative in their political ideology. Without more information I would not conclude this is a causal relationship and would want to further examine the relationship, including whehter the relationship could be inferred in the reverse.\nNow let’s examine the high school GPA and TV watching habits model. Below is the summary of the model.\n\n\nCode\nsummary(gpa_tv_mod)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey.hw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThe model here is statistically significant with a p = 0.039 so under the 0.05 standard threshold. On the other hand, the model explans very little of the variation in high school GPA with an R2 of 0.072. The relationship is negative with a one hour increase in average weekly TV watching resulting in an expected decrease in high school GPA of 0.02. With such a low R2 I would hesitate to conclude any strong evidence in an explanatory relationship, while observing that they do have a statistically significant correlation."
  },
  {
    "objectID": "posts/Homework1QH.html",
    "href": "posts/Homework1QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework1QH.html#a",
    "href": "posts/Homework1QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/Homework1QH.html#d",
    "href": "posts/Homework1QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/Homework1QH.html#e",
    "href": "posts/Homework1QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/Homework1QH.html#f",
    "href": "posts/Homework1QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/Homework1QH.html#a-1",
    "href": "posts/Homework1QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/Homework1QH.html#b",
    "href": "posts/Homework1QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/Homework1QH.html#c",
    "href": "posts/Homework1QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/Homework1QH.html#d-1",
    "href": "posts/Homework1QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/Homework1QH.html#e-1",
    "href": "posts/Homework1QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/Homework1QH.html#f-1",
    "href": "posts/Homework1QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html",
    "href": "posts/HW3_ShoshanaBuck.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-1",
    "href": "posts/HW3_ShoshanaBuck.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\n\n1.1.1\nThe predictor is the ppgdp and the response is fertility.\n\n\n1.1.2\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nfertility_ppgdp<- ggplot(data = UN11, aes(x=ppgdp, y= fertility)) + geom_point() + labs(title = \"Fertility vs. ppgdp,United Nations (2011)\")\nfertility_ppgdp\n\n\n\n\n\nThe scatterplot shows the relationship between fertility and the ppgdp. There is a strong correlation between fertility and ppgdp when fertility is high, however, the more ppgdp increases fertility decreases. As a result, a straight-line mean function doesn’t seem plausible for a summary of this graph.\n\n\n1.1.3\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nlog_ppgdp<- ggplot(data = UN11, aes(x = log(ppgdp), y= fertility)) + geom_point() + labs(title = \"Natural Log of fertility vs. ppgdp, United Nations (2011)\")\nlog_ppgdp\n\n\n\n\n\nNow using the log() function with ppgdp the relationship between ppgdp vs. fertility seems to look more like a straight line, making plausible to use a straight-line mean function to summarize the graph.\n\n\nCode\nlog_fertility<- ggplot(data = UN11, aes(x =ppgdp, y= log(fertility))) + geom_point() + labs(title = \"Natural Log of fertility vs. ppgdp, United Nations (2011)\")\nlog_fertility\n\n\n\n\n\nUsing the log() function for fertility shows a similar relationship to the first graph, not making it plausible to use a straight-line mean function to summarize the graph."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-2",
    "href": "posts/HW3_ShoshanaBuck.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nA\nHow, if at all, does the slope of the prediction equation change?\nThe slope of the prediction equation would increase by 1.33 because that is the conversion rate between British pounds sterling and US dollars.\n\n\nB\nHow, if at all, does the correlation change?\nNo, the correlation does not change because it is the standardized version of the slope and the unit of measurement does not affect the slope."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-3",
    "href": "posts/HW3_ShoshanaBuck.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\nwater\n\n\n   Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1  1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2  1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3  1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4  1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5  1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6  1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n7  1954  5.02  1.45    1.77 13.57 12.45   13.32  65356\n8  1955  6.70  7.44    6.51  9.28  9.65    9.80  67909\n9  1956 10.50  5.85    3.38 21.20 18.55   17.42  92715\n10 1957  9.10  6.13    4.08  9.55  9.20    8.25  70024\n11 1958  8.75  5.23    5.90 15.25 14.80   17.48  99216\n12 1959  8.10  3.77    4.56  9.05  6.85    9.56  55786\n13 1960  3.75  1.47    1.78  4.57  6.10    7.65  46153\n14 1961 10.15  5.09    4.86  8.90  7.15    9.00  47947\n15 1962  6.15  3.52    3.30 16.90 14.75   17.68  76877\n16 1963 12.75  8.17   10.16 16.75 11.55   15.53  88443\n17 1964  7.35  4.33    4.85  5.25  7.45    8.20  54634\n18 1965 11.25  6.56    7.60  8.40 13.20   13.29  78806\n19 1966  4.05  1.90    2.00 10.85  8.25   12.56  56542\n20 1967 12.65  6.62    7.14 23.25 17.00   23.66 116244\n21 1968  4.65  3.84    3.34  7.10  6.80    8.28  60857\n22 1969  5.35  3.62    4.62 43.37 24.85   33.07 146345\n23 1970  4.05  1.98    2.94  8.95 11.25   11.00  73726\n24 1971  5.90  5.72    5.42  8.45 10.90   10.82  65530\n25 1972  9.45  4.82    6.79  7.90  7.60    8.06  60772\n26 1973  3.45  2.63    2.88 14.80 14.70   15.86  91696\n27 1974  4.25  2.54    2.36 18.05 16.90   16.42  87377\n28 1975  7.90  4.42    6.78 11.50  9.55   12.56  77306\n29 1976  9.38  8.30    9.70  6.80  5.25    4.73  44756\n30 1977  7.08  4.40    3.90  4.05  4.35    4.60  41785\n31 1978 11.92  5.78    6.70 25.30 20.55   21.94 112653\n32 1979  3.88  2.26    3.10 15.97 11.83   13.88  79975\n33 1980  5.80  3.10    3.34 24.40 19.15   23.78 106821\n34 1981  2.70  2.22    2.48  8.99  9.45   12.14  69177\n35 1982 18.08 11.96   13.02 18.55 18.40   19.45 120463\n36 1983  8.20  4.98    5.76 19.25 22.90   23.86 135043\n37 1984  7.65  5.30    5.74 14.45 13.15   14.42 102001\n38 1985  5.22  4.42    4.04 11.45 10.16   13.06  77790\n39 1986  4.93  3.26    4.58 26.47 15.33   26.46 118144\n40 1987  5.99  2.76    3.98  4.80  6.85    6.36  61229\n41 1988  6.83  6.82    5.18  7.20  9.01    9.88  58942\n42 1989  8.80  5.06    4.92  8.05  9.60    9.58  53965\n43 1990  7.10  5.06    6.05  5.80  6.50    8.41  49774\n\n\n\n\nCode\npairs(water)\n\n\n\n\n\nThere seems to be a positive correlation between the stream runoff (BSAAM) and water availability in three of the perceptitaion measurement sites (OPSLAKE, OPRC, and OPBPC). The other three precipitation measurement sites- (APSLAKE, APSAB, and APMAM ) show a distribution of values without having a positive or negative correlation between the stream runoff and water availability as the graph ."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-4",
    "href": "posts/HW3_ShoshanaBuck.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nRateprof\n\n\n    gender numYears numRaters numCourses pepper discipline\n1     male        7        11          5     no        Hum\n2     male        6        11          5     no        Hum\n3     male       10        43          2     no        Hum\n4     male       11        24          5     no        Hum\n5     male       11        19          7     no        Hum\n6     male       10        15          9     no        Hum\n7     male        7        17          3     no        Hum\n8     male       11        16          3     no        Hum\n9     male       11        12          4     no        Hum\n10    male        7        18          4     no        Hum\n11    male       11        11          4     no        Hum\n12    male        6        33          4    yes        Hum\n13    male        4        23          5     no        Hum\n14    male       10        34          2    yes        Hum\n15    male       11        23          8    yes        Hum\n16    male        3        27          5    yes        Hum\n17    male        1        14          2    yes        Hum\n18    male       11        19          7     no        Hum\n19    male        6        11          5     no        Hum\n20    male       11        14          5     no        Hum\n21    male        7        59          9     no        Hum\n22    male       10        28          5     no        Hum\n23    male        1        13          3     no        Hum\n24    male       10        15          5     no        Hum\n25    male        7        16          2     no        Hum\n26    male       11        35          6     no        Hum\n27    male        7        11          2     no        Hum\n28    male       11        23         10     no        Hum\n29    male       11        27          6     no        Hum\n30    male       11        42          5    yes        Hum\n31    male       11        38          4     no        Hum\n32    male       11        20          8     no        Hum\n33    male       11        44          6     no        Hum\n34    male       11        57          3    yes        Hum\n35    male       10        15          3     no        Hum\n36    male        9        46          5     no        Hum\n37    male       11        10          4     no        Hum\n38    male        3        18          2     no        Hum\n39    male       11        79          7     no        Hum\n40    male       11        18          2     no        Hum\n41    male        3        26          5     no        Hum\n42    male       11        52          2     no        Hum\n43    male       11        10          2     no        Hum\n44    male        7        26          5     no        Hum\n45    male       10        15          3     no        Hum\n46    male       11        45          8     no        Hum\n47    male        3        12          3    yes        Hum\n48    male        7        12          2     no        Hum\n49    male       11        16          3     no        Hum\n50    male        9        36          8     no        Hum\n51    male       11        24          5     no        Hum\n52    male        3        29          4     no        Hum\n53    male       11        65          4     no        Hum\n54    male       11        11          4     no        Hum\n55    male        2        14          4    yes        Hum\n56    male       11        14          7     no        Hum\n57    male       11        29          6     no        Hum\n58    male       11        54          6    yes        Hum\n59    male        4        12          3     no        Hum\n60    male       11        57          5     no        Hum\n61    male        9        13          6     no        Hum\n62    male        5        33          3     no        Hum\n63    male        8        46          7     no        Hum\n64    male       11        19          4     no        Hum\n65  female       11        18          6    yes        Hum\n66  female        9        28          3     no        Hum\n67  female       11        21          7     no        Hum\n68  female        8        26          8    yes        Hum\n69  female       11        10          2     no        Hum\n70  female        2        14          4     no        Hum\n71  female        7        27          6     no        Hum\n72  female       10        22          3     no        Hum\n73  female        4        36          6     no        Hum\n74  female        2        14          4    yes        Hum\n75  female       11        19          6     no        Hum\n76  female       11        86          8     no        Hum\n77  female        8        25          8     no        Hum\n78  female        8        26          7    yes        Hum\n79  female       10        16          2     no        Hum\n80  female        9        20          3     no        Hum\n81  female       11        30          4     no        Hum\n82  female        7        12          3     no        Hum\n83  female       10        21          3     no        Hum\n84  female        1        19          4    yes        Hum\n85  female       11        12          3    yes        Hum\n86  female        4        31          3    yes        Hum\n87  female        4        22          2    yes        Hum\n88  female        4        13          5     no        Hum\n89  female       11        22          5     no        Hum\n90  female       11        26          8     no        Hum\n91  female       11        15          3     no        Hum\n92  female        2        20          5     no        Hum\n93  female       11        24          7     no        Hum\n94  female       11        69          3     no        Hum\n95  female       10        37          6     no        Hum\n96  female        9        36          7     no        Hum\n97  female        7        32          3     no        Hum\n98  female        2        11          3     no        Hum\n99  female       11        41          2     no        Hum\n100 female        7        34          7     no        Hum\n101 female        5        15          4     no        Hum\n102 female       11        38          3     no        Hum\n103 female        2        10          4    yes        Hum\n104 female       11        29          5     no        Hum\n105 female        1        10          4    yes        Hum\n106 female       11        27          5     no        Hum\n107 female       11        36          4     no        Hum\n108 female        9        19          3     no        Hum\n109 female       11        14          2     no        Hum\n110 female        8        17          5     no        Hum\n111 female       10        54          3     no        Hum\n112 female       11        21          5     no        Hum\n113 female        5        33          6     no        Hum\n114 female       11        47          8     no        Hum\n115 female       11        58          5     no        Hum\n116 female        7        14          4    yes        Hum\n117 female       11        10          5     no        Hum\n118 female        9        36          6     no        Hum\n119 female        7        14          5    yes        Hum\n120 female       11        26          5     no        Hum\n121 female        8        39          5     no        Hum\n122 female       11        16          6     no        Hum\n123 female        5        67          3     no        Hum\n124 female        9        29          4     no        Hum\n125 female       11        26          6     no        Hum\n126 female        6        24          7     no        Hum\n127 female        7        16          5    yes        Hum\n128 female        5        20          2     no        Hum\n129 female       10        35          6     no        Hum\n130 female        5        36          9     no        Hum\n131 female        4        28          3     no        Hum\n132 female       11        10          4     no        Hum\n133 female        9        53          7     no        Hum\n134 female        2        13          2     no        Hum\n135   male        7        10          2     no     SocSci\n136   male        7        21          2     no     SocSci\n137   male        2        33          2     no     SocSci\n138   male        7        11          2     no     SocSci\n139   male        6        18          3     no     SocSci\n140   male       11        30          2     no     SocSci\n141   male        7        14          2     no     SocSci\n142   male        4        15          5    yes     SocSci\n143   male       11        50          4     no     SocSci\n144   male        3        10          3     no     SocSci\n145   male       11        30          5     no     SocSci\n146   male       11        53          3     no     SocSci\n147   male       11        46          4    yes     SocSci\n148   male       11        30          5     no     SocSci\n149   male       11        42          7     no     SocSci\n150   male       11        74          8     no     SocSci\n151   male       10        51          4     no     SocSci\n152   male       11        31          4     no     SocSci\n153   male       11        19          2     no     SocSci\n154   male       11        56          4     no     SocSci\n155   male       11        72          7     no     SocSci\n156   male        8        67          4     no     SocSci\n157   male       11        85          7     no     SocSci\n158   male        9        14          1     no     SocSci\n159   male        9        47          7     no     SocSci\n160   male       10        69          2     no     SocSci\n161   male        2        11          2     no     SocSci\n162   male       11        56          6     no     SocSci\n163   male       10        14          5     no     SocSci\n164   male        7        23          7     no     SocSci\n165   male        6        36          5     no     SocSci\n166   male       11        44          8     no     SocSci\n167 female        6        19          4     no     SocSci\n168 female        4        11          5     no     SocSci\n169 female       11        10          1    yes     SocSci\n170 female       10        56          2     no     SocSci\n171 female       11        85          3     no     SocSci\n172 female        2        16          1    yes     SocSci\n173 female        7        39          5    yes     SocSci\n174 female       11        31          4     no     SocSci\n175 female        8        24          3     no     SocSci\n176 female       11        67          3     no     SocSci\n177 female        2        15          3     no     SocSci\n178 female        6        46          3     no     SocSci\n179 female        9        13          6     no     SocSci\n180 female        4        27          3     no     SocSci\n181 female       11        35          3     no     SocSci\n182 female        1        32          4     no     SocSci\n183 female        3        22          2    yes     SocSci\n184 female       11        51          3     no     SocSci\n185 female        8        11          2     no     SocSci\n186 female       10        52          3     no     SocSci\n187 female       10        62          5     no     SocSci\n188 female        3        13          1     no     SocSci\n189 female        2        20          2     no     SocSci\n190 female       11        32          4     no     SocSci\n191 female        5        23          3     no     SocSci\n192 female       11        62          1     no     SocSci\n193 female        5        42          6    yes     SocSci\n194 female       11        24          2     no     SocSci\n195 female        5        26          6     no     SocSci\n196 female        3        12          4    yes     SocSci\n197 female       11        32          4     no     SocSci\n198 female        9        50          5     no     SocSci\n199 female       11        57          4     no     SocSci\n200 female       11        15          4     no     SocSci\n201   male        8        32          4    yes       STEM\n202   male       11        17          9     no       STEM\n203   male       11        23          5     no       STEM\n204   male       11        13          4     no       STEM\n205   male        1        13          2     no       STEM\n206   male        6        18          7     no       STEM\n207   male        1        11          3    yes       STEM\n208   male       10        26          4     no       STEM\n209   male        6        15          4     no       STEM\n210   male       11        25          2     no       STEM\n211   male       11        21          3     no       STEM\n212   male       11        43          3     no       STEM\n213   male       11        51          6     no       STEM\n214   male       11        11          2     no       STEM\n215   male       11        27          2     no       STEM\n216   male       11        37          5     no       STEM\n217   male       11        35          3     no       STEM\n218   male       11        29          2     no       STEM\n219   male        4        30          2     no       STEM\n220   male       11        28          6    yes       STEM\n221   male       11        33          3     no       STEM\n222   male       11        28          6     no       STEM\n223   male       11        62          5     no       STEM\n224   male        5        38          7     no       STEM\n225   male       11        14          1     no       STEM\n226   male        5        25          3     no       STEM\n227   male       11        53          8     no       STEM\n228   male        7        20          3     no       STEM\n229   male       11        21         10     no       STEM\n230   male        6        40          6     no       STEM\n231   male       11        13          3     no       STEM\n232   male       11        67          4     no       STEM\n233   male       11        19          5     no       STEM\n234   male       10        17          2     no       STEM\n235   male       11        22          7     no       STEM\n236   male       11        16          3     no       STEM\n237   male        6        35          6     no       STEM\n238   male        8        38          5     no       STEM\n239   male       11        67          6     no       STEM\n240   male       11        35          4     no       STEM\n241   male       11        32          5     no       STEM\n242   male       11        58          4     no       STEM\n243   male       11        21          3     no       STEM\n244   male       10        12          4     no       STEM\n245   male       11        45          7     no       STEM\n246   male        3        14          2     no       STEM\n247   male        9        41          5     no       STEM\n248   male        1        10          3     no       STEM\n249   male       11        30          3     no       STEM\n250   male       11        25         12     no       STEM\n251   male       11        37          7     no       STEM\n252   male       11        23          4     no       STEM\n253   male       11        65          3     no       STEM\n254   male       11        54          7     no       STEM\n255   male        8        41          5     no       STEM\n256   male        9        13          6     no       STEM\n257   male       11        52          6     no       STEM\n258   male        4        33          8    yes       STEM\n259   male        8        20          6     no       STEM\n260   male        2        13          2     no       STEM\n261   male       11        52          5     no       STEM\n262   male        6        20          3     no       STEM\n263   male        5        39          3     no       STEM\n264   male       11        53          8     no       STEM\n265   male       11        49          7     no       STEM\n266   male       11        49          6     no       STEM\n267   male       11        57          7     no       STEM\n268   male       11        17          5     no       STEM\n269   male       11        30         10     no       STEM\n270   male       11        13          3     no       STEM\n271   male       11        29          6     no       STEM\n272   male       11        29          3     no       STEM\n273   male       11        11          6     no       STEM\n274   male       10        12          2     no       STEM\n275   male       11        18          5     no       STEM\n276   male        3        15          3     no       STEM\n277 female       11        25          3     no       STEM\n278 female        8        10          4     no       STEM\n279 female       11        13          3    yes       STEM\n280 female       11        47          8     no       STEM\n281 female        5        11          6     no       STEM\n282 female       11        18          5     no       STEM\n283 female        4        21          1     no       STEM\n284 female        7        42          2     no       STEM\n285 female        9        33          1     no       STEM\n286 female        3        27          3     no       STEM\n287 female       11        12          3     no       STEM\n288 female       11        45          4     no       STEM\n289 female       11        49          3     no       STEM\n290 female       11        36          3    yes       STEM\n291 female       11        54          4     no       STEM\n292 female       10        31          6     no       STEM\n293 female        6        17          4     no       STEM\n294 female        7        30          4     no       STEM\n295 female        4        11          3     no       STEM\n296 female        2        17          5     no       STEM\n297 female        8        80          8     no       STEM\n298 female       11        39          4     no       STEM\n299 female       11        26          4     no       STEM\n300 female       11        29          4     no       STEM\n301 female       11        60          7     no       STEM\n302 female        1        16          3     no       STEM\n303 female        3        10          5     no       STEM\n304   male        3        14          2     no   Pre-prof\n305   male        6        14          7     no   Pre-prof\n306   male        7        12          4     no   Pre-prof\n307   male       11        13          3     no   Pre-prof\n308   male       11        11          5     no   Pre-prof\n309   male        8        39          2     no   Pre-prof\n310   male        8        32          3    yes   Pre-prof\n311   male        2        10          2     no   Pre-prof\n312   male       10        21          5    yes   Pre-prof\n313   male       11        10          3     no   Pre-prof\n314   male        7        12          3     no   Pre-prof\n315   male        5        15          3    yes   Pre-prof\n316   male       11        53          6     no   Pre-prof\n317   male        8        68          3     no   Pre-prof\n318   male        7        48          4     no   Pre-prof\n319   male        2        10          1     no   Pre-prof\n320   male       11        36          3     no   Pre-prof\n321   male        8        15          2    yes   Pre-prof\n322   male        3        32          3    yes   Pre-prof\n323   male        9        23          5     no   Pre-prof\n324   male        4        14          4     no   Pre-prof\n325   male        8        12          4     no   Pre-prof\n326   male       11        31          2     no   Pre-prof\n327   male       11        24          3    yes   Pre-prof\n328   male        8        33          4     no   Pre-prof\n329   male        6        50          2    yes   Pre-prof\n330   male        8        11          6     no   Pre-prof\n331   male       11        19          3     no   Pre-prof\n332   male        6        19          1     no   Pre-prof\n333   male       11        57          5     no   Pre-prof\n334   male       11        34          3     no   Pre-prof\n335   male        6        16          2     no   Pre-prof\n336   male        8        29          3     no   Pre-prof\n337   male       11        45          6     no   Pre-prof\n338   male       11        21          4     no   Pre-prof\n339 female        9        10          1     no   Pre-prof\n340 female        7        10          4     no   Pre-prof\n341 female       11        24          3     no   Pre-prof\n342 female        5        10          2     no   Pre-prof\n343 female        7        10          2     no   Pre-prof\n344 female       11        16          4     no   Pre-prof\n345 female       11        16          2     no   Pre-prof\n346 female       11        10          5     no   Pre-prof\n347 female        2        15          1     no   Pre-prof\n348 female        2        29          2    yes   Pre-prof\n349 female        3        24          2     no   Pre-prof\n350 female        2        11          4     no   Pre-prof\n351 female       11        22          9    yes   Pre-prof\n352 female       11        62          3     no   Pre-prof\n353 female       11        21          3     no   Pre-prof\n354 female        9        53          4     no   Pre-prof\n355 female        5        12          2     no   Pre-prof\n356 female       11        11          2     no   Pre-prof\n357 female       11        38          2     no   Pre-prof\n358 female       11        46          4     no   Pre-prof\n359 female       11        10          3     no   Pre-prof\n360 female        3        24          4     no   Pre-prof\n361 female       11        27          4     no   Pre-prof\n362 female        5        21          3     no   Pre-prof\n363 female        4        15          2     no   Pre-prof\n364 female        2        10          3     no   Pre-prof\n365 female        9        11          5     no   Pre-prof\n366 female        2        11          4     no   Pre-prof\n                           dept  quality helpfulness  clarity easiness\n1                       English 4.636364    4.636364 4.636364 4.818182\n2             Religious Studies 4.318182    4.545455 4.090909 4.363636\n3                           Art 4.790698    4.720930 4.860465 4.604651\n4                       English 4.250000    4.458333 4.041667 2.791667\n5                       Spanish 4.684211    4.684211 4.684211 4.473684\n6                       Spanish 4.233333    4.266667 4.200000 4.533333\n7                       Spanish 4.382353    4.352941 4.411765 4.117647\n8                       English 2.062500    2.062500 2.062500 1.437500\n9                         Music 2.041667    2.166667 2.000000 1.750000\n10                      English 4.111111    4.222222 4.000000 3.666667\n11                   Philosophy 4.727273    4.909091 4.545455 4.000000\n12                   Philosophy 3.724242    3.848485 3.606060 4.242424\n13                        Music 2.804348    2.695652 2.913043 2.217391\n14                        Music 4.838235    4.823529 4.852941 4.676471\n15                      Spanish 4.565217    4.565217 4.565217 2.826087\n16            Religious Studies 4.944444    4.962963 4.925926 3.703704\n17                      English 4.464286    4.714286 4.214286 3.214286\n18                      History 4.184211    4.368421 4.000000 3.631579\n19                          Art 3.909091    4.090909 3.727273 2.272727\n20                          Art 3.500000    3.285714 3.714286 3.285714\n21                   Philosophy 3.474576    3.542373 3.406780 2.355932\n22                      English 3.696429    3.714286 3.678571 3.642857\n23                      English 3.576923    3.461538 3.692308 4.615385\n24                   Philosophy 1.633333    1.600000 1.666667 2.266667\n25                      History 3.531250    3.312500 3.750000 2.562500\n26                      History 3.114286    3.057143 3.171429 3.857143\n27                        Music 4.909091    5.000000 4.818182 4.181818\n28                      English 4.239130    4.434783 4.043478 2.826087\n29                      Spanish 3.981481    4.037037 3.925926 3.148148\n30                      English 4.392857    4.500000 4.285714 3.166667\n31                      History 4.526316    4.473684 4.578947 3.131579\n32                       German 4.075000    4.150000 4.000000 3.250000\n33                   Philosophy 2.829546    3.022727 2.636364 2.045454\n34            Religious Studies 4.552632    4.561404 4.543860 3.614035\n35                        Music 3.700000    3.600000 3.800000 3.133333\n36                      English 1.891304    2.239130 1.543478 3.869565\n37                       French 4.277778    4.222222 4.333333 3.888889\n38                      English 4.416667    4.611111 4.222222 3.111111\n39                      History 2.569620    2.696203 2.443038 2.658228\n40                      Theater 3.472222    3.111111 3.833333 3.388889\n41                      English 2.442308    2.692308 2.192308 2.230769\n42            Religious Studies 3.067308    3.403846 2.692308 3.076923\n43            Religious Studies 3.600000    3.600000 3.600000 3.200000\n44                      Spanish 4.115385    4.230769 4.000000 2.884615\n45                      English 2.900000    3.200000 2.600000 3.800000\n46                      English 3.388889    3.711111 3.111111 2.844444\n47                   Philosophy 4.708334    4.750000 4.666666 3.666667\n48                         FLTR 3.291667    3.333333 3.250000 3.333333\n49                      History 2.250000    2.062500 2.437500 2.812500\n50                      English 3.555556    3.722222 3.388889 2.777778\n51                       French 3.854167    3.875000 3.833333 3.000000\n52                      History 3.913793    3.965517 3.862069 2.965517\n53                      History 3.469231    3.353846 3.584615 3.061538\n54                          Art 3.045455    3.090909 3.000000 3.363636\n55                      Spanish 3.464286    3.285714 3.642857 3.357143\n56                       German 4.000000    4.000000 4.000000 2.500000\n57                        Music 3.982759    3.862069 4.103448 3.310345\n58                      History 4.518519    4.518519 4.518519 4.074074\n59                      Theater 2.958333    2.833333 3.083333 2.500000\n60                      History 2.085965    2.280702 1.894737 2.631579\n61                      Theater 3.538462    3.461538 3.615385 3.230769\n62                      English 2.924242    2.696970 3.151515 3.454545\n63                      English 2.293478    2.282609 2.304348 2.456522\n64                      English 3.684211    3.684211 3.684211 3.157895\n65                       German 4.666666    4.888889 4.444445 3.055556\n66               Womens Studies 4.732143    4.857143 4.607143 4.500000\n67               Womens Studies 4.880952    4.952381 4.809524 4.809524\n68                      English 4.653846    4.615385 4.692308 4.076923\n69                      History 3.100000    3.500000 2.700000 3.000000\n70                      Spanish 2.107143    1.928571 2.285714 2.000000\n71                      Spanish 4.180000    4.320000 4.040000 3.720000\n72                      English 3.500000    3.500000 3.500000 2.272727\n73                      English 4.625000    4.666667 4.583333 4.527778\n74                      Spanish 4.428571    4.642857 4.214286 4.000000\n75                      English 2.394737    2.263158 2.526316 1.736842\n76                      English 1.674419    1.686047 3.360465 1.558140\n77                      English 3.760000    3.840000 3.680000 2.920000\n78                      English 4.538462    4.769231 4.307692 3.500000\n79            Religious Studies 1.875000    2.187500 1.562500 2.687500\n80            Religious Studies 4.025000    3.950000 4.100000 2.650000\n81                      Spanish 3.550000    3.666667 3.433333 1.966667\n82                      Spanish 3.166667    3.333333 3.000000 3.916667\n83                      English 2.523810    2.428571 2.619048 1.857143\n84                      Spanish 3.710526    3.894737 3.526316 2.789474\n85                          Art 4.458333    4.500000 4.416667 4.166667\n86                      Spanish 4.709677    4.806452 4.612903 3.645161\n87                      English 4.500000    4.545455 4.454545 3.636364\n88                       French 3.461538    3.307692 3.615385 2.923077\n89                       French 4.545455    4.545455 4.545455 3.500000\n90                      English 3.500000    3.807692 3.192308 3.692308\n91                      English 2.200000    2.000000 2.400000 2.400000\n92                      English 3.925000    4.000000 3.500000 3.850000\n93                      History 3.958333    3.916667 4.000000 2.750000\n94            Religious Studies 2.782609    3.000000 2.565218 3.768116\n95                      English 1.756757    1.810811 1.702703 1.783784\n96                      English 4.851351    4.945946 4.756757 3.729730\n97                      English 3.218750    3.343750 3.093750 2.156250\n98                      Spanish 3.312500    3.416667 3.208333 2.458333\n99               Womens Studies 3.695122    3.731707 3.658537 3.390244\n100                 Art History 3.794118    3.794118 3.794118 4.088235\n101                     Spanish 3.233333    3.266667 3.200000 3.400000\n102                     Spanish 3.236842    3.631579 2.842105 3.421053\n103                     English 4.700000    4.700000 4.700000 3.700000\n104                     English 1.482759    1.517241 1.448276 2.068966\n105                     Spanish 3.950000    3.900000 4.000000 3.400000\n106                     English 3.537037    3.629630 3.444444 2.814815\n107                     English 3.208333    3.222222 3.194444 2.083333\n108                     English 2.394737    2.684211 2.105263 1.789474\n109                       Dance 3.500000    3.714286 3.285714 3.500000\n110                     English 3.470588    3.529412 3.411765 3.294118\n111                     English 4.305556    4.259259 4.351852 3.203704\n112                     English 3.095238    3.142857 3.047619 3.857143\n113                  Philosophy 2.469697    2.454545 2.484848 2.666667\n114                     English 2.904255    3.021277 2.787234 3.361702\n115                     History 3.189655    3.206897 3.172414 3.672414\n116                     Spanish 4.678571    4.714286 4.642857 4.071429\n117              Art and design 3.200000    2.800000 3.600000 2.000000\n118                     History 3.833333    3.805556 3.861111 2.638889\n119                       Music 3.785714    3.500000 4.071429 3.285714\n120                       Music 3.442308    3.346154 3.500000 4.038462\n121                     History 2.589744    2.641026 2.538462 3.923077\n122                     History 2.437500    2.375000 2.500000 3.187500\n123                     English 2.977612    2.955224 3.000000 2.373134\n124                     English 3.224138    3.172414 3.275862 2.965517\n125                      German 2.750000    2.961539 2.538461 3.038461\n126                     English 3.729167    4.041666 3.416667 3.166667\n127                    Japanese 4.625000    4.812500 4.437500 3.062500\n128                     English 3.825000    4.000000 3.650000 2.550000\n129                     History 3.200000    3.171429 3.228571 3.028571\n130                     English 3.347222    3.333333 3.361111 3.500000\n131                     English 3.000000    3.074074 2.851852 3.592593\n132                       Music 4.100000    4.100000 4.100000 2.600000\n133                     English 2.792453    2.698113 2.886792 2.735849\n134                     Spanish 3.076923    3.076923 3.076923 3.461538\n135                  Psychology 4.800000    4.900000 4.700000 4.400000\n136               Communication 4.571429    4.571429 4.571429 4.238095\n137                  Psychology 4.757576    4.818182 4.696970 3.969697\n138               Communication 3.636364    3.545455 3.727273 3.727273\n139               Communication 3.583333    3.222222 3.944444 3.833333\n140                  Psychology 4.400000    4.366667 4.433333 4.133333\n141               Communication 4.071429    4.071429 4.142857 3.142857\n142                  Psychology 4.333333    4.333333 4.333333 2.933333\n143                   Sociology 4.250000    4.220000 4.280000 2.480000\n144                   Geography 4.150000    4.100000 4.200000 2.800000\n145           Political Science 3.366667    3.566667 3.166667 2.366667\n146                Anthropology 4.490566    4.283019 4.698113 3.603774\n147           Political Science 4.130435    3.934783 4.326087 3.217391\n148                  Psychology 2.700000    2.866667 2.533333 1.633333\n149           Political Science 3.142857    3.119048 3.166667 2.547619\n150                   Geography 2.621622    2.662162 2.581081 2.175676\n151                  Psychology 4.372549    4.176471 4.568627 3.588235\n152                  Psychology 1.467742    1.483871 1.451613 2.483871\n153                  Psychology 3.421053    3.578947 3.263158 4.368421\n154                Anthropology 2.419643    2.375000 2.464286 2.142857\n155           Political Science 3.833333    3.805556 3.861111 2.611111\n156           Political Science 4.216418    4.104477 4.328358 2.537314\n157           Political Science 2.494118    2.764706 2.223529 2.082353\n158                  Psychology 4.178571    4.142857 4.214286 2.428571\n159                   Sociology 2.872340    3.340426 2.404255 3.808511\n160                   Sociology 3.086957    3.420290 2.753623 3.579710\n161                Anthropology 3.409091    3.454545 3.363636 3.818182\n162           Political Science 2.535714    2.446429 2.625000 2.500000\n163               Communication 1.604167    1.500000 1.708333 2.083333\n164               Communication 2.630435    3.173913 2.086957 3.173913\n165               Communication 2.916667    3.138889 2.694444 3.500000\n166                   Sociology 2.733333    2.622222 2.844444 3.177778\n167     Communication Disorders 4.684211    4.894737 4.473684 4.210526\n168               Communication 4.000000    4.272727 3.727273 3.636364\n169               Communication 4.600000    4.700000 4.500000 4.500000\n170                   Sociology 4.544643    4.517857 4.571429 4.303571\n171                  Psychology 4.311765    4.305882 4.317647 4.435294\n172                  Psychology 4.937500    4.937500 4.937500 4.000000\n173                  Psychology 4.115385    4.179487 4.051282 2.410256\n174               Communication 2.435484    2.451613 2.419355 1.870968\n175                  Psychology 4.574074    4.703704 4.444444 2.888889\n176                   Sociology 3.880597    3.746269 4.029851 3.910448\n177                   Sociology 3.666667    3.866667 3.466667 3.533333\n178                   Sociology 4.195652    4.413043 3.978261 3.869565\n179                Anthropology 4.153846    4.384615 3.923077 4.076923\n180               Communication 2.000000    1.962963 2.037037 3.185185\n181               Communication 3.742857    3.714286 3.771429 2.885714\n182                   Sociology 2.171875    2.437500 1.906250 4.093750\n183                  Psychology 4.295455    4.318182 4.272727 3.954545\n184                  Psychology 3.686275    3.686275 3.686275 2.764706\n185               Communication 3.636364    4.090909 3.181818 3.363636\n186                  Psychology 4.076923    4.076923 4.076923 3.423077\n187                Anthropology 3.435484    3.274194 3.596774 2.322581\n188               Communication 2.730769    2.230769 3.230769 3.461538\n189                  Psychology 3.625000    3.550000 3.700000 3.500000\n190                  Psychology 4.562500    4.468750 4.656250 3.875000\n191                   Sociology 3.173913    3.130435 3.217391 3.173913\n192               Communication 3.741935    3.612903 3.870968 3.483871\n193                  Psychology 4.535714    4.547619 4.523810 2.500000\n194               Communication 4.083333    3.958333 4.208333 3.333333\n195     Communication Disorders 2.807692    2.730769 2.884615 3.576923\n196                   Geography 3.791667    3.500000 4.083334 3.000000\n197               Communication 3.343750    3.218750 3.468750 2.718750\n198                  Psychology 2.580000    2.600000 2.560000 2.860000\n199           Political Science 2.728070    2.526316 2.929825 2.684210\n200               Communication 3.966667    3.800000 4.133333 3.400000\n201                        Math 4.921875    5.000000 4.843750 3.812500\n202                        Math 2.529412    2.411765 2.647059 2.235294\n203                   Chemistry 2.565218    2.565218 2.565218 1.391304\n204                   Chemistry 4.269231    4.461538 4.076923 2.615385\n205                        Math 3.730769    3.692308 3.769231 3.846154\n206                        Math 2.638889    2.833333 2.444444 2.055556\n207                     Geology 4.954545    4.909091 5.000000 4.363636\n208                   Chemistry 4.480769    4.384615 4.576923 3.038462\n209                     Biology 2.646667    2.800000 2.533333 2.000000\n210                     Physics 4.720000    4.760000 4.680000 2.560000\n211                   Chemistry 2.619048    2.904762 2.333333 1.571429\n212           Astronomy/Physics 4.918605    5.000000 4.837209 3.279070\n213                        Math 4.549020    4.627451 4.470588 4.039216\n214                     Biology 4.045455    4.090909 4.000000 3.000000\n215                   Chemistry 4.796296    4.925926 4.666667 3.148148\n216                   Chemistry 3.067568    3.108108 3.027027 2.189189\n217                     Biology 3.385714    3.028571 3.742857 1.628571\n218                   Chemistry 4.000000    3.655172 4.344828 2.137931\n219                   Chemistry 1.750000    2.166667 1.333333 2.666667\n220            Computer Science 4.321429    4.392857 4.250000 2.714286\n221                     Physics 4.666667    4.757576 4.575758 3.212121\n222                        Math 2.714286    2.892857 2.535714 2.750000\n223                        Math 3.338710    3.645161 3.048387 3.193548\n224                        Math 3.986842    4.026316 3.947368 3.842105\n225       Physics and Astronomy 3.821429    4.071429 3.571429 3.214286\n226                     Geology 3.482759    3.482759 3.482759 3.206897\n227                   Chemistry 3.490566    3.924528 3.056604 3.000000\n228                     Geology 4.625000    4.600000 4.650000 3.700000\n229         Physics & Astronomy 3.452381    3.523810 3.380952 2.238095\n230                     Biology 2.287500    2.450000 2.125000 1.900000\n231                     Biology 3.038462    3.384615 2.692308 2.230769\n232                     Biology 4.365672    4.447761 4.283582 1.820896\n233                     Physics 3.684211    3.947368 3.421053 2.157895\n234                     Geology 3.500000    3.294118 3.705882 2.823529\n235                     Physics 4.159091    4.363636 3.954545 3.090909\n236                        Math 3.562500    3.437500 3.687500 3.375000\n237                   Chemistry 3.562500    3.437500 3.687500 3.375000\n238                     Geology 3.960526    4.000000 3.921053 2.578947\n239                        Math 4.231343    4.164179 4.298507 4.164179\n240                     Biology 2.671429    2.885714 2.457143 1.971429\n241                        Math 4.015625    3.937500 4.093750 4.406250\n242                     Biology 3.103448    3.275862 2.931034 2.224138\n243       Physics and Astronomy 4.166667    4.428571 3.904762 3.095238\n244                     Biology 2.791667    2.750000 2.833333 1.750000\n245                        Math 3.966667    4.088889 3.844444 2.533333\n246                     Geology 4.107143    4.142857 4.142857 3.071429\n247                   Chemistry 3.329268    3.512195 3.146341 3.000000\n248                     Geology 2.800000    2.900000 2.700000 2.400000\n249                     Biology 4.066667    3.900000 4.233333 2.933333\n250            Computer Science 4.620000    4.720000 4.520000 2.480000\n251                        Math 3.000000    3.081081 2.918919 3.054054\n252                   Chemistry 2.586957    2.608696 2.565217 2.695652\n253                        Math 1.915385    2.076923 1.723077 2.107692\n254                     Geology 4.527778    4.537037 4.518519 2.888889\n255                   Chemistry 3.890244    4.073171 3.707317 2.634146\n256                        Math 4.384615    4.461538 4.307692 3.846154\n257                        Math 3.451923    3.596154 3.269231 3.692308\n258                        Math 3.878788    4.030303 3.727273 3.515152\n259                     Physics 3.950000    3.900000 4.000000 3.200000\n260                   Chemistry 4.846154    4.923077 4.769231 3.769231\n261                        Math 2.865385    2.826923 2.903846 2.250000\n262                   Chemistry 3.000000    3.400000 2.600000 2.300000\n263                     Biology 3.166667    3.333333 3.000000 2.153846\n264                        Math 3.264151    3.339623 3.188679 3.528302\n265                        Math 2.928571    3.081633 2.775510 3.591837\n266         Physics & Astronomy 4.265306    4.265306 4.265306 3.204082\n267                        Math 2.508772    2.561404 2.473684 3.017544\n268                   Chemistry 4.235294    4.235294 4.235294 2.705882\n269            Computer Science 2.850000    2.933333 2.766667 2.200000\n270                     Geology 3.307692    3.461539 3.153846 2.769231\n271                        Math 2.620690    2.758621 2.482759 3.034483\n272                   Chemistry 3.103448    3.172414 3.034483 2.758621\n273            Computer Science 3.727273    4.000000 3.454545 3.272727\n274                     Geology 2.375000    2.416667 2.333333 2.750000\n275                        Math 3.722222    3.944444 3.500000 3.500000\n276                   Chemistry 2.500000    2.733333 2.266667 2.733333\n277                        Math 4.940000    5.000000 4.880000 4.800000\n278                     Biology 2.900000    3.100000 2.700000 1.500000\n279                     Biology 4.692308    4.769231 4.615385 4.230769\n280                        Math 3.436170    3.531915 3.340425 4.744681\n281                        Math 3.772727    3.818182 3.727273 3.727273\n282                   Chemistry 2.750000    2.777778 2.722222 2.444444\n283                   Chemistry 2.619048    3.190476 2.047619 2.523810\n284                     Geology 4.285714    4.452381 4.119048 3.619048\n285                     Biology 4.015152    4.181818 3.848485 1.878788\n286                        Math 4.981481    5.000000 4.962963 3.962963\n287                        Math 4.583333    4.500000 4.666667 3.666667\n288                     Biology 2.422222    2.533333 2.311111 2.022222\n289                        Math 2.336735    2.591837 2.081633 2.183673\n290                     Biology 4.500000    4.555556 4.444444 3.305556\n291                     Geology 3.555556    3.555556 3.555556 2.500000\n292                        Math 4.064516    4.096774 4.032258 3.258065\n293                     Biology 3.529412    3.470588 3.588235 2.117647\n294                     Biology 4.233333    4.366667 4.100000 2.933333\n295                     Biology 3.454545    3.363636 3.545455 2.636364\n296                        Math 2.352941    2.411765 2.294118 2.000000\n297                        Math 2.087500    2.375000 1.800000 2.387500\n298                     Physics 3.653846    4.076923 3.230769 2.794872\n299                     Biology 2.153846    2.346154 1.961538 2.115385\n300            Computer Science 2.655172    2.689655 2.620690 2.655172\n301                        Math 2.608333    2.716667 2.466667 3.483333\n302       Physics and Astronomy 2.406250    2.375000 2.437500 1.937500\n303                        Math 2.900000    3.200000 2.600000 3.100000\n304                        Kins 2.392857    2.142857 2.642857 1.428572\n305                        Kins 4.892857    4.928571 4.857143 4.857143\n306                        Kins 2.958333    3.000000 2.916667 3.833333\n307                        Kins 2.000000    2.000000 2.000000 2.846154\n308                    Business 3.954545    4.363636 3.545455 3.636364\n309                   Economics 4.294872    4.384615 4.205128 3.769231\n310                        Kins 4.453125    4.343750 4.562500 4.625000\n311                  Accounting 4.200000    4.600000 4.200000 2.400000\n312                        Kins 4.904762    4.857143 4.952381 4.809524\n313                     Finance 3.100000    3.500000 2.700000 1.900000\n314 Environmental Public Health 3.166667    3.166667 3.250000 3.583333\n315         Information Systems 3.433333    3.533333 3.333333 3.600000\n316                   Economics 4.811321    4.849057 4.773585 4.132075\n317                   Economics 3.919118    4.250000 3.588235 2.911765\n318                  Accounting 4.062500    4.229167 3.895833 1.750000\n319                    Business 2.600000    3.100000 2.200000 4.300000\n320                    Business 2.791667    2.750000 2.777778 2.194444\n321                     Finance 4.166667    4.266667 4.066667 3.000000\n322         Information Systems 4.250000    4.218750 4.281250 4.125000\n323                   Economics 4.260870    4.478261 4.043478 4.130435\n324                    Business 4.178571    4.071429 4.285714 3.142857\n325          Managerial Science 2.916667    3.000000 2.833333 2.333333\n326                  Accounting 2.919355    2.645161 3.193548 2.322581\n327                   Marketing 4.562500    4.791667 4.333333 3.666667\n328                   Economics 4.393939    4.424242 4.363636 3.454545\n329            Criminal Justice 4.640000    4.560000 4.720000 3.940000\n330  Curriculum and Instruction 2.590909    2.636364 2.545454 2.272727\n331             Library Science 4.394737    4.473684 4.315789 3.368421\n332                     Finance 3.447368    3.105263 3.789474 2.473684\n333                   Economics 3.719298    3.912281 3.526316 2.561404\n334                    Business 2.720588    2.764706 2.676471 2.647059\n335                    Business 3.687500    3.812500 3.562500 3.437500\n336           Special Education 3.136364    3.000000 3.272727 2.090909\n337                   Economics 4.288889    4.466667 4.111111 3.155556\n338                 Social Work 3.095238    2.809524 3.380952 2.666667\n339                    Business 4.050000    3.900000 4.200000 4.900000\n340           Special Education 4.850000    4.900000 4.800000 4.900000\n341                  Accounting 4.861111    4.833333 4.888889 4.722222\n342                        Kins 4.850000    4.800000 4.900000 4.000000\n343                  Management 3.350000    3.600000 3.100000 3.700000\n344                   Marketing 3.562500    3.687500 3.437500 4.125000\n345                    Business 4.687500    4.625000 4.750000 3.500000\n346                   Marketing 3.600000    3.600000 3.600000 2.700000\n347          Managerial Science 3.433333    3.666667 3.200000 3.200000\n348                   Economics 4.120690    4.344828 3.896552 4.034483\n349                   Economics 4.083330    4.583330 3.583330 4.166670\n350                 Social Work 3.181818    3.272727 3.090909 3.545455\n351                        Kins 4.340909    4.545455 4.090909 4.454545\n352                  Accounting 3.491935    3.612903 3.370968 2.467742\n353                  Management 4.500000    4.428571 4.571429 3.619048\n354                   Economics 3.584906    3.415094 3.754717 3.037736\n355                        Kins 3.708333    3.583333 3.833333 4.250000\n356                  Accounting 3.318182    3.090909 3.545455 2.181818\n357                   Economics 3.513158    3.657895 3.368421 3.868421\n358                   Economics 4.228261    4.304348 4.152174 3.369565\n359                        Kins 1.900000    1.900000 1.900000 3.800000\n360                   Economics 1.937500    2.250000 1.625000 2.333333\n361                    Business 3.462963    3.333333 3.592593 3.111111\n362                 Social Work 2.619048    2.714286 2.523810 3.619048\n363                  Accounting 2.966667    3.066667 2.866667 2.666667\n364                  Accounting 3.250000    3.200000 3.300000 3.000000\n365                     Nursing 1.909091    1.909091 1.909091 2.272727\n366                   Marketing 1.409091    1.363636 1.454545 2.636364\n    raterInterest  sdQuality sdHelpfulness sdClarity sdEasiness sdRaterInterest\n1        3.545455 0.55185640     0.6741999 0.5045250  0.4045199       1.1281521\n2        4.000000 0.90201795     0.9341987 0.9438798  0.5045250       1.0744356\n3        3.432432 0.45293432     0.6663898 0.4129681  0.5407021       1.2369438\n4        3.181818 0.93250483     0.9315329 0.9990938  0.5882300       1.3322506\n5        4.214286 0.65001124     0.8200699 0.5823927  0.6117753       0.9749613\n6        3.916667 0.86327170     1.0327956 0.7745967  0.6399405       0.6685579\n7        3.812500 0.94421613     0.9963167 0.9393364  0.6966305       1.2230427\n8        2.937500 1.19547760     1.3400871 1.1236103  0.7274384       1.6111590\n9        3.750000 1.07573090     1.3371159 1.0444659  0.7537783       1.2880570\n10       4.176471 0.90025413     1.0032627 0.9074852  0.7669650       1.5381123\n11       2.900000 0.34377584     0.3015113 0.5222330  0.7745967       1.1972190\n12       3.333333 1.23870660     1.3257359 1.2975793  0.7917663       1.2909944\n13       3.217391 0.98556780     1.0632191 1.1246431  0.7952428       1.1660548\n14       4.718750 0.51816386     0.6262243 0.4357058  0.8060600       0.5811210\n15       3.952381 0.99206334     1.0368697 0.9920634  0.8340577       1.0712698\n16       3.592593 0.16012815     0.1924501 0.2668803  0.8688992       1.1522306\n17       2.357143 0.60333323     0.4688072 0.8925824  0.8925824       1.1507284\n18       3.526316 0.69142620     0.6839856 0.8164966  0.8950808       1.1239030\n19       4.000000 1.26131250     1.4459976 1.1908744  0.9045340       1.0000000\n20       3.230769 1.25575600     1.3827827 1.2666473  0.9138736       1.2351685\n21       3.537037 1.59585600     1.6433989 1.5878705  0.9240599       1.3418231\n22       3.250000 1.23482860     1.2724180 1.3348206  0.9511898       1.3228756\n23       2.923077 1.57911040     1.6641006 1.6012815  0.9607689       1.4978617\n24       3.400000 0.71879530     0.9102590 1.0465362  0.9611501       1.2983506\n25       3.800000 0.93930380     1.1383468 0.9309493  0.9639329       0.9411239\n26       3.000000 1.27236300     1.3271566 1.3823619  0.9744639       1.5811388\n27       4.909091 0.20225996     0.0000000 0.4045199  0.9816498       0.3015113\n28       3.333333 1.12683750     1.1609591 1.2605288  0.9840627       1.3284223\n29       4.153846 1.08735290     1.1596247 1.1410496  0.9885383       0.8338972\n30       3.516129 1.00932060     1.1529390 0.9947598  1.0101115       1.3384311\n31       4.029412 0.62544435     0.6872130 0.7580765  1.0179750       1.0294245\n32       3.631579 1.38862860     1.3484884 1.4867839  1.0195458       1.1160708\n33       3.475000 1.35519330     1.5017607 1.3655378  1.0332731       1.3772417\n34       3.901961 0.72999640     0.8867586 0.8252743  1.0480319       1.1533413\n35       3.933333 1.26491100     1.2983506 1.3201731  1.0600988       0.7988086\n36       2.347826 1.03769530     1.4634022 0.8084697  1.0668478       1.1588767\n37       3.882353 0.84404874     1.0602750 0.7669650  1.0786096       0.9926198\n38       3.500000 0.57522374     0.9785276 0.7320845  1.0786096       0.9851844\n39       2.956522 1.17597260     1.3141676 1.2273260  1.0846963       1.3109806\n40       4.000000 1.18162680     1.4095844 1.0981267  1.0921586       1.0954451\n41       2.692308 1.37351320     1.5942203 1.2967415  1.1066234       1.2890068\n42       3.442308 1.14630200     1.2408033 1.2452020  1.1175280       1.1784580\n43       4.300000 1.50554530     1.5055453 1.5776213  1.1352924       0.4830459\n44       4.320000 0.86380196     0.8629110 1.0954451  1.1428709       0.9882645\n45       2.733333 1.28452320     1.3732131 1.3522468  1.1464230       1.3870146\n46       3.185185 1.42577290     1.4866408 1.5109031  1.1472409       1.3877773\n47       3.416667 0.54181236     0.4522670 0.6513389  1.1547005       1.4433757\n48       3.416667 1.33923880     1.3026779 1.5447860  1.1547005       1.4433757\n49       3.200000 0.96609180     1.1236103 1.0935416  1.1672617       1.2649110\n50       3.290322 1.40802690     1.4660171 1.4595063  1.1737878       1.2556325\n51       4.227273 1.27244340     1.2958965 1.3405601  1.1795356       0.9725675\n52       4.320000 1.40196910     1.4010904 1.4814119  1.1796718       0.9451631\n53       3.280702 1.28049350     1.3855366 1.3450407  1.1842313       1.3059532\n54       3.636364 1.42222620     1.6403991 1.2649111  1.2060454       0.9244163\n55       2.785714 1.51231200     1.7288756 1.3926810  1.2157393       1.4238934\n56       3.875000 1.19292780     1.4142136 1.0377490  1.2247449       0.6408699\n57       4.333333 1.24987690     1.4814119 1.2633523  1.2846191       1.2038585\n58       4.340909 0.70015470     0.7458246 0.8182065  1.2863852       1.0330173\n59       3.500000 1.23322070     1.4034589 1.3113722  1.3142575       1.0000000\n60       3.166667 1.15177480     1.2642670 1.2054076  1.3447196       1.4241846\n61       3.800000 1.16299780     1.5607362 0.8697185  1.3634421       1.6193277\n62       2.696970 1.28160800     1.3803271 1.3257359  1.3713795       1.2370542\n63       3.806452 1.33988430     1.4089089 1.3311576  1.3777038       1.3271361\n64       2.933333 1.18098970     1.2495613 1.2495613  1.3849652       1.1629192\n65       3.937500 0.42008403     0.3233808 0.7047922  1.0556415       0.7719024\n66       3.333333 0.44058344     0.4483951 0.5669467  0.5091751       1.2089410\n67       3.428571 0.21821790     0.2182179 0.4023739  0.5117663       1.2478553\n68       3.550000 0.62879616     0.6972473 0.6793662  0.6275716       1.0505954\n69       2.800000 1.12546290     1.3540064 1.0593499  0.6666667       1.2292726\n70       3.166667 1.04105290     1.2066665 1.1387288  0.6793662       1.4034589\n71       3.840000 0.77567180     0.8524475 0.8888194  0.7371115       0.9865766\n72       3.000000 1.38873020     1.5039630 1.4392458  0.7672969       1.4832397\n73       3.722222 0.59009683     0.6324555 0.7319251  0.7740842       1.0852547\n74       3.214286 0.26726124     0.4972452 0.5789342  0.7844645       1.1217138\n75       3.066667 1.51454940     1.6276126 1.6113632  0.8056816       1.4375906\n76       2.814815 1.07585110     1.1808435 1.0584168  0.8059288       1.2258784\n77       3.631579 1.20864940     1.2330483 1.2489996  0.8124038       1.2115429\n78       3.730769 0.54631630     0.4296689 0.7358930  0.8602325       1.1156233\n79       2.666667 0.82663980     1.1086779 0.8139410  0.8732125       1.2909944\n80       3.812500 1.15251450     1.2343760 1.1652874  0.8750940       1.1672618\n81       4.000000 1.15482500     1.0933445 1.2507469  0.8899180       1.1126973\n82       3.777778 1.05169420     1.0730867 1.2060454  0.9003366       1.7159384\n83       2.450000 1.25972410     1.4342743 1.3219754  0.9102590       1.3562720\n84       4.157895 1.26178660     1.2864567 1.3485968  0.9176629       1.2588865\n85       3.750000 0.68947720     0.9045340 0.6685579  0.9374369       1.1381804\n86       3.870968 0.49622230     0.4774484 0.6672041  0.9503819       1.4081178\n87       3.545455 0.80178374     0.8578641 0.9116846  0.9534626       1.0107646\n88       3.153846 0.98871840     1.1094004 1.1929279  0.9540736       1.2142318\n89       3.818182 0.87163080     1.0568269 0.8004328  0.9636241       1.2960145\n90       3.153846 1.20830460     1.3272296 1.2006409  0.9703290       1.3473621\n91       2.769231 1.34695420     1.3093073 1.4540584  0.9856108       1.4232502\n92       3.250000 1.05475120     1.1697953 1.0894228  0.9880869       1.2926920\n93       3.666667 1.13172300     1.4116492 0.9780193  0.9890707       1.3904436\n94       2.903226 1.24699130     1.3612279 1.2541491  1.0021291       1.3755437\n95       2.545455 1.06472230     1.1263964 1.1514451  1.0037467       1.5429458\n96       3.750000 0.30878040     0.2292434 0.4947168  1.0178586       1.2181424\n97       2.937500 1.24393690     1.3102419 1.4223872  1.0194678       1.1341474\n98       3.125000 1.46563900     1.5012072 1.5316705  1.0206207       1.2618999\n99       3.358974 1.24939010     1.3968606 1.2571745  1.0217154       1.1582014\n100      2.794118 1.14228400     1.2499554 1.2739681  1.0259555       1.2739681\n101      3.400000 1.34783780     1.4864467 1.3732131  1.0555973       1.3522468\n102      3.457143 1.42722960     1.4597492 1.5338754  1.0560400       1.2912114\n103      3.400000 0.78881060     0.9486833 0.6749486  1.0593499       1.0749677\n104      2.400000 0.66120505     0.7847060 0.6316762  1.0667385       1.3844373\n105      3.900000 0.89597870     1.4491377 0.6666667  1.0749677       0.8755950\n106      3.166667 1.31504550     1.3414650 1.3681355  1.0754976       1.4345630\n107      2.647059 1.15495820     1.3117297 1.0907257  1.0790207       1.2524486\n108      2.842105 1.03519920     1.2495613 1.1002392  1.0841765       1.3849652\n109      3.846154 1.17669680     1.2666474 1.2043876  1.0919284       1.7246330\n110      2.875000 1.15204420     1.3284223 1.1757351  1.1048024       1.6683325\n111      2.920000 1.01133510     1.1190492 0.9935150  1.1053836       1.2590894\n112      2.850000 1.51343190     1.6212870 1.5961263  1.1084094       1.0021622\n113      3.363636 1.36324180     1.4809395 1.4603341  1.1086779       1.0552897\n114      3.380952 1.35389260     1.5671819 1.3011061  1.1117071       1.1032630\n115      3.333333 1.21694030     1.3861295 1.2860647  1.1299423       0.9712859\n116      4.071429 0.63872296     0.6112498 0.8418974  1.1411388       1.1411388\n117      3.000000 1.41813650     1.6193277 1.3498971  1.1547005       1.2472191\n118      3.166667 1.06904500     1.2608261 1.0994226  1.1748016       1.2761549\n119      3.714286 0.87077080     1.0919285 0.7300459  1.2043875       1.2043875\n120      1.098039 1.57052170     1.7191232 1.7191232  1.2159200       1.1972190\n121      3.105263 1.33215200     1.4232502 1.3542556  1.2222631       1.4292216\n122      3.062500 1.22304260     1.5438048 1.0954451  1.2230427       1.4361407\n123      2.552239 1.47307360     1.6554120 1.4142136  1.2288923       1.1452614\n124      2.827586 1.36660560     1.5369024 1.3600565  1.2387424       1.4159541\n125      3.277778 1.19373370     1.3705698 1.1740791  1.2483835       1.2307339\n126      3.333333 1.33025940     1.3376712 1.5012072  1.2740442       1.1671841\n127      4.533333 0.64549720     0.5439056 0.8920949  1.2893797       0.6399405\n128      3.235294 1.29039980     1.3764944 1.3869694  1.3168943       1.1472473\n129      3.657143 1.31842150     1.3823619 1.3303187  1.3169866       1.1867617\n130      3.500000 1.53910660     1.5856499 1.6062873  1.3416408       1.3471506\n131      3.148148 1.27097790     1.3846945 1.3503192  1.3660516       1.0635103\n132      3.300000 1.24275680     1.3703203 1.1972190  1.4298407       1.6363917\n133      2.913043 1.35317990     1.3948381 1.4366143  1.4431662       1.2441166\n134      2.615385 1.32045050     1.4978617 1.3204506  1.4500221       1.0439078\n135      4.500000 0.25819890     0.3162278 0.4830459  0.5163978       0.5477226\n136      4.095238 0.63807750     0.6761234 0.6761234  0.7003401       0.9952267\n137      4.000000 0.28287620     0.3916747 0.4666937  0.7699370       0.7905694\n138      3.333333 1.22659910     1.3684763 1.1908744  0.7862454       1.3228757\n139      3.000000 1.15363870     1.2628425 1.2113300  0.8574929       1.2649111\n140      4.000000 0.75885576     0.7648905 0.8583598  0.8603661       0.9325048\n141      3.500000 1.10692130     1.3847680 1.0271052  0.8644378       1.0801235\n142      3.933333 0.91936830     1.0465362 0.8997354  0.8837151       0.5936168\n143      3.312500 0.66432410     0.7899884 0.8091316  0.8861750       1.1697654\n144      3.800000 1.17968920     1.1972190 1.2292726  0.9189366       1.3165612\n145      3.296296 1.22427560     1.3047217 1.3412124  0.9278575       1.0308628\n146      3.428571 0.62396500     0.9482242 0.5401177  0.9474586       1.3844373\n147      3.057143 0.87834935     0.9752988 0.9440892  0.9640895       1.4939655\n148      3.192308 1.36836170     1.5698305 1.2793677  0.9643055       1.2335066\n149      3.428571 1.23607220     1.2916690 1.3954048  0.9678334       1.5324595\n150      3.218750 1.30806460     1.4456077 1.3447692  0.9702155       1.2657735\n151      4.083334 0.76709280     1.0675591 0.6963624  0.9900296       0.7810788\n152      3.225806 0.88445354     0.9956896 0.8500474  0.9956896       1.5429339\n153      3.277778 1.49316180     1.6095475 1.5578513  1.0116283       1.6379886\n154      3.111111 1.40057380     1.4962073 1.4008346  1.0344708       1.3963114\n155      3.606557 1.17485390     1.2631159 1.1904024  1.0421478       1.2685778\n156      3.500000 0.91793525     1.0020332 0.9752709  1.0777130       1.2460464\n157      3.222222 1.24521490     1.4528056 1.2571365  1.0824549       1.3862730\n158      3.785714 1.01160850     1.0994504 1.1217138  1.0894096       1.3688047\n159      2.857143 1.13476720     1.4337030 1.0766206  1.0962049       1.2010448\n160      3.019608 1.21557450     1.3438947 1.1931890  1.1298854       1.2726381\n161      3.090909 1.39316510     1.5075567 1.4333686  1.1677484       1.3003496\n162      2.365854 1.39106610     1.4259879 1.5203170  1.1908744       1.3182583\n163      2.454545 0.90864410     0.9780193 0.9990938  1.2128539       1.4384937\n164      3.650000 1.17953560     1.5270939 1.0406748  1.2303796       1.0399899\n165      2.861111 1.33897610     1.5147424 1.3484265  1.2305632       1.2224747\n166      3.097561 1.33399600     1.4027390 1.4027390  1.3973279       1.0721450\n167      4.368421 0.34199280     0.3153018 0.5129892  0.6306035       0.7608859\n168      3.636364 0.80622580     0.9045340 0.9045340  0.6741999       0.9244163\n169      3.400000 0.65828060     0.6749486 0.7071068  0.7071068       1.5776213\n170      3.285714 0.51589980     0.6321988 0.6566344  0.7608522       1.0738933\n171      3.705882 0.91604894     0.9883070 0.9905718  0.7935313       1.1070847\n172      2.187500 0.17078250     0.2500000 0.2500000  0.8164966       1.5585784\n173      3.828571 1.23789690     1.2746887 1.2343487  0.8181477       1.0427823\n174      3.357143 1.19542130     1.2606535 1.2589465  0.8462441       1.3666473\n175      3.833333 0.58348596     0.5417078 0.7510676  0.8473185       1.0494995\n176      3.566667 1.06986810     1.2103504 1.0726652  0.8656989       1.1404232\n177      3.466667 1.34518540     1.5055453 1.4573296  0.9154754       1.1872337\n178      3.369565 0.98589080     1.1070699 0.9997584  0.9335403       1.1226694\n179      3.692308 1.16161910     1.1929279 1.2557560  0.9540736       1.1094004\n180      2.740741 1.05611770     1.2854655 1.0912759  0.9622504       1.1298654\n181      3.483871 1.24482120     1.4465258 1.2622509  0.9631880       1.1509697\n182      3.281250 1.29271770     1.6051831 1.2536238  0.9954534       1.3009767\n183      3.590909 0.95940320     1.0413528 0.9847319  0.9989172       0.7341397\n184      3.500000 0.96416175     1.0486219 1.0860975  1.0116963       1.1832160\n185      4.714286 1.00227010     1.4459976 1.0787198  1.0269106       0.4879500\n186      3.659574 1.21826220     1.3112807 1.2342522  1.0355666       1.1087909\n187      3.241935 1.17516610     1.3203248 1.2208527  1.0366117       1.1878850\n188      2.923077 1.12944280     1.3634421 1.0127394  1.0500305       1.3204506\n189      3.950000 1.26569100     1.5035047 1.2607433  1.0513150       0.9986833\n190      4.266667 0.83037110     1.0467885 0.7006621  1.0701221       0.9802650\n191      4.086957 1.54925720     1.5463843 1.6502485  1.0724727       0.6683115\n192      2.870968 1.07787720     1.2460408 1.0937059  1.0825279       1.1232837\n193      3.761905 0.66619470     0.8025077 0.7066960  1.0876244       0.8499505\n194      2.708333 1.26548390     1.3014763 1.2846643  1.1293194       1.1970677\n195      4.272727 1.64970860     1.7101507 1.6811168  1.2057554       1.1621744\n196      3.833333 0.81067690     1.2431631 0.5149286  1.2060454       1.2673044\n197      2.586207 1.33463480     1.4081416 1.3674647  1.2504032       1.0527936\n198      3.100000 1.30290100     1.4568627 1.3425532  1.2779128       1.2152872\n199      2.957447 1.24308870     1.3771770 1.2936610  1.3114591       1.3180593\n200      3.307692 1.21694387     1.3732131 1.2459458  1.5946339       1.4366985\n201      2.900000 0.18445101     0.0000000 0.3689020  0.5922892       1.3222238\n202      3.333333 1.21797200     1.2776357 1.2718675  0.6642112       0.8164966\n203      3.650000 1.38419940     1.3968564 1.4405203  0.7223151       1.2258188\n204      3.333333 0.63296210     0.7762500 0.6405126  0.7679476       1.3228757\n205      2.846154 1.21818480     1.1821319 1.3008873  0.8006408       0.9870962\n206      2.444444 1.30390310     1.4652846 1.2935233  0.8023658       1.2472191\n207      3.181818 0.15075567     0.3015113 0.0000000  0.8090398       1.4012981\n208      4.043478 0.84238670     1.0228166 0.8086075  0.8236878       1.2239378\n209      3.200000 1.46671000     1.6124516 1.3557637  0.8451543       1.3201731\n210      3.320000 0.43493295     0.5228129 0.4760952  0.8698659       1.2819256\n211      3.142857 1.46547570     1.6704718 1.3540064  0.8701396       1.1952286\n212      3.047619 0.21630646     0.0000000 0.4326129  0.8817078       1.2484601\n213      3.000000 0.59375840     0.7472827 0.6435197  0.8935499       1.3070323\n214      2.909091 1.12815210     1.3003496 1.0954451  0.8944272       1.2210279\n215      2.904762 0.31802452     0.2668803 0.5547002  0.9073929       1.3749459\n216      3.028571 1.42965700     1.5773357 1.4431156  0.9079231       1.3169866\n217      3.228571 1.20101500     1.4242793 1.2209653  0.9102590       1.2622509\n218      3.107143 0.91612536     1.1733914 0.9364012  0.9151166       1.3968029\n219      2.961538 0.80676025     1.2058288 0.6064784  0.9222661       1.1482428\n220      3.642857 0.91504186     0.9164863 1.0408330  0.9371803       0.9440892\n221      3.354839 0.60810500     0.6139169 0.7084447  0.9603898       1.4270806\n222      3.346154 1.22798060     1.4991179 1.1379690  0.9670497       1.1642099\n223      2.796610 1.15508210     1.2557641 1.2337789  0.9723844       1.1563865\n224      3.894737 1.19394220     1.4423487 1.0640912  0.9733285       1.1806886\n225      3.357143 0.91161615     1.1411388 0.9376145  0.9749613       1.2774459\n226      3.896552 1.27813170     1.6822750 1.1838403  0.9775812       1.2913124\n227      2.851064 1.00320000     1.1715584 1.0413763  0.9784634       1.3984265\n228      3.882353 0.55901700     0.6805570 0.5871429  0.9787210       1.1114379\n229      2.722222 1.42218820     1.5039630 1.5321942  0.9952267       1.2274103\n230      3.375000 1.18693470     1.4667249 1.1137256  1.0076629       1.3716451\n231      4.000000 1.39136530     1.5566236 1.4366985  1.0127394       1.2472191\n232      3.611940 1.03199600     1.0770834 1.1120002  1.0139239       1.5270810\n233      3.000000 1.01667380     1.2681432 1.2163602  1.0145145       1.3228757\n234      2.875000 1.03077640     0.9195587 1.3585243  1.0145993       1.1474610\n235      3.363636 0.89157915     0.9021379 1.0455016  1.0192944       1.3471507\n236      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n237      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n238      3.200000 1.01596430     1.1624764 1.0496223  1.0301293       1.2319282\n239      2.650794 0.97440100     1.0814835 1.0152612  1.0386036       1.2202423\n240      3.645161 1.24819200     1.3233520 1.3578282  1.0427823       1.1704241\n241      2.281250 1.31053040     1.3897667 1.3526408  1.0429293       1.1139692\n242      3.810345 1.23102660     1.4605106 1.1826534  1.0436626       1.4244112\n243      3.350000 1.01653000     1.1212238 1.0910895  1.0442587       1.2258187\n244      3.000000 1.21465170     1.2880570 1.1934163  1.0552897       1.1832160\n245      3.333333 1.35847510     1.4588635 1.3809673  1.0574412       1.2841818\n246      3.000000 0.62568640     0.9492623 0.5345225  1.0716117       1.0377490\n247      3.028571 1.21248590     1.5020311 1.1524100  1.0723805       1.3169866\n248      2.700000 1.22927260     1.2866839 1.2516656  1.0749677       1.2516656\n249      3.413793 0.76263310     1.0618786 0.7738544  1.0806554       0.9826074\n250      4.117647 0.72571800     0.6782330 0.9183318  1.0847427       1.2187264\n251      2.343750 1.33853150     1.4601822 1.3617468  1.1041826       1.1247760\n252      2.333333 1.29379800     1.4996706 1.3082287  1.1051443       1.0846523\n253      2.122807 0.97855616     1.2413160 0.9438567  1.1057107       1.2965638\n254      3.078431 0.84032700     0.8403270 0.7948120  1.1271381       1.2139710\n255      3.341463 1.20162490     1.2326850 1.2892615  1.1348149       1.2963363\n256      3.076923 1.13933180     1.1266014 1.1821319  1.1435437       1.4411534\n257      2.787234 1.20562260     1.2873383 1.2225878  1.1468364       1.2146957\n258      3.303030 1.17944840     1.3342800 1.1256311  1.1489455       1.3574988\n259      2.950000 1.30686450     1.5525870 1.1697953  1.1516578       1.1459310\n260      4.153846 0.31521258     0.2773501 0.4385290  1.1657506       1.2810252\n261      2.285714 1.25680650     1.3535886 1.2408033  1.1694644       1.3385315\n262      3.050000 1.15849270     1.2732057 1.1876558  1.1742859       1.1459310\n263      3.615385 1.20488770     1.3245324 1.2139540  1.1818465       1.1148610\n264      2.978723 1.13765860     1.3147594 1.1938400  1.1865233       1.1700866\n265      2.950000 1.17703720     1.2883571 1.2460823  1.2062056       1.1972190\n266      3.891304 1.15064310     1.3034491 1.1686087  1.2412962       1.0160547\n267      2.350877 1.35781870     1.5355030 1.2970470  1.2746320       1.2318863\n268      4.090909 1.14724730     1.2004901 1.1472473  1.3117119       1.0444659\n269      3.724138 1.34645130     1.5297810 1.3565507  1.3235272       1.2217245\n270      3.692308 1.31558700     1.3301244 1.3445045  1.3634421       1.1094004\n271      3.173913 1.25822410     1.4054784 1.1838403  1.3753638       1.0724727\n272      2.962963 1.30506780     1.3645765 1.3491468  1.4054784       1.2241632\n273      4.181818 1.64869094     1.6124516 1.8090681  1.4206273       0.9816498\n274      2.090909 1.13066760     1.4433757 1.3026779  1.4222262       0.8312094\n275      2.444444 1.36362650     1.4741786 1.4652846  1.4245742       1.3382263\n276      3.000000 1.06904500     1.4864467 1.0997835  1.4375906       1.4142136\n277      4.240000 0.16583124     0.0000000 0.3316625  0.5000000       0.8793937\n278      3.000000 1.02198060     0.8755950 1.3374935  0.5270463       1.3333333\n279      3.250000 0.59646390     0.4385290 0.8697185  0.7250111       1.1381804\n280      2.914894 1.11129090     1.2131716 1.1661270  0.7362680       1.4269120\n281      3.000000 1.45539750     1.6624188 1.3483997  0.9045340       1.4832397\n282      3.588235 1.37466570     1.5550886 1.4061025  0.9217772       0.9393364\n283      2.904762 1.19557330     1.4359334 1.1169687  0.9283883       1.0442587\n284      3.459459 0.99885650     0.9160461 1.1305596  0.9358023       1.1924437\n285      3.961538 0.79534630     0.8083372 0.9721501  0.9603898       0.8708970\n286      3.407407 0.09622505     0.0000000 0.1924501  0.9798541       1.3376000\n287      3.000000 0.73340220     0.6741999 0.8876254  0.9847319       1.6514456\n288      3.850000 1.27009100     1.4238233 1.2581050  1.0110501       1.1220403\n289      2.302326 1.20064120     1.3527498 1.2047948  1.0139335       0.9888638\n290      3.656250 0.70710677     0.6946508 0.8432740  1.0642085       1.0957211\n291      2.607843 1.06251160     1.2539247 1.0757475  1.0772081       1.2973578\n292      2.320000 1.40084460     1.4225526 1.4255729  1.0944631       1.3453624\n293      3.352941 1.23073390     1.3284223 1.2277430  1.1114379       1.2718675\n294      3.366667 1.08860350     1.2452207 1.0618786  1.1121068       1.2726116\n295      3.272727 1.40453820     1.5666989 1.3684763  1.1200649       1.4893562\n296      2.470588 0.91454744     0.9393364 1.1599949  1.1726039       1.2307339\n297      2.733333 1.09883430     1.3442329 1.0482052  1.2169036       1.2339054\n298      3.421053 1.27296710     1.2852322 1.4227760  1.2178386       1.0813300\n299      2.884615 1.02731920     1.3547637 0.9583640  1.2434443       1.4234303\n300      4.068966 1.34365870     1.4418106 1.3993313  1.2614012       1.3074248\n301      2.250000 1.35637750     1.4390636 1.4078071  1.2688132       1.0823902\n302      2.875000 1.14336860     1.3102163 1.0935416  1.3400871       1.3102163\n303      1.900000 1.07496770     1.3165612 0.9660918  1.3703203       1.1005049\n304      2.714286 1.19580300     1.2924124 1.1507283  0.5135526       1.2043875\n305      4.000000 0.28946713     0.2672612 0.3631365  0.5345225       1.1766968\n306      3.416667 1.49443412     1.4298407 1.6329932  0.6992059       1.3540064\n307      2.769231 1.24163870     1.5275252 1.0801234  0.8006408       1.4232502\n308      2.909091 1.15009880     1.2060454 1.2135598  0.8090398       1.1361818\n309      3.055556 0.76706850     0.8148421 0.8938234  0.8098583       1.1697239\n310      3.593750 0.87399140     1.0957211 0.8775883  0.8327955       1.5420844\n311      3.900000 0.78881064     0.6992059 0.6324555  0.8432740       1.2866839\n312      3.894737 0.20118695     0.3585686 0.2182179  0.8728716       1.1969747\n313      2.500000 1.32916010     1.5811388 1.4181365  0.8755950       0.9258201\n314      3.500000 1.11464080     1.6422453 1.0552897  0.9003366       1.1677484\n315      3.666667 1.26585190     1.3557637 1.2909944  0.9102590       1.5886502\n316      3.133333 0.34287268     0.4112002 0.4658123  0.9206548       1.2720778\n317      2.919355 1.06368230     1.1638203 1.2366939  0.9261587       1.1205734\n318      3.348837 1.09459510     1.0766296 1.2070640  0.9339917       1.3252802\n319      2.600000 1.02198060     1.3703203 1.3984118  0.9486833       1.3498971\n320      3.114286 1.38551180     1.5743479 1.3333333  0.9803627       1.2312459\n321      3.357143 0.79432510     0.9611501 0.7988086  1.0000000       1.4468609\n322      3.093750 1.03954090     1.2374369 1.0544644  1.0080323       1.4448881\n323      2.952381 0.91539320     0.8979555 1.0650762  1.0137396       1.2835961\n324      4.000000 1.11987540     1.1411388 1.2043876  1.0271052       1.0377490\n325      3.750000 1.41153260     1.8090681 1.1934162  1.0730867       0.7537783\n326      2.741935 1.17564420     1.2529535 1.2759142  1.0766335       1.2101719\n327      3.863636 0.86366886     0.8329709 1.0494995  1.0901403       1.0371873\n328      3.285714 0.94998010     1.0615526 0.9623598  1.0923286       1.1818737\n329      4.160000 0.63116350     0.8121526 0.5728554  1.0956314       0.8417668\n330      3.636364 1.13618180     1.3618170 1.3684763  1.1037128       1.5015144\n331      3.421053 0.71838840     0.7723284 0.8200699  1.1160708       1.5024347\n332      3.000000 1.41317940     1.5949482 1.4749368  1.1239030       1.1547005\n333      3.092593 1.04803190     1.1538863 1.1036508  1.1498066       1.2017051\n334      3.030303 1.29796560     1.4367224 1.3644976  1.1516090       1.1315048\n335      2.769231 1.34008700     1.3768926 1.3647344  1.1528949       1.4232502\n336      4.000000 1.30558240     1.4832397 1.3483997  1.2210279       0.8944272\n337      3.184211 1.00277390     1.0135446 1.0917505  1.2239198       1.3326516\n338      3.444444 1.15778930     1.3645163 1.1608700  1.2382784       1.0416176\n339      3.200000 0.92646280     1.1005049 0.9189366  0.3162278       1.5491933\n340      4.600000 0.24152295     0.3162278 0.4216370  0.3162278       0.6992059\n341      4.647059 0.41322106     0.5144958 0.3233808  0.5745131       0.6063391\n342      4.200000 0.33747430     0.4216370 0.3162278  0.6666667       0.6324555\n343      3.875000 1.54650140     1.5776213 1.7288403  0.6749486       1.3562027\n344      3.437500 1.44769930     1.5370426 1.4127397  0.7187953       1.0935416\n345      3.187500 0.57373047     0.6191392 0.5773503  0.8164966       0.9810708\n346      4.250000 1.64654520     1.7126977 1.6465452  0.8232726       0.8864053\n347      3.000000 1.48644670     1.5886502 1.4242793  0.9411239       1.4638501\n348      3.034483 0.96042377     1.2614012 0.9001916  0.9813532       1.0850529\n349      2.625000 0.68630000     0.7172800 0.8297000  1.0072200       1.0959400\n350      3.363636 1.30905940     1.4893562 1.2210279  1.0357255       0.6741999\n351      3.636364 0.80750000     0.9625004 0.9714540  1.0568269       1.3988245\n352      2.966102 1.37152520     1.4525821 1.4169220  1.0669052       1.3641481\n353      3.428571 0.80622580     1.0757057 0.7464200  1.0712698       1.0281745\n354      2.825000 1.14243270     1.2315400 1.2074394  1.0734965       1.0594508\n355      2.000000 1.43745890     1.5050420 1.4034589  1.1381804       1.4142136\n356      2.727273 1.67738970     1.7002674 1.7529196  1.1677484       1.4206273\n357      2.823529 1.14777600     1.2363048 1.2610817  1.2119016       1.1407225\n358      2.906977 0.97585590     1.0081791 1.0534049  1.2176242       1.1508579\n359      2.200000 1.02198060     1.1005049 1.1972190  1.2292726       1.1352924\n360      2.562500 1.18241190     1.3593477 1.1726039  1.2740441       1.3149778\n361      2.555556 1.32958880     1.4935760 1.2787993  1.3397283       1.4232502\n362      3.411765 1.54842470     1.5537972 1.6917165  1.4309504       1.0036697\n363      2.533333 1.28822500     1.3345233 1.3557637  1.4474937       1.1254629\n364      2.700000 1.20761480     1.4757296 1.0593499  1.5634719       0.9486833\n365      2.545455 1.37510340     1.5782614 1.3003496  1.6180797       1.6348478\n366      2.272727 0.91701096     0.9244163 1.0357255  1.6292776       1.4206273\n\n\n\n\nCode\nratemyprof<- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\nratemyprof\n\n\n     quality helpfulness  clarity easiness raterInterest\n1   4.636364    4.636364 4.636364 4.818182      3.545455\n2   4.318182    4.545455 4.090909 4.363636      4.000000\n3   4.790698    4.720930 4.860465 4.604651      3.432432\n4   4.250000    4.458333 4.041667 2.791667      3.181818\n5   4.684211    4.684211 4.684211 4.473684      4.214286\n6   4.233333    4.266667 4.200000 4.533333      3.916667\n7   4.382353    4.352941 4.411765 4.117647      3.812500\n8   2.062500    2.062500 2.062500 1.437500      2.937500\n9   2.041667    2.166667 2.000000 1.750000      3.750000\n10  4.111111    4.222222 4.000000 3.666667      4.176471\n11  4.727273    4.909091 4.545455 4.000000      2.900000\n12  3.724242    3.848485 3.606060 4.242424      3.333333\n13  2.804348    2.695652 2.913043 2.217391      3.217391\n14  4.838235    4.823529 4.852941 4.676471      4.718750\n15  4.565217    4.565217 4.565217 2.826087      3.952381\n16  4.944444    4.962963 4.925926 3.703704      3.592593\n17  4.464286    4.714286 4.214286 3.214286      2.357143\n18  4.184211    4.368421 4.000000 3.631579      3.526316\n19  3.909091    4.090909 3.727273 2.272727      4.000000\n20  3.500000    3.285714 3.714286 3.285714      3.230769\n21  3.474576    3.542373 3.406780 2.355932      3.537037\n22  3.696429    3.714286 3.678571 3.642857      3.250000\n23  3.576923    3.461538 3.692308 4.615385      2.923077\n24  1.633333    1.600000 1.666667 2.266667      3.400000\n25  3.531250    3.312500 3.750000 2.562500      3.800000\n26  3.114286    3.057143 3.171429 3.857143      3.000000\n27  4.909091    5.000000 4.818182 4.181818      4.909091\n28  4.239130    4.434783 4.043478 2.826087      3.333333\n29  3.981481    4.037037 3.925926 3.148148      4.153846\n30  4.392857    4.500000 4.285714 3.166667      3.516129\n31  4.526316    4.473684 4.578947 3.131579      4.029412\n32  4.075000    4.150000 4.000000 3.250000      3.631579\n33  2.829546    3.022727 2.636364 2.045454      3.475000\n34  4.552632    4.561404 4.543860 3.614035      3.901961\n35  3.700000    3.600000 3.800000 3.133333      3.933333\n36  1.891304    2.239130 1.543478 3.869565      2.347826\n37  4.277778    4.222222 4.333333 3.888889      3.882353\n38  4.416667    4.611111 4.222222 3.111111      3.500000\n39  2.569620    2.696203 2.443038 2.658228      2.956522\n40  3.472222    3.111111 3.833333 3.388889      4.000000\n41  2.442308    2.692308 2.192308 2.230769      2.692308\n42  3.067308    3.403846 2.692308 3.076923      3.442308\n43  3.600000    3.600000 3.600000 3.200000      4.300000\n44  4.115385    4.230769 4.000000 2.884615      4.320000\n45  2.900000    3.200000 2.600000 3.800000      2.733333\n46  3.388889    3.711111 3.111111 2.844444      3.185185\n47  4.708334    4.750000 4.666666 3.666667      3.416667\n48  3.291667    3.333333 3.250000 3.333333      3.416667\n49  2.250000    2.062500 2.437500 2.812500      3.200000\n50  3.555556    3.722222 3.388889 2.777778      3.290322\n51  3.854167    3.875000 3.833333 3.000000      4.227273\n52  3.913793    3.965517 3.862069 2.965517      4.320000\n53  3.469231    3.353846 3.584615 3.061538      3.280702\n54  3.045455    3.090909 3.000000 3.363636      3.636364\n55  3.464286    3.285714 3.642857 3.357143      2.785714\n56  4.000000    4.000000 4.000000 2.500000      3.875000\n57  3.982759    3.862069 4.103448 3.310345      4.333333\n58  4.518519    4.518519 4.518519 4.074074      4.340909\n59  2.958333    2.833333 3.083333 2.500000      3.500000\n60  2.085965    2.280702 1.894737 2.631579      3.166667\n61  3.538462    3.461538 3.615385 3.230769      3.800000\n62  2.924242    2.696970 3.151515 3.454545      2.696970\n63  2.293478    2.282609 2.304348 2.456522      3.806452\n64  3.684211    3.684211 3.684211 3.157895      2.933333\n65  4.666666    4.888889 4.444445 3.055556      3.937500\n66  4.732143    4.857143 4.607143 4.500000      3.333333\n67  4.880952    4.952381 4.809524 4.809524      3.428571\n68  4.653846    4.615385 4.692308 4.076923      3.550000\n69  3.100000    3.500000 2.700000 3.000000      2.800000\n70  2.107143    1.928571 2.285714 2.000000      3.166667\n71  4.180000    4.320000 4.040000 3.720000      3.840000\n72  3.500000    3.500000 3.500000 2.272727      3.000000\n73  4.625000    4.666667 4.583333 4.527778      3.722222\n74  4.428571    4.642857 4.214286 4.000000      3.214286\n75  2.394737    2.263158 2.526316 1.736842      3.066667\n76  1.674419    1.686047 3.360465 1.558140      2.814815\n77  3.760000    3.840000 3.680000 2.920000      3.631579\n78  4.538462    4.769231 4.307692 3.500000      3.730769\n79  1.875000    2.187500 1.562500 2.687500      2.666667\n80  4.025000    3.950000 4.100000 2.650000      3.812500\n81  3.550000    3.666667 3.433333 1.966667      4.000000\n82  3.166667    3.333333 3.000000 3.916667      3.777778\n83  2.523810    2.428571 2.619048 1.857143      2.450000\n84  3.710526    3.894737 3.526316 2.789474      4.157895\n85  4.458333    4.500000 4.416667 4.166667      3.750000\n86  4.709677    4.806452 4.612903 3.645161      3.870968\n87  4.500000    4.545455 4.454545 3.636364      3.545455\n88  3.461538    3.307692 3.615385 2.923077      3.153846\n89  4.545455    4.545455 4.545455 3.500000      3.818182\n90  3.500000    3.807692 3.192308 3.692308      3.153846\n91  2.200000    2.000000 2.400000 2.400000      2.769231\n92  3.925000    4.000000 3.500000 3.850000      3.250000\n93  3.958333    3.916667 4.000000 2.750000      3.666667\n94  2.782609    3.000000 2.565218 3.768116      2.903226\n95  1.756757    1.810811 1.702703 1.783784      2.545455\n96  4.851351    4.945946 4.756757 3.729730      3.750000\n97  3.218750    3.343750 3.093750 2.156250      2.937500\n98  3.312500    3.416667 3.208333 2.458333      3.125000\n99  3.695122    3.731707 3.658537 3.390244      3.358974\n100 3.794118    3.794118 3.794118 4.088235      2.794118\n101 3.233333    3.266667 3.200000 3.400000      3.400000\n102 3.236842    3.631579 2.842105 3.421053      3.457143\n103 4.700000    4.700000 4.700000 3.700000      3.400000\n104 1.482759    1.517241 1.448276 2.068966      2.400000\n105 3.950000    3.900000 4.000000 3.400000      3.900000\n106 3.537037    3.629630 3.444444 2.814815      3.166667\n107 3.208333    3.222222 3.194444 2.083333      2.647059\n108 2.394737    2.684211 2.105263 1.789474      2.842105\n109 3.500000    3.714286 3.285714 3.500000      3.846154\n110 3.470588    3.529412 3.411765 3.294118      2.875000\n111 4.305556    4.259259 4.351852 3.203704      2.920000\n112 3.095238    3.142857 3.047619 3.857143      2.850000\n113 2.469697    2.454545 2.484848 2.666667      3.363636\n114 2.904255    3.021277 2.787234 3.361702      3.380952\n115 3.189655    3.206897 3.172414 3.672414      3.333333\n116 4.678571    4.714286 4.642857 4.071429      4.071429\n117 3.200000    2.800000 3.600000 2.000000      3.000000\n118 3.833333    3.805556 3.861111 2.638889      3.166667\n119 3.785714    3.500000 4.071429 3.285714      3.714286\n120 3.442308    3.346154 3.500000 4.038462      1.098039\n121 2.589744    2.641026 2.538462 3.923077      3.105263\n122 2.437500    2.375000 2.500000 3.187500      3.062500\n123 2.977612    2.955224 3.000000 2.373134      2.552239\n124 3.224138    3.172414 3.275862 2.965517      2.827586\n125 2.750000    2.961539 2.538461 3.038461      3.277778\n126 3.729167    4.041666 3.416667 3.166667      3.333333\n127 4.625000    4.812500 4.437500 3.062500      4.533333\n128 3.825000    4.000000 3.650000 2.550000      3.235294\n129 3.200000    3.171429 3.228571 3.028571      3.657143\n130 3.347222    3.333333 3.361111 3.500000      3.500000\n131 3.000000    3.074074 2.851852 3.592593      3.148148\n132 4.100000    4.100000 4.100000 2.600000      3.300000\n133 2.792453    2.698113 2.886792 2.735849      2.913043\n134 3.076923    3.076923 3.076923 3.461538      2.615385\n135 4.800000    4.900000 4.700000 4.400000      4.500000\n136 4.571429    4.571429 4.571429 4.238095      4.095238\n137 4.757576    4.818182 4.696970 3.969697      4.000000\n138 3.636364    3.545455 3.727273 3.727273      3.333333\n139 3.583333    3.222222 3.944444 3.833333      3.000000\n140 4.400000    4.366667 4.433333 4.133333      4.000000\n141 4.071429    4.071429 4.142857 3.142857      3.500000\n142 4.333333    4.333333 4.333333 2.933333      3.933333\n143 4.250000    4.220000 4.280000 2.480000      3.312500\n144 4.150000    4.100000 4.200000 2.800000      3.800000\n145 3.366667    3.566667 3.166667 2.366667      3.296296\n146 4.490566    4.283019 4.698113 3.603774      3.428571\n147 4.130435    3.934783 4.326087 3.217391      3.057143\n148 2.700000    2.866667 2.533333 1.633333      3.192308\n149 3.142857    3.119048 3.166667 2.547619      3.428571\n150 2.621622    2.662162 2.581081 2.175676      3.218750\n151 4.372549    4.176471 4.568627 3.588235      4.083334\n152 1.467742    1.483871 1.451613 2.483871      3.225806\n153 3.421053    3.578947 3.263158 4.368421      3.277778\n154 2.419643    2.375000 2.464286 2.142857      3.111111\n155 3.833333    3.805556 3.861111 2.611111      3.606557\n156 4.216418    4.104477 4.328358 2.537314      3.500000\n157 2.494118    2.764706 2.223529 2.082353      3.222222\n158 4.178571    4.142857 4.214286 2.428571      3.785714\n159 2.872340    3.340426 2.404255 3.808511      2.857143\n160 3.086957    3.420290 2.753623 3.579710      3.019608\n161 3.409091    3.454545 3.363636 3.818182      3.090909\n162 2.535714    2.446429 2.625000 2.500000      2.365854\n163 1.604167    1.500000 1.708333 2.083333      2.454545\n164 2.630435    3.173913 2.086957 3.173913      3.650000\n165 2.916667    3.138889 2.694444 3.500000      2.861111\n166 2.733333    2.622222 2.844444 3.177778      3.097561\n167 4.684211    4.894737 4.473684 4.210526      4.368421\n168 4.000000    4.272727 3.727273 3.636364      3.636364\n169 4.600000    4.700000 4.500000 4.500000      3.400000\n170 4.544643    4.517857 4.571429 4.303571      3.285714\n171 4.311765    4.305882 4.317647 4.435294      3.705882\n172 4.937500    4.937500 4.937500 4.000000      2.187500\n173 4.115385    4.179487 4.051282 2.410256      3.828571\n174 2.435484    2.451613 2.419355 1.870968      3.357143\n175 4.574074    4.703704 4.444444 2.888889      3.833333\n176 3.880597    3.746269 4.029851 3.910448      3.566667\n177 3.666667    3.866667 3.466667 3.533333      3.466667\n178 4.195652    4.413043 3.978261 3.869565      3.369565\n179 4.153846    4.384615 3.923077 4.076923      3.692308\n180 2.000000    1.962963 2.037037 3.185185      2.740741\n181 3.742857    3.714286 3.771429 2.885714      3.483871\n182 2.171875    2.437500 1.906250 4.093750      3.281250\n183 4.295455    4.318182 4.272727 3.954545      3.590909\n184 3.686275    3.686275 3.686275 2.764706      3.500000\n185 3.636364    4.090909 3.181818 3.363636      4.714286\n186 4.076923    4.076923 4.076923 3.423077      3.659574\n187 3.435484    3.274194 3.596774 2.322581      3.241935\n188 2.730769    2.230769 3.230769 3.461538      2.923077\n189 3.625000    3.550000 3.700000 3.500000      3.950000\n190 4.562500    4.468750 4.656250 3.875000      4.266667\n191 3.173913    3.130435 3.217391 3.173913      4.086957\n192 3.741935    3.612903 3.870968 3.483871      2.870968\n193 4.535714    4.547619 4.523810 2.500000      3.761905\n194 4.083333    3.958333 4.208333 3.333333      2.708333\n195 2.807692    2.730769 2.884615 3.576923      4.272727\n196 3.791667    3.500000 4.083334 3.000000      3.833333\n197 3.343750    3.218750 3.468750 2.718750      2.586207\n198 2.580000    2.600000 2.560000 2.860000      3.100000\n199 2.728070    2.526316 2.929825 2.684210      2.957447\n200 3.966667    3.800000 4.133333 3.400000      3.307692\n201 4.921875    5.000000 4.843750 3.812500      2.900000\n202 2.529412    2.411765 2.647059 2.235294      3.333333\n203 2.565218    2.565218 2.565218 1.391304      3.650000\n204 4.269231    4.461538 4.076923 2.615385      3.333333\n205 3.730769    3.692308 3.769231 3.846154      2.846154\n206 2.638889    2.833333 2.444444 2.055556      2.444444\n207 4.954545    4.909091 5.000000 4.363636      3.181818\n208 4.480769    4.384615 4.576923 3.038462      4.043478\n209 2.646667    2.800000 2.533333 2.000000      3.200000\n210 4.720000    4.760000 4.680000 2.560000      3.320000\n211 2.619048    2.904762 2.333333 1.571429      3.142857\n212 4.918605    5.000000 4.837209 3.279070      3.047619\n213 4.549020    4.627451 4.470588 4.039216      3.000000\n214 4.045455    4.090909 4.000000 3.000000      2.909091\n215 4.796296    4.925926 4.666667 3.148148      2.904762\n216 3.067568    3.108108 3.027027 2.189189      3.028571\n217 3.385714    3.028571 3.742857 1.628571      3.228571\n218 4.000000    3.655172 4.344828 2.137931      3.107143\n219 1.750000    2.166667 1.333333 2.666667      2.961538\n220 4.321429    4.392857 4.250000 2.714286      3.642857\n221 4.666667    4.757576 4.575758 3.212121      3.354839\n222 2.714286    2.892857 2.535714 2.750000      3.346154\n223 3.338710    3.645161 3.048387 3.193548      2.796610\n224 3.986842    4.026316 3.947368 3.842105      3.894737\n225 3.821429    4.071429 3.571429 3.214286      3.357143\n226 3.482759    3.482759 3.482759 3.206897      3.896552\n227 3.490566    3.924528 3.056604 3.000000      2.851064\n228 4.625000    4.600000 4.650000 3.700000      3.882353\n229 3.452381    3.523810 3.380952 2.238095      2.722222\n230 2.287500    2.450000 2.125000 1.900000      3.375000\n231 3.038462    3.384615 2.692308 2.230769      4.000000\n232 4.365672    4.447761 4.283582 1.820896      3.611940\n233 3.684211    3.947368 3.421053 2.157895      3.000000\n234 3.500000    3.294118 3.705882 2.823529      2.875000\n235 4.159091    4.363636 3.954545 3.090909      3.363636\n236 3.562500    3.437500 3.687500 3.375000      2.812500\n237 3.562500    3.437500 3.687500 3.375000      2.812500\n238 3.960526    4.000000 3.921053 2.578947      3.200000\n239 4.231343    4.164179 4.298507 4.164179      2.650794\n240 2.671429    2.885714 2.457143 1.971429      3.645161\n241 4.015625    3.937500 4.093750 4.406250      2.281250\n242 3.103448    3.275862 2.931034 2.224138      3.810345\n243 4.166667    4.428571 3.904762 3.095238      3.350000\n244 2.791667    2.750000 2.833333 1.750000      3.000000\n245 3.966667    4.088889 3.844444 2.533333      3.333333\n246 4.107143    4.142857 4.142857 3.071429      3.000000\n247 3.329268    3.512195 3.146341 3.000000      3.028571\n248 2.800000    2.900000 2.700000 2.400000      2.700000\n249 4.066667    3.900000 4.233333 2.933333      3.413793\n250 4.620000    4.720000 4.520000 2.480000      4.117647\n251 3.000000    3.081081 2.918919 3.054054      2.343750\n252 2.586957    2.608696 2.565217 2.695652      2.333333\n253 1.915385    2.076923 1.723077 2.107692      2.122807\n254 4.527778    4.537037 4.518519 2.888889      3.078431\n255 3.890244    4.073171 3.707317 2.634146      3.341463\n256 4.384615    4.461538 4.307692 3.846154      3.076923\n257 3.451923    3.596154 3.269231 3.692308      2.787234\n258 3.878788    4.030303 3.727273 3.515152      3.303030\n259 3.950000    3.900000 4.000000 3.200000      2.950000\n260 4.846154    4.923077 4.769231 3.769231      4.153846\n261 2.865385    2.826923 2.903846 2.250000      2.285714\n262 3.000000    3.400000 2.600000 2.300000      3.050000\n263 3.166667    3.333333 3.000000 2.153846      3.615385\n264 3.264151    3.339623 3.188679 3.528302      2.978723\n265 2.928571    3.081633 2.775510 3.591837      2.950000\n266 4.265306    4.265306 4.265306 3.204082      3.891304\n267 2.508772    2.561404 2.473684 3.017544      2.350877\n268 4.235294    4.235294 4.235294 2.705882      4.090909\n269 2.850000    2.933333 2.766667 2.200000      3.724138\n270 3.307692    3.461539 3.153846 2.769231      3.692308\n271 2.620690    2.758621 2.482759 3.034483      3.173913\n272 3.103448    3.172414 3.034483 2.758621      2.962963\n273 3.727273    4.000000 3.454545 3.272727      4.181818\n274 2.375000    2.416667 2.333333 2.750000      2.090909\n275 3.722222    3.944444 3.500000 3.500000      2.444444\n276 2.500000    2.733333 2.266667 2.733333      3.000000\n277 4.940000    5.000000 4.880000 4.800000      4.240000\n278 2.900000    3.100000 2.700000 1.500000      3.000000\n279 4.692308    4.769231 4.615385 4.230769      3.250000\n280 3.436170    3.531915 3.340425 4.744681      2.914894\n281 3.772727    3.818182 3.727273 3.727273      3.000000\n282 2.750000    2.777778 2.722222 2.444444      3.588235\n283 2.619048    3.190476 2.047619 2.523810      2.904762\n284 4.285714    4.452381 4.119048 3.619048      3.459459\n285 4.015152    4.181818 3.848485 1.878788      3.961538\n286 4.981481    5.000000 4.962963 3.962963      3.407407\n287 4.583333    4.500000 4.666667 3.666667      3.000000\n288 2.422222    2.533333 2.311111 2.022222      3.850000\n289 2.336735    2.591837 2.081633 2.183673      2.302326\n290 4.500000    4.555556 4.444444 3.305556      3.656250\n291 3.555556    3.555556 3.555556 2.500000      2.607843\n292 4.064516    4.096774 4.032258 3.258065      2.320000\n293 3.529412    3.470588 3.588235 2.117647      3.352941\n294 4.233333    4.366667 4.100000 2.933333      3.366667\n295 3.454545    3.363636 3.545455 2.636364      3.272727\n296 2.352941    2.411765 2.294118 2.000000      2.470588\n297 2.087500    2.375000 1.800000 2.387500      2.733333\n298 3.653846    4.076923 3.230769 2.794872      3.421053\n299 2.153846    2.346154 1.961538 2.115385      2.884615\n300 2.655172    2.689655 2.620690 2.655172      4.068966\n301 2.608333    2.716667 2.466667 3.483333      2.250000\n302 2.406250    2.375000 2.437500 1.937500      2.875000\n303 2.900000    3.200000 2.600000 3.100000      1.900000\n304 2.392857    2.142857 2.642857 1.428572      2.714286\n305 4.892857    4.928571 4.857143 4.857143      4.000000\n306 2.958333    3.000000 2.916667 3.833333      3.416667\n307 2.000000    2.000000 2.000000 2.846154      2.769231\n308 3.954545    4.363636 3.545455 3.636364      2.909091\n309 4.294872    4.384615 4.205128 3.769231      3.055556\n310 4.453125    4.343750 4.562500 4.625000      3.593750\n311 4.200000    4.600000 4.200000 2.400000      3.900000\n312 4.904762    4.857143 4.952381 4.809524      3.894737\n313 3.100000    3.500000 2.700000 1.900000      2.500000\n314 3.166667    3.166667 3.250000 3.583333      3.500000\n315 3.433333    3.533333 3.333333 3.600000      3.666667\n316 4.811321    4.849057 4.773585 4.132075      3.133333\n317 3.919118    4.250000 3.588235 2.911765      2.919355\n318 4.062500    4.229167 3.895833 1.750000      3.348837\n319 2.600000    3.100000 2.200000 4.300000      2.600000\n320 2.791667    2.750000 2.777778 2.194444      3.114286\n321 4.166667    4.266667 4.066667 3.000000      3.357143\n322 4.250000    4.218750 4.281250 4.125000      3.093750\n323 4.260870    4.478261 4.043478 4.130435      2.952381\n324 4.178571    4.071429 4.285714 3.142857      4.000000\n325 2.916667    3.000000 2.833333 2.333333      3.750000\n326 2.919355    2.645161 3.193548 2.322581      2.741935\n327 4.562500    4.791667 4.333333 3.666667      3.863636\n328 4.393939    4.424242 4.363636 3.454545      3.285714\n329 4.640000    4.560000 4.720000 3.940000      4.160000\n330 2.590909    2.636364 2.545454 2.272727      3.636364\n331 4.394737    4.473684 4.315789 3.368421      3.421053\n332 3.447368    3.105263 3.789474 2.473684      3.000000\n333 3.719298    3.912281 3.526316 2.561404      3.092593\n334 2.720588    2.764706 2.676471 2.647059      3.030303\n335 3.687500    3.812500 3.562500 3.437500      2.769231\n336 3.136364    3.000000 3.272727 2.090909      4.000000\n337 4.288889    4.466667 4.111111 3.155556      3.184211\n338 3.095238    2.809524 3.380952 2.666667      3.444444\n339 4.050000    3.900000 4.200000 4.900000      3.200000\n340 4.850000    4.900000 4.800000 4.900000      4.600000\n341 4.861111    4.833333 4.888889 4.722222      4.647059\n342 4.850000    4.800000 4.900000 4.000000      4.200000\n343 3.350000    3.600000 3.100000 3.700000      3.875000\n344 3.562500    3.687500 3.437500 4.125000      3.437500\n345 4.687500    4.625000 4.750000 3.500000      3.187500\n346 3.600000    3.600000 3.600000 2.700000      4.250000\n347 3.433333    3.666667 3.200000 3.200000      3.000000\n348 4.120690    4.344828 3.896552 4.034483      3.034483\n349 4.083330    4.583330 3.583330 4.166670      2.625000\n350 3.181818    3.272727 3.090909 3.545455      3.363636\n351 4.340909    4.545455 4.090909 4.454545      3.636364\n352 3.491935    3.612903 3.370968 2.467742      2.966102\n353 4.500000    4.428571 4.571429 3.619048      3.428571\n354 3.584906    3.415094 3.754717 3.037736      2.825000\n355 3.708333    3.583333 3.833333 4.250000      2.000000\n356 3.318182    3.090909 3.545455 2.181818      2.727273\n357 3.513158    3.657895 3.368421 3.868421      2.823529\n358 4.228261    4.304348 4.152174 3.369565      2.906977\n359 1.900000    1.900000 1.900000 3.800000      2.200000\n360 1.937500    2.250000 1.625000 2.333333      2.562500\n361 3.462963    3.333333 3.592593 3.111111      2.555556\n362 2.619048    2.714286 2.523810 3.619048      3.411765\n363 2.966667    3.066667 2.866667 2.666667      2.533333\n364 3.250000    3.200000 3.300000 3.000000      2.700000\n365 1.909091    1.909091 1.909091 2.272727      2.545455\n366 1.409091    1.363636 1.454545 2.636364      2.272727\n\n\n\n\nCode\npairs(ratemyprof)\n\n\n\n\n\nQuality, helpfulness, and clarity all have a positive correlation with one another which show that these rating have the most influence on how a professor is rated. The other variables easiness and raterInterest have dispersed data and seem to effect the other variables in a similar way which shows that they don’t have much a sway with rating a professor."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-5",
    "href": "posts/HW3_ShoshanaBuck.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.You can use student.survey in the R console, after loading the package, to see what each variable means.\n\nA\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\n\nCode\nstudent.survey\n\n\nError in eval(expr, envir, enclos): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#i-y-political-ideology-and-x-religiosity",
    "href": "posts/HW3_ShoshanaBuck.html#i-y-political-ideology-and-x-religiosity",
    "title": "Homework 3",
    "section": "(i) y = political ideology and x = religiosity",
    "text": "(i) y = political ideology and x = religiosity\n\n\nCode\ns<- ggplot(data = student.survey, aes( x= as.numeric(pi), y= as.numeric(re))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(pi), y = as.numeric(re))): object 'student.survey' not found\n\n\nCode\ns\n\n\nError in eval(expr, envir, enclos): object 's' not found"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#ii-y-high-school-gpa-and-x-hours-of-tv-watching",
    "href": "posts/HW3_ShoshanaBuck.html#ii-y-high-school-gpa-and-x-hours-of-tv-watching",
    "title": "Homework 3",
    "section": "(ii) y = high school GPA and x = hours of TV watching",
    "text": "(ii) y = high school GPA and x = hours of TV watching\n\n\nCode\np<- ggplot(data = student.survey, aes( x= tv, y= hi)) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = tv, y = hi)): object 'student.survey' not found\n\n\nCode\np\n\n\nError in eval(expr, envir, enclos): object 'p' not found\n\n\n\nB\nThe first graph that shows a positive correlation with political ideology and religiosity, the stronger political beliefs one has the more religious they are. The second graph shows that the more tv people watch the lower ones high school GPA will be."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html",
    "href": "posts/HW3_KarenKimble.html",
    "title": "DACSS 603 HW 3",
    "section": "",
    "text": "Code\n# Setup\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)"
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-1",
    "href": "posts/HW3_KarenKimble.html#question-1",
    "title": "DACSS 603 HW 3",
    "section": "Question 1",
    "text": "Question 1\n\nPart 1\nThe predictor is the ppgdp, the gross national product per person in U.S. dollars, and the response is fertility, the birth rate per 1000 females.\n\n\nPart 2\n\n\nCode\ndata(UN11)\noptions(scipen = 999)\nplot(fertility ~ ppgdp, data = UN11, main=\"Fertility by GDP per Capita\", xlab = \"GDP per Capita\", ylab = \"Fertility Rate per 1000 Females\")\n\n\n\n\n\nA straight-line function does not seem applicable to this graph because the trend of the data appears closer to a quadratic or exponential function. There are very many high values of fertility rate on the very low end of the x axis, but then this sharply changes between 10,000 and 20,000.\n\n\nPart 3\n\n\nCode\nplot(log(fertility) ~ log(ppgdp), data = UN11, main=\"Log of Fertility by Log of GDP per Capita\", xlab = \"Log of GDP per Capita\", ylab = \"Log of Fertility Rate per 1000 Females\")\n\n\n\n\n\nUsing the logs of each variable results in a different graph where a straight line would fit better than in the previous graph. A simple linear regression model seems plausible here."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-2",
    "href": "posts/HW3_KarenKimble.html#question-2",
    "title": "DACSS 603 HW 3",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\nThe slope of the prediction equation might decrease because the British pound is worth more U.S. dollars, so the slope would be increasing by the same number of units (for both 1.33 U.S. dollars and 1 British pound) but over a larger span of the x-axis.\n\n\nPart B\nThe correlation would not change because the relationship between annual income and the dependent/response variable is the same regardless of units of measurement."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-3",
    "href": "posts/HW3_KarenKimble.html#question-3",
    "title": "DACSS 603 HW 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\npairs(water, upper.panel = NULL)\n\n\n\n\n\nIt looks like there has been a lot more runoff at the OPRC, OPSLAKE, BSAAM, and APSLAkE in recent years. Runoff in the earliest years seemed to me more random and spread out."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-4",
    "href": "posts/HW3_KarenKimble.html#question-4",
    "title": "DACSS 603 HW 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\npairs(Rateprof[,8:12], upper.panel = NULL)\n\n\n\n\n\nIt looks like as the ratings of professors’ quality of teaching increased, helpfulness and clarity ratings also increased. However, easiness and interest ratings don’t seem to be as correalted with these variables."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-5",
    "href": "posts/HW3_KarenKimble.html#question-5",
    "title": "DACSS 603 HW 3",
    "section": "Question 5",
    "text": "Question 5\n\nPolitical Ideology vs Religiosity\n\nPart A\n\n\nCode\ndata(student.survey)\nstudent.survey$pi <- unclass(student.survey$pi)\nstudent.survey$re <- unclass(student.survey$re)\n\n# In the political ideology variable, very conservative = 7 and very liberal = 1\n\n# In the religiosity variable, attending religious services every week = 4, and never = 1\n\nplot(pi ~ re, data = student.survey, main = \"Political Ideology by Religiosity\", xlab = \"Religiousity\", ylab = \"Political Ideology\")\n\n\n\n\n\n\n\nPart B\n\n\nCode\nfit1 <- lm(pi ~ re, data = student.survey)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189     0.0327 *  \nre            0.9704     0.1792   5.416 0.00000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 0.000001221\n\n\nFor the variables political ideology and religiosity, there seems to be a correlation between them. The p-value for religiosity is much smaller than the alpha value of 0.05, indicating that there is statistically significant evidence showing religiosity affects students’ political ideology. The two are positively related, meaning that, within the regression model, for an increase in religiosity (attending more religious services) by 1, there is a 0.97 increase in political ideology (more conservative or less liberal).\n\n\n\nHigh School GPA versus TV Watching\n\nPart A\n\n\nCode\nplot(hi ~ tv, data = student.survey, main = \"High School GPA by Hours of TV Watched\", xlab = \"Avg Hours/Week Watching TV\", ylab = \"High School GPA\")\n\n\n\n\n\n\n\nPart B\n\n\nCode\nfit2 <- lm(hi ~ tv, data = student.survey)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323 <0.0000000000000002 ***\ntv          -0.018305   0.008658  -2.114              0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThere also seems to be a correlation between the variables high school GPA and hours of TV watched per week. The p-value for hours of TV is less than the alpha value of 0.05, indicating that there is statistically significant evidence showing TV watching affects students’ high school GPA. The variables are negatively related: for every hour of TV watched per week (on average), there is a 0.018 decrease in high school GPA based on the regression model."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html",
    "href": "posts/RahulGundeti_DACSS603_HW2.html",
    "title": "DACSS603_HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "title": "DACSS603_HW2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "title": "DACSS603_HW2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\nsample_size <- c(539, 847)\nmwt <- c(19, 18)\ns_stddev <- c(10, 9)\n\nsurgery <- data.frame(procedure, sample_size, mwt, s_stddev)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstd_error <- s_stddev / sqrt(sample_size)\nstd_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = sample_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nConfidence_Interval <- c(mwt - t_score * std_error,\n        mwt + t_score * std_error)\nConfidence_Interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe above results are obtained by fitting the 90% confidence interval level for the sample mean wait time for both the bypass surgery and the angiograph.\nBypass Surgery mean wait time : 18.29029 and 19.70971 days\nAngiograph mean wait time: 17.49078 and 18.50922 days\nThe comparision shows that the wait time for Angiograph is shorter than that of Bypass Surgery."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "title": "DACSS603_HW2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n95 percent confidence interval: 0.5189682 0.5805580\nThe sample estimate for the point p, from the sample who believes that college is necessary for success is: 0.5499515"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "title": "DACSS603_HW2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary sample size is: 278."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "title": "DACSS603_HW2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "title": "DACSS603_HW2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "title": "DACSS603_HW2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "title": "DACSS603_HW2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "title": "DACSS603_HW2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW3_SteveONeill.html",
    "href": "posts/HW3_SteveONeill.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\ndata(\"UN11\")\ndata(\"water\")\ndata(\"Rateprof\")\ndata(\"student.survey\")\nQuestions:"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-1",
    "href": "posts/HW3_SteveONeill.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n1.1.1. Identify the predictor and the response.\nThe independent (predictor) variable is ppgdp. The response (dependent) variable is fertility.\n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\nThis data looks curvilinear. A straight-line mean function is not favored.\n\n\nCode\nlibrary(ggplot2)\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\n\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\nYes, in the Log-Log model, linear regression seems more likely to work.\n\n\nCode\nlibrary(ggplot2)\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  ggtitle(\"Log-Log\") +\n  geom_smooth(method = 'lm', se = F)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-2",
    "href": "posts/HW3_SteveONeill.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\n\nHow, if at all, does the slope of the prediction equation change?\n\n\nI assume “responses” mean responses to a survey. I have created fake survey data to understand the question:\n\n\nCode\nsatisfaction <- c(1,2,3,4,5)\nus_income <- c(20010,30450,40000,51000,75000)\nuk_income <- c(us_income / 1.33) \n\nmod_us <- lm(satisfaction ~ us_income)\nsummary(mod_us)\n\n\n\nCall:\nlm(formula = satisfaction ~ us_income)\n\nResiduals:\n       1        2        3        4        5 \n-0.29520 -0.05966  0.24105  0.43559 -0.32178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.700e-01  4.294e-01  -0.396    0.719   \nus_income    7.322e-05  9.092e-06   8.054    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3839 on 3 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9411 \nF-statistic: 64.86 on 1 and 3 DF,  p-value: 0.003999\n\n\nCode\nmod_uk <- lm(satisfaction ~ uk_income)\nsummary(mod_uk)\n\n\n\nCall:\nlm(formula = satisfaction ~ uk_income)\n\nResiduals:\n       1        2        3        4        5 \n-0.29520 -0.05966  0.24105  0.43559 -0.32178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.700e-01  4.294e-01  -0.396    0.719   \nuk_income    9.739e-05  1.209e-05   8.054    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3839 on 3 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9411 \nF-statistic: 64.86 on 1 and 3 DF,  p-value: 0.003999\n\n\nIn the UK version, the slope is multiplied by 1.33.\n\n\nHow, if at all, does the correlation change?\n\n\nSince only the units are changing, the correlation does not change. See:\n\n\nCode\ncor(satisfaction,us_income)\n\n\n[1] 0.9776455\n\n\nCode\ncor(satisfaction,uk_income)\n\n\n[1] 0.9776455"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-3",
    "href": "posts/HW3_SteveONeill.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water)\n\n\n\n\n\npair() shows pairwise correlations. It looks like OPBC, OPRC, OPSLAKE, and BSAAM are very similar to each other. Likewise, APSLAK, APSAB, and APMAM are similar to each other. Taken separately, the two groups are not similar to each other. There is no pattern in the year of observation. I would be comfortable making predictions within the groups individually, but not based off the year."
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-4",
    "href": "posts/HW3_SteveONeill.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\nQuality, helpfulness, and clarity are all highly correlated. This tells me that perceptions of quality really hinge on helpfulness and clarity from the professor. However, classes which are high quality, helpful, and clear are not always easy (although there is some kind of positive correlation).\nTo some degree, it does look like each pairwise combination is positively correlated, with the possible exception of raterInterest and easiness.\n\n\nCode\nratings <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(ratings)"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-5",
    "href": "posts/HW3_SteveONeill.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n\n\n\nCode\n#Levels look to be correct already:\n#as.factor(student.survey$pi)\n\nsummary(lm(as.numeric(student.survey$pi) ~ as.numeric(student.survey$re)))\n\n\n\nCall:\nlm(formula = as.numeric(student.survey$pi) ~ as.numeric(student.survey$re))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     0.9308     0.4252   2.189   0.0327 *  \nas.numeric(student.survey$re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nsummary(lm(as.numeric(student.survey$hi) ~ as.numeric(student.survey$tv)))\n\n\n\nCall:\nlm(formula = as.numeric(student.survey$hi) ~ as.numeric(student.survey$tv))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                    3.441353   0.085345  40.323   <2e-16 ***\nas.numeric(student.survey$tv) -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n(a)Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n\nCode\n#as.numeric(student.survey$pi)\n\n#  (i) y = political ideology and x = religiosity\n\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point()\n\n\n\n\n\nCode\npi_re <- student.survey %>% select(pi, re)\npairs(pi_re)\n\n\n\n\n\nCode\n#Alternate way:\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  ggtitle (\"Religiosity and Political Ideology\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n#(ii) y = high school GPA and x = hours of TV watching\n\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = hi, y = tv)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  ggtitle (\"High School GPA and TV watched (in hours)\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\n#Alternate way:\n\nhi_tv  <- student.survey %>% select(hi, tv)\npairs(hi_tv)\n\n\n\n\n\n\n\nSummarize and interpret results of inferential analyses.\n\n\n\nPolitical Ideology ~ Religiosity\nPolitical ideology is slightly correlated with religiosity with an R-squared of 0.3359. The results are statistically significant. However, I would not say this is quite enough for predictive purposes because of that low R-squared value.\n\n\nHigh School GPA ~ TV Watched\nLooking at the scatterplot makes it appear that this data is all over the place with outliers. It does seem the higher GPA students do watch the least TV, and the linear regression is technically statistically significant with a p-value of 0.0388. However, the R-squared value is a very low 0.07156. I would not make any inferences about GPA’s causal effect on TV."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html",
    "href": "posts/KalimahMuhammad_hw3.html",
    "title": "Homework #3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-1-united-nations",
    "href": "posts/KalimahMuhammad_hw3.html#question-1-united-nations",
    "title": "Homework #3",
    "section": "Question 1: United Nations",
    "text": "Question 1: United Nations\n\n\nCode\ndata(UN11) #load United Nations data\n\n\n\n1.1.1\nThe predictor variable is ppgdp (the gross national product per person in USD) and the response or outcome variable in fertility (the birth rate per 1000 females).\n\n\n1.1.2\n\n\nCode\nggplot(UN11, aes(x=ppgdp, y=fertility))+ geom_point()\n\n\n\n\n\nFertility has the most variability at zero gross national product per person (ppgdp) where countries range from birthrate of 1 to 7 per 1000 females. There is a sharp decline thereafter where the fertility rate is consistently 3 or under and hovers between slightly above 2 and 1 for countries with a ppgdp above 30,000.\n\n\nCode\nplot(x=UN11$ppgdp, y=UN11$fertility)\n\n\n\n\n\nBased on the plot above, there is little difference between the two plots.\n\n\n1.1.3.\n\n\nCode\nplot(x= log(UN11$ppgdp), y=UN11$fertility)\n\n\n\n\n\nUsing natural logarithms, the model seems more plausible as the data becomes normalized."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-2-annual-income",
    "href": "posts/KalimahMuhammad_hw3.html#question-2-annual-income",
    "title": "Homework #3",
    "section": "Question 2: Annual Income",
    "text": "Question 2: Annual Income\n\n2a.\nWhen the British pound is used instead of the dollar, the steep of the slop minimizes.\n\n\n2b.\nTHe correlation does not change when factoring in pound in lieu of the dollar."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-3-water-runoff-in-the-sierras",
    "href": "posts/KalimahMuhammad_hw3.html#question-3-water-runoff-in-the-sierras",
    "title": "Homework #3",
    "section": "Question 3: Water runoff in the Sierras",
    "text": "Question 3: Water runoff in the Sierras\nUsing the pairwise scatterplot, we find some pairs are better represented by a straight line than others. This seems to be common among mountain ranges with similar starting initials (i.e. of the ranges starting with “A” or those starting with “O”).In each case, those pairs have a clear positive slope. Examining the percipitation by year and mountain range shows a wide range of variability with a singular outlier for each mountain range.\n\n\nCode\ndata(water) #load water data\npairs(water) #plot pairs\n\n\n\n\n\n\n\nCode\nfit_water <- lm(Year~APMAM+ APSAB+ APSLAKE+ OPBPC+ OPRC+ OPSLAKE+ BSAAM, data=water) #create linear regression model based on year and mountain range\nsummary(fit_water) #summarize model\n\n\n\nCall:\nlm(formula = Year ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE + BSAAM, data = water)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.135  -7.762  -1.500   8.473  27.092 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.962e+03  7.979e+00 245.831   <2e-16 ***\nAPMAM       -1.256e+00  1.158e+00  -1.084    0.286    \nAPSAB       -7.327e-01  2.494e+00  -0.294    0.771    \nAPSLAKE      2.351e+00  2.276e+00   1.033    0.309    \nOPBPC       -7.900e-02  7.543e-01  -0.105    0.917    \nOPRC        -1.975e+00  1.170e+00  -1.687    0.100    \nOPSLAKE      5.219e-01  1.369e+00   0.381    0.705    \nBSAAM        3.353e-04  2.722e-04   1.232    0.226    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.34 on 35 degrees of freedom\nMultiple R-squared:  0.1949,    Adjusted R-squared:  0.03388 \nF-statistic:  1.21 on 7 and 35 DF,  p-value: 0.3231"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-4-professor-ratings",
    "href": "posts/KalimahMuhammad_hw3.html#question-4-professor-ratings",
    "title": "Homework #3",
    "section": "Question 4: Professor ratings",
    "text": "Question 4: Professor ratings\n\n\nCode\ndata(\"Rateprof\") #load data\nRateprof%>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)%>%\npairs()\n\n\n\n\n\nBased on the scatterplot, quality appears to be positive related to helpfulness and clarity. There’s more variablity in results for easiness and aterInterest."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-5-student-survey",
    "href": "posts/KalimahMuhammad_hw3.html#question-5-student-survey",
    "title": "Homework #3",
    "section": "Question 5: Student Survey",
    "text": "Question 5: Student Survey\n\n\nCode\ndata(\"student.survey\")\nfit_smss<- lm(factor(pi) ~ re, data = student.survey)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\nCode\nplot(x=student.survey$re, y=student.survey$pi)\n\n\n\n\n\n\n\nCode\nfit_smss2<- lm(tv ~ hi, data = student.survey)\nsummary(fit_smss2)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.600 -3.790 -1.167  2.408 27.746 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.200      6.175   3.271   0.0018 **\nhi            -3.909      1.849  -2.114   0.0388 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.528 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\nplot(x=student.survey$hi, y=student.survey$tv)\n\n\n\n\n\nThe summary above shows there is a statistically signifcate relationship between high school GPA and the number of TV hours watched per week."
  },
  {
    "objectID": "posts/hw1.html",
    "href": "posts/hw1.html",
    "title": "Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot)\n\n\nError in library(ggplot): there is no package called 'ggplot'\n\n\nCode\nlungcap<- read_excel(\"LungCapData.xls\") \n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/hw1.html#lungcapdata",
    "href": "posts/hw1.html#lungcapdata",
    "title": "Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n``\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n \n### 1b. Compare the probability distribution of the LungCap with respect to Males and Females? \n\n\nError: attempt to use zero-length variable name\n\n\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Gender): object 'lungcap' not found\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Smoke): object 'lungcap' not found\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\nError in ggplot(lungcap, aes(x = LungCap)): object 'lungcap' not found\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nLung capacity for those under age 13 is 6.36 for non-smokers and 7.20 for smokers.\n\n\nCode\nlungcap %>%\n+ filter(Age <= 13)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 14 to 15\nlungcap%>%\n+ filter(Age=<15 & Age >=14)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 16 to 17\nlungcap%>%\n+     filter(Age=<17>=16)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those 18 and older\nlungcap%>%\n+ filter(Age>=18)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\n\nError: <text>:6:6: unexpected symbol\n5: \n6: Lung capacity\n        ^\n\n\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/hw1.html#inmate-data",
    "href": "posts/hw1.html#inmate-data",
    "title": "Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\nx<- c(0, 1, 2, 3, 4)\ny<- c(128, 434, 160, 64, 24)\nprison <-data.frame(x,y)\nView(prison)\n\n\nWarning in View(prison): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20% 2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69% 2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89% 2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11% 2e. What is the expected value for the number of prior convictions? 84% 2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison, y= NULL)\nsd(rnorm(810))\nThe standard deviation is 1.02.\n\n\nError: <text>:3:5: unexpected symbol\n2: sd(rnorm(810))\n3: The standard\n       ^"
  },
  {
    "objectID": "posts/KPopiela_HW1.html",
    "href": "posts/KPopiela_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(lsr)\n\nError in library(lsr): there is no package called 'lsr'\n#Question 1"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "href": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "title": "HW1",
    "section": "1a/1b. What does the distribution of LungCap look like?",
    "text": "1a/1b. What does the distribution of LungCap look like?\n\nHint: Plot a histogram with probability density on the y axis\n\n\nHint: make boxplots separated by gender using the boxplot() function\n\nLungCap <- read_xls(\"_data/LungCapData.xls\")\nhead(LungCap)\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n\nhist(LungCap$LungCap)\n\n\n\n\n\nLungCap_MF <- LungCap %>%\n  arrange(LungCap, Gender) %>%\n  group_by(Gender)\nboxplot(LungCap_MF$LungCap ~ LungCap_MF$Gender)\n\n\n\n#I wanted to change the axis labels to \"Gender\" (x) and \"Lung Capacity\" (y), but after an hour and a half of trying to no avail, I had to call it for my own sanity.\n\n\ncolnames(LungCap)\n\n[1] \"LungCap\"   \"Age\"       \"Height\"    \"Smoke\"     \"Gender\"    \"Caesarean\""
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "HW1",
    "section": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?",
    "text": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\nLungCap_smoke <- LungCapData %>%\n  select(LungCap, Smoke) %>%\n  group_by(Smoke)\n\nError in select(., LungCap, Smoke): object 'LungCapData' not found\n\nhead(LungCap_smoke)\n\nError in head(LungCap_smoke): object 'LungCap_smoke' not found\n\n\n\nsummarise(LungCap_smoke, mean(LungCap))\n\nError in summarise(LungCap_smoke, mean(LungCap)): object 'LungCap_smoke' not found\n\n#The mean lung capacities for non-smokers and smokers is 7.77 and 8.65 respectively. Does this make sense? No. One would expect that the mean lung capacity for non-smokers would be higher, but that is not the case here. Let's do some digging to see what the range of values for smokers' and non-smokers' lung capacity. I also want to look at how many people voted \"yes\" or \"no\"; it could be that fewer people (with higher lung capacity) voted \"yes,\" contributing to the higher mean.\n\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"yes\")\n\nError in filter(., Smoke == \"yes\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"no\")\n\nError in filter(., Smoke == \"no\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\n##Lung capacity for smokers ranges from 3.850 to 13.325, while the range for non-smokers is 0.507 to 14.675. Right off the bat, smokers have a higher minimum value, which prevents the mean from being dragged down during calculation. Non-smokers' minimum value is 0.507, an outlier which does seem to have an effect on this category's mean. \n\n\nLungCap_smoke %>%\n  count(Smoke)\n\nError in count(., Smoke): object 'LungCap_smoke' not found\n\n#Out of 725 respondents only 77 voted yes and 648 voted no, so I was right with my guess as to what caused the difference in mean lung capacity between smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "HW1",
    "section": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.",
    "text": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#To start, I'm going to calculate the range and mean of each of the above age groups, as well as a tally of how many are and aren't smokers.\n\n#a) Less than or equal to 13\nLC_Age13 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age <= 13)\nrange(LC_Age13$LungCap)\n\n[1]  0.507 12.050\n\n#The range of lung capacity values for children under the age of 13 is 0.507 to 12.050. The mean is 6.412.\n\n\nsummarise(LC_Age13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.41\n\n\n\nLC_Age13 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no      401\n2 yes      27\n\n#401 individuals 13 and under responded that they don't smoke, while 27 said they do. Compared to the initial calculations for the whole survey, the mean value is slightly lower, which is likely indicative of the fact that children have smaller lungs than adults and therefore have less lung capacity. Something important to note, however, is that this age group accounts for 428 of the total 725 responses (about 59%).\n\n\n#b) 14 to 15 \nLC_Age145 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 14:15)\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age145$LungCap)\n\n[1]  5.625 12.900\n\n#The minimum and maximum lung capacity values for individuals aged 14-15 are 5.625 and 12.900. The mean is 8.842.\n\n\nsummarise(LC_Age145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.84\n\n\n\nLC_Age145 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       44\n2 yes       8\n\n#Out of the 52 respondents in this age group, 44 stated that they don't smoke and 8 said that they do. The 14-15y/o age group is MUCH smaller than the \"13 and under\" one (it makes up only 12% of total responses). The percentage of smokers to non-smokers in each of the above age groups,is 7% and 18% respectively. If you were to take these percentages at face value without taking sample size into account, it would look as if the 14-15 y/o age group makes up 18% of the total 725 responses. In reality, this sample accounts for 6% of the total, making its impact relatively low.\n\n\n#c) 16 to 17\nLC_Age167 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 16:17)\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age167$LungCap)\n\n[1]  5.675 13.375\n\n#The minimum and maximum lung capacity values for individuals ages 16 to 17 are 5.675 and 13.375. The mean is 10.058.\n\n\nsummarise(LC_Age167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.1\n\n\n\nLC_Age167 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       40\n2 yes       8\n\n#This sample is similar in size to the previous (14-15 year olds) with only 48 responses, and smokers make up 20% of the responses. Now lets discuss the other figures.  \n\n#The mean lung capacity for 16-17 year olds is 10.058, 1.216 units higher than the previous age group, and 3.646 units higher than the \"13 and under\" age group. As of this point in my calculations, the only relationship seems to be between age and lung capacity rather than smoking and lung capacity; this is due to the facts that: 1) not that many people ages 0-17 smoke, and 2) the sample sizes for the 14-17 age group is 100 compared to the 428 responses in the \"13 and under\" group.\n\n\nLC_Age18p <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age >= 18)\nrange(LC_Age18p$LungCap)\n\n[1]  7.750 14.675\n\n#The minimum and maximum range values for the 18+ age group are 7.750 and 14.675. The mean is 10.965.\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\nLC_Age18p %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       65\n2 yes      15\n\n#Ok so what we've learned here is that this survey was HEAVILY focused on kids 13 and younger; although the sample for this age group is larger than the previous 2, it's still only 80 out of 725 responses (about 11% of total respondents). The mean lung capacity for this group is the highest of all of them at 10.965, but this still doesn't seem to show a relationship between smoking and lung capacity. Rather, at least to me, it shows a relationship between age and lung capacity (i.e. stage of lung development and lung capacity).\n\n##1e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n#I will place a comparison of all values produced by the following calculations at the bottom. (I will go through the smoker and non-smoker calculations first)\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"yes\")\nrange(LCu13$LungCap)\n\n[1]  3.850 10.275\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            7.20\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"yes\")\nrange(LCu145$LungCap)\n\n[1]  6.225 11.025\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.36\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"yes\")\nrange(LCu167$LungCap)\n\n[1]  7.550 11.775\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            9.06\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"yes\")\nrange(LCu18p$LungCap)\n\n[1]  8.200 13.325\n\n\n\nsummarise(LCu18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.5\n\n\n\n#Now for the non-smoker calculations.\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"no\")\nrange(LCu13$LungCap)\n\n[1]  0.507 12.050\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.36\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"no\")\nrange(LCu145$LungCap)\n\n[1]  5.625 12.900\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.93\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"no\")\nrange(LCu167$LungCap)\n\n[1]  5.675 13.375\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.3\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"no\")\nrange(LCu18p$LungCap)\n\n[1]  7.750 14.675\n\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\n#Comparison!!  \n  # 13 and under smokers: range = 3.850 to 10.275, mean = 7.202\n  # 13 and under non-smokers: range = 0.507 and 12.050, mean = 6.359 \n    \n  # 14-15 smokers: range = 6.225 and 11.025, mean = 8.359\n  # 14-15 non-smokers: range = 5.625 and 12.900, mean = 8.930  \n  \n  # 16-17 smokers: range = 7.550 and 11.775, mean = 9.063\n  # 16-17 non-smokers: range = 5.675 and 13.375, mean = 10.257  \n    \n  # 18+ smokers: range = 8.200 and 13.325,mean = 10.513 \n  # 18+ non-smokers: range = 7.750 and 14.675, mean = 10.965\n\n\n#The answers I got for smokers vs. non-smokers are obviously different, but I wouldn't say they necessarily convey something different to what I interpreted from 1d. I don't see a relationship between smoking and lung capacity, and I certainly don't see a massive difference in the values comparing the lung capacity of smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "href": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "title": "HW1",
    "section": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.",
    "text": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\ncov(LungCap$LungCap, LungCap$Age)\n\n[1] 8.738289\n\n\n\ncor(LungCap$LungCap, LungCap$Age)\n\n[1] 0.8196749\n\n\n\n#Covariance between lung capacity and age: 8.738.  \n#Correlation between lung capacity and age: 0.820  \n\n#I'm not totally confident in my understanding of covariance yet, but from what I know, it's the positive or negative relationship between two variables and the further the value is from 0, the stronger the relationship is. And the covariance between lung capacity and age is 8.738. Correlation gets stronger the closer the value gets to 1 or -1; the correlation between lung capacity and age for 'LungCap' dataset is 0.820, a figure relatively close to 1, so I would say there is a moderate to strong correlation between the two variables in question here.\n\n#Question 2\n###Let X=number of prior convictions for prisoners at a state prison at which there are 810 inmates."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "HW1",
    "section": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nconrange <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nconrange\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n#To start I just wanted to present a visualization of the frequency of each categorical variable.  \n  # 0 prior convictions = 128  \n  # 1 prior conviction = 434  \n  # 2 prior convictions = 160  \n  # 3 prior convictions = 64  \n  # 4 prior convictions = 24\n\n\nprop.table(table(conrange))[0:2]\n\nconrange\n        0         1 \n0.1580247 0.5358025 \n\n#By combining the probability values for 0 and 1, we can see that the probability of a randomly selected inmate having fewer than 2 prior convictions is 0.694."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "HW1",
    "section": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nprop.table(table(conrange))[0:3]\n\nconrange\n        0         1         2 \n0.1580247 0.5358025 0.1975309 \n\n#By using the same math above, the probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "HW1",
    "section": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nprop.table(table(conrange))[4:5]\n\nconrange\n         3          4 \n0.07901235 0.02962963 \n\n#The probability that a randomly selected inmate has more than 2 prior convictions is 0.108."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "href": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "title": "HW1",
    "section": "2d. What is the expected value for the number of prior convictions?",
    "text": "2d. What is the expected value for the number of prior convictions?\n\nprior_con_range <- c(0,1,2,3,4)\nprobs <- c(0.158,0.535,0.197,0.079,0.029)\nc(prior_con_range %*% probs)\n\n[1] 1.282\n\n#The expected value for the number of prior convictions is 1.282"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "href": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "title": "HW1",
    "section": "2e. Calculate the variance and the standard deviation for the Prior Convictions.",
    "text": "2e. Calculate the variance and the standard deviation for the Prior Convictions.\n\nvar(conrange)\n\n[1] 0.8572937\n\nsd(conrange)\n\n[1] 0.9259016\n\n#The variance and standard deviation for prior convictions are 0.857 and 0.925 respectively."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 603 Introduction to Quantitative Analysis",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 603, Introduction to Quantitative Analysis.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nHW1 Quat\n\n\nAmy Quarkume\n\n\n\n\n\n\nGraphs and Probz\n\n\n\n\n\n\n\n\nKPopiela_final p1\n\n\n\n\n\n\n\n\nKPopiela_Finalp2\n\n\n\n\n\n\n\n\nKPopiela HW2\n\n\n\n\n\n\n\n\nHW3\n\n\nKatie Popiela\n\n\n\n\n\n\ntheme: default\n\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\n\n\n\n\nHW1\n\n\nKatie Popiela\n\n\n\n\nNov 12, 2022\n\n\nFinal Project Part 2\n\n\nKen Docekal\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nEmily Duryea\n\n\n\n\nNov 11, 2022\n\n\nHomework 3\n\n\nMani Kanta\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Update\n\n\nSaaradhaa M\n\n\n\n\nNov 8, 2022\n\n\nFinal Part 2\n\n\nCaleb Hill\n\n\n\n\nNov 4, 2022\n\n\nHomework 3\n\n\nNick Boonstra\n\n\n\n\nNov 1, 2022\n\n\nHomework 3\n\n\nOmer Yalcin\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nLindsay Jones\n\n\n\n\nOct 31, 2022\n\n\nHomework 3 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nNiyati Sharma\n\n\n\n\nOct 28, 2022\n\n\nHomework 3\n\n\nSaaradhaa M\n\n\n\n\nOct 27, 2022\n\n\nHomework 3\n\n\nEthan Campbell\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nDonny Snyder\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nLindsay Jones\n\n\n\n\nOct 16, 2022\n\n\nHomework 2 Solution\n\n\nDane Shelton\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Final Part 1\n\n\nTory Bartelloni\n\n\n\n\nNov 11, 2022\n\n\nFinal Project\n\n\nEthan Campbell\n\n\n\n\nOct 31, 2022\n\n\nHW3_EmmaRasmussen\n\n\nEmma Rasmussen\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 18, 2022\n\n\nHomework 2\n\n\nRoy Yoon\n\n\n\n\nOct 17, 2022\n\n\nHW2\n\n\nSteph Roberts\n\n\n\n\nOct 17, 2022\n\n\nKimble HW 2\n\n\nKaren Kimble\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nKalimah Muhammad\n\n\n\n\nOct 17, 2022\n\n\nHW2\n\n\nKen Docekal\n\n\n\n\nOct 15, 2022\n\n\nHomework 2\n\n\nEthan Campbell\n\n\n\n\nOct 14, 2022\n\n\nHomework 2\n\n\nCaleb Hill\n\n\n\n\nOct 12, 2022\n\n\nFinal Project: Part 1\n\n\nDane Shelton\n\n\n\n\nOct 11, 2022\n\n\nProject Proposal\n\n\nMEGHA JOSEPH\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Submission 1\n\n\nKaushika Potluri\n\n\n\n\nOct 11, 2022\n\n\nFinal_Project_1\n\n\nMani Kanta Gogula & Rahul Gundeti\n\n\n\n\nOct 11, 2022\n\n\nFinal project part 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 10, 2022\n\n\nFinal Project Proposal\n\n\nEmily Duryea\n\n\n\n\nOct 9, 2022\n\n\nFinal Project Proposal\n\n\nSaaradhaa M\n\n\n\n\nOct 9, 2022\n\n\nFinal Project: Diabetes Prediction\n\n\nSteph Roberts\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Proposal\n\n\nLindsay Jones\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project - Proposal\n\n\n\n\n\n\nOct 5, 2022\n\n\nHomework 1\n\n\nNick Boonstra\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603: Homework 1\n\n\nTory Bartelloni\n\n\n\n\nOct 3, 2022\n\n\nShoshanaBuck-HW1\n\n\nShoshana Buck\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nSteph Roberts\n\n\n\n\nOct 3, 2022\n\n\nDuryea Homework 1\n\n\nEmily Duryea\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nSteph Roberts\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKaren Detter\n\n\n\n\nOct 2, 2022\n\n\nDACSS603_HW1\n\n\nRahul Gundeti\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nLindsay Jones\n\n\n\n\nOct 1, 2022\n\n\nHomework 1\n\n\nSaaradhaa M\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 - Donny Snyder\n\n\nDonny Snyder\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 Solution\n\n\nDane Shelton\n\n\n\n\nSep 29, 2022\n\n\nHomework 1\n\n\nEmma Rasmussen\n\n\n\n\nSep 21, 2022\n\n\nHomework 1\n\n\nEthan Campbell\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nSteve O’Neill\n\n\n\n\nSep 20, 2022\n\n\nHomework 2\n\n\nSteve O’Neill\n\n\n\n\nAug 2, 2022\n\n\nHomework 1\n\n\nKaushika Potluri\n\n\n\n\nAug 2, 2022\n\n\nHW3\n\n\nKaren Detter\n\n\n\n\nAug 2, 2022\n\n\nHW 3 Solution\n\n\nDane Shelton\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nMegha joseph\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nDonny Snyder\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nEmma Rasmussen\n\n\n\n\nNov 11, 2022\n\n\nFinal Project\n\n\nKaren Detter\n\n\n\n\nNov 10, 2022\n\n\nfinalpart2\n\n\nShoshana Buck & Roy Yoon\n\n\n\n\nNov 1, 2022\n\n\nHomework 2\n\n\nOmer Yalcin\n\n\n\n\nOct 31, 2022\n\n\nHomework 3 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nOct 31, 2022\n\n\nHome Work 3\n\n\nMegha Joseph\n\n\n\n\nOct 31, 2022\n\n\nHW3\n\n\nKen Docekal\n\n\n\n\nOct 31, 2022\n\n\nHW3\n\n\nSteph Roberts\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nRoy Yoon\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nShoshana Buck\n\n\n\n\nOct 31, 2022\n\n\nDACSS 603 HW 3\n\n\nKaren Kimble\n\n\n\n\nOct 31, 2022\n\n\nHomework #3\n\n\nKalimah Muhammad\n\n\n\n\nOct 30, 2022\n\n\nDACSS 603: Homework 3\n\n\nTory Bartelloni\n\n\n\n\nOct 29, 2022\n\n\nHomework 3\n\n\nSteve O’Neill\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nMani Kanta Gogula\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nShoshana Buck\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nNiyati Sharma\n\n\n\n\nOct 17, 2022\n\n\nHomework 3\n\n\nDonny Snyder\n\n\n\n\nOct 17, 2022\n\n\nHW 2\n\n\nKaren Detter\n\n\n\n\nOct 17, 2022\n\n\nDACSS603_HW2\n\n\nRahul Gundeti\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nKaushika Potluri\n\n\n\n\nOct 15, 2022\n\n\nFinal Project\n\n\nEthan Campbell\n\n\n\n\nOct 14, 2022\n\n\nHomework 3\n\n\nCaleb Hill\n\n\n\n\nOct 13, 2022\n\n\nHomework 2\n\n\nEmma Rasmussen\n\n\n\n\nOct 12, 2022\n\n\nVoter Turnout and Partisan Bias in U.S. Presidential Elections\n\n\nNicholas Boonstra\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Part 1\n\n\nEmma Rasmussen\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nNiyati Sharma\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nKaren Detter\n\n\n\n\nOct 11, 2022\n\n\nfinalpart1\n\n\nShoshana Buck & Roy Yoon\n\n\n\n\nOct 11, 2022\n\n\nProject Rough Draft Proposal\n\n\nYakub Rabiutheen\n\n\n\n\nOct 10, 2022\n\n\nHomework 2\n\n\nSaaradhaa M\n\n\n\n\nOct 10, 2022\n\n\nFinal Project 1\n\n\nKen Docekal\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Part 1\n\n\nDonny Snyder\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project Pt 1\n\n\nKaren Kimble\n\n\n\n\nOct 5, 2022\n\n\nHomework 1 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 4, 2022\n\n\nHomework 1\n\n\nOmer Yalcin\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nMani Kanta Gogula\n\n\n\n\nOct 3, 2022\n\n\nHW1\n\n\nNiyati Sharma\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 3, 2022\n\n\nHOME WORK1 603\n\n\nMegha Joseph\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603 HW 1 Kimble\n\n\nKaren Kimble\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKen Docekal\n\n\n\n\nOct 3, 2022\n\n\nResubmission: Homework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 3, 2022\n\n\nHomework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nCaleb Hill\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nSep 20, 2022\n\n\nHomework 3\n\n\nYakub Rabiutheen\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Homework 2\n\n\nTory Bartelloni\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nNick Boonstra\n\n\n\n\nOct 2, 2022\n\n\nFinal Part 1\n\n\nCaleb Hill\n\n\n\n\nOct 1, 2022\n\n\nFinal Part 1\n\n\nSteve O’Neill\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nYakub Rabiutheen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/AboutTemplate_mani.html",
    "href": "about/AboutTemplate_mani.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#educationwork-background",
    "href": "about/AboutTemplate_mani.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#r-experience",
    "href": "about/AboutTemplate_mani.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#research-interests",
    "href": "about/AboutTemplate_mani.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hometown",
    "href": "about/AboutTemplate_mani.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hobbies",
    "href": "about/AboutTemplate_mani.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#fun-fact",
    "href": "about/AboutTemplate_mani.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Your Name\n\n\n\n\n\n\n\nNo matching items"
  }
]