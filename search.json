[
  {
    "objectID": "about/AboutTemplate_mani.html",
    "href": "about/AboutTemplate_mani.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#educationwork-background",
    "href": "about/AboutTemplate_mani.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#r-experience",
    "href": "about/AboutTemplate_mani.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#research-interests",
    "href": "about/AboutTemplate_mani.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hometown",
    "href": "about/AboutTemplate_mani.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hobbies",
    "href": "about/AboutTemplate_mani.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#fun-fact",
    "href": "about/AboutTemplate_mani.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Your Name\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 603 Introduction to Quantitative Analysis",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 603, Introduction to Quantitative Analysis.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\n\n\n\n\n-\n\n\n\n\n\n\n\n\nGraphs and Probz\n\n\n\n\n\n\n\n\ntheme: default\n\n\n\n\n\n\n\n\nKPopiela_Finalp2\n\n\n\n\n\n\n\n\nKPopiela_final p1\n\n\n\n\n\n\n\n\nHW1\n\n\nKatie Popiela\n\n\n\n\n\n\nKPopiela HW2\n\n\n\n\n\n\n\n\nHW3\n\n\nKatie Popiela\n\n\n\n\n\n\nHW4\n\n\n\n\n\n\n\n\nHW5\n\n\n\n\n\n\n\n\nHW1 Quat\n\n\nAmy Quarkume\n\n\n\n\nFeb 11, 2023\n\n\nHomework 4\n\n\nKaushika Potluri\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nEmily Duryea\n\n\n\n\nDec 8, 2022\n\n\nDACSS 603 Final Project Write-Up\n\n\nKaren Kimble\n\n\n\n\nDec 4, 2022\n\n\nHomework 5\n\n\nLindsay Jones\n\n\n\n\nDec 1, 2022\n\n\nFinal Project Part-2\n\n\nNiharika Pola\n\n\n\n\nDec 1, 2022\n\n\nFinal Project Part-1\n\n\nNiharika Pola\n\n\n\n\nNov 21, 2022\n\n\nFinal Part 2\n\n\nQuinn He\n\n\n\n\nNov 20, 2022\n\n\nDACSS 603: Final Part 2\n\n\nTory Bartelloni\n\n\n\n\nNov 14, 2022\n\n\nDACSS 603 Final Project Pt 2\n\n\nKaren Kimble\n\n\n\n\nNov 14, 2022\n\n\nHomework 4\n\n\nLindsay Jones\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nEmma Rasmussen\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Submission 2\n\n\nKaushika Potluri\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Proposal\n\n\nLindsay Jones\n\n\n\n\nNov 11, 2022\n\n\nFinal Project\n\n\nEthan Campbell\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nEmily Duryea\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Part 2\n\n\nDonny Snyder\n\n\n\n\nNov 11, 2022\n\n\nFinal project part 2\n\n\nMani Shanker Kamarapu\n\n\n\n\nNov 11, 2022\n\n\nFinal Project Update\n\n\nSaaradhaa M\n\n\n\n\nNov 10, 2022\n\n\nfinalpart2\n\n\nShoshana Buck & Roy Yoon\n\n\n\n\nNov 8, 2022\n\n\nFinal Part 2\n\n\nCaleb Hill\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nLindsay Jones\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nLindsay Jones\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nKaushika Potluri\n\n\n\n\nOct 15, 2022\n\n\nFinal Project\n\n\nEthan Campbell\n\n\n\n\nOct 11, 2022\n\n\nfinalpart1\n\n\nShoshana Buck & Roy Yoon\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Part 1\n\n\nEmma Rasmussen\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Submission 1\n\n\nKaushika Potluri\n\n\n\n\nOct 11, 2022\n\n\nFinal project part 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 11, 2022\n\n\nProject Rough Draft Proposal\n\n\nYakub Rabiutheen\n\n\n\n\nOct 11, 2022\n\n\nFinal_Project_1\n\n\nMani Kanta Gogula & Rahul Gundeti\n\n\n\n\nOct 10, 2022\n\n\nFinal Project Proposal\n\n\nEmily Duryea\n\n\n\n\nOct 9, 2022\n\n\nFinal Project: Diabetes Prediction\n\n\nSteph Roberts\n\n\n\n\nOct 9, 2022\n\n\nFinal Project Proposal\n\n\nSaaradhaa M\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project - Proposal\n\n\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Proposal\n\n\nLindsay Jones\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Part 1\n\n\nDonny Snyder\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project Pt 1\n\n\nKaren Kimble\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603 HW 1 Kimble\n\n\nKaren Kimble\n\n\n\n\nOct 3, 2022\n\n\nDuryea Homework 1\n\n\nEmily Duryea\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nCaleb Hill\n\n\n\n\nOct 2, 2022\n\n\nFinal Part 1\n\n\nCaleb Hill\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nLindsay Jones\n\n\n\n\nOct 1, 2022\n\n\nFinal Part 1\n\n\nSteve O’Neill\n\n\n\n\nAug 2, 2022\n\n\nHomework 1\n\n\nKaushika Potluri\n\n\n\n\nAug 2, 2022\n\n\nFinal Project Part 1\n\n\nQuinn He\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Final Part 1\n\n\nTory Bartelloni\n\n\n\n\nMay 10, 2023\n\n\nHomework-2\n\n\nNiharika Pola\n\n\n\n\nNov 27, 2022\n\n\nHomework 3\n\n\nNiharika Pola\n\n\n\n\nNov 14, 2022\n\n\nHomework 4 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nNov 7, 2022\n\n\nHomework 3\n\n\nKaushika Potluri\n\n\n\n\nNov 4, 2022\n\n\nHomework 3\n\n\nNick Boonstra\n\n\n\n\nNov 1, 2022\n\n\nHomework 2\n\n\nOmer Yalcin\n\n\n\n\nNov 1, 2022\n\n\nHomework 3\n\n\nOmer Yalcin\n\n\n\n\nOct 31, 2022\n\n\nHomework 3 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nOct 31, 2022\n\n\nHW3_EmmaRasmussen\n\n\nEmma Rasmussen\n\n\n\n\nOct 31, 2022\n\n\nDACSS 603 HW 3\n\n\nKaren Kimble\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 31, 2022\n\n\nHomework 3 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nRoy Yoon\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nShoshana Buck\n\n\n\n\nOct 28, 2022\n\n\nHomework 3\n\n\nSaaradhaa M\n\n\n\n\nOct 27, 2022\n\n\nHomework 3\n\n\nEthan Campbell\n\n\n\n\nOct 18, 2022\n\n\nHomework 2\n\n\nRoy Yoon\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nDonny Snyder\n\n\n\n\nOct 17, 2022\n\n\nKimble HW 2\n\n\nKaren Kimble\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nMani Kanta Gogula\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nShoshana Buck\n\n\n\n\nOct 17, 2022\n\n\nHW2\n\n\nSteph Roberts\n\n\n\n\nOct 17, 2022\n\n\nHomework 3\n\n\nDonny Snyder\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 15, 2022\n\n\nHomework 2\n\n\nEthan Campbell\n\n\n\n\nOct 14, 2022\n\n\nHomework 2\n\n\nCaleb Hill\n\n\n\n\nOct 14, 2022\n\n\nHomework 3\n\n\nCaleb Hill\n\n\n\n\nOct 13, 2022\n\n\nHomework 2\n\n\nEmma Rasmussen\n\n\n\n\nOct 10, 2022\n\n\nHomework 2\n\n\nSaaradhaa M\n\n\n\n\nOct 5, 2022\n\n\nHomework 1\n\n\nNick Boonstra\n\n\n\n\nOct 5, 2022\n\n\nHomework 1 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 4, 2022\n\n\nHomework 1\n\n\nOmer Yalcin\n\n\n\n\nOct 3, 2022\n\n\nHomework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKaren Detter\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nSteph Roberts\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603: Homework 1\n\n\nTory Bartelloni\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 - Donny Snyder\n\n\nDonny Snyder\n\n\n\n\nOct 1, 2022\n\n\nHomework 1\n\n\nSaaradhaa M\n\n\n\n\nSep 29, 2022\n\n\nHomework 1\n\n\nEmma Rasmussen\n\n\n\n\nSep 21, 2022\n\n\nHomework 1\n\n\nEthan Campbell\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nSteve O’Neill\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nYakub Rabiutheen\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nNick Boonstra\n\n\n\n\nSep 20, 2022\n\n\nHomework 2\n\n\nSteve O’Neill\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Homework 2\n\n\nTory Bartelloni\n\n\n\n\nDec 11, 2022\n\n\nHomework 5\n\n\nOmer Yalcin\n\n\n\n\nDec 10, 2022\n\n\nHomework 4\n\n\nOmer Yalcin\n\n\n\n\nDec 10, 2022\n\n\nHW5\n\n\nKen Docekal\n\n\n\n\nDec 9, 2022\n\n\nDACSS 603 HW 5\n\n\nKaren Kimble\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nMani Shanker Kamarapu\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nShoshana Buck\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nMani Kanta Gogula\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nQuinn He\n\n\n\n\nDec 9, 2022\n\n\nHomework #5\n\n\nKalimah Muhammad\n\n\n\n\nDec 9, 2022\n\n\nHW 5\n\n\nKaren Detter\n\n\n\n\nDec 9, 2022\n\n\nHomework 5\n\n\nNiyati Sharma\n\n\n\n\nDec 8, 2022\n\n\nHomework 5\n\n\nDonny Snyder\n\n\n\n\nDec 8, 2022\n\n\nHomework 5\n\n\nCaleb Hill\n\n\n\n\nDec 8, 2022\n\n\nHomework 5\n\n\nEthan Campbell\n\n\n\n\nDec 8, 2022\n\n\nHomework 5 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nDec 6, 2022\n\n\nHomework 5\n\n\nSaaradhaa M\n\n\n\n\nDec 2, 2022\n\n\nFinal Pt.2: Updates\n\n\nKalimah Muhammad\n\n\n\n\nDec 1, 2022\n\n\nHomework 5\n\n\n\n\n\n\nNov 27, 2022\n\n\nHomework 4\n\n\nNiharika Pola\n\n\n\n\nNov 27, 2022\n\n\nDACSS 603: Homework 4\n\n\nTory Bartelloni\n\n\n\n\nNov 27, 2022\n\n\nHomework 5\n\n\nSteve O’Neill\n\n\n\n\nNov 25, 2022\n\n\nDACSS603_HW4\n\n\nRahul Gundeti\n\n\n\n\nNov 24, 2022\n\n\nDACSS603_HW3\n\n\nRahul Gundeti\n\n\n\n\nNov 20, 2022\n\n\nFinal Project Proposal\n\n\nNiyati Sharma\n\n\n\n\nNov 17, 2022\n\n\nHomework 4 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nNov 16, 2022\n\n\nHomework 4\n\n\nNick Boonstra\n\n\n\n\nNov 16, 2022\n\n\nHomework 4\n\n\nSteve O’Neill\n\n\n\n\nNov 16, 2022\n\n\nHomework 4\n\n\nNiyati Sharma\n\n\n\n\nNov 14, 2022\n\n\nHomework 4\n\n\nDonny Snyder\n\n\n\n\nNov 14, 2022\n\n\nHomework 4\n\n\nEthan Campbell\n\n\n\n\nNov 14, 2022\n\n\nDACSS 603 HW 4\n\n\nKaren Kimble\n\n\n\n\nNov 14, 2022\n\n\nHomework 4\n\n\nSaaradhaa M\n\n\n\n\nNov 14, 2022\n\n\nHW4_ShoshanaBuck\n\n\nShoshana Buck\n\n\n\n\nNov 14, 2022\n\n\nHW4\n\n\nSteph Roberts\n\n\n\n\nNov 14, 2022\n\n\nHomework #4\n\n\nKalimah Muhammad\n\n\n\n\nNov 14, 2022\n\n\nHW4\n\n\nKen Docekal\n\n\n\n\nNov 14, 2022\n\n\nHOME WORK 4\n\n\nMegha Joseph\n\n\n\n\nNov 14, 2022\n\n\nDACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)\n\n\nNicholas Boonstra\n\n\n\n\nNov 14, 2022\n\n\nHomework 4 Solution\n\n\nDane Shelton\n\n\n\n\nNov 13, 2022\n\n\nHomework 4\n\n\nEmma Rasmussen\n\n\n\n\nNov 12, 2022\n\n\nHomework 4\n\n\nCaleb Hill\n\n\n\n\nNov 12, 2022\n\n\nHomework 4\n\n\nMani Shanker Kamarapu\n\n\n\n\nNov 12, 2022\n\n\nHomework 4\n\n\nMani Kanta Gogula\n\n\n\n\nNov 12, 2022\n\n\nFinal Project Part 2\n\n\nKen Docekal\n\n\n\n\nNov 12, 2022\n\n\nFinal Project: Part 2 (Update)\n\n\nDane Shelton\n\n\n\n\nNov 11, 2022\n\n\nHomework 4\n\n\nHW4\n\n\n\n\nNov 11, 2022\n\n\nHomework 3\n\n\nMani Kanta\n\n\n\n\nNov 11, 2022\n\n\nFinal Part 2\n\n\nKalimah Muhammad\n\n\n\n\nNov 11, 2022\n\n\nFinal Project - Part 2\n\n\nKaren Detter\n\n\n\n\nOct 31, 2022\n\n\nHW3\n\n\nSteph Roberts\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nQuinn He\n\n\n\n\nOct 31, 2022\n\n\nHomework #3\n\n\nKalimah Muhammad\n\n\n\n\nOct 31, 2022\n\n\nHW3\n\n\nKen Docekal\n\n\n\n\nOct 31, 2022\n\n\nHome Work 3\n\n\nMegha Joseph\n\n\n\n\nOct 31, 2022\n\n\nHomework 3\n\n\nNiyati Sharma\n\n\n\n\nOct 30, 2022\n\n\nDACSS 603: Homework 3\n\n\nTory Bartelloni\n\n\n\n\nOct 29, 2022\n\n\nHomework 3\n\n\nSteve O’Neill\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nQuinn He\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nKalimah Muhammad\n\n\n\n\nOct 17, 2022\n\n\nHW 2\n\n\nKaren Detter\n\n\n\n\nOct 17, 2022\n\n\nHW2\n\n\nKen Docekal\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nNiyati Sharma\n\n\n\n\nOct 17, 2022\n\n\nDACSS603_HW2\n\n\nRahul Gundeti\n\n\n\n\nOct 16, 2022\n\n\nHomework 2 Solution\n\n\nDane Shelton\n\n\n\n\nOct 12, 2022\n\n\nVoter Turnout and Partisan Bias in U.S. Presidential Elections\n\n\nNicholas Boonstra\n\n\n\n\nOct 12, 2022\n\n\nFinal Project: Part 1\n\n\nDane Shelton\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nKaren Detter\n\n\n\n\nOct 11, 2022\n\n\nProject Proposal\n\n\nMEGHA JOSEPH\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nNiyati Sharma\n\n\n\n\nOct 11, 2022\n\n\nProject Rough Draft Proposal\n\n\nYakub Rabiutheen\n\n\n\n\nOct 11, 2022\n\n\nProject Rough Draft Proposal\n\n\nYakub Rabiutheen\n\n\n\n\nOct 10, 2022\n\n\nFinal Project 1\n\n\nKen Docekal\n\n\n\n\nOct 9, 2022\n\n\nFinal Project: Diabetes Prediction - Part 2\n\n\nSteph Roberts\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nMani Kanta Gogula\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 3, 2022\n\n\nResubmission: Homework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKen Docekal\n\n\n\n\nOct 3, 2022\n\n\nHOME WORK1 603\n\n\nMegha Joseph\n\n\n\n\nOct 3, 2022\n\n\nHW1\n\n\nNiyati Sharma\n\n\n\n\nOct 3, 2022\n\n\nShoshanaBuck-HW1\n\n\nShoshana Buck\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nOct 2, 2022\n\n\nDACSS603_HW1\n\n\nRahul Gundeti\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 Solution\n\n\nDane Shelton\n\n\n\n\nSep 20, 2022\n\n\nHomework 3\n\n\nYakub Rabiutheen\n\n\n\n\nAug 2, 2022\n\n\nHomework 4\n\n\nQuinn He\n\n\n\n\nAug 2, 2022\n\n\nHW3\n\n\nKaren Detter\n\n\n\n\nAug 2, 2022\n\n\nHW 4\n\n\nKaren Detter\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nMegha joseph\n\n\n\n\nAug 2, 2022\n\n\nHW 3 Solution\n\n\nDane Shelton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html",
    "href": "posts/Blog Post 2_Kaushika Potluri.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "title": "Homework 1",
    "section": "1(a) Distribution of LungCap:",
    "text": "1(a) Distribution of LungCap:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe distribution appears to be very similar to a normal distribution, according to the histogram."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "title": "Homework 1",
    "section": "1(b)",
    "text": "1(b)\nThe boxplots below show the probability distributions grouped by Gender.\n\n\nCode\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nLooks like males have a slightly higher lung capacity than females."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nSurprisingly, the mean lung capacity is higher for smokers than it is for non-smokers."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n\nCode\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and lower\", Age == 14 | Age == 15 ~ \"14-15\", Age == 16 | Age == 17 ~ \"16-17\", Age >= 18 ~ \"18 and above\"))\narrange(df, Age)\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeGroup    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <chr>       \n 1   5.88      3   55.9 no    male   no        13 and lower\n 2   0.507     3   51.6 no    female yes       13 and lower\n 3   1.18      3   51.9 no    male   no        13 and lower\n 4   4.7       3   52.7 no    male   no        13 and lower\n 5   5.48      3   52.9 no    male   no        13 and lower\n 6   1.02      3   47   no    female no        13 and lower\n 7   2         3   51   no    female no        13 and lower\n 8   1.68      3   51.9 no    male   no        13 and lower\n 9   4.08      3   53.6 no    male   yes       13 and lower\n10   1.45      3   45.3 no    female no        13 and lower\n# … with 715 more rows\n\n\nCode\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMajority seem to be non-smokers, and looks like non-smokers seem to have higher lung capacity."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n\nCode\nclass(df$AgeGroup)\n\n\n[1] \"character\"\n\n\n\n\nCode\ndf$AgeGroup <- as.factor(df$AgeGroup) #converting to factor\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke `mean(LungCap)`\n  <fct>        <chr>           <dbl>\n1 13 and lower no               6.36\n2 13 and lower yes              7.20\n3 14-15        no               9.14\n4 14-15        yes              8.39\n5 16-17        no              10.5 \n6 16-17        yes              9.38\n7 18 and above no              11.1 \n8 18 and above yes             10.5 \n\n\nThe mean lung capacity for smokers aged 13 and under is greater than that of non-smokers in the same age group which is different from expectation. Non-smokers have higher mean lung capacity for ages 14-15, 16-17 and 18 and above. Either there may be an error or extreme outlier in the data for smokers aged 13 and under."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n\nCode\ncor(df$LungCap,df$Age)\n\n\n[1] 0.8196749\n\n\n\n\nCode\ncov(df$LungCap,df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity and age have a high positive correlation of 0.82, meaning that as age increases, lung capacity also does. The covariance is a little more challenging to interpret; the positive number indicates a positive association between lung capacity and age, but because covariance varies from negative infinity to infinity, it is difficult to judge the strength of the relationship. In most situations, I would choose to employ correlation."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "title": "Homework 1",
    "section": "2",
    "text": "2\n\n\nCode\ndf1 <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(df1, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "title": "Homework 1",
    "section": "2(a)",
    "text": "2(a)\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n# A tibble: 5 × 3\n    df1 Inmate_count Probability\n  <int>        <dbl>       <dbl>\n1     0          128      0.158 \n2     1          434      0.536 \n3     2          160      0.198 \n4     3           64      0.0790\n5     4           24      0.0296\n\n\n\n\nCode\nIP %>%\n  filter(df1 == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability is about 19.75%."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "title": "Homework 1",
    "section": "(b)",
    "text": "(b)\n\n\nCode\ndf2 <- IP %>%\n  filter(df1 < 2)\nsum(df2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "title": "Homework 1",
    "section": "2(c)",
    "text": "2(c)\n\n\nCode\ndf3 <- IP %>%\n  filter(df1 <= 2)\nsum(df3$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "title": "Homework 1",
    "section": "2(d)",
    "text": "2(d)\n\n\nCode\ndf4 <- IP %>%\n  filter(df1 > 2)\nsum(df4$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "title": "Homework 1",
    "section": "2(e)",
    "text": "2(e)\n\n\nCode\nIP <- mutate(IP, X = df1*Probability)\nexpectedvalue<- sum(IP$X)\nexpectedvalue\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.2864198. We can round this to 1."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "title": "Homework 1",
    "section": "2(f)",
    "text": "2(f)\n\n\nCode\nvar1 <-sum(((IP$df1-expectedvalue)^2)*IP$Probability)\nvar1\n\n\n[1] 0.8562353\n\n\n\n\nCode\nsqrt(var1)\n\n\n[1] 0.9253298\n\n\nThe variance and the standard deviation for prior convictions are 0.8562353 and 0.9253298 respectively."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html",
    "href": "posts/Buck_Yoon_finalpart1.html",
    "title": "finalpart1",
    "section": "",
    "text": "we are going to be using the National Longitudinal Study of Adolescent to Adult Health, 1994-2018 we are interested in exploring the relation between education levels and health.\n#Some of our research questions are:\nWhat is the correlation and relationship between someone’s education and health? Does the type and duration of education matter? Are there fields that may be “more healthy”? How does the relationship between education and health differ among the education levels/ is there a difference?\nWhat does this data set have to say to a possible causal link between education and health? Does the data set provide apt data to establish a causal link?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "href": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "title": "finalpart1",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are going to be using the hypothesis from researchers Eric R. Ride and Mark H. Showalter, but using the data from the National Longitudinal Study\nThere hypothesis was: ’The empirical link between education and health is firmly established. Numerous studies document that higher levels of education are positively associated with longer life and better health throughout the lifespan…But measuring the causal links between education and health is a more challenging task.” Estimating the relation between health and education: what do we know and what do we need to know?\nWe are hypothesizing that a positive correlation exists between education and health; the more education an individual receives, the better health the individual may have.\nWe want to look at the National Longitudinal Study of Adolescent to Adult Health 1992-2018 and observe what other factors beyond education there is that can affect the correlation to health. What are the potential moderating or mediating variables?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "href": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "title": "finalpart1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis is an overview of the entire data set we are still determining which specific sections we want to analyze for our final project.\nAccording to ICPSR:\nStudy Purpose: Add Health was developed in response to a mandate from the U.S. Congress to fund a study of adolescent health. Waves I and II focused on the forces that may influence adolescents’ health and risk behaviors, including personal traits, families, friendships, romantic relationships, peer groups, schools, neighborhoods, and communities. As participants aged into adulthood, the scientific goals of the study expanded and evolved. Wave III explored adolescent experiences and behaviors related to decisions, behavior, and health outcomes in the transition to adulthood. Wave IV expanded to examine developmental and health trajectories across the life course of adolescence into young adulthood, using an integrative study design which combined social, behavioral, and biomedical measures data collection. Wave V aimed to track the emergence of chronic disease as the cohort aged into their 30s and early 40s.\nStudy Design: Add health is a school-based longitudinal study of a nationally-representative sample of adolescents in grates 7-12 in the United States in 1945-45. Over more than 20 years of data collection, data have been collected from adolescents, their fellow students, school administrators, parents, siblings, friends, and romantic partners through multiple data collection components. In addition, existing databases with information about respondents’ neighborhoods and communities have been merged with Add Health data, including variables on income poverty, unemployment, availability and utilization of health services, crime, church membership, and social programs and policies.\nSample:\n\nWave I: The Stage 1 in-school sample was a stratified, random sample of all high schools in the United States. A school was eligible for the sample if it included an 11th grade and had a minimum enrollment of 30 students. A feeder school – a school that sent graduates to the high school and that included a 7th grade – was also recruited from the community. The in-school questionnaire was administered to more than 90,000 students in grades 7 through 12. The Stage 2 in-home sample of 27,000 adolescents consisted of a core sample from each community, plus selected special over samples. Eligibility for over samples was determined by an adolescent’s responses on the in-school questionnaire. Adolescents could qualify for more than one sample.\nWave II: The Wave II in-home interview surveyed almost 15,000 of the same students one year after Wave I.\nWave III: The in-home Wave III sample consists of over 15,000 Wave I respondents who could be located and re-interviewed six years later.\nWave IV: All original Wave I in-home respondents were eligible for in-home interviews at Wave IV. At Wave IV, the Add Health sample was dispersed across the nation with respondents living in all 50 states. Administrators were able to locate 92.5% of the Wave IV sample and interviewed 80.3% of eligible sample members.\nWave V: All Wave I respondents who were still living were eligible at Wave V, yielding a pool of 19,828 persons. This pool was split into three stratified random samples for the purposes of survey design testing.\nTime Method: Longitudinal:Panel\nUniverse: Adolescents in grades 7 through 12 during the 1994-1995 school year. Respondents were geographically located in the United States.\nUnits of Observation: Individual\nData Types: Survey Data\nTime periods: 1994 - 2018\nDate of Collections: Wave 1(1994-01 - 1995-12), Wave II(1996-04 - 1996-09), Wave III(2001-04 - 2002 -04), Wave IV(2007-04 - 2009-01), Wave V(2016-03 - 2018-11)\nResponse Rates: Wave 1(79%), Wave 2(88.6%), Wave III(77.4%), Wave IV(80.3%), Wave V(71.8%)."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html",
    "href": "posts/Buck_Yoon_finalpart2.html",
    "title": "finalpart2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(gapminder)\nlibrary(readr)\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#research-question",
    "href": "posts/Buck_Yoon_finalpart2.html#research-question",
    "title": "finalpart2",
    "section": "Research Question",
    "text": "Research Question\nwe are going to be using the National Longitudinal Study of Adolescent to Adult Health, 1994-2018 we are interested in exploring the relation between education levels and health."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#hypothesis",
    "href": "posts/Buck_Yoon_finalpart2.html#hypothesis",
    "title": "finalpart2",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are going to be using the hypothesis from researchers Eric R. Ride and Mark H. Showalter, but using the data from the National Longitudinal Study\nThere hypothesis was: ’The empirical link between education and health is firmly established. Numerous studies document that higher levels of education are positively associated with longer life and better health throughout the lifespan…But measuring the causal links between education and health is a more challenging task.” Estimating the relation between health and education: what do we know and what do we need to know?\nWe are hypothesizing that a positive correlation exists between education and health; the more education an individual receives, the better health the individual may have.\nWe want to look at the National Longitudinal Study of Adolescent to Adult Health 1992-2018 and observe what other factors beyond education there is that can affect the correlation to health. What are the potential moderating or mediating variables?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#descriptive-statistics",
    "href": "posts/Buck_Yoon_finalpart2.html#descriptive-statistics",
    "title": "finalpart2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis is an overview of the entire data set we are still determining which specific sections we want to analyze for our final project.\nAccording to ICPSR:\nStudy Purpose: Add Health was developed in response to a mandate from the U.S. Congress to fund a study of adolescent health. Waves I and II focused on the forces that may influence adolescents’ health and risk behaviors, including personal traits, families, friendships, romantic relationships, peer groups, schools, neighborhoods, and communities. As participants aged into adulthood, the scientific goals of the study expanded and evolved. Wave III explored adolescent experiences and behaviors related to decisions, behavior, and health outcomes in the transition to adulthood. Wave IV expanded to examine developmental and health trajectories across the life course of adolescence into young adulthood, using an integrative study design which combined social, behavioral, and biomedical measures data collection. Wave V aimed to track the emergence of chronic disease as the cohort aged into their 30s and early 40s.\nStudy Design: Add health is a school-based longitudinal study of a nationally-representative sample of adolescents in grates 7-12 in the United States in 1945-45. Over more than 20 years of data collection, data have been collected from adolescents, their fellow students, school administrators, parents, siblings, friends, and romantic partners through multiple data collection components. In addition, existing databases with information about respondents’ neighborhoods and communities have been merged with Add Health data, including variables on income poverty, unemployment, availability and utilization of health services, crime, church membership, and social programs and policies.\nSample:\n\nWave I: The Stage 1 in-school sample was a stratified, random sample of all high schools in the United States. A school was eligible for the sample if it included an 11th grade and had a minimum enrollment of 30 students. A feeder school – a school that sent graduates to the high school and that included a 7th grade – was also recruited from the community. The in-school questionnaire was administered to more than 90,000 students in grades 7 through 12. The Stage 2 in-home sample of 27,000 adolescents consisted of a core sample from each community, plus selected special over samples. Eligibility for over samples was determined by an adolescent’s responses on the in-school questionnaire. Adolescents could qualify for more than one sample.\nWave II: The Wave II in-home interview surveyed almost 15,000 of the same students one year after Wave I.\nWave III: The in-home Wave III sample consists of over 15,000 Wave I respondents who could be located and re-interviewed six years later.\nWave IV: All original Wave I in-home respondents were eligible for in-home interviews at Wave IV. At Wave IV, the Add Health sample was dispersed across the nation with respondents living in all 50 states. Administrators were able to locate 92.5% of the Wave IV sample and interviewed 80.3% of eligible sample members.\nWave V: All Wave I respondents who were still living were eligible at Wave V, yielding a pool of 19,828 persons. This pool was split into three stratified random samples for the purposes of survey design testing.\nTime Method: Longitudinal:Panel\nUniverse: Adolescents in grades 7 through 12 during the 1994-1995 school year. Respondents were geographically located in the United States.\nUnits of Observation: Individual\nData Types: Survey Data\nTime periods: 1994 - 2018\nDate of Collections: Wave 1(1994-01 - 1995-12), Wave II(1996-04 - 1996-09), Wave III(2001-04 - 2002 -04), Wave IV(2007-04 - 2009-01), Wave V(2016-03 - 2018-11)\nResponse Rates: Wave 1(79%), Wave 2(88.6%), Wave III(77.4%), Wave IV(80.3%), Wave V(71.8%)."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart2.html#part-2",
    "href": "posts/Buck_Yoon_finalpart2.html#part-2",
    "title": "finalpart2",
    "section": "Part 2",
    "text": "Part 2"
  },
  {
    "objectID": "posts/CalebHill_HW1.html",
    "href": "posts/CalebHill_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nNext, let’s compare the probability distribution of the LungCap with respect to Males and Females, using a boxplot.\n\n\nCode\nboxplot(LungCap ~ Gender, df)\n\n\n\n\n\nThe minimum and mean are very similar to each other, with the minimum around 1 and the mean around 8. The maximum does differ though by gender, at 13 to 14/15 respectively.\n\n\n\nFor the third question, we’re going to compare the mean lung capacities for smokers and non-smokers. To compare the mean, we’ll again use the box-plot.\n\n\nCode\nboxplot(LungCap ~ Smoke, df)\n\n\n\n\n\nWhile the mean is very similar, hovering between 8 and 9, the range is what is substantial. A smoker’s lung capacity has a much smaller range, 4 - 13, compared to non-smokers, at 1 - 15. This makes sense, as a smoker’s lungs would start to have less capacity through consistent substance abuse.\n\n\n\nFor question four, we need to create a new variable, Age Group, followed by comparing the relationship between Smoking and Lung Capacity, broken down by Age Group. First, we’ll create the new column, referencing the Age column to determine groups.\n\n\nCode\ndf_new <- df %>%\n  mutate(\n    Age_Group = dplyr::case_when(\n      Age <= 13 ~ \"Less than or equal to 13\",\n      Age == 14 | Age == 15 ~ \"14 or 15\",\n      Age == 16 | Age == 17 ~ \"16 or 17\",\n      Age >= 18 ~ \"Greater than or equal to 18\"\n    ),\n    Age_Group = factor(\n      Age_Group,\n      level = c(\"Less than or equal to 13\", \"14 or 15\", \"16 or 17\", \"Greater than or equal to 18\")\n    )\n  )\nhead(df_new)\n\n\n# A tibble: 6 × 7\n  LungCap   Age Height Smoke Gender Caesarean Age_Group                  \n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>                      \n1    6.48     6   62.1 no    male   no        Less than or equal to 13   \n2   10.1     18   74.7 yes   female no        Greater than or equal to 18\n3    9.55    16   69.7 no    female yes       16 or 17                   \n4   11.1     14   71   no    male   no        14 or 15                   \n5    4.8      5   56.9 no    male   no        Less than or equal to 13   \n6    6.22    11   58.7 no    female no        Less than or equal to 13   \n\n\nGood. Now we can place a histogram to better understand the relationship between LungCap and Smoking status. To view it by age group, we’ll add a facet wrap to the visualization.\n\n\nCode\nggplot(df_new, aes(LungCap, color=Smoke)) +\n  geom_histogram() +\n  facet_wrap(~Age_Group)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThose that are smokers have a smaller sample size than non-smokers. Looking purely at the distribution of each, we can see that three of the four age groups follow a normal distribution, save the 14 or 15 group that has a somewhat “two hump” distribution.\nEven so, smoking status does seem to mirror the non-smoker distribution, when it comes to the overall sample count and LungCap.\n\n\n\nFor the fifth question, we’ll compare the lung capacities for smokers and non-smokers within each age group. We’ll use a box-plot and facet wrap this visualization again by Age Group.\n\n\nCode\nggplot(df_new, aes(LungCap, Smoke)) +\n  geom_boxplot() +\n  facet_wrap(~Age_Group)\n\n\n\n\n\nWe can readily see that smokers, irrespective of age, have a substantially smaller lung capacity range compared to non-smokers. While the mean might be similar, sometimes even smaller for “13 years old or less”, the length of each capacity varies for non-smokers where it doesn’t for smokers.\n\n\n\nFor the sixth question, we shall calculate the covariance and correlation between LungCap and Age.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is the relationship between a pair of random variables where change in one variable causes change in another variable. With a covariance of 8.73, that means that there is a positive relationship between the two variables and that, by every 1 point change of Age, that can result in an average of 8.73 point change in LungCap.\nCorrelations show whether and how strongly pairs or variables are related to one another. Correlation can range from 0.0 to 1.0. With a result of 0.81, that means there is a high correlation between LungCap and Age."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#a-1",
    "href": "posts/CalebHill_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nLet’s calculate the probability that a randomly selected inmate has EXACTLY 2 prior convictions.\n\n\nCode\ndbinom(2, 810, 0.1975)\n\n\n[1] 7.90917e-74\n\n\nSo 7.9%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#b-1",
    "href": "posts/CalebHill_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nLet’s calculate the probability that a randomly selected inmate has FEWER THAN 2 prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975, lower.tail=FALSE)\n\n\n[1] 1\n\n\nNot sure why it’s pulling 1."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#c-1",
    "href": "posts/CalebHill_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLet’s calculate the probability that a randomly selected inmate has 2 OR FEWER prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975)\n\n\n[1] 7.989018e-74\n\n\nSo 7.98%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#d-1",
    "href": "posts/CalebHill_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nLet’s calculate the probability that a randomly selected inmate has MORE THAN 2 prior convictions."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#e-1",
    "href": "posts/CalebHill_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nLet’s calculate the expected value for the number of prior convictions. As I am unable to calculate sections B and D, I’m unable to determine the expected value."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#f-1",
    "href": "posts/CalebHill_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nLet’s calculate the variance and the standard deviation for the Prior Convictions.As I am unable to determine the expected value, I cannot calculate the variance and standard deviation either."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html",
    "href": "posts/DACSS 603 Final Part 1.html",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv' does not exist.\n\n\nCode\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\n\nError in number(x = x, accuracy = accuracy, scale = scale, prefix = prefix, : object 'NYC_2019' not found\n\n\nCode\nNYC_2021 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv' does not exist.\n\n\nCode\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n\nError in rbind(NYC_2019, NYC_2021): object 'NYC_2019' not found\n\n\nCode\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n\nError in recode(school_data$`% Poverty`, `Above 95%` = \"95%\", `Below 5%` = \"5%\"): object 'school_data' not found\n\n\nCode\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\n\nError in lapply(X = X, FUN = FUN, ...): object 'school_data' not found\n\n\nCode\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found\n\n\nCode\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found"
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#research-question",
    "href": "posts/DACSS 603 Final Part 1.html#research-question",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "href": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "href": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nError in glimpse(school_data): object 'school_data' not found\n\n\n\n\nCode\nsummary(school_data)\n\n\nError in summary(school_data): object 'school_data' not found\n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#references",
    "href": "posts/DACSS 603 Final Part 1.html#references",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html",
    "href": "posts/DACSS 603 HW 1.html",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.5\n✔ tibble  3.1.8     ✔ stringr 1.4.1\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n✔ readr   2.1.3     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n\n# Reading in File\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram above shows that the Lung Cap data is roughly normally distributed because a majority of the observations are centered around the mean. There are fewer observations at the tail ends of the histogram.\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, main = \"Lung Capacity by Gender\",\n        xlab = \"Gender\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the box-plots above, it appears that males in this study had slightly higher lung capacities than females, with the median for males at 9 and the median for females at 8. However, both genders had large ranges, but these ranges reflected the overall pattern of males having slightly higher lung capacities.\n\n\n\n\n\nCode\nsmokers <- filter(LungCapData, Smoke == \"yes\")\nmean(smokers$LungCap)\n\n\n[1] 8.645455\n\n\nCode\nnonsmokers <- filter(LungCapData, Smoke == \"no\")\nmean(nonsmokers$LungCap)\n\n\n[1] 7.770188\n\n\nThe mean lung capacity for smokers (8.65) is higher than the mean lung capacity for non-smokers (7.77). Based on what we now know about how smoking affects the lungs, these results don’t seem to make sense. However, there is the possibility that smokers may be more used to deep inhales/exhales and therefore could have better lung capacity until the substance has more of an effect on their lungs. There may also be external factors that led to these results that aren’t clear from the data right now.\n\n\n\n\n\nCode\nLungCapData <- within(LungCapData, {\n  Age.group <- NA\n  Age.group[Age <= 13] <- \"13 and Under\"\n  Age.group[Age >= 14 & Age <= 15] <- \"14-15\"\n  Age.group[Age >= 16 & Age <= 17] <- \"16-17\"\n  Age.group[Age >= 18] <- \"18 and Over\"\n} )\n\n\n\n\n\n\nCode\n# Boxplots\n\nsmoking_age <- filter(LungCapData, Smoke == \"yes\")\n\nboxplot(LungCap ~ Age.group, data = smoking_age,\n        main = \"Lung Capacity of Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the boxplot above, we can see that smokers’ lung capacities reach about a maximum of 12 as age increases, but there is not very much improvement in the maximums. The medians move a bit more as age increases, but still not very dramatically after ages 14 and 15. Smokers that are 18 and over have higher lung capacities overall, but this may just be because of natural aging processes and development.\n\n\nCode\n# Means\n\nsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  7.20\n2 14-15         8.39\n3 16-17         9.38\n4 18 and Over  10.5 \n\n\nWe see the same trend in means as in the medians: mean lung capacity to increases as the age increases.\n\n\n\n\n\nCode\n# Boxplot\n\nnonsmoking_age <- filter(LungCapData, Smoke == \"no\")\n\nboxplot(LungCap ~ Age.group, data = nonsmoking_age,\n        main = \"Lung Capacity of Non-Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nIn non-smokers, we see the same trend of increasing lung capacities as age increases, but the median lung capacities in the two older age groups in the non-smoking group are higher than those in the smoking group. There are also more outliers for non-smokers, especially in the 14-15 category.\n\n\nCode\n# Means\n\nnonsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  6.36\n2 14-15         9.14\n3 16-17        10.5 \n4 18 and Over  11.1 \n\n\nThe means of the non-smoking group by age follow the same trend as the medians, as well as in the smoking group. However, the mean lung capacity for the oldest two age groups in the non-smoking category are higher than the means for those groups in the smoking category.\n\n\n\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"13 and Under\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     6.36\n2 yes    7.20\n\n\nThe mean lung capacity for smokers is higher than the mean lung capacity for non-smokers in the age group 13 and under, which mirrors the general means we found earlier. However, from the boxplot of Smokers by Age Group, we can see that there is a very low outlier in this age group, which might be affecting the mean for this group as well as overall smokers.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"14-15\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nIn this age group, the mean lung capacity for non-smokers is higher than the mean lung capacity for smokers–unlike the younger group.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"16-17\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nThe same trend continues in this age group, with the mean lung capacity in non-smokers ages 16 and 17 higher than the mean lung capacity of smokers in this group. Yet as the ages increase, the mean lung capacities for non-smokers and smokers increase about the same amount (by 1).\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"18 and Over\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     11.1\n2 yes    10.5\n\n\nIn this oldest age group, the same trend continues: the mean lung capacity for non-smokers is higher than that of smokers. This pattern in the groups 18+, 16-17, and 14-15 are not found in the overall means for smokers and nonsmokers, suggesting that the outlier in the 13 and Under group might have brought down the overall mean for smokers.\n\n\n\n\n\n\nCode\n# Correlation\n\ncor(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 0.8196749\n\n\nThe correlation between lung capacity and age is positive and strong. As age increases, lung capacity also increases. The value of 0.8 is close to 1, meaning there is a somewhat strong relationship between the two variables.\n\n\nCode\n# Covariance\n\ncov(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 8.738289\n\n\nThe covariance is positive, meaning that there is a positive relationship between the varaibles, which is also clear from the correlation (since the correlation coefficient is a function of the covariance). Age and lung capacity have an overall positive relationship: as age increases, so does lung capacity."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-a",
    "href": "posts/DACSS 603 HW 1.html#part-a",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part A",
    "text": "Part A\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-b",
    "href": "posts/DACSS 603 HW 1.html#part-b",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part B",
    "text": "Part B\n\n\nCode\n(434 + 128)/810\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-c",
    "href": "posts/DACSS 603 HW 1.html#part-c",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part C",
    "text": "Part C\n\n\nCode\n(160 + 434 + 128)/810\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-d",
    "href": "posts/DACSS 603 HW 1.html#part-d",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part D",
    "text": "Part D\n\n\nCode\n(64 + 24)/810\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-e",
    "href": "posts/DACSS 603 HW 1.html#part-e",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part E",
    "text": "Part E\n\n\nCode\n# Creating vector\nconvict <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\n\nweighted.mean(convict)\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.27–but since prior convictions have to be a whole number, that would be rounded to 1."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-f",
    "href": "posts/DACSS 603 HW 1.html#part-f",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part F",
    "text": "Part F\n\n\nCode\nvar(convict)\n\n\n[1] 0.8572937\n\n\nCode\nsd(convict)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/Emily Duryea Homework 1.html",
    "href": "posts/Emily Duryea Homework 1.html",
    "title": "Duryea Homework 1",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")  \n\nhist(df$LungCap)\n\n\n\n\n\nPart A: Plotting probability density histogram\n\n\nCode\nhist(df$LungCap, \n     col=\"yellow\",\n     border=\"black\",\n     prob = TRUE,\n     xlab = \"LungCap\",\n     main = \"Density Plot\")\n\nlines(density(df$LungCap),\n      lwd = 2,\n      col = \"chocolate3\")\n\n\n\n\n\nPart B: Compare the probability distribution of the LungCap with respect to Males and Females\n\n\nCode\nggplot(df, aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"LungCap Probability Distribution for Males and Females\", y = \"Probability density\")\n\n\n\n\n\nPart C: Compare the mean lung capacities for smokers and non-smokers\n\n\nCode\nmean_smoking <- df %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nmean_smoking\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThe means of smokers vs non smokers does not make sense since non smokers have a lower mean lung cap, when one would think it would be the other way around. However, limited data is provided on the sample, so there could be other factors in play.\nPart D: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”\n\n\nCode\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\ndf %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGroup)) +\n  theme_classic() + \n  labs(title = \"LungCap and Smoke based on age groups\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nBased on the histograms, Part D seems to contrast with Part C, since the plots seem to demonstrate non-smokers having higher lung capacity than smokers in all age groups. Additionally, lung capacity appears to decrease with age based on the graph.\nPart E: Compare the lung capacities for smokers and non-smokers within each age group\n\n\nCode\ndf %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(title = \"LungCap and Smoke based on age and smoker vs nonsmoker\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nBased on information gained in Part D and Part E, it appears that lung capacity decreases with age, and, despite the means in Part C, lung capacity is higher for non-smokers.\nPart F: Calculate the correlation and covariance between Lung Capacity and Age\n\n\nCode\nCov_lungcapage <- cov(df$LungCap, df$Age)\nCor_lungcapeage <- cor(df$LungCap, df$Age)\nCov_lungcapage\n\n\n[1] 8.738289\n\n\nCode\nCor_lungcapeage\n\n\n[1] 0.8196749\n\n\nBecause both the covariance and correlation are positive numbers, the relationship between lung capacity and age are positively related, meaning as one increases, the other also increases in a proportional manner.\nQuestion 2\n\n\nCode\nPrior_Convictions <- c(0:4)\nInmate_Number <- c(128, 434, 160, 64, 24)\nip <- tibble(Prior_Convictions, Inmate_Number)\n\nip <- mutate(ip, Probability = Inmate_Number/sum(Inmate_Number))\nip\n\n\n# A tibble: 5 × 3\n  Prior_Convictions Inmate_Number Probability\n              <int>         <dbl>       <dbl>\n1                 0           128      0.158 \n2                 1           434      0.536 \n3                 2           160      0.198 \n4                 3            64      0.0790\n5                 4            24      0.0296\n\n\nPart A: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nip %>%\n  filter(Prior_Convictions == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability that a randomly selected inmate has exactly two prior convictions is 0.1975309.\nPart B: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\npartb <- ip %>%\n  filter(Prior_Convictions < 2)\nsum(partb$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than two prior convictions is 0.6938272.\nPart C: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\npartc <- ip %>%\n  filter(Prior_Convictions <= 2)\nsum(partc$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has two or fewer prior convictions is 0.891358.\nPart D: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\npartd <- ip %>%\n  filter(Prior_Convictions > 2)\nsum(partd$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than two prior convictions is 0.108642.\nPart E: What is the expected value for the number of prior convictions?\n\n\nCode\nip <- mutate(ip, vl = Prior_Convictions*Probability)\nparte <- sum(ip$vl)\nparte\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.28642.\nPart F: Calculate the variance and the standard deviation for the Prior Convictions\n\n\nCode\nip_var <-sum(((ip$Prior_Convictions-parte)^2)*ip$Probability)\nip_var\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(ip_var)\n\n\n[1] 0.9253298\n\n\nThe variance for prior convictions is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html",
    "href": "posts/EmmaRasmussenFinalPart1.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(googlesheets4)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "href": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question:",
    "text": "Research Question:\nDoes political partisanship correlate with COVID-19 death rates?\nThe COVID-19 pandemic became a political matter. Behaviors associated with COVID-19 prevention were adopted on partisan lines (masking, social distancing, and vaccine uptake). Early in the pandemic, mask mandates were protested in some communities. My research question is have these behaviors affected COVID-19 death rates along partisan lines? If so, public health interventions could target communities that may be higher risk for COVID-19 deaths based on political partisanship.\nI am thinking death toll would make the most sense to measure than infection rates as infection rates are constantly changing (other studies have looked at infection rates over waves of the pandemic, see this study from the Pew Research Center (Jones 2022)). I also think that one way to measure partisanship will be the 2020 county-level election results (% voting for Trump). In other words, my research is looking to see if (county-level) Trump support correlates with COVID-19 death rates. Both these variables can be found in county-level data sets so I can join multiple dataset with county name (or FIPS code) as the “key”.\nOther variables to consider at the county-level (confounding variables): vaccine (and booster) uptake, average age of population"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "href": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "title": "Final Project Part 1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\nWhile I came up with this research idea on my own, other organizations such as NPR (Wood and Brumfiel 2021) and the Pew Research Center ()have already tested this. For this project, I will use the most recent data I can find. I was hoping to consider the confounding variable of population density, for instance I am guessing more urban populations will tend to vote democratic but these more densely populated places may also have higher infection rates. However, I cannot find any county level population density data sets, so I may use the “Urban Rural Description” variable in one of my datasets.\nH0: B1 (and all beta values) is zero. There is no correlation Ha: B1 (or any beta value) is not zero. There is a correlation between partisanship and COVID-19 death rates."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "href": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics:",
    "text": "Descriptive Statistics:\n\n#Reading in the data from google sheets\ngs4_deauth()\n\nvotedf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1fmxoA_bibvsxsvgRdVPCgMA7DkmJNZfxiWgLgCLcsOY/edit#gid=937778872\")\n\ncoviddf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1Hy2O3HxhZGF_fhu6jgmoC2ibWwJTlI7pQOESBOd4hTU/edit#gid=787918384\")\n\n\n#Changing fips code to character format and adding in leading zeros\ncoviddf$\"FIPS Code\" <- as.character(coviddf$\"FIPS Code\")\ncoviddf<-mutate(coviddf, FIPSNEW=str_pad(coviddf$\"FIPS Code\", 5, pad = \"0\"))\nhead(coviddf, 12)\n\n# A tibble: 12 × 22\n   `Data as of`        `Start Date`        `End Date`          State County Na…¹\n   <dttm>              <dttm>              <dttm>              <chr> <chr>      \n 1 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 2 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 3 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 4 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 5 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 6 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 7 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 8 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 9 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n10 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n11 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n12 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n# … with 17 more variables: `Urban Rural Code` <dbl>, `FIPS State` <dbl>,\n#   `FIPS County` <dbl>, `FIPS Code` <chr>, Indicator <chr>,\n#   `Total deaths` <dbl>, `COVID-19 Deaths` <dbl>, `Non-Hispanic White` <dbl>,\n#   `Non-Hispanic Black` <dbl>,\n#   `Non-Hispanic American Indian or Alaska Native` <dbl>,\n#   `Non-Hispanic Asian` <dbl>,\n#   `Non-Hispanic Native Hawaiian or Other Pacific Islander` <dbl>, …\n\nvotedf$county_fips <- as.character(votedf$county_fips)\nvotedf<-mutate(votedf, county_fipsNEW=str_pad(votedf$county_fips, 5, pad = \"0\"))\nhead(votedf, 12)\n\n# A tibble: 12 × 13\n    year state   state_po county_…¹ count…² office candi…³ party candi…⁴ total…⁵\n   <dbl> <chr>   <chr>    <chr>     <chr>   <chr>  <chr>   <chr>   <dbl>   <dbl>\n 1  2000 ALABAMA AL       AUTAUGA   1001    US PR… AL GORE DEMO…    4942   17208\n 2  2000 ALABAMA AL       AUTAUGA   1001    US PR… GEORGE… REPU…   11993   17208\n 3  2000 ALABAMA AL       AUTAUGA   1001    US PR… RALPH … GREEN     160   17208\n 4  2000 ALABAMA AL       AUTAUGA   1001    US PR… OTHER   OTHER     113   17208\n 5  2000 ALABAMA AL       BALDWIN   1003    US PR… AL GORE DEMO…   13997   56480\n 6  2000 ALABAMA AL       BALDWIN   1003    US PR… GEORGE… REPU…   40872   56480\n 7  2000 ALABAMA AL       BALDWIN   1003    US PR… RALPH … GREEN    1033   56480\n 8  2000 ALABAMA AL       BALDWIN   1003    US PR… OTHER   OTHER     578   56480\n 9  2000 ALABAMA AL       BARBOUR   1005    US PR… AL GORE DEMO…    5188   10395\n10  2000 ALABAMA AL       BARBOUR   1005    US PR… GEORGE… REPU…    5096   10395\n11  2000 ALABAMA AL       BARBOUR   1005    US PR… RALPH … GREEN      46   10395\n12  2000 ALABAMA AL       BARBOUR   1005    US PR… OTHER   OTHER      65   10395\n# … with 3 more variables: version <dbl>, mode <chr>, county_fipsNEW <chr>, and\n#   abbreviated variable names ¹​county_name, ²​county_fips, ³​candidate,\n#   ⁴​candidatevotes, ⁵​totalvotes\n\n\n\nsummary(votedf)\n\n      year         state             state_po         county_name       \n Min.   :2000   Length:72617       Length:72617       Length:72617      \n 1st Qu.:2004   Class :character   Class :character   Class :character  \n Median :2012   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2011                                                           \n 3rd Qu.:2020                                                           \n Max.   :2020                                                           \n county_fips           office           candidate            party          \n Length:72617       Length:72617       Length:72617       Length:72617      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n candidatevotes      totalvotes         version             mode          \n Min.   :      0   Min.   :      0   Min.   :20220315   Length:72617      \n 1st Qu.:    115   1st Qu.:   5175   1st Qu.:20220315   Class :character  \n Median :   1278   Median :  11194   Median :20220315   Mode  :character  \n Mean   :  10782   Mean   :  42514   Mean   :20220315                     \n 3rd Qu.:   5848   3rd Qu.:  29855   3rd Qu.:20220315                     \n Max.   :3028885   Max.   :4264365   Max.   :20220315                     \n county_fipsNEW    \n Length:72617      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nsummary(coviddf)\n\n   Data as of           Start Date            End Date         \n Min.   :2022-10-05   Min.   :2020-01-01   Min.   :2022-10-01  \n 1st Qu.:2022-10-05   1st Qu.:2020-01-01   1st Qu.:2022-10-01  \n Median :2022-10-05   Median :2020-01-01   Median :2022-10-01  \n Mean   :2022-10-05   Mean   :2020-01-01   Mean   :2022-10-01  \n 3rd Qu.:2022-10-05   3rd Qu.:2020-01-01   3rd Qu.:2022-10-01  \n Max.   :2022-10-05   Max.   :2020-01-01   Max.   :2022-10-01  \n                                                               \n    State           County Name        Urban Rural Code   FIPS State   \n Length:3495        Length:3495        Min.   :1.000    Min.   : 1.00  \n Class :character   Class :character   1st Qu.:2.000    1st Qu.:18.00  \n Mode  :character   Mode  :character   Median :4.000    Median :33.00  \n                                       Mean   :3.645    Mean   :30.47  \n                                       3rd Qu.:5.000    3rd Qu.:42.00  \n                                       Max.   :6.000    Max.   :56.00  \n                                                                       \n  FIPS County      FIPS Code          Indicator          Total deaths   \n Min.   :  1.00   Length:3495        Length:3495        Min.   :   621  \n 1st Qu.: 31.00   Class :character   Class :character   1st Qu.:  1690  \n Median : 71.00   Mode  :character   Mode  :character   Median :  3284  \n Mean   : 99.37                                         Mean   :  7163  \n 3rd Qu.:121.00                                         3rd Qu.:  6990  \n Max.   :840.00                                         Max.   :220829  \n                                                                        \n COVID-19 Deaths   Non-Hispanic White Non-Hispanic Black\n Min.   :  101.0   Min.   :0.0270     Min.   :0.0010    \n 1st Qu.:  176.0   1st Qu.:0.6677     1st Qu.:0.0230    \n Median :  364.0   Median :0.8300     Median :0.0690    \n Mean   :  852.7   Mean   :0.7742     Mean   :0.1242    \n 3rd Qu.:  844.0   3rd Qu.:0.9290     3rd Qu.:0.1800    \n Max.   :31013.0   Max.   :1.0000     Max.   :0.7610    \n                   NA's   :3          NA's   :592       \n Non-Hispanic American Indian or Alaska Native Non-Hispanic Asian\n Min.   :0.0000                                Min.   :0.0010    \n 1st Qu.:0.0020                                1st Qu.:0.0070    \n Median :0.0040                                Median :0.0130    \n Mean   :0.0214                                Mean   :0.0261    \n 3rd Qu.:0.0100                                3rd Qu.:0.0280    \n Max.   :0.8610                                Max.   :0.5170    \n NA's   :1701                                  NA's   :1360      \n Non-Hispanic Native Hawaiian or Other Pacific Islander    Hispanic     \n Min.   :0.0000                                         Min.   :0.0030  \n 1st Qu.:0.0000                                         1st Qu.:0.0220  \n Median :0.0010                                         Median :0.0480  \n Mean   :0.0023                                         Mean   :0.0987  \n 3rd Qu.:0.0010                                         3rd Qu.:0.1090  \n Max.   :0.2000                                         Max.   :0.9870  \n NA's   :2183                                           NA's   :740     \n     Other        Urban Rural Description   Footnote           FIPSNEW         \n Min.   :0.0010   Length:3495             Length:3495        Length:3495       \n 1st Qu.:0.0090   Class :character        Class :character   Class :character  \n Median :0.0150   Mode  :character        Mode  :character   Mode  :character  \n Mean   :0.0174                                                                \n 3rd Qu.:0.0220                                                                \n Max.   :0.2410                                                                \n NA's   :1633                                                                  \n\n\nThis data is going to require some tidying before merging. In the coviddf, each county is listed 3 times, (once per indicator) so I will likely filter out just the indicator “Distribution of COVID-19 deaths (%)” so each county is listed only once. Similarly, the votedf contains extra years. For my research, I am only concerned with 2016 data so I will filter out % voting for Trump in 2016 as a measure of political affiliation/partisanship. Then I will merge the two dfs based on county names (will also require some data tidying).\nThe votedf was compiled by the MIT Election Data and Science Lab. It was first published in 2018 and has been updated with the 2020 election. It contains county-level presidential election data beginning in 2000 and going up to the 2020 election. The data has 12 columns, and 72,617 rows (many of which I will filter out before conducting analysis.) There are 1,892 distinct county names in the data set.\nThe coviddf only has 857 unique county names in the data frame. This may be because not all counties reported COVID-19 death counts. When I join the data sets, I will join so as to only include observations that we have information from both data frames. The coviddf is provisional, meaning that it is consistently updated (I believe on a weekly basis) with current COVID-19 death toll data. It is likely compiled by counties/towns reporting these numbers to the CDC. This data has limitations, not all counties report this, and not all report it accurately/ attribute COVID-19 as the true cause of death in all circumstances. Using the summary function, we can see the “mean” COVID-19 deaths by county is 852.7, however this isn’t super meaningful given each county has this reported 3 times in the data and the median is significantly lower. Statistics provided by the summary function will be more meaningful once the data is tidied."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#references",
    "href": "posts/EmmaRasmussenFinalPart1.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nJones, B. (2022). The Changing Political Geography of COVID-19 Over the Last Two Years. Pew Research Center. March 3, 2022. https://www.pewresearch.org/politics/2022/03/03/the-changing-political-geography-of-covid-19-over-the-last-two-years/\nMIT Election Data and Science Lab. (2021) County Presidential Election Returns 2000-2020. Accessed from the Harvard Dataverse [October 11, 2022]. https://doi.org/10.7910/DVN/VOQCHQ\nNational Center for Health Statistics. (2022). Provisional COVID-19 Deaths by County, and Race and Hispanic Origin. Accessed from the Centers for Disease Control [October 11, 2022]. https://data.cdc.gov/d/k8wy-p9cg\nWood, D. and Brumfiel, G. (2021). Pro-Trump counties now have far higher COVID death rates. Misinformation is to blame. NPR. December 5, 2021. https://www.npr.org/sections/health-shots/2021/12/05/1059828993/data-vaccine-misinformation-trump-counties-covid-death-rate\n[Need to add italics to references]"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html",
    "href": "posts/EmmaRasmussenFinalPart2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(googlesheets4)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#research-question",
    "href": "posts/EmmaRasmussenFinalPart2.html#research-question",
    "title": "Final Project Part 2",
    "section": "Research Question",
    "text": "Research Question\nDoes political partisanship correlate with COVID-19 death rates?"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#introduction",
    "href": "posts/EmmaRasmussenFinalPart2.html#introduction",
    "title": "Final Project Part 2",
    "section": "Introduction",
    "text": "Introduction\nThe COVID-19 pandemic became a political matter. Behaviors associated with COVID-19 prevention were adopted on partisan lines (masking, social distancing, and vaccine uptake). Early in the pandemic, mask mandates were protested in some communities. My research question is have these behaviors affected COVID-19 death rates along partisan lines? If so, public health interventions could target communities that may be higher risk for COVID-19 deaths based on political partisanship.\nFor this analysis I used cumulative COVID-19 death toll as opposed to infection rates as infection rates are constantly changing over time. Other studies have looked at infection rates on partisan lines over waves of the pandemic, see this study from the Pew Research Center (Jones 2022)). I measured partisanship using 2016 county-level election results (% voting for Trump). My research is looking to see if (county-level) Trump support correlates with COVID-19 death rates. The unit of analysis for this study was U.S. counties.\nI also control for age (percent population over 65), income (median household income in 2020), urbanization (Urban-Rural Continuum Code), and policy (2020 governor dummy variable)."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#hypothesis",
    "href": "posts/EmmaRasmussenFinalPart2.html#hypothesis",
    "title": "Final Project Part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nWhile I came up with this research idea on my own, other organizations such as NPR (Wood and Brumfiel 2021) and the Pew Research Center (Jones 2022) have already tested this and found a significant correlation in Trump support and COVID-19 death and infection rates. For this project, I will use more recent data and include additional control variables that were not accounted for in these previous studies.\nH0: B1 is zero. There is no correlation between political partisanship and COVID-19 death rates. Ha: B1 is not zero. There is a correlation between partisanship and COVID-19 death rates."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#variables",
    "href": "posts/EmmaRasmussenFinalPart2.html#variables",
    "title": "Final Project Part 2",
    "section": "Variables",
    "text": "Variables\n\nPolitical Partisanship\nFor this project, I measured partisanship as percentage of the county that voted for Trump in the 2016 election against Clinton. I did not use 2020 election results in case counties “flipped” support as a result of COVID-19. Below I tidied the data, filtering out only 2016 election results, created a variable: percent voting for Trump in 2016, and selected the percent_trump and FIPS Code variables to use in the analysis.\nThe data used for this variable came from the MIT Election Data and Science Lab (2021).\n\ngs4_deauth()\n\n#2016 election df\n\nvotedf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1fmxoA_bibvsxsvgRdVPCgMA7DkmJNZfxiWgLgCLcsOY/edit#gid=937778872\")\n\nvotedf$county_fips <- as.character(votedf$county_fips)\nvotedf<-mutate(votedf, county_fipsNEW=str_pad(votedf$county_fips, 5, pad = \"0\"))\nvotedf<-votedf %>% \n  filter(year==2016, candidate==\"DONALD TRUMP\")\n  votedf$percent_trump <-votedf$candidatevotes/votedf$totalvotes\n  \nvotedf<- select(votedf, county_fipsNEW, percent_trump)\nhead(votedf)\n\n# A tibble: 6 × 2\n  county_fipsNEW percent_trump\n  <chr>                  <dbl>\n1 01001                  0.728\n2 01003                  0.765\n3 01005                  0.521\n4 01007                  0.764\n5 01009                  0.893\n6 01011                  0.242\n\n\n\n\nCOVID-19 Cumulative Death Rate\nThis data comes from USAFacts.org. Their collection methods are thoroughly described on their website (see USA Facts 2022 in References). Originally I planned on using CDC/NIH data, however they only had data for a little over 1,000 counties, compared to USAFacts which had data for over 3,000. USAFacts compiles their data from CDC, but also town, county, and state leading to a larger number of observations in their dataset.\nCumulative county-level death toll is taken as of March 19, 2022, when many states stopped regularly reporting COVID-19 deaths (and beginning late in January 2020) (USA Facts 2022).\n\ncoviddf_2<-read_sheet(\"https://docs.google.com/spreadsheets/d/1ZKa3sg_UdtyX5z0OGGVZN6nuqcC_OKWcsOjm1ZHNjQY/edit#gid=716391091\")\n\n#adding leading 0 back to fips\ncoviddf_2<-mutate(coviddf_2, county_fipsNEW=str_pad(coviddf_2$\"countyFIPS\", 5, pad = \"0\"))\n\n#selecting march 19, 2022, the day before the first day of spring in 2022, when many states stopped/slowed reporting (USA Facts)\ncoviddf_2<-select(coviddf_2, county_fipsNEW, \"County Name\", State, \"2022-03-19\")\ncoviddf_2<-rename(coviddf_2, \"covid_deaths\" = \"2022-03-19\")\n\n\n\nControl: Age\nBecause age correlates with political party and older Americans (over 65) were disproportionately more likely to die as a result of COVID-19, this is included in the analysis as a control. According to an article from the Mayo Clinic Website, 81% of COVID-19 deaths occured in individuals 65 or over (Mayo Clinic Staff 2022). Previous studies have controlled for age as well (Brumfiel and Wood 2021). I controlled for age by creating the variable: percent of the county’s population over 65 years of age.\n\n#age df\n\n#Please see pdf of code tidying I did to make df into a manageable size for google sheets\n\nage<- read_sheet(\"https://docs.google.com/spreadsheets/d/1EysREWJ61NCSYyiYH8-2pmZC_TnuLTgqa8Lz5ZpsPZ4/edit#gid=271068707\")\nhead(age)\n\n# A tibble: 6 × 6\n  STNAME  CTYNAME        county_fipsNEW tot_pop over65 over65_pct         \n  <chr>   <chr>          <chr>          <chr>   <chr>  <chr>              \n1 Alabama Autauga County 01001          55869   8924   0.1597307988329843 \n2 Alabama Baldwin County 01003          223234  46830  0.20977987224168362\n3 Alabama Barbour County 01005          24686   4861   0.19691323017094708\n4 Alabama Bibb County    01007          22394   3733   0.16669643654550326\n5 Alabama Blount County  01009          57826   10814  0.18700930377338915\n6 Alabama Bullock County 01011          10101   1711   0.1693891693891694 \n\n\n\n\nControl: Policy Dummy Variable\nThis variable (2020 governor party) attempts to control for local and state policy such as mask mandates, stay-at-home orders, and vaccine requirements to attend large events that contribute to COVID-19 death and infection rates. This data set was created from a table of current U.S. governors and their political party on Ballotpedia.org (2022). I adjusted a couple observations when governors had been elected more recently than March 11, when the WHO declared COVID-19 a global pandemic. This data represents the political party of state governors as of March 11, 2020.\n\n#Governor/policy df\n\n#reading in gov dummy variable: whoever was in office March 11, 2020 (when WHO declared covid-19 a global pandemic)\n\ngov2020<-read_sheet(\"https://docs.google.com/spreadsheets/d/1-pToTikvnXl1-lT-xazjoxX0sOCwOH5-7Bl8vJqu9Tk/edit#gid=0\")\n\ngov2020$Office<-str_remove(gov2020$Office, \"Governor of \")\ngov2020<-rename(gov2020, \"STNAME\" = Office)\ngov2020<-select(gov2020, STNAME, Name, Party)\nhead(gov2020)\n\n# A tibble: 6 × 3\n  STNAME         Name                 Party     \n  <chr>          <chr>                <chr>     \n1 Alabama        Kay Ivey             Republican\n2 Alaska         Mike Dunleavy        Republican\n3 American Samoa Lemanu Palepoi Mauga Democratic\n4 Arizona        Doug Ducey           Republican\n5 Arkansas       Asa Hutchinson       Republican\n6 California     Gavin Newsom         Democratic\n\n\n\n\nControl: Income\nMedian household income by county in 2020 is taken from this dataset, from the Economic Research Service at United States Department of Agriculture (2022). Income likely correlates with both political affiliation and COVID-19 death rates (access to medical care, preventative treatment etc), which is why it is included in the analysis.\n\n\nControl: Rural-Urban Continuum Code\nThis is also taken from the Economic Research Service/USDA dataset. According to the USDA, the 2013 rural-urban continuum code is a “classification scheme that distinguishes metropolitan counties by the population size of their metro area, and nonmetropolitan counties by degree of urbanization and adjacency to a metro area”. In my opinion, this makes more sense to include than a simple calculation of population density, because it takes into account cities/ how close people settle to cities rather than just population divided by land area. (For instance, a large county, land-wise, may have a majority of the population in a large city, however this could still result in a fairly small population density depending on land area). The Rural Urban Continuum code is on an integer scale from 1 to 9, where 1 represents the most urban/metro counties, and 9 represents the most rural counties.\nGenerally speaking, more rural counties tend to favor Trump while more metropolitan areas tend to be more democratic. At the same time, one would expect that more densely populated areas/ people living closer together in cities would experience higher infection (and therefore death rates) of COVID-19.\n\n#Income df\n\n#reading in income variable sheet, renaming variables, and selecting only relevant columns, then renaming fips code variable to join to other df's\n\nincome<-read_sheet(\"https://docs.google.com/spreadsheets/d/1ntReIIrpzjRvGabr64-91xEpSJk10r6Er1CX3pG5zBg/edit#gid=1233692484\", skip=4) %>% \n  rename(\"med_income_2020\" = Median_Household_Income_2020, \"county_fipsNEW\" = FIPS_code) %>% \n  select(county_fipsNEW, State, Area_name, med_income_2020, Rural_urban_continuum_code_2013)\nhead(income)\n\n# A tibble: 6 × 5\n  county_fipsNEW State Area_name          med_income_2020 Rural_urban_continuu…¹\n  <chr>          <chr> <chr>                        <dbl>                  <dbl>\n1 00000          US    United States                67340                     NA\n2 01000          AL    Alabama                      53958                     NA\n3 01001          AL    Autauga County, AL           67565                      2\n4 01003          AL    Baldwin County, AL           71135                      3\n5 01005          AL    Barbour County, AL           38866                      6\n6 01007          AL    Bibb County, AL              50907                      1\n# … with abbreviated variable name ¹​Rural_urban_continuum_code_2013"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#analysis",
    "href": "posts/EmmaRasmussenFinalPart2.html#analysis",
    "title": "Final Project Part 2",
    "section": "Analysis",
    "text": "Analysis\n\nJoining the Dataframes\nBelow I join the data based on FIPS code (except for the governor policy dummy variable, which is joined by state).\n\ncovidvote_2<-votedf %>% \n  left_join(coviddf_2, by=\"county_fipsNEW\")\nhead(covidvote_2)\n\n# A tibble: 6 × 5\n  county_fipsNEW percent_trump `County Name`  State covid_deaths\n  <chr>                  <dbl> <chr>          <chr> <chr>       \n1 01001                  0.728 Autauga County AL    210         \n2 01003                  0.765 Baldwin County AL    669         \n3 01005                  0.521 Barbour County AL    94          \n4 01007                  0.764 Bibb County    AL    100         \n5 01009                  0.893 Blount County  AL    230         \n6 01011                  0.242 Bullock County AL    52          \n\ncovid_vote_3 <- covidvote_2 %>% \n  left_join(age, by= \"county_fipsNEW\")\nhead(covid_vote_3)\n\n# A tibble: 6 × 10\n  county_f…¹ perce…² Count…³ State covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>        <dbl> <chr>   <chr> <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001        0.728 Autaug… AL    210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003        0.765 Baldwi… AL    669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005        0.521 Barbou… AL    94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007        0.764 Bibb C… AL    100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009        0.893 Blount… AL    230     Alaba… Blount… 57826   10814  0.1870…\n6 01011        0.242 Bulloc… AL    52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with abbreviated variable names ¹​county_fipsNEW, ²​percent_trump,\n#   ³​`County Name`, ⁴​covid_deaths, ⁵​over65_pct\n\ncovid_vote_4<- covid_vote_3 %>% \n  left_join(gov2020, by=\"STNAME\")\nhead(covid_vote_4)\n\n# A tibble: 6 × 12\n  county_f…¹ perce…² Count…³ State covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>        <dbl> <chr>   <chr> <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001        0.728 Autaug… AL    210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003        0.765 Baldwi… AL    669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005        0.521 Barbou… AL    94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007        0.764 Bibb C… AL    100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009        0.893 Blount… AL    230     Alaba… Blount… 57826   10814  0.1870…\n6 01011        0.242 Bulloc… AL    52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with 2 more variables: Name <chr>, Party <chr>, and abbreviated variable\n#   names ¹​county_fipsNEW, ²​percent_trump, ³​`County Name`, ⁴​covid_deaths,\n#   ⁵​over65_pct\n\ncovid_vote_5<- covid_vote_4 %>% \n  left_join(income, by= \"county_fipsNEW\")\nhead(covid_vote_5)\n\n# A tibble: 6 × 16\n  county…¹ perce…² Count…³ State.x covid…⁴ STNAME CTYNAME tot_pop over65 over6…⁵\n  <chr>      <dbl> <chr>   <chr>   <chr>   <chr>  <chr>   <chr>   <chr>  <chr>  \n1 01001      0.728 Autaug… AL      210     Alaba… Autaug… 55869   8924   0.1597…\n2 01003      0.765 Baldwi… AL      669     Alaba… Baldwi… 223234  46830  0.2097…\n3 01005      0.521 Barbou… AL      94      Alaba… Barbou… 24686   4861   0.1969…\n4 01007      0.764 Bibb C… AL      100     Alaba… Bibb C… 22394   3733   0.1666…\n5 01009      0.893 Blount… AL      230     Alaba… Blount… 57826   10814  0.1870…\n6 01011      0.242 Bulloc… AL      52      Alaba… Bulloc… 10101   1711   0.1693…\n# … with 6 more variables: Name <chr>, Party <chr>, State.y <chr>,\n#   Area_name <chr>, med_income_2020 <dbl>,\n#   Rural_urban_continuum_code_2013 <dbl>, and abbreviated variable names\n#   ¹​county_fipsNEW, ²​percent_trump, ³​`County Name`, ⁴​covid_deaths, ⁵​over65_pct\n\n#covid_vote_5 has all 5 dataframes joined together\n\nMore tidying:\n\n#converting covid deaths and total population to numeric variables\n\ncovid_vote_5$covid_deaths<-as.numeric(covid_vote_5$covid_deaths)\ncovid_vote_5$tot_pop<-as.numeric(covid_vote_5$tot_pop)\ncovid_vote_5$over65_pct<-as.numeric(covid_vote_5$over65_pct)\n\n#creating a death rate variable (COVID-19 deaths per 100,000 by county from January 2020-March 2022)\ncovid_vote_5$\"covid_death_rate\" = (covid_vote_5$covid_deaths / covid_vote_5$tot_pop)*100000\n\n#selecting only relevant columns for analysis\ncovid_vote_5<- select(covid_vote_5, county_fipsNEW, STNAME, CTYNAME, covid_death_rate, percent_trump, over65_pct, Party, med_income_2020, tot_pop, Rural_urban_continuum_code_2013)\nhead(covid_vote_5)\n\n# A tibble: 6 × 10\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with abbreviated variable names ¹​county_fipsNEW, ²​covid_death_rate,\n#   ³​percent_trump, ⁴​over65_pct, ⁵​med_income_2020,\n#   ⁶​Rural_urban_continuum_code_2013\n\n\n\n# Creating df where counties with death rates of exactly 0 (no covid deaths) are excluded. This will allow me to take the log  of death rates in later analysis, and exclude counties that potentially did not report COVID-19 deaths at all. Exluding counties where covid_death_pct is greater than zero removes 66 counties from the analysis.\n\ncovid_vote_5_no_zero<-filter(covid_vote_5, covid_death_rate >0)\nhead(covid_vote_5_no_zero)\n\n# A tibble: 6 × 10\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with abbreviated variable names ¹​county_fipsNEW, ²​covid_death_rate,\n#   ³​percent_trump, ⁴​over65_pct, ⁵​med_income_2020,\n#   ⁶​Rural_urban_continuum_code_2013\n\n\n\n\nExplotatory Analysis\n\n#Scatter plot of percent trump and percent population dying of covid. \nvisual1<-ggplot(covid_vote_5, aes(x=percent_trump, y=covid_death_rate))+geom_point()+geom_smooth()+labs(x= \"(%) Votes for Trump in 2016\", y=\"COVID-19 Deaths per 100,000 People\", title= \"COVID-19 Death Rate in U.S. Counties Based on 2016 Trump Support\")\nggplotly(visual1)\n\n\n\n\n\nThere appears to be a slight positive trend, especially among counties where percentage Trump votes was over 50%. Interestingly, on this plot, the death rate appears to decrease slightly from 0 to 40% percent voting for Trump, and then increases. Perhaps this could be due to the fact that highly democratic counties are more likely to be more urban- therefore people live more close together and infection rate is likely higher. From this visual, it appears that the county with the highest COVID-19 death rate (1123 per 100,000) voted heavily for Trump in 2016 (over 90%). The counties with death rates of 0 appear to come from across parties (however these counties might not have reported COVID-19 deaths at all).\n\n#Creating a new variable, whether trump or clinton was the majority vote\ncovid_vote_5<- covid_vote_5%>% \n  mutate(majority= case_when(percent_trump > 0.5 ~ \"Trump_favor\",\n                                             percent_trump < 0.5 ~ \"Clinton_favor\"))\nhead(covid_vote_5)\n\n# A tibble: 6 × 11\n  county_…¹ STNAME CTYNAME covid…² perce…³ over6…⁴ Party med_i…⁵ tot_pop Rural…⁶\n  <chr>     <chr>  <chr>     <dbl>   <dbl>   <dbl> <chr>   <dbl>   <dbl>   <dbl>\n1 01001     Alaba… Autaug…    376.   0.728   0.160 Repu…   67565   55869       2\n2 01003     Alaba… Baldwi…    300.   0.765   0.210 Repu…   71135  223234       3\n3 01005     Alaba… Barbou…    381.   0.521   0.197 Repu…   38866   24686       6\n4 01007     Alaba… Bibb C…    447.   0.764   0.167 Repu…   50907   22394       1\n5 01009     Alaba… Blount…    398.   0.893   0.187 Repu…   55203   57826       1\n6 01011     Alaba… Bulloc…    515.   0.242   0.169 Repu…   33124   10101       6\n# … with 1 more variable: majority <chr>, and abbreviated variable names\n#   ¹​county_fipsNEW, ²​covid_death_rate, ³​percent_trump, ⁴​over65_pct,\n#   ⁵​med_income_2020, ⁶​Rural_urban_continuum_code_2013\n\n#creating a boxplot to compare means of two groups\nggplot(na.omit(covid_vote_5), aes(x=majority, y=covid_death_rate))+geom_boxplot()+labs(x=\"Majority\", y=\"COVID-19 Deaths per 100,000 People\", title= \"COVID-19 Death Rates in Clinton and Trump Majority Counties\")\n\n\n\n\nAccording to this boxplot, there appears to be a difference in median COVID-19 death rates based on if a county voted majority Trump of Clinton.\n\n\nModeling this Relationship\nBelow I try two different models including all of the control variables identified above. In the second model I omit counties where covid_death_rate is zero so I can log transform covid_death_rate. I think a log transformation could make sense in this case- perhaps only counties that had higher percentage Trump votes also had higher corresponding COVID-19 death rates (see scatterplot above).\n\n#Model with covid_death_pct with 0 values included\nsummary(lm(covid_death_rate ~ percent_trump + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5))\n\n\nCall:\nlm(formula = covid_death_rate ~ percent_trump + over65_pct + \n    med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-447.34  -82.23   -3.53   79.19  765.10 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      5.406e+02  2.043e+01  26.463  < 2e-16 ***\npercent_trump                    1.635e+02  1.778e+01   9.196  < 2e-16 ***\nover65_pct                      -6.920e+01  5.964e+01  -1.160    0.246    \nmed_income_2020                 -4.955e-03  1.968e-04 -25.182  < 2e-16 ***\nPartyRepublican                  2.536e+01  5.252e+00   4.829 1.44e-06 ***\nRural_urban_continuum_code_2013 -8.644e-01  1.160e+00  -0.745    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.6 on 3107 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.2668,    Adjusted R-squared:  0.2656 \nF-statistic: 226.1 on 5 and 3107 DF,  p-value: < 2.2e-16\n\nfit1<-lm(covid_death_rate ~ percent_trump + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5)\n\n#Model with covid_death_pct with 0 values excluded and logged covid_death_pct\nsummary(lm(log(covid_death_rate) ~ percent_trump + over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, data= covid_vote_5_no_zero))\n\n\nCall:\nlm(formula = log(covid_death_rate) ~ percent_trump + over65_pct + \n    med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5_no_zero)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.15841 -0.19737  0.06346  0.27447  1.16727 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      6.382e+00  6.656e-02  95.877  < 2e-16 ***\npercent_trump                    7.718e-01  5.789e-02  13.331  < 2e-16 ***\nover65_pct                      -2.974e-01  1.941e-01  -1.532  0.12557    \nmed_income_2020                 -1.720e-05  6.410e-07 -26.828  < 2e-16 ***\nPartyRepublican                  5.297e-02  1.705e-02   3.106  0.00192 ** \nRural_urban_continuum_code_2013 -1.826e-02  3.787e-03  -4.821  1.5e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4484 on 3085 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.2852,    Adjusted R-squared:  0.2841 \nF-statistic: 246.2 on 5 and 3085 DF,  p-value: < 2.2e-16\n\nfit2<-lm(log(covid_death_rate) ~ percent_trump + over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, data= covid_vote_5_no_zero)\n\nThe second model, where covid_death_rate is log transformed and the 66 observations where COVID-19 death rate is zero are omitted has the higher adjusted r-squared.\nIn both models, the coefficient of interest percent_trump, is significant at the 0.001 significance level and has a positive coefficient. This model suggests there is evidence that counties that vote higher for Trump experience higher COVID-19 death rates. Still, the adjusted R-squared is not very high.\nConcerns with this model: - Potential multicollinearity between over65_pct and med_income_2020? and between gov dummy and percent trump? - why is the coefficient for percent over 65 negative?\nBelow I test a polynomial model (where at first death rate decreases and then decreases as the curved line in the scatterplot above), however, the adjusted R squared is still smaller than in the above model.\n\n#polynomial model\nsummary(lm(covid_death_rate ~ poly(percent_trump, 2, raw=TRUE) + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5))\n\n\nCall:\nlm(formula = covid_death_rate ~ poly(percent_trump, 2, raw = TRUE) + \n    over65_pct + med_income_2020 + Party + Rural_urban_continuum_code_2013, \n    data = covid_vote_5)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-470.63  -82.97   -3.05   76.94  770.31 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                          6.393e+02  3.181e+01  20.093  < 2e-16 ***\npoly(percent_trump, 2, raw = TRUE)1 -2.056e+02  9.310e+01  -2.209   0.0273 *  \npoly(percent_trump, 2, raw = TRUE)2  3.286e+02  8.136e+01   4.039 5.50e-05 ***\nover65_pct                          -4.899e+01  5.970e+01  -0.821   0.4120    \nmed_income_2020                     -5.004e-03  1.967e-04 -25.443  < 2e-16 ***\nPartyRepublican                      2.273e+01  5.279e+00   4.305 1.72e-05 ***\nRural_urban_continuum_code_2013     -1.759e+00  1.178e+00  -1.492   0.1357    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.2 on 3106 degrees of freedom\n  (45 observations deleted due to missingness)\nMultiple R-squared:  0.2706,    Adjusted R-squared:  0.2692 \nF-statistic:   192 on 6 and 3106 DF,  p-value: < 2.2e-16\n\nfit3<-lm(covid_death_rate ~ poly(percent_trump, 2, raw=TRUE) + over65_pct + med_income_2020 + Party+ Rural_urban_continuum_code_2013, data= covid_vote_5)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#diagnositics",
    "href": "posts/EmmaRasmussenFinalPart2.html#diagnositics",
    "title": "Final Project Part 2",
    "section": "Diagnositics",
    "text": "Diagnositics\n\npar(mfrow= c(2,3)); plot(fit1, which=1:6)\n\n\n\npar(mfrow= c(2,3)); plot(fit2, which=1:6)\n\n\n\npar(mfrow= c(2,3)); plot(fit3, which=1:6)\n\n\n\n\nAccording to the diagnostic plots, none of the models seem to fit super well. For fit2, (the one where covid_death_rate is log transformed,) the residuals seem to have a trend (higher fitted values have lower residuals). Same with the Q-Q, plot, lower theoretical quantiles gave significantly lower standardized residuals. The scale location graph has a negative trend, suggesting variance may not be constant. There may be a couple outliers significantly affecting the model according 4/n which is approximately 0.0013. Residuals versus leverage looks ok, there does not appear to be any super concerning observations. Finally, Cooks dist to leverage 2898 has a high cooks distance and leverage and likely has a large influence on the model. The other models display similar issues.\nMoving into part three of the project, I may look into other control variables that may improve the model or other transformations to improve R squared. Regardless there does not seem to be a super strong observable trend."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart2.html#references",
    "href": "posts/EmmaRasmussenFinalPart2.html#references",
    "title": "Final Project Part 2",
    "section": "References",
    "text": "References\nBallotpedia. (2022). Partisan composition of governors. Accessed [November 10, 2022]. https://ballotpedia.org/Partisan_composition_of_governors\nEconomic Research Service. (2022). Unemployment and median household income for the U.S., States, and counties, 2000-2021. Accessed from the United States Department of Agriculture [November 10, 2022]. https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data/\nJones, B. (2022). The Changing Political Geography of COVID-19 Over the Last Two Years. Pew Research Center. March 3, 2022. https://www.pewresearch.org/politics/2022/03/03/the-changing-political-geography-of-covid-19-over-the-last-two-years/\nMayo Clinic Staff. (2022). “COVID-19: Who’s at Higher Risk of Serious Symptoms?” Accessed from Mayo Clinic, [November 11, 2022]. https://www.mayoclinic.org/diseases-conditions/coronavirus/in-depth/coronavirus-who-is-at-risk/art-20483301\nMIT Election Data and Science Lab. (2021) County Presidential Election Returns 2000-2020. Accessed from the Harvard Dataverse [October 11, 2022]. https://doi.org/10.7910/DVN/VOQCHQ\nNational Center for Health Statistics. (2022). Provisional COVID-19 Deaths by County, and Race and Hispanic Origin. Accessed from the Centers for Disease Control [October 11, 2022]. https://data.cdc.gov/d/k8wy-p9cg\nUSA Facts. (2022).US COVID-19 cases and deaths by state. Accessed [November 10, 2022]. https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/?utm_source=usnews&utm_medium=partnership&utm_campaign=2020&utm_content=healthiestcommunitiescovid\nUnited States Census Bureau. (2019). Annual County Resident Population Estimates by Age, Sex, Race, and Hispanic Origin: April 1, 2010 to July 1, 2019 (CC-EST2019-ALLDATA). Accessed from Census.gov [November 11, 2022].https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html\nWood, D. and Brumfiel, G. (2021). Pro-Trump counties now have far higher COVID death rates. Misinformation is to blame. NPR. December 5, 2021. https://www.npr.org/sections/health-shots/2021/12/05/1059828993/data-vaccine-misinformation-trump-counties-covid-death-rate\n[Need to add italics to references]"
  },
  {
    "objectID": "posts/Final Part 2.html",
    "href": "posts/Final Part 2.html",
    "title": "Final Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapview)\n#library(summarytools)\nlibrary(GGally)\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Part 2.html#notes-from-class",
    "href": "posts/Final Part 2.html#notes-from-class",
    "title": "Final Part 2",
    "section": "Notes from class",
    "text": "Notes from class\nWhy did I choose particular interaction terms?\nWhy did I choose the variables I did for certain models? There must be a reason for each different model"
  },
  {
    "objectID": "posts/Final Part 2.html#data-read-in",
    "href": "posts/Final Part 2.html#data-read-in",
    "title": "Final Part 2",
    "section": "Data Read-in",
    "text": "Data Read-in\nHere is the Miami housing dataset I am using for the project.\n\n\nCode\nmiami_housing <- read_csv(\"~/Documents/DACSS Program/data/miami-housing.csv\")\n\n\nError: '~/Documents/DACSS Program/data/miami-housing.csv' does not exist.\n\n\nBelow I read in data I may end up using that relates to the county and state election data. For now, my main data set is miami_housing.\n\n\nCode\ncounty_election <- read_csv(\"~/Documents/DACSS Program/data/president_county.csv\")\n\n\nError: '~/Documents/DACSS Program/data/president_county.csv' does not exist.\n\n\nCode\nstate_election <- read_csv(\"~/Documents/DACSS Program/data/president_state.csv\")\n\n\nError: '~/Documents/DACSS Program/data/president_state.csv' does not exist.\n\n\nCode\npresident_county <- read_csv(\"~/Documents/DACSS Program/data/president_county_candidate.csv\")\n\n\nError: '~/Documents/DACSS Program/data/president_county_candidate.csv' does not exist."
  },
  {
    "objectID": "posts/Final Part 2.html#information-about-data-set",
    "href": "posts/Final Part 2.html#information-about-data-set",
    "title": "Final Part 2",
    "section": "Information about data set",
    "text": "Information about data set\nThe dataset contains information on 13,932 single-family homes sold in Miami .\nNames before column rename: PARCELNO: unique identifier for each property. About 1% appear multiple times. SALE_PRC: sale price (\\() LND_SQFOOT: land area (square feet) TOTLVGAREA: floor area (square feet) SPECFEATVAL: value of special features (e.g., swimming pools) (\\)) RAIL_DIST: distance to the nearest rail line (an indicator of noise) (feet) OCEAN_DIST: distance to the ocean (feet) WATER_DIST: distance to the nearest body of water (feet) CNTR_DIST: distance to the Miami central business district (feet) SUBCNTR_DI: distance to the nearest subcenter (feet) HWY_DIST: distance to the nearest highway (an indicator of noise) (feet) age: age of the structure avno60plus: dummy variable for airplane noise exceeding an acceptable level structure_quality: quality of the structure month_sold: sale month in 2016 (1 = jan) LATITUDE LONGITUDE\nI first want to rename some columns because I am not a fan of the format of the stock column names,\n\n\nCode\nmiami_housing <- miami_housing %>% \n  rename(\"latitude\" = \"LATITUDE\",\n         \"longitude\" = \"LONGITUDE\",\n         \"sale_price\" = \"SALE_PRC\",  \n         \"land_sqfoot\" = \"LND_SQFOOT\",  \n         \"floor_sqfoot\" = \"TOT_LVG_AREA\",\n         \"special_features\" = \"SPEC_FEAT_VAL\",  \n         \"dist_2_rail\" = \"RAIL_DIST\",  \n         \"dist_2_ocean\" = \"OCEAN_DIST\", \n         \"dist_2_nearest_water\" = \"WATER_DIST\",  \n         \"dist_2_biz_center\" = \"CNTR_DIST\",  \n         \"dis_2_nearest_subcenter\"= \"SUBCNTR_DI\", \n         \"dist_2_hiway\" = \"HWY_DIST\", #close distance is a negative trait \n         \"home_age\" = \"age\") \n\n\nError in rename(., latitude = \"LATITUDE\", longitude = \"LONGITUDE\", sale_price = \"SALE_PRC\", : object 'miami_housing' not found\n\n\nI wonder if it would be worth creating another column called “county” based off of the longitude and latitude coordinates. This would make some graphs more interesting since I could fill by county in a ggplot graph."
  },
  {
    "objectID": "posts/Final Part 2.html#research-question",
    "href": "posts/Final Part 2.html#research-question",
    "title": "Final Part 2",
    "section": "Research Question",
    "text": "Research Question\nDoes the saying “location, location, location” really matter when it comes to housing prices in Miami or are there other factors as well that contribute to price?"
  },
  {
    "objectID": "posts/Final Part 2.html#hypothesis",
    "href": "posts/Final Part 2.html#hypothesis",
    "title": "Final Part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nOutcome variable: Sale price of houses in Miami\nExplanatory variable: Distance from the ocean, measured in feet\nHypothesis: Houses closer (lower dist_2_ocean) will have a higher price than houses farther."
  },
  {
    "objectID": "posts/Final Part 2.html#descriptive-statistics",
    "href": "posts/Final Part 2.html#descriptive-statistics",
    "title": "Final Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI’ll have to go further in depth into this research that I found.\nAccording to Redfin, the median sale price for houses in Miami is $530,000. https://www.redfin.com/city/11458/FL/Miami/housing-market\nhttps://www.proquest.com/docview/222418064?pq-origsite=gscholar&fromopenview=true - This study found that increase in house prices from 2003-2004 was largely due to interest rates, housing construction, unemployment, and household income.\nhttps://link.springer.com/article/10.1023/A:1007751112669\nhttps://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2005.00243.x?casa_token=Lk-oinHQfCoAAAAA:3YFA9GRG6r9UwIJa8-8Z40x77ENRHm07d9CQ7dLdRZD5VoGjSVB4hUUcKU8yWiJapg0csSONiwxZzqWJ\nhttps://www.sciencedirect.com/science/article/pii/S1051137712000228?casa_token=DwJ93qbzqLAAAAAA:8vClfNoxs09D_UL4BCg-Ds36vurfjk8t0uK6Hft50ytFeWo3-XaFlsj5r0WBW4lB1jGqgoaqwXA\nhttps://link.springer.com/article/10.1007/s11146-007-9053-7\nhttps://eprints.gla.ac.uk/221903/1/221903.pdf"
  },
  {
    "objectID": "posts/Final Part 2.html#exploratory-analysis",
    "href": "posts/Final Part 2.html#exploratory-analysis",
    "title": "Final Part 2",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\n\nCode\nstr(miami_housing)\n\n\nError in str(miami_housing): object 'miami_housing' not found\n\n\n\n\nCode\nsummary(miami_housing)\n\n\nError in summary(miami_housing): object 'miami_housing' not found\n\n\nAt a glance, I do not care about summary statistics for longitude and latitude. Sale price is worth noting with a minimum of $72,000 and a max of $2.6 million. There are some pretty old homes in the data set with the most at almost 100 years old. A minimum age of zero may be something to watch out for because it is not clear as to if the home is less than 1 year old. I must also find out how the distance is measured. I believe distance in this data set is measured in feet so we can see there may be a few water front properties that I will guess fetch a high sale price. Homes farther away may be cheaper if they are not located near another desirable location.\nWhile typically a negative, the distance to highway can be viewed as a positive trait to a home. For the sake of this analysis and to go with typical association with close proximity to a highway, I will view closer distance as a negative.\nAnother note, is I will have one linear model, m1, to act as the model with all of the variables in it. I want it as a baseline model to see the factor all of the variables play in house price. Since multicollinearity may be a factor, I will not use this model too much.\n\n\nCode\nm1 <- lm(sale_price ~ ., data = miami_housing)\n\n\nError in is.data.frame(data): object 'miami_housing' not found"
  },
  {
    "objectID": "posts/final part-2.html",
    "href": "posts/final part-2.html",
    "title": "Final Project Part-2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(corrgram)\nlibrary(ggpubr)\nlibrary(caTools)\nlibrary(GGally)"
  },
  {
    "objectID": "posts/final part-2.html#backgroundmotivation",
    "href": "posts/final part-2.html#backgroundmotivation",
    "title": "Final Project Part-2",
    "section": "Background/Motivation",
    "text": "Background/Motivation\nWhat is takes for a country or a continent to be happy? Is it the economy, life-expectancy, freedom or the trust in the government? What are the factors that affect a country’s or continents overall happiness? Can we predict the happiness score? The curiosity to find answers to these questions made me explore the world happiness data of 2022.\n“This year marks the 10th anniversary of the World Happiness Report, which uses global survey data to report how people evaluate their own lives in more than 150 countries worldwide. The World Happiness Report 2022 reveals a bright light in dark times. The pandemic brought not only pain and suffering but also an increase in social support and benevolence. As we battle the ills of disease and war, it is essential to remember the universal desire for happiness and the capacity of individuals to rally to each other’s support in times of great need.” - World Happiness Report 2022"
  },
  {
    "objectID": "posts/final part-2.html#research-question",
    "href": "posts/final part-2.html#research-question",
    "title": "Final Project Part-2",
    "section": "Research Question",
    "text": "Research Question\nThe World happiness data tries to measure the happiness of the populace of every country and comes up with a score which connotes the level of happiness of the populace.\nThe data set uses various variables to measure happiness such as the GDP per capita, Freedom to make choices, life expectancy, the perception of corruption, generosity and social support.\nIn this study, I aim to find out answers to the following research questions:\n\nWhat are the variables or factors that are affecting world’s happiness, with a focus on individual countries & continents. This includes analyzing the correlation between most effective variables.\nTo find out which model accurately predicts the happiness score."
  },
  {
    "objectID": "posts/final part-2.html#hypothesis",
    "href": "posts/final part-2.html#hypothesis",
    "title": "Final Project Part-2",
    "section": "Hypothesis",
    "text": "Hypothesis\nI wish to test the following hypothesis,\n\nBetter economy of a country would lead to happiness\nLonger life expectancy would lead to happiness\nHaving family/social support leads to happiness\nFreedom leads to happiness\nPeople’s trust in the Government leads to happiness\nGenerosity leads to happiness"
  },
  {
    "objectID": "posts/final part-2.html#data-preparation",
    "href": "posts/final part-2.html#data-preparation",
    "title": "Final Project Part-2",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nReading the data set\nprimary <- read.csv(\"project datasets/World Happiness Report 2022.csv\")\nhead(primary)\nstr(primary)\nstr(primary$Country)\nThe dataset that I have chosen is happiness 2022 dataset, one of Kaggle’s dataset. This dataset gives the happiness rank and happiness score of 147 countries around the world based on 8 factors including GDP per capita, Social support, Health life expectancy, freedom to make life choices, Generosity, Perceptions of corruption and dystopia residual. The higher value of each of these 8 factors means the level of happiness is higher. Dystopia is the opposite of utopia and has the lowest happiness level. Dystopia will be considered as a reference for other countries to show how far they are from being the poorest country regarding happiness level.\nSource of the data: World Happiness Report 2022 use data from the Gallup World Poll surveys from 2019 to 2021. They are based on answers to the main life evaluation question asked in the poll.\nSome of the variable names are not clear enough and I decided to change the name of several of them a little bit. Also, I will remove whisker low and whisker high variables from my dataset because these variables give only the lower and upper confidence interval of happiness score and there is no need to use them for visualization and prediction.\nThe next step is adding another column to the dataset which is continent. I want to work on different continents to discover whether there are different trends for them regarding which factors play a significant role in gaining higher happiness score. Asia, Africa, North America, South America, Europe, and Australia are our six continents in this dataset. Then I moved the position of the continent column to the second column because I think with this position arrange, dataset looks better. Finally, I changed the type of continent variable to factor to be able to work with it easily for visualization."
  },
  {
    "objectID": "posts/final part-2.html#preparation-of-the-data",
    "href": "posts/final part-2.html#preparation-of-the-data",
    "title": "Final Project Part-2",
    "section": "Preparation of the data",
    "text": "Preparation of the data\n\n\nCode\n# Changing the name of columns\ncolnames (primary) <- c(\"Happiness.Rank\", \"Country\",  \"Happiness.Score\",\n                          \"Whisker.High\", \"Whisker.Low\", \"Dystopia.Residual\",   \"Economy\", \"Family\", \"Life.Expectancy\", \"Freedom\", \"Generosity\",\n                          \"Trust\")\n\n\nError in colnames(primary) <- c(\"Happiness.Rank\", \"Country\", \"Happiness.Score\", : object 'primary' not found\n\n\nCode\n# Country: Name of countries\n# Happiness.Rank: Rank of the country based on the Happiness Score\n# Happiness.Score: Happiness measurement on a scale of 0 to 10\n# Whisker.High: Upper confidence interval of happiness score\n# Whisker.Low: Lower confidence interval of happiness score\n# Economy: The value of all final goods and services produced within a nation in a given year\n# Family: Importance of having a family\n# Life.Expectancy: Importance of health and amount of time prople expect to live\n# Freedom: Importance of freedom in each country\n# Generosity: The quality of being kind and generous\n# Trust: Perception of corruption in a government\n# Dystopia.Residual: Plays as a reference\n\n# Deleting unnecessary columns (Whisker.high and Whisker.low)\n\nprimary <- primary[, -c(4,5)]\n\n\nError in eval(expr, envir, enclos): object 'primary' not found\n\n\n\n\nCode\nprimary$Continent <- NA\n\n\nError in primary$Continent <- NA: object 'primary' not found\n\n\nCode\nprimary$Continent[which(primary$Country %in% c(\"Israel\", \"United Arab Emirates\", \"Singapore\", \"Thailand\", \"Taiwan Province of China\",\n                                   \"Qatar\", \"Saudi Arabia\", \"Kuwait\", \"Bahrain\", \"Malaysia\", \"Uzbekistan\", \"Japan\",\n                                   \"South Korea\", \"Turkmenistan\", \"Kazakhstan\", \"Turkey\", \"Hong Kong S.A.R., China\", \"Philippines\",\n                                   \"Jordan\", \"China\", \"Pakistan\", \"Indonesia\", \"Azerbaijan\", \"Lebanon\", \"Vietnam\",\n                                   \"Tajikistan\", \"Bhutan\", \"Kyrgyzstan\", \"Nepal\", \"Mongolia\", \"Palestinian Territories\",\n                                   \"Iran\", \"Bangladesh\", \"Myanmar\", \"Iraq\", \"Sri Lanka\", \"Armenia\", \"India\", \"Georgia\",\n                                   \"Cambodia\", \"Afghanistan\", \"Yemen\", \"Syria\"))] <- \"Asia\"\n\n\nError in primary$Continent[which(primary$Country %in% c(\"Israel\", \"United Arab Emirates\", : object 'primary' not found\n\n\nCode\nprimary$Continent[which(primary$Country %in% c(\"Norway\", \"Denmark\", \"Iceland\", \"Switzerland\", \"Finland\",\n                                   \"Netherlands\", \"Sweden\", \"Austria\", \"Ireland\", \"Germany\",\n                                   \"Belgium\", \"Luxembourg\", \"United Kingdom\", \"Czech Republic\",\n                                   \"Malta\", \"France\", \"Spain\", \"Slovakia\", \"Poland\", \"Italy\",\n                                   \"Russia\", \"Lithuania\", \"Latvia\", \"Moldova\", \"Romania\",\n                                   \"Slovenia\", \"North Cyprus\", \"Cyprus\", \"Estonia\", \"Belarus\",\n                                   \"Serbia\", \"Hungary\", \"Croatia\", \"Kosovo\", \"Montenegro\",\n                                   \"Greece\", \"Portugal\", \"Bosnia and Herzegovina\", \"Macedonia\",\n                                   \"Bulgaria\", \"Albania\", \"Ukraine\"))] <- \"Europe\"\n\n\nError in primary$Continent[which(primary$Country %in% c(\"Norway\", \"Denmark\", : object 'primary' not found\n\n\nCode\nprimary$Continent[which(primary$Country %in% c(\"Canada\", \"Costa Rica\", \"United States\", \"Mexico\",  \n                                   \"Panama\",\"Trinidad and Tobago\", \"El Salvador\", \"Belize\", \"Guatemala\",\n                                   \"Jamaica\", \"Nicaragua\", \"Dominican Republic\", \"Honduras\",\n                                   \"Haiti\"))] <- \"North America\"\n\n\nError in primary$Continent[which(primary$Country %in% c(\"Canada\", \"Costa Rica\", : object 'primary' not found\n\n\nCode\nprimary$Continent[which(primary$Country %in% c(\"Chile\", \"Brazil\", \"Argentina\", \"Uruguay\",\n                                   \"Colombia\", \"Ecuador\", \"Bolivia\", \"Peru\",\n                                   \"Paraguay\", \"Venezuela\"))] <- \"South America\"\n\n\nError in primary$Continent[which(primary$Country %in% c(\"Chile\", \"Brazil\", : object 'primary' not found\n\n\nCode\nprimary$Continent[which(primary$Country %in% c(\"New Zealand\", \"Australia\"))] <- \"Australia\"\n\n\nError in primary$Continent[which(primary$Country %in% c(\"New Zealand\", : object 'primary' not found\n\n\nCode\nprimary$Continent[which(is.na(primary$Continent))] <- \"Africa\"\n\n\nError in primary$Continent[which(is.na(primary$Continent))] <- \"Africa\": object 'primary' not found\n\n\nCode\n# Moving the continent column's position in the dataset to the second column\n\nprimary <- primary %>% select(Country,Continent, everything())\n\n\nError in select(., Country, Continent, everything()): object 'primary' not found\n\n\nCode\n#Renaming the final dataframe to happy\n\nhappy <- primary\n\n\nError in eval(expr, envir, enclos): object 'primary' not found\n\n\nCode\nstr(happy)\n\n\n'data.frame':   51020 obs. of  10 variables:\n $ id     : num  1 2 3 4 5 6 7 8 9 10 ...\n $ happy  : Factor w/ 3 levels \"not too happy\",..: 1 1 2 1 2 2 1 1 2 2 ...\n $ year   : num  1972 1972 1972 1972 1972 ...\n $ age    : num  23 70 48 27 61 26 28 27 21 30 ...\n $ sex    : Factor w/ 2 levels \"male\",\"female\": 2 1 2 2 2 1 1 1 2 2 ...\n $ marital: Factor w/ 5 levels \"married\",\"never married\",..: 2 1 1 1 1 2 3 2 2 1 ...\n $ degree : Factor w/ 5 levels \"lt high school\",..: 4 1 2 4 2 2 2 4 2 2 ...\n $ finrela: Factor w/ 5 levels \"far below average\",..: 3 4 3 3 4 4 4 3 3 2 ...\n $ health : Factor w/ 4 levels \"poor\",\"fair\",..: 3 2 4 3 3 3 4 3 4 2 ...\n $ wtssall: num  0.445 0.889 0.889 0.889 0.889 ..."
  },
  {
    "objectID": "posts/final part-2.html#visualization",
    "href": "posts/final part-2.html#visualization",
    "title": "Final Project Part-2",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "posts/final part-2.html#analyzing-the-correlation-between-each-numeric-variable",
    "href": "posts/final part-2.html#analyzing-the-correlation-between-each-numeric-variable",
    "title": "Final Project Part-2",
    "section": "Analyzing the correlation between each numeric variable",
    "text": "Analyzing the correlation between each numeric variable\nAs we already know that the sum of these numeric variables gives the happiness score and there is an inverse relationship between happiness score and happiness rank. The higher the happiness score, the lower the happiness rank. Hence there is no need of looking at the correlation between each numeric variable and happiness rank. We can directly look at the happiness score and other numeric variables.\n\n\nCode\n# Create a correlation plot\nggcorr(dataset, label = TRUE, label_round = 2, label_size = 3.5, size = 2, hjust = .85) +\n  ggtitle(\"Correlation Heatmap\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in ggcorr(dataset, label = TRUE, label_round = 2, label_size = 3.5, : object 'dataset' not found\n\n\nAccording to the above cor plot, Economy, life expectancy, and family play the most significant role in contributing to happiness. Trust and generosity have the lowest impact on the happiness score.\n\nComparing different continents regarding their happiness variables\nLet’s calculate the average happiness score and the average of the other seven variables for each continent. Then melt it to have variables and values in separate columns. Finally, using ggplot to show the difference between continents.\n\n\nCode\nhappy.Continent <- happy %>%\n                          select(-3) %>%\n                          group_by(Continent) %>%\n                          summarise_at(vars(-Country), funs(mean(., na.rm=TRUE)))\n\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Continent` is not found.\n\n\nCode\n# Or we can use aggregate\n# aggregate(happy[, 4:11], list(happy$Continent), mean)\n\n# Melting the \"happy.Continent\" dataset\nhappy.Continent.melt <- melt(happy.Continent)\n\n\nError in melt(happy.Continent): object 'happy.Continent' not found\n\n\nCode\n# Faceting\nggplot(happy.Continent.melt, aes(y=value, x=Continent, color=Continent, fill=Continent)) + \n  geom_bar( stat=\"identity\") +    \n  facet_wrap(~variable) + theme_bw() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Average value of happiness variables for different continents\", \n       y = \"Average value\") \n\n\nError in ggplot(happy.Continent.melt, aes(y = value, x = Continent, color = Continent, : object 'happy.Continent.melt' not found\n\n\nWe can see that Australia has approximately the highest average in all fields except dystopia residual, after that Europe, North America, and South America are roughly the same regarding happiness score and the other seven factors. Finally, Asia and Africa have the lowest scores in all fields.\n\n\nCorrelation plot for each continent\nLet’s see the correlation between variables for each continent.\n\n\nCode\nggcorr(happy %>% select(-3) %>% filter(Continent == \"Africa\"), label = TRUE, label_round = 2, label_size = 3.5, size = 3.5, hjust = .85) +\n  ggtitle(\"Happiness Matrix for Africa\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Continent == \"Africa\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Continent' not found\n\n\nCorrelation between “Happiness Score” and the other variables in Africa:\nEconomy > Life Expectancy > Family > Dystopia Residual > Freedom\nHappiness score and Generosity are inversely correlated.\nThere is no correlation between happiness score and trust.\n\n\n\nCode\nggcorr(happy %>% select(-3) %>% filter(Continent == \"Asia\"), label = TRUE, label_round = 2, label_size = 3.5, size = 3.5, hjust = .85) +\n  ggtitle(\"Happiness Matrix for Asia\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Continent == \"Asia\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Continent' not found\n\n\nCorrelation between “Happiness Score” and the other variables in Asia:\nFamily > Dystopia.Residual > Freedom > Economy > Life Expectancy > Trust\n\nThere is no correlation between happiness score and Generosity.\n\n\nCode\nggcorr(happy %>% select(-3) %>% filter(Continent == \"Europe\"), label = TRUE, label_round = 2, label_size = 3.5, size = 3.5, hjust = .85) +\n  ggtitle(\"Happiness Matrix for Europe\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Continent == \"Europe\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Continent' not found\n\n\nCorrelation between “Happiness Score” and the other variables in Europe:\nTrust > Economy > Freedom > Life Expectancy > Dystopia Residual > Family > Generosity\n\n\n\nCode\nggcorr(happy %>% select(-3) %>% filter(Continent == \"North America\"), label = TRUE, label_round = 2, label_size = 3.5, size = 3.5, hjust = .85) +\n  ggtitle(\"Happiness Matrix for North America\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Continent == \"North America\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Continent' not found\n\n\nCorrelation between “Happiness Score” and the other variables in North America:\nEconomy > Life Expectancy > Family > Generosity > Trust\nThere is an inverse correlation between happiness score and dystopia residual.\n\n\nCode\nggcorr(happy %>% select(-3) %>% filter(Continent == \"South America\"), label = TRUE, label_round = 2, label_size = 3.5, size = 3.5, hjust = .85) +\n  ggtitle(\"Happiness Matrix for South America\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\nError in `filter()`:\n! Problem while computing `..1 = Continent == \"South America\"`.\nCaused by error in `mask$eval_all_filter()`:\n! object 'Continent' not found\n\n\nCorrelation between “Happiness Score” and the other variables in South America:\nDystopia.Residual > Trust > Family > Freedom > Life Expectancy\nThere is an inverse correlation between happiness score and dystopia residual, Generosity.\n\n\nHappiness score comparison on different continents\nWe will use scatter plot, box plot, and violin plot to see the happiness score distribution in different countries, how this score is populated in these continents and also will calculate the mean and median of happiness score for each of these continents.\n\n\nCode\n####### Happiness score for each continent\n\ngg1 <- ggplot(happy,\n              aes(x=Continent,\n                  y=Happiness.Score,\n                  color=Continent))+\n  geom_point() + theme_bw() +\n  theme(axis.title = element_text(family = \"Helvetica\", size = (8)))\n\ngg2 <- ggplot(happy , aes(x = Continent, y = Happiness.Score)) +\n  geom_boxplot(aes(fill=Continent)) + theme_bw() +\n  theme(axis.title = element_text(family = \"Helvetica\", size = (8)))\n\ngg3 <- ggplot(happy,aes(x=Continent,y=Happiness.Score))+\n  geom_violin(aes(fill=Continent),alpha=0.7)+ theme_bw() +\n  theme(axis.title = element_text(family = \"Helvetica\", size = (8)))\n\n# Compute descriptive statistics by groups\nstable <- desc_statby(happy, measure.var = \"Happiness.Score\",\n                      grps = \"Continent\")\n\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Continent` is not found.\n\n\nCode\nstable <- stable[, c(\"Continent\",\"mean\",\"median\")]\n\n\nError in eval(expr, envir, enclos): object 'stable' not found\n\n\nCode\nnames(stable) <- c(\"Continent\", \"Mean of happiness score\",\"Median of happiness score\")\n\n\nError in names(stable) <- c(\"Continent\", \"Mean of happiness score\", \"Median of happiness score\"): object 'stable' not found\n\n\nCode\n# Summary table plot\nstable.p <- ggtexttable(stable,rows = NULL, \n                         theme = ttheme(\"classic\"))\n\n\nError in as.matrix(d): object 'stable' not found\n\n\nCode\nggarrange(gg1, gg2, ncol = 1, nrow = 2)\n\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'Continent' not found\n\n\n\n\nCode\nggarrange(gg3, stable.p, ncol = 1, nrow = 2)\n\n\nError in ggarrange(gg3, stable.p, ncol = 1, nrow = 2): object 'stable.p' not found\n\n\nAs we have seen before, Australia has the highest median happiness score. Europe, South America, and North America are in the second place regarding median happiness score. Asia has the lowest median after Africa. We can see the range of happiness score for different continents, and also the concentration of happiness score."
  },
  {
    "objectID": "posts/final part-2.html#find-relationship-using-scatter-plot-of-happiness-score-with-each-variable-include-regression-line",
    "href": "posts/final part-2.html#find-relationship-using-scatter-plot-of-happiness-score-with-each-variable-include-regression-line",
    "title": "Final Project Part-2",
    "section": "Find Relationship using Scatter Plot of Happiness Score with each variable (include regression line)",
    "text": "Find Relationship using Scatter Plot of Happiness Score with each variable (include regression line)\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Life.Expectancy, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Life Expectancy)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nThe correlation between life expectancy and happiness score in Europe, North America, and Asia is more significant than the other continents. Worth mentioning that we will not take Australia into account because there are just two countries in Australia and creating scatter plot with the regression line for this continent will not give us any insight.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Economy, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Economy)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nWe can see pretty the same result here for the correlation between happiness score and economy. Africa has the lowest relationship in this regard.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Freedom, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Freedom)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nFreedom in Europe and North America is more correlated to happiness score than any other continents.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Trust, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Trust)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nApproximately there is no correlation between trust and happiness score in Africa.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Generosity, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Generosity)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nThe regression line has a positive slope only for Europe and South America. For Asia the line is horizontal, and for Africa and North America the slope is negative.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Family, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Family)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nIn South America with increase in the family score, the happiness score remains constant.\n\n\nCode\nggplot(subset(happy, happy$Continent != \"Australia\"), aes(x = Dystopia.Residual, y = Happiness.Score)) + \n  geom_point(aes(color=Continent), size = 3, alpha = 0.8) +  \n  geom_smooth(aes(color = Continent, fill = Continent), \n              method = \"lm\", fullrange = TRUE) +\n  facet_wrap(~Continent) +\n  theme_bw() + labs(title = \"Scatter plot with regression line (Happiness Score & Dystopia Residual)\")\n\n\nError in happy$Continent: $ operator is invalid for atomic vectors\n\n\nAll continents act pretty the same regarding dystopia residual."
  },
  {
    "objectID": "posts/final part-2.html#prediction-using-regression-models",
    "href": "posts/final part-2.html#prediction-using-regression-models",
    "title": "Final Project Part-2",
    "section": "Prediction using Regression Models",
    "text": "Prediction using Regression Models\nIn this section, we will implement several machine learning algorithms to predict happiness score. First, we should split our dataset into training and test set. Our dependent variable is happiness score, and the independent variables are family, economy, life expectancy, trust, freedom, generosity, and dystopia residual.\n\n\nCode\n# Splitting the dataset into the Training set and Test set\nset.seed(123)\ndataset <- happy[4:11]\n\n\nError in `[.data.frame`(happy, 4:11): undefined columns selected\n\n\nCode\nsplit = sample.split(dataset$Happiness.Score, SplitRatio = 0.8)\n\n\nError in sample.split(dataset$Happiness.Score, SplitRatio = 0.8): object 'dataset' not found\n\n\nCode\ntraining_set = subset(dataset, split == TRUE)\n\n\nError in subset(dataset, split == TRUE): object 'dataset' not found\n\n\nCode\ntest_set = subset(dataset, split == FALSE)\n\n\nError in subset(dataset, split == FALSE): object 'dataset' not found\n\n\n\n\nCode\n# Fitting Multiple Linear Regression to the Training set\nregressor_lm = lm(formula = Happiness.Score ~ .,\n               data = training_set)\n\n\nError in is.data.frame(data): object 'training_set' not found\n\n\nCode\nsummary(regressor_lm)\n\n\nError in summary(regressor_lm): object 'regressor_lm' not found\n\n\nThe summary shows that all independent variables have a significant impact, and adjusted R squared is 1! As we discussed, it is clear that there is a linear correlation between dependent and independent variables. Again, I should mention that the sum of the independent variables is equal to the dependent variable which is the happiness score. This is the justification for having an adjusted R squared equals to 1. As a result, I guess Multiple Linear Regression will predict happiness scores with 100 % accuracy!\n\n\nCode\n####### Predicting the Test set results\ny_pred_lm = predict(regressor_lm, newdata = test_set)\n\n\nError in predict(regressor_lm, newdata = test_set): object 'regressor_lm' not found\n\n\nCode\nPred_Actual_lm <- as.data.frame(cbind(Prediction = y_pred_lm, Actual = test_set$Happiness.Score))\n\n\nError in cbind(Prediction = y_pred_lm, Actual = test_set$Happiness.Score): object 'y_pred_lm' not found\n\n\nCode\ngg.lm <- ggplot(Pred_Actual_lm, aes(Actual, Prediction )) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"Multiple Linear Regression\", x = \"Actual happiness score\",\n       y = \"Predicted happiness score\") +\n  theme(plot.title = element_text(family = \"Helvetica\", face = \"bold\", size = (15)), \n        axis.title = element_text(family = \"Helvetica\", size = (10)))\n\n\nError in ggplot(Pred_Actual_lm, aes(Actual, Prediction)): object 'Pred_Actual_lm' not found\n\n\nCode\ngg.lm\n\n\nError in eval(expr, envir, enclos): object 'gg.lm' not found\n\n\nAs we expected, actual versus predicted plot shows the accuracy of our model.\n\n\nCode\n# Fitting Multiple Linear Regression to the Training set\nregressor_lm_2 = lm(formula = happy$Happiness.Score ~ happy$Trust+happy$Life.Expectancy+ happy$Generosity + happy$Economy+ happy$Family + happy$Freedom+happy$Dystopia.Residual,\n               data = training_set)\n\n\nError in is.data.frame(data): object 'training_set' not found\n\n\nCode\nsummary(regressor_lm_2)\n\n\nError in summary(regressor_lm_2): object 'regressor_lm_2' not found\n\n\n\n\nCode\n####### Predicting the Test set results\ny_pred_lm = predict(regressor_lm_2, newdata = test_set)\n\n\nError in predict(regressor_lm_2, newdata = test_set): object 'regressor_lm_2' not found\n\n\nCode\nPred_Actual_lm <- as.data.frame(cbind(Prediction = y_pred_lm, Actual = test_set$Happiness.Score))\n\n\nError in cbind(Prediction = y_pred_lm, Actual = test_set$Happiness.Score): object 'y_pred_lm' not found\n\n\nCode\ngg.lm <- ggplot(Pred_Actual_lm, aes(Actual, Prediction )) +\n  geom_point() + theme_bw() + geom_abline() +\n  labs(title = \"Multiple Linear Regression\", x = \"Actual happiness score\",\n       y = \"Predicted happiness score\") +\n  theme(plot.title = element_text(family = \"Helvetica\", face = \"bold\", size = (15)), \n        axis.title = element_text(family = \"Helvetica\", size = (10)))\n\n\nError in ggplot(Pred_Actual_lm, aes(Actual, Prediction)): object 'Pred_Actual_lm' not found\n\n\nCode\ngg.lm\n\n\nError in eval(expr, envir, enclos): object 'gg.lm' not found"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html",
    "href": "posts/Final Project - Working Draft.html",
    "title": "",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ readr   2.1.2     ✓ stringr 1.4.0\n✓ purrr   0.3.4     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#research-question",
    "href": "posts/Final Project - Working Draft.html#research-question",
    "title": "",
    "section": "Research Question",
    "text": "Research Question\nI am interested in examining the relationship between exports from China to the US, and the increase in Co2 emissions over the years. China is marked to be the highest Co2 emitting country followed by the US. However, it is clear that China pulls some of the carbon weight for the United States by manufacturing a wealth of goods. I would like to explore the connection between exports to the US and increasing carbon emissions. This could be helpful in guiding policy in international trade and climate change moving forward. I would also like to specify the industries and goods more closely related to these emissions, if possible."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#hypothesis",
    "href": "posts/Final Project - Working Draft.html#hypothesis",
    "title": "",
    "section": "Hypothesis",
    "text": "Hypothesis\nI am starting with the hypothesis that there exports from China to the US is positively related with its Co2 emissions. However, taken together, its also significant to ask whether or not export process alone accounts for the greater sum of Co2 emission increases. The manufacturing process likely plays a role here, and may be considered for further analyses. Exports may be used in an inferential manner, suggesting that increased exports indicate higher rates of manufacturing that could thereby increase Co2 emissions. As such, exports would be an indirect measure of domestic activity, the correlation of which could lead to further insights. A search of datasets with more direct measures of carbon emissions in China relating to trade and supply of goods to the US may also be considered."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#export-data",
    "href": "posts/Final Project - Working Draft.html#export-data",
    "title": "",
    "section": "Export Data",
    "text": "Export Data\nTwo initial datasets have been pulled for the purpose of this study. The first set includes data on exports from China to the US. The set has 2 columns of interest representing the date, value (in US dollars”). The overall dataset has 3 columns, each containing 30 rows. This dataset was chosen because it covers a relatively adequate sample range from 1992-2020. This data was pulled from the United Nations COMTRADE database on comerce and trade.\n\ncomtrade_historical_CHNUSA00002 <- read.csv(\"~/Downloads/comtrade_historical_CHNUSA00002.csv\")\nexports<-comtrade_historical_CHNUSA00002\nglimpse(exports)\n\nRows: 30\nColumns: 3\n$ symbol <chr> \"CHNUSA00002\", \"CHNUSA00002\", \"CHNUSA00002\", \"CHNUSA00002\", \"CH…\n$ date   <chr> \"1992-12-31T00:00:00\", \"1993-12-31T00:00:00\", \"1994-12-31T00:00…\n$ value  <dbl> 8599371576, 16972667973, 21474839665, 24728628807, 26705626240,…\n\nsummary(exports)\n\n    symbol              date               value          \n Length:30          Length:30          Min.   :8.599e+09  \n Class :character   Class :character   1st Qu.:4.454e+10  \n Mode  :character   Mode  :character   Median :2.125e+11  \n                                       Mean   :2.187e+11  \n                                       3rd Qu.:3.815e+11  \n                                       Max.   :5.771e+11"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#carbon-data",
    "href": "posts/Final Project - Working Draft.html#carbon-data",
    "title": "",
    "section": "Carbon Data",
    "text": "Carbon Data\nCarbon data was pulled to show the difference in carbon emissions from china between the years 2010-2020. It is unclear, yet, if this dataset will be used for final drafts, as it exludes a number of years reviewed in the export data. As a result, the gaps in years may lead to weaker analyses. For now, this data will be considered.\n\nstatistic_id270499_global_co2_emissions_by_select_country_2010_2020<-read_excel(\"Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx\")\n\nError: `path` does not exist: 'Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx'\n\n\n\ncarbon<-statistic_id270499_global_co2_emissions_by_select_country_2010_2020\n\nError in eval(expr, envir, enclos): object 'statistic_id270499_global_co2_emissions_by_select_country_2010_2020' not found\n\nsummarize(carbon)\n\nError in summarize(carbon): object 'carbon' not found\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html",
    "href": "posts/Final Project 1_Kaushika Potluri.html",
    "title": "Final Project Submission 1",
    "section": "",
    "text": "the research question that I have been interested in is the impact of education about sex and fertility for women and how that changes the fetility rate. Women’s education raises the value of time spent working in the market and, as a result, the opportunity cost of spending time to take care of their child seems less. Across time and places, there is a clear negative link between women’s education and fertility, although its meaning is ambiguous. Women’s level of education may impact fertility through its effects on children’s health, the number of children desired, and women’s ability to give birth and understanding of various birth control options. Each of these are influenced by local, institutional, and national circumstances. Their relative importance may fluctuate as a society develops economically. Since having children affects how much mothers must pay for childcare, women’s education may also be correlated with fertility. The data was acquired from various years of the National Opinion Resource Center’s General Social Survey. Compared to other women, mothers who stay at home with their kids are less likely to invest more money in their education. The correlation between women’s education and unobservable qualities that are jointly linked with fertility may be even more significant.\n###Hypothesis It can be thought of as the total number of unplanned and intended children. The number of kids a family can have, the number of kids the family desires, and the capability to regulate birth through the availability of modern contraceptives and the knowledge of how to use them are all impacted by advancements in women’s education. The number of children a woman has is halfway between the amount she wants and her level of natural fertility. Age and fertility control are the determining variables.If there was a variation by region in birth control availability, such information might be valuable. However, our data set does not contain geographical information (parameters). My assumption would be that if the level of education increases, the number of children would decrease.\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "title": "Final Project Submission 1",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "title": "Final Project Submission 1",
    "section": "Reading in Data:",
    "text": "Reading in Data:\nThe data was acquired from Professor Sander’s article that he used.\n\n\nCode\nWomendata <-  read.csv(\"_data/data.csv\")"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "title": "Final Project Submission 1",
    "section": "Summary of the data",
    "text": "Summary of the data\n\n\nCode\nsummary(Womendata)\n\n\n       X           mnthborn         yearborn          age       \n Min.   :   1   Min.   : 1.000   Min.   :38.00   Min.   :15.00  \n 1st Qu.:1091   1st Qu.: 3.000   1st Qu.:55.00   1st Qu.:20.00  \n Median :2181   Median : 6.000   Median :62.00   Median :26.00  \n Mean   :2181   Mean   : 6.331   Mean   :60.43   Mean   :27.41  \n 3rd Qu.:3271   3rd Qu.: 9.000   3rd Qu.:68.00   3rd Qu.:33.00  \n Max.   :4361   Max.   :12.000   Max.   :73.00   Max.   :49.00  \n                                                                \n    electric          radio              tv             bicycle      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1402   Mean   :0.7018   Mean   :0.09291   Mean   :0.2758  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n NA's   :3        NA's   :2        NA's   :2         NA's   :3       \n      educ             ceb            agefbrth        children     \n Min.   : 0.000   Min.   : 0.000   Min.   :10.00   Min.   : 0.000  \n 1st Qu.: 3.000   1st Qu.: 1.000   1st Qu.:17.00   1st Qu.: 0.000  \n Median : 7.000   Median : 2.000   Median :19.00   Median : 2.000  \n Mean   : 5.856   Mean   : 2.442   Mean   :19.01   Mean   : 2.268  \n 3rd Qu.: 8.000   3rd Qu.: 4.000   3rd Qu.:20.00   3rd Qu.: 4.000  \n Max.   :20.000   Max.   :13.000   Max.   :38.00   Max.   :13.000  \n                                   NA's   :1088                    \n    knowmeth         usemeth          monthfm          yearfm     \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.00   Min.   :50.00  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.: 3.00   1st Qu.:72.00  \n Median :1.0000   Median :1.0000   Median : 6.00   Median :78.00  \n Mean   :0.9633   Mean   :0.5776   Mean   : 6.27   Mean   :76.91  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 9.00   3rd Qu.:83.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :12.00   Max.   :88.00  \n NA's   :7        NA's   :71       NA's   :2282    NA's   :2282   \n     agefm          idlnchld          heduc            agesq       \n Min.   :10.00   Min.   : 0.000   Min.   : 0.000   Min.   : 225.0  \n 1st Qu.:17.00   1st Qu.: 3.000   1st Qu.: 0.000   1st Qu.: 400.0  \n Median :20.00   Median : 4.000   Median : 6.000   Median : 676.0  \n Mean   :20.69   Mean   : 4.616   Mean   : 5.145   Mean   : 826.5  \n 3rd Qu.:23.00   3rd Qu.: 6.000   3rd Qu.: 8.000   3rd Qu.:1089.0  \n Max.   :46.00   Max.   :20.000   Max.   :20.000   Max.   :2401.0  \n NA's   :2282    NA's   :120      NA's   :2405                     \n     urban           urb_educ          spirit          protest      \n Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median : 0.000   Median :0.0000   Median :0.0000  \n Mean   :0.5166   Mean   : 3.469   Mean   :0.4222   Mean   :0.2277  \n 3rd Qu.:1.0000   3rd Qu.: 7.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :20.000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n    catholic         frsthalf          educ0           evermarr     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.1025   Mean   :0.5405   Mean   :0.2078   Mean   :0.4767  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n\n\n\n\nCode\nglimpse(Womendata)\n\n\nRows: 4,361\nColumns: 28\n$ X        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ mnthborn <int> 5, 1, 7, 11, 5, 8, 7, 9, 12, 9, 6, 10, 12, 2, 1, 6, 1, 8, 4, …\n$ yearborn <int> 64, 56, 58, 45, 45, 52, 51, 70, 53, 39, 46, 59, 42, 40, 53, 6…\n$ age      <int> 24, 32, 30, 42, 43, 36, 37, 18, 34, 49, 42, 29, 45, 48, 35, 2…\n$ electric <int> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ radio    <int> 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tv       <int> 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ bicycle  <int> 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…\n$ educ     <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ ceb      <int> 0, 3, 1, 3, 2, 1, 4, 0, 1, 0, 3, 3, 4, 10, 3, 0, 4, 2, 0, 1, …\n$ agefbrth <int> NA, 25, 27, 17, 24, 26, 20, NA, 19, NA, 25, 23, 18, 19, 23, N…\n$ children <int> 0, 3, 1, 2, 2, 1, 4, 0, 1, 0, 3, 3, 2, 8, 3, 0, 4, 2, 0, 1, 0…\n$ knowmeth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ usemeth  <int> 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1…\n$ monthfm  <int> NA, 11, 6, 1, 3, 11, 5, NA, 7, 11, 6, 1, 1, 10, 1, NA, NA, NA…\n$ yearfm   <int> NA, 80, 83, 61, 66, 76, 78, NA, 72, 61, 70, 84, 66, 66, 74, N…\n$ agefm    <int> NA, 24, 24, 15, 20, 24, 26, NA, 18, 22, 24, 24, 23, 26, 21, N…\n$ idlnchld <int> 2, 3, 5, 3, 2, 4, 4, 4, 4, 4, 3, 6, 6, 4, 3, 4, 5, 1, 2, 3, 2…\n$ heduc    <int> NA, 12, 7, 11, 14, 9, 17, NA, 3, 1, 16, 7, NA, 3, 16, NA, NA,…\n$ agesq    <int> 576, 1024, 900, 1764, 1849, 1296, 1369, 324, 1156, 2401, 1764…\n$ urban    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ urb_educ <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ spirit   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0…\n$ protest  <int> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1…\n$ catholic <int> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ frsthalf <int> 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0…\n$ educ0    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ evermarr <int> 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1…\n\n\nWe can see that we have 28 variables and 4361 observations in this dataset. The dependent variable of interest - number of living children Then I will perform data manipulation to tidy the data. The variables of interest are age, yearborn, month born, urban education and many more variables that seem intriguing. Variables like radio, bicycle, electric can be ignored in this.\n###References [1] The effect of women’s schooling on fertility by W Sander · 1992 [2] The Impact of Women’s Schooling on Fertility and Contraceptive Use by M Ainsworth · 1996"
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html",
    "href": "posts/Final Project 2_Kaushika Potluri.html",
    "title": "Final Project Submission 2",
    "section": "",
    "text": "#Introduction The research question that I have been interested in is the impact of education about sex and fertility for women and how that changes the fertility rate. Women’s education raises the value of time spent working in the market and, as a result, the opportunity cost of spending time to take care of their child seems less. Across time and places, there is a clear negative link between women’s education and fertility, although its meaning is ambiguous. Women’s level of education may impact fertility through its effects on children’s health, the number of children desired, and women’s ability to give birth and understanding of various birth control options. Each of these are influenced by local, institutional, and national circumstances. Their relative importance may fluctuate as a society develops economically. We analyse the education–fertility relationship by using data on women from Botswana. A realistic quantification of such a relationship can be problematic for various reasons. First, factors such as motivation and ability are associated with fertility and education but cannot be observed and as a consequence cannot be included in the model."
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Final Project 2_Kaushika Potluri.html#loading-in-packages",
    "title": "Final Project Submission 2",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(DataExplorer)\n\n\nError in library(DataExplorer): there is no package called 'DataExplorer'\n\n\nCode\nlibrary(summarytools)\n\n\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\nCode\nlibrary(lmtest)\n\n\nWarning: package 'lmtest' was built under R version 4.2.2\n\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCode\nlibrary(reshape)\n\n\nWarning: package 'reshape' was built under R version 4.2.2\n\n\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths"
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#reading-in-data",
    "href": "posts/Final Project 2_Kaushika Potluri.html#reading-in-data",
    "title": "Final Project Submission 2",
    "section": "Reading in Data:",
    "text": "Reading in Data:\nThe data was acquired from Professor Sander’s article that he used.\n\n\nCode\nWomendata <-  read.csv(\"_data/data.csv\")\n\n\n\n\n\nVariable\n\n\n\n\nchildren\n\n\neducation\n\n\nelectricity\n\n\ntv\n\n\nurban\n\n\nevermarr\n\n\nradio\n\n\nbicycle\n\n\nknowmeth\n\n\nusemeth\n\n\nage\n\n\nfirsthalf"
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#descriptive-statistics",
    "href": "posts/Final Project 2_Kaushika Potluri.html#descriptive-statistics",
    "title": "Final Project Submission 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nsummary(Womendata)\n\n\n       X           mnthborn         yearborn          age       \n Min.   :   1   Min.   : 1.000   Min.   :38.00   Min.   :15.00  \n 1st Qu.:1091   1st Qu.: 3.000   1st Qu.:55.00   1st Qu.:20.00  \n Median :2181   Median : 6.000   Median :62.00   Median :26.00  \n Mean   :2181   Mean   : 6.331   Mean   :60.43   Mean   :27.41  \n 3rd Qu.:3271   3rd Qu.: 9.000   3rd Qu.:68.00   3rd Qu.:33.00  \n Max.   :4361   Max.   :12.000   Max.   :73.00   Max.   :49.00  \n                                                                \n    electric          radio              tv             bicycle      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1402   Mean   :0.7018   Mean   :0.09291   Mean   :0.2758  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n NA's   :3        NA's   :2        NA's   :2         NA's   :3       \n      educ             ceb            agefbrth        children     \n Min.   : 0.000   Min.   : 0.000   Min.   :10.00   Min.   : 0.000  \n 1st Qu.: 3.000   1st Qu.: 1.000   1st Qu.:17.00   1st Qu.: 0.000  \n Median : 7.000   Median : 2.000   Median :19.00   Median : 2.000  \n Mean   : 5.856   Mean   : 2.442   Mean   :19.01   Mean   : 2.268  \n 3rd Qu.: 8.000   3rd Qu.: 4.000   3rd Qu.:20.00   3rd Qu.: 4.000  \n Max.   :20.000   Max.   :13.000   Max.   :38.00   Max.   :13.000  \n                                   NA's   :1088                    \n    knowmeth         usemeth          monthfm          yearfm     \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.00   Min.   :50.00  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.: 3.00   1st Qu.:72.00  \n Median :1.0000   Median :1.0000   Median : 6.00   Median :78.00  \n Mean   :0.9633   Mean   :0.5776   Mean   : 6.27   Mean   :76.91  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 9.00   3rd Qu.:83.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :12.00   Max.   :88.00  \n NA's   :7        NA's   :71       NA's   :2282    NA's   :2282   \n     agefm          idlnchld          heduc            agesq       \n Min.   :10.00   Min.   : 0.000   Min.   : 0.000   Min.   : 225.0  \n 1st Qu.:17.00   1st Qu.: 3.000   1st Qu.: 0.000   1st Qu.: 400.0  \n Median :20.00   Median : 4.000   Median : 6.000   Median : 676.0  \n Mean   :20.69   Mean   : 4.616   Mean   : 5.145   Mean   : 826.5  \n 3rd Qu.:23.00   3rd Qu.: 6.000   3rd Qu.: 8.000   3rd Qu.:1089.0  \n Max.   :46.00   Max.   :20.000   Max.   :20.000   Max.   :2401.0  \n NA's   :2282    NA's   :120      NA's   :2405                     \n     urban           urb_educ          spirit          protest      \n Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median : 0.000   Median :0.0000   Median :0.0000  \n Mean   :0.5166   Mean   : 3.469   Mean   :0.4222   Mean   :0.2277  \n 3rd Qu.:1.0000   3rd Qu.: 7.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :20.000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n    catholic         frsthalf          educ0           evermarr     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.1025   Mean   :0.5405   Mean   :0.2078   Mean   :0.4767  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n\n\n\n\nCode\nstr(Womendata)\n\n\n'data.frame':   4361 obs. of  28 variables:\n $ X       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ mnthborn: int  5 1 7 11 5 8 7 9 12 9 ...\n $ yearborn: int  64 56 58 45 45 52 51 70 53 39 ...\n $ age     : int  24 32 30 42 43 36 37 18 34 49 ...\n $ electric: int  1 1 1 1 1 1 1 1 0 1 ...\n $ radio   : int  1 1 0 0 1 0 1 1 1 1 ...\n $ tv      : int  1 1 0 1 1 0 1 1 0 0 ...\n $ bicycle : int  1 1 0 0 1 0 1 1 0 0 ...\n $ educ    : int  12 13 5 4 11 7 16 10 5 4 ...\n $ ceb     : int  0 3 1 3 2 1 4 0 1 0 ...\n $ agefbrth: int  NA 25 27 17 24 26 20 NA 19 NA ...\n $ children: int  0 3 1 2 2 1 4 0 1 0 ...\n $ knowmeth: int  1 1 1 1 1 1 1 1 1 1 ...\n $ usemeth : int  0 1 0 0 1 1 1 1 1 0 ...\n $ monthfm : int  NA 11 6 1 3 11 5 NA 7 11 ...\n $ yearfm  : int  NA 80 83 61 66 76 78 NA 72 61 ...\n $ agefm   : int  NA 24 24 15 20 24 26 NA 18 22 ...\n $ idlnchld: int  2 3 5 3 2 4 4 4 4 4 ...\n $ heduc   : int  NA 12 7 11 14 9 17 NA 3 1 ...\n $ agesq   : int  576 1024 900 1764 1849 1296 1369 324 1156 2401 ...\n $ urban   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ urb_educ: int  12 13 5 4 11 7 16 10 5 4 ...\n $ spirit  : int  0 0 1 0 0 0 0 0 0 1 ...\n $ protest : int  0 0 0 0 1 0 0 0 1 0 ...\n $ catholic: int  0 0 0 0 0 0 1 1 0 0 ...\n $ frsthalf: int  1 1 0 0 1 0 0 0 0 0 ...\n $ educ0   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ evermarr: int  0 1 1 1 1 1 1 0 1 1 ...\n\n\n\n\nCode\nprint(dfSummary(Womendata, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.30, style = \"grid\", valid.col = FALSE), \n      method = 'render', table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nWomendata\nDimensions: 4361 x 28\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      X\n[integer]\n      Mean (sd) : 2181 (1259.1)min ≤ med ≤ max:1 ≤ 2181 ≤ 4361IQR (CV) : 2180 (0.6)\n      4361 distinct values\n(Integer sequence)\n      \n      0\n(0.0%)\n    \n    \n      mnthborn\n[integer]\n      Mean (sd) : 6.3 (3.3)min ≤ med ≤ max:1 ≤ 6 ≤ 12IQR (CV) : 6 (0.5)\n      12 distinct values\n      \n      0\n(0.0%)\n    \n    \n      yearborn\n[integer]\n      Mean (sd) : 60.4 (8.7)min ≤ med ≤ max:38 ≤ 62 ≤ 73IQR (CV) : 13 (0.1)\n      36 distinct values\n      \n      0\n(0.0%)\n    \n    \n      age\n[integer]\n      Mean (sd) : 27.4 (8.7)min ≤ med ≤ max:15 ≤ 26 ≤ 49IQR (CV) : 13 (0.3)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      electric\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:3747(86.0%)1:611(14.0%)\n      \n      3\n(0.1%)\n    \n    \n      radio\n[integer]\n      Min  : 0Mean : 0.7Max  : 1\n      0:1300(29.8%)1:3059(70.2%)\n      \n      2\n(0.0%)\n    \n    \n      tv\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:3954(90.7%)1:405(9.3%)\n      \n      2\n(0.0%)\n    \n    \n      bicycle\n[integer]\n      Min  : 0Mean : 0.3Max  : 1\n      0:3156(72.4%)1:1202(27.6%)\n      \n      3\n(0.1%)\n    \n    \n      educ\n[integer]\n      Mean (sd) : 5.9 (3.9)min ≤ med ≤ max:0 ≤ 7 ≤ 20IQR (CV) : 5 (0.7)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ceb\n[integer]\n      Mean (sd) : 2.4 (2.4)min ≤ med ≤ max:0 ≤ 2 ≤ 13IQR (CV) : 3 (1)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      agefbrth\n[integer]\n      Mean (sd) : 19 (3.1)min ≤ med ≤ max:10 ≤ 19 ≤ 38IQR (CV) : 3 (0.2)\n      28 distinct values\n      \n      1088\n(24.9%)\n    \n    \n      children\n[integer]\n      Mean (sd) : 2.3 (2.2)min ≤ med ≤ max:0 ≤ 2 ≤ 13IQR (CV) : 4 (1)\n      14 distinct values\n      \n      0\n(0.0%)\n    \n    \n      knowmeth\n[integer]\n      Min  : 0Mean : 1Max  : 1\n      0:160(3.7%)1:4194(96.3%)\n      \n      7\n(0.2%)\n    \n    \n      usemeth\n[integer]\n      Min  : 0Mean : 0.6Max  : 1\n      0:1812(42.2%)1:2478(57.8%)\n      \n      71\n(1.6%)\n    \n    \n      monthfm\n[integer]\n      Mean (sd) : 6.3 (3.6)min ≤ med ≤ max:1 ≤ 6 ≤ 12IQR (CV) : 6 (0.6)\n      12 distinct values\n      \n      2282\n(52.3%)\n    \n    \n      yearfm\n[integer]\n      Mean (sd) : 76.9 (7.8)min ≤ med ≤ max:50 ≤ 78 ≤ 88IQR (CV) : 11 (0.1)\n      38 distinct values\n      \n      2282\n(52.3%)\n    \n    \n      agefm\n[integer]\n      Mean (sd) : 20.7 (5)min ≤ med ≤ max:10 ≤ 20 ≤ 46IQR (CV) : 6 (0.2)\n      35 distinct values\n      \n      2282\n(52.3%)\n    \n    \n      idlnchld\n[integer]\n      Mean (sd) : 4.6 (2.2)min ≤ med ≤ max:0 ≤ 4 ≤ 20IQR (CV) : 3 (0.5)\n      19 distinct values\n      \n      120\n(2.8%)\n    \n    \n      heduc\n[integer]\n      Mean (sd) : 5.1 (4.8)min ≤ med ≤ max:0 ≤ 6 ≤ 20IQR (CV) : 8 (0.9)\n      21 distinct values\n      \n      2405\n(55.1%)\n    \n    \n      agesq\n[integer]\n      Mean (sd) : 826.5 (526.9)min ≤ med ≤ max:225 ≤ 676 ≤ 2401IQR (CV) : 689 (0.6)\n      35 distinct values\n      \n      0\n(0.0%)\n    \n    \n      urban\n[integer]\n      Min  : 0Mean : 0.5Max  : 1\n      0:2108(48.3%)1:2253(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      urb_educ\n[integer]\n      Mean (sd) : 3.5 (4.3)min ≤ med ≤ max:0 ≤ 0 ≤ 20IQR (CV) : 7 (1.2)\n      21 distinct values\n      \n      0\n(0.0%)\n    \n    \n      spirit\n[integer]\n      Min  : 0Mean : 0.4Max  : 1\n      0:2520(57.8%)1:1841(42.2%)\n      \n      0\n(0.0%)\n    \n    \n      protest\n[integer]\n      Min  : 0Mean : 0.2Max  : 1\n      0:3368(77.2%)1:993(22.8%)\n      \n      0\n(0.0%)\n    \n    \n      catholic\n[integer]\n      Min  : 0Mean : 0.1Max  : 1\n      0:3914(89.8%)1:447(10.2%)\n      \n      0\n(0.0%)\n    \n    \n      frsthalf\n[integer]\n      Min  : 0Mean : 0.5Max  : 1\n      0:2004(46.0%)1:2357(54.0%)\n      \n      0\n(0.0%)\n    \n    \n      educ0\n[integer]\n      Min  : 0Mean : 0.2Max  : 1\n      0:3455(79.2%)1:906(20.8%)\n      \n      0\n(0.0%)\n    \n    \n      evermarr\n[integer]\n      Min  : 0Mean : 0.5Max  : 1\n      0:2282(52.3%)1:2079(47.7%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-11\n\n\n\n\n\nCode\nglimpse(Womendata)\n\n\nRows: 4,361\nColumns: 28\n$ X        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ mnthborn <int> 5, 1, 7, 11, 5, 8, 7, 9, 12, 9, 6, 10, 12, 2, 1, 6, 1, 8, 4, …\n$ yearborn <int> 64, 56, 58, 45, 45, 52, 51, 70, 53, 39, 46, 59, 42, 40, 53, 6…\n$ age      <int> 24, 32, 30, 42, 43, 36, 37, 18, 34, 49, 42, 29, 45, 48, 35, 2…\n$ electric <int> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ radio    <int> 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tv       <int> 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ bicycle  <int> 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…\n$ educ     <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ ceb      <int> 0, 3, 1, 3, 2, 1, 4, 0, 1, 0, 3, 3, 4, 10, 3, 0, 4, 2, 0, 1, …\n$ agefbrth <int> NA, 25, 27, 17, 24, 26, 20, NA, 19, NA, 25, 23, 18, 19, 23, N…\n$ children <int> 0, 3, 1, 2, 2, 1, 4, 0, 1, 0, 3, 3, 2, 8, 3, 0, 4, 2, 0, 1, 0…\n$ knowmeth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ usemeth  <int> 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1…\n$ monthfm  <int> NA, 11, 6, 1, 3, 11, 5, NA, 7, 11, 6, 1, 1, 10, 1, NA, NA, NA…\n$ yearfm   <int> NA, 80, 83, 61, 66, 76, 78, NA, 72, 61, 70, 84, 66, 66, 74, N…\n$ agefm    <int> NA, 24, 24, 15, 20, 24, 26, NA, 18, 22, 24, 24, 23, 26, 21, N…\n$ idlnchld <int> 2, 3, 5, 3, 2, 4, 4, 4, 4, 4, 3, 6, 6, 4, 3, 4, 5, 1, 2, 3, 2…\n$ heduc    <int> NA, 12, 7, 11, 14, 9, 17, NA, 3, 1, 16, 7, NA, 3, 16, NA, NA,…\n$ agesq    <int> 576, 1024, 900, 1764, 1849, 1296, 1369, 324, 1156, 2401, 1764…\n$ urban    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ urb_educ <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ spirit   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0…\n$ protest  <int> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1…\n$ catholic <int> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ frsthalf <int> 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0…\n$ educ0    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ evermarr <int> 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1…\n\n\nWe can see that we have 28 variables and 4361 observations in this dataset. The dependent variable of interest - number of living children Then I will perform data manipulation to tidy the data. The variables of interest are age, yearborn, month born, urban education and many more variables that seem intriguing. Variables like radio, bicycle, electric can be ignored in this.\n\n\nCode\nhead(Womendata)\n\n\n  X mnthborn yearborn age electric radio tv bicycle educ ceb agefbrth children\n1 1        5       64  24        1     1  1       1   12   0       NA        0\n2 2        1       56  32        1     1  1       1   13   3       25        3\n3 3        7       58  30        1     0  0       0    5   1       27        1\n4 4       11       45  42        1     0  1       0    4   3       17        2\n5 5        5       45  43        1     1  1       1   11   2       24        2\n6 6        8       52  36        1     0  0       0    7   1       26        1\n  knowmeth usemeth monthfm yearfm agefm idlnchld heduc agesq urban urb_educ\n1        1       0      NA     NA    NA        2    NA   576     1       12\n2        1       1      11     80    24        3    12  1024     1       13\n3        1       0       6     83    24        5     7   900     1        5\n4        1       0       1     61    15        3    11  1764     1        4\n5        1       1       3     66    20        2    14  1849     1       11\n6        1       1      11     76    24        4     9  1296     1        7\n  spirit protest catholic frsthalf educ0 evermarr\n1      0       0        0        1     0        0\n2      0       0        0        1     0        1\n3      1       0        0        0     0        1\n4      0       0        0        0     0        1\n5      0       1        0        1     0        1\n6      0       0        0        0     0        1\n\n\n##Tidying the data\n\n\nCode\nWomendata <- data.frame(Womendata)\n\nWomendata$mnthborn <- as.factor(Womendata$mnthborn)\nWomendata$age <- as.factor(Womendata$age)\nWomendata$electric <- as.factor(Womendata$electric)\nWomendata$radio <- as.factor(Womendata$radio)\nWomendata$tv <- as.factor(Womendata$tv)\nWomendata$bicycle <- as.factor(Womendata$bicycle)\nWomendata$educ <- as.factor(Womendata$educ)\nWomendata$children <- as.factor(Womendata$children)\nWomendata$knowmeth <- as.factor(Womendata$knowmeth)\n\n\nThe dependent variable of interest – number of living children (children) or number of children ever born (ceb) is a count variable.\n\n\nCode\ntable(is.na(Womendata))\n\n\n\n FALSE   TRUE \n111561  10547 \n\n\nHere we can see that we have some missing values in our dataset. Plotting the missing values we can check if they are important or not using ‘plot_missing’ by loading library DataExplorer.\n\n\nCode\nplot_missing(data = Womendata, geom_label_args = list(size = 1.4), theme_config=list(axis.text=element_text(size = 6 )))\n\n\nError in plot_missing(data = Womendata, geom_label_args = list(size = 1.4), : could not find function \"plot_missing\"\n\n\nWe can see how these missing values do not cause much of an issue since these missing observations(parameters) convey less important information than the other parameters. Hence we can ignore these values.\n##Removing missing values and parameters that are not required The aim is to estimate the effect of education on women’s fertility flexibly, while controlling for possible linear and non-linear effects of observable and unobservable confounding factors, and to analyse how the effect of education changes when considering different expectiles of the response variable distribution. We also include two variables regarding the knowledge about and the use of birth control methods as well as marital status.All three obviously influence the number of children. Further, we include variables indicating wealth, e.g. about the availability of electricity, a television set or a bicycle.\nExcluding variables that have no significant importance from our data.\n\n\nCode\nWomendata <- subset(Womendata, select = -c(agefm,yearfm,monthfm,heduc))\n\n\n\n\nCode\nWomendatacleaned <-Womendata[complete.cases(Womendata), ]\nplot_missing(data = Womendatacleaned, geom_label_args = list(size = 1.4), theme_config=list(axis.text=element_text(size = 6 )))\n\n\nError in plot_missing(data = Womendatacleaned, geom_label_args = list(size = 1.4), : could not find function \"plot_missing\""
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#detecting-outliers.",
    "href": "posts/Final Project 2_Kaushika Potluri.html#detecting-outliers.",
    "title": "Final Project Submission 2",
    "section": "Detecting outliers.",
    "text": "Detecting outliers.\n\n\nCode\nWomendata %>%\n  ggplot(aes(educ)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nWe can see that the variable educ i.e education has some outliers. Mostly for having education of more than 15 years, but they cannot potentially affect the data set.\n\n\nCode\nWomendata %>%\n  ggplot(aes(age)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, age variable has no outliers.\n\n\nCode\nWomendata %>%\n  ggplot(aes(children)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom the above plot we can see that the variable children does have outliers but nothing to be concerned about.\n\n\nCode\nWomendata %>%\n  ggplot(aes(urb_educ)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom the above plot we can see that the variable urban education does have outliers but nothing to be concerned about.\n\n\nCode\nWomendata %>%\n  ggplot(aes(yearborn)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom the above box plot, yearborn variable has no outliers.\n\n\nCode\nWomendata %>%\n  ggplot(aes(mnthborn)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom the above box plot, mnthborn variable has no outliers.\n#Exploratory Data Analysis (EDA) Here variables indicating wealth, e.g. about the availability of electricity, a television set or a bicycle can say something about a women’s knowledge & use of birth control/ and its impact on how many children they have.\n\n\nCode\nggplot(Womendatacleaned,aes(x=factor(children),fill=factor(tv)))+\n  \ngeom_bar()+theme(axis.text.x = element_text(face=\"bold\", size=15),axis.text.y = element_text(face=\"bold\", size=15))+\n  \nlabs(\n    title = \"Number of Children based on if they own a Tv or not\",\n    x = \"Number of children\",\n    y = \"Count\",size=15) +\n   \nscale_fill_manual(\n    name = \"Access to Telivision or not\",\n    breaks = c(\"0\", \"1\"),\n    labels = c(\"No Telivision\", \"Owns/ has access to a telivision\"),\n    values = c(\"0\" = \"orange\", \"1\"=\"yellow\")\n  )\n\n\n\n\n\nWe can see that most mothers do not own a TV here.\n\n\nCode\nggplot(Womendatacleaned,aes(x=factor(children),fill=factor(radio)))+\n  \ngeom_bar()+theme(axis.text.x = element_text(face=\"bold\", size=15),axis.text.y = element_text(face=\"bold\", size=15))+\n  \nlabs(\n    title = \"Number of Children based on if they own a radio or not\",\n    x = \"Number of children\",\n    y = \"Count\",size=15) +\n   \nscale_fill_manual(\n    name = \"Access to Radio or not\",\n    breaks = c(\"0\", \"1\"),\n    labels = c(\"No Radio\", \"Owns/ has access to a Radio\"),\n    values = c(\"0\" = \"green\", \"1\"=\"pink\")\n  )\n\n\n\n\n\n\n\nCode\nggplot(Womendatacleaned,aes(x=factor(children),fill=factor(electric)))+\n  \ngeom_bar()+theme(axis.text.x = element_text(face=\"bold\", size=15),axis.text.y = element_text(face=\"bold\", size=15))+\n  \nlabs(\n    title = \"Number of Children based on if they have electricity or not\",\n    x = \"Number of children\",\n    y = \"Count\",size=15) +\n   \nscale_fill_manual(\n    name = \"Access to Electricity or not\",\n    breaks = c(\"0\", \"1\"),\n    labels = c(\"No Electricity\", \"Has access to Electricity\"),\n    values = c(\"0\" = \"pink\", \"1\"=\"yellow\")\n  )\n\n\n\n\n\nHere from our bar graph we can see that most mothers that have children do not have electricity.\n\n\nCode\np <- Womendata %>%\n  ggplot() +\n  geom_bar(aes(Womendata$usemeth)) +\n  ggtitle(\"Individuals that ever used birth control\") + labs(title = \"Individuals that have ever used  birth control\", \nx = \"No. of individuals that have ever used birth control\",y = \"Count\")\n  theme_classic()\n\n\nList of 94\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : NULL\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                :List of 6\n  ..$ colour       : chr \"grey20\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : 'rel' num 1\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : chr \"white\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : 'rel' num 2\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.clip                : chr \"inherit\"\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\nCode\np\n\n\nWarning: Use of `Womendata$usemeth` is discouraged.\nℹ Use `usemeth` instead.\n\n\nWarning: Removed 71 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\nMajority of women have used birth control atleast once in their life.\n\n\nCode\nk <- Womendata %>%\n  ggplot() +\n  geom_bar(aes(Womendata$knowmeth)) +\n  ggtitle(\"Individuals that know about birth control\") + labs(title = \"Individual knows about birth control\", \nx = \"No. of individuals that know about birth control\",y = \"Count\")\n  theme_classic()\n\n\nList of 94\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : NULL\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                :List of 6\n  ..$ colour       : chr \"grey20\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : 'rel' num 1\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.minor          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : chr \"white\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : 'rel' num 2\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.clip                : chr \"inherit\"\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\nCode\nk\n\n\nWarning: Use of `Womendata$knowmeth` is discouraged.\nℹ Use `knowmeth` instead.\n\n\n\n\n\nHere, we can see that most individuals know about birth control.\n\n\nCode\nggplot(data = Womendata,\n       aes(\n         x = children,\n         y = prop.table(stat(count)),\n         fill = factor(usemeth), width = 1,\n         label = scales::percent(prop.table(stat(count)))\n       )) +\n  geom_bar(position = position_dodge()) +\n  geom_text(\n    stat = \"count\",\n    position = position_dodge(0.2),\n    vjust = -1,\n    size = 1.5\n  ) + scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Number of children based on birth control\",\n       x = \"Number of Children\",\n       y = \"Count\") +\n  theme_classic() +\n  scale_fill_discrete(\n    name = \"Birth Control\",\n    labels = c(\"Use birth control\", \"Never used birth control\")\n  )\n\n\nWarning: `stat(count)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nHere we can see that the number of individuals using birth control is higher than individuals that never used birth control only for individuals have zero children. As the number of children go up we can see that individuals most individuals that have never used birth control are higher than the indivuals that have used birth control. This can imply that the percentage individuals that have children and use birth control are lesser than the percentage individuals that have children and never used birth control.\n\n\nCode\nggplot(Womendatacleaned,aes(x=factor(children),fill=factor(evermarr)))+\n  \ngeom_bar()+theme(axis.text.x = element_text(face=\"bold\", size=15),axis.text.y = element_text(face=\"bold\", size=15))+\n  \nlabs(\n    title = \"Number of Children based on Marriage status\",\n    x = \"Number of children\",\n    y = \"Count\",size=15) +\n   \nscale_fill_manual(\n    name = \"Married or not\",\n    breaks = c(\"0\", \"1\"),\n    labels = c(\"Not Married\", \"Married\"),\n    values = c(\"0\" = \"blue\", \"1\"=\"red\")\n  )\n\n\n\n\n\nMost women have 1 child in majority. Majority of those mothers are not married. This could say something about our data.\n\n\nCode\nWomendata %>%\n  ggplot() +\n  geom_bar(aes(educ)) +\n  theme_classic() + labs(title = \"Number of children based on years of schooling\",\n       x = \"Number of years of schooling\",\n       y = \"Count\")\n\n\n\n\n\nCode\n  ggtitle(\"No of indviduals educated\")\n\n\n$title\n[1] \"No of indviduals educated\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nFrom the bar graph we can say that majority of women in our data have education of atleast 5-7 years and the next highest is women with 0 years of education. This can say a lot about our data when we are talking about the relationship between education and fertility.\n\n\nCode\nggplot(Womendatacleaned,aes(x=factor(age),fill=factor(usemeth)))+\n  \ngeom_bar()+theme(axis.text.x = element_text(face=\"bold\", size=5),axis.text.y = element_text(face=\"bold\", size=15))+\n  \nlabs(\n    title = \"Number of individuals that have used birth control based their age\",\n    x = \"Age\",\n    y = \"Count\",size=15) +\n   \nscale_fill_manual(\n    name = \"Use birth control or not\",\n    breaks = c(\"0\", \"1\"),\n    labels = c(\"Never used birth control\", \"Has used birth control\"),\n    values = c(\"0\" = \"brown\", \"1\"=\"green\")\n  )\n\n\n\n\n\n\n\nCode\nggplot(data = Womendata, aes(x=mnthborn, y= children)) + \n  geom_boxplot(outlier.color = \"red\", outlier.shape = 1, show.legend = T) + \n  facet_wrap(~mnthborn)\n\n\n\n\n\n\n\nCode\nggplot(data = Womendata) + \n  geom_violin(mapping = aes(y=children, x = educ,fill=educ), trim = TRUE, draw_quantiles = c(0.25, 0.5, 0.75))\n\n\nWarning: Groups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\nGroups with fewer than two data points have been dropped.\n\n\n\n\n\n\n\nCode\nplot_bar(data = Womendata)\n\n\nError in plot_bar(data = Womendata): could not find function \"plot_bar\"\n\n\n\n\nCode\nWomendata$educ <-as.integer(as.character(Womendata$educ))\nWomendata$age <- as.integer(as.character(Womendata$age))\n\nWomendata$mnthborn <- as.integer(as.character(Womendata$mnthborn))\nWomendata$electric <- as.integer(as.character(Womendata$electric))\n\nWomendata$radio <- as.integer(as.character(Womendata$radio))\nWomendata$tv <- as.integer(as.character(Womendata$tv))\n\nWomendata$bicycle <- as.integer(as.character(Womendata$bicycle))\nWomendata$children <- as.integer(as.character(Womendata$children))\n\nWomendata$knowmeth <- as.integer(as.character(Womendata$knowmeth))\nplot_histogram(data = Womendata)\n\n\nError in plot_histogram(data = Womendata): could not find function \"plot_histogram\""
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#corelation",
    "href": "posts/Final Project 2_Kaushika Potluri.html#corelation",
    "title": "Final Project Submission 2",
    "section": "Corelation",
    "text": "Corelation\n\n\nCode\nlibrary(corrplot)\n\n\ncorrplot 0.92 loaded\n\n\nCode\nlibrary(RColorBrewer)\n\nM <-cor(Womendata %>% \n         dplyr::select(age, yearborn, educ, ceb, agefbrth, children, usemeth, knowmeth))\n\ncorrplot(M, type=\"upper\", order = \"original\",col=brewer.pal(n=8, name=\"RdYlBu\"))\n\n\n\n\n\n\n\nCode\ncor(Womendata$educ, Womendata$children)\n\n\n[1] -0.3705226\n\n\nHere we can see that education and number of children have a negative correlation. Negative correlation is a relationship between two variables in which one variable increases as the other decreases, and vice versa.\n\n\nCode\ncor(Womendata$educ, Womendata$ceb)\n\n\n[1] -0.3842877\n\n\nHere we can see that education and number of children ever born have a negative correlation as well. This does say lot about education and number of children.\n\n\nCode\ncor(Womendata$age, unclass(Womendata$educ))\n\n\n[1] -0.3096017\n\n\nWe can see that age and education have a negative correlation.\n\n\nCode\ncor(Womendata$age, Womendata$children)\n\n\n[1] 0.7325709\n\n\nAge and number of children have a positive correlation.\n\n\nCode\nlibrary(PerformanceAnalytics)\n\n\nError in library(PerformanceAnalytics): there is no package called 'PerformanceAnalytics'\n\n\nCode\nchart.Correlation(Womendata %>% \n              dplyr::select(age,yearborn, educ, ceb, agefbrth, children), histogram=TRUE, pch=19)\n\n\nError in chart.Correlation(Womendata %>% dplyr::select(age, yearborn, : could not find function \"chart.Correlation\"\n\n\n\n\nCode\nWomendata$educ <- as.factor(Womendata$educ)\nWomendata$age <- as.factor(Womendata$age)\n\nWomendata$mnthborn <- as.factor(Womendata$mnthborn)\nWomendata$electric <- as.factor(Womendata$electric)\n\nWomendata$radio <- as.factor(Womendata$radio)\nWomendata$tv <- as.factor(Womendata$tv)\n\nWomendata$bicycle <- as.factor(Womendata$bicycle)\nWomendata$children <- as.factor(Womendata$children)\n\nWomendata$knowmeth <- as.factor(Womendata$knowmeth)\nWomendata$catholic <- as.factor(Womendata$catholic)\n\nWomendata$frsthalf <- as.factor(Womendata$frsthalf)\nWomendata$educ0 <- as.factor(Womendata$educ0)\n\nWomendata$evermarr <- as.factor(Womendata$evermarr)\nWomendata$protest <- as.factor(Womendata$protest)\nWomendata$spirit <- as.factor(Womendata$spirit)\n\nWomendata$urban <- as.factor(Womendata$urban)\nWomendata$spirit <- as.factor(Womendata$spirit)\n\n\n\n\nCode\nsummary(Womendata)\n\n\n       X           mnthborn       yearborn          age       electric   \n Min.   :   1   6      : 623   Min.   :38.00   18     : 243   0   :3747  \n 1st Qu.:1091   3      : 406   1st Qu.:55.00   20     : 219   1   : 611  \n Median :2181   9      : 382   Median :62.00   22     : 206   NA's:   3  \n Mean   :2181   1      : 380   Mean   :60.43   16     : 205              \n 3rd Qu.:3271   8      : 363   3rd Qu.:68.00   19     : 205              \n Max.   :4361   7      : 358   Max.   :73.00   26     : 201              \n                (Other):1849                   (Other):3082              \n  radio         tv       bicycle          educ           ceb        \n 0   :1300   0   :3954   0   :3156   7      :1162   Min.   : 0.000  \n 1   :3059   1   : 405   1   :1202   0      : 906   1st Qu.: 1.000  \n NA's:   2   NA's:   2   NA's:   3   10     : 527   Median : 2.000  \n                                     6      : 298   Mean   : 2.442  \n                                     5      : 234   3rd Qu.: 4.000  \n                                     9      : 232   Max.   :13.000  \n                                     (Other):1002                   \n    agefbrth        children    knowmeth       usemeth          idlnchld     \n Min.   :10.00   0      :1132   0   : 160   Min.   :0.0000   Min.   : 0.000  \n 1st Qu.:17.00   1      : 907   1   :4194   1st Qu.:0.0000   1st Qu.: 3.000  \n Median :19.00   2      : 696   NA's:   7   Median :1.0000   Median : 4.000  \n Mean   :19.01   3      : 528               Mean   :0.5776   Mean   : 4.616  \n 3rd Qu.:20.00   4      : 392               3rd Qu.:1.0000   3rd Qu.: 6.000  \n Max.   :38.00   5      : 255               Max.   :1.0000   Max.   :20.000  \n NA's   :1088    (Other): 451               NA's   :71       NA's   :120     \n     agesq        urban       urb_educ      spirit   protest  catholic frsthalf\n Min.   : 225.0   0:2108   Min.   : 0.000   0:2520   0:3368   0:3914   0:2004  \n 1st Qu.: 400.0   1:2253   1st Qu.: 0.000   1:1841   1: 993   1: 447   1:2357  \n Median : 676.0            Median : 0.000                                      \n Mean   : 826.5            Mean   : 3.469                                      \n 3rd Qu.:1089.0            3rd Qu.: 7.000                                      \n Max.   :2401.0            Max.   :20.000                                      \n                                                                               \n educ0    evermarr\n 0:3455   0:2282  \n 1: 906   1:2079  \n                  \n                  \n                  \n                  \n                  \n\n\n\n\nCode\nWomendata$age <- unclass(Womendata$age)\nWomendata$children <- unclass(Womendata$children)\nWomendata$educ <- unclass(Womendata$educ)\n\n\nRegression Models :\n\n\nCode\nmodel1 <- lm(children ~ educ, data = Womendata)\nsummary(model1)\n\n\n\nCall:\nlm(formula = children ~ educ, data = Womendata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.495 -1.496 -0.399  1.182  9.505 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.70519    0.06289   74.81   <2e-16 ***\neduc        -0.20965    0.00796  -26.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.064 on 4359 degrees of freedom\nMultiple R-squared:  0.1373,    Adjusted R-squared:  0.1371 \nF-statistic: 693.7 on 1 and 4359 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nmodel2 <- lm(children ~., data = Womendata)\nsummary(model1)\n\n\n\nCall:\nlm(formula = children ~ educ, data = Womendata)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.495 -1.496 -0.399  1.182  9.505 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.70519    0.06289   74.81   <2e-16 ***\neduc        -0.20965    0.00796  -26.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.064 on 4359 degrees of freedom\nMultiple R-squared:  0.1373,    Adjusted R-squared:  0.1371 \nF-statistic: 693.7 on 1 and 4359 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nmodel3 <- lm(children ~ educ + age + mnthborn + bicycle + urb_educ + evermarr + yearborn + radio + agefbrth +idlnchld + ceb, data = Womendata)\nsummary(model3)\n\n\n\nCall:\nlm(formula = children ~ educ + age + mnthborn + bicycle + urb_educ + \n    evermarr + yearborn + radio + agefbrth + idlnchld + ceb, \n    data = Womendata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0221 -0.0315  0.0745  0.2482  1.2733 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.0207133  3.1695672  -0.322   0.7474    \neduc        -0.0001782  0.0032429  -0.055   0.9562    \nage          0.0248719  0.0427747   0.581   0.5610    \nmnthborn2   -0.0238081  0.0456270  -0.522   0.6018    \nmnthborn3    0.0671309  0.0426656   1.573   0.1157    \nmnthborn4    0.0529723  0.0451366   1.174   0.2406    \nmnthborn5    0.0374369  0.0462253   0.810   0.4181    \nmnthborn6    0.0572035  0.0388489   1.472   0.1410    \nmnthborn7    0.0378128  0.0439550   0.860   0.3897    \nmnthborn8    0.0419064  0.0450599   0.930   0.3524    \nmnthborn9    0.0501796  0.0453065   1.108   0.2681    \nmnthborn10   0.0197417  0.0503030   0.392   0.6947    \nmnthborn11   0.0135532  0.0561693   0.241   0.8093    \nmnthborn12   0.0537444  0.0613665   0.876   0.3812    \nbicycle1     0.0488972  0.0211513   2.312   0.0209 *  \nurb_educ     0.0019654  0.0028657   0.686   0.4929    \nevermarr1    0.0428377  0.0208819   2.051   0.0403 *  \nyearborn     0.0269700  0.0428099   0.630   0.5287    \nradio1       0.0220766  0.0212752   1.038   0.2995    \nagefbrth     0.0054808  0.0035900   1.527   0.1269    \nidlnchld    -0.0083450  0.0044706  -1.867   0.0620 .  \nceb          0.9004369  0.0067066 134.262   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5175 on 3155 degrees of freedom\n  (1184 observations deleted due to missingness)\nMultiple R-squared:  0.9368,    Adjusted R-squared:  0.9363 \nF-statistic:  2226 on 21 and 3155 DF,  p-value: < 2.2e-16\n\n\nWith an adjusted R-squared of 0.9363 model 3 best fits our data.\n\n\nCode\nlibrary(MASS)\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\nmodel4<- lm(log(ceb)~., \n                data = na.omit(Womendata))\n\n#Using stepAIC search method for feature selection to simplify model without impacting much on the performance.\nstep.model <- stepAIC(model4,direction = \"both\",trace = FALSE)\n\nsummary(step.model)\n\n\n\nCall:\nlm(formula = log(ceb) ~ yearborn + age + electric + radio + educ + \n    agefbrth + children + usemeth + agesq + urban + evermarr, \n    data = na.omit(Womendata))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11133 -0.14413  0.01042  0.13234  1.31158 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.109e+00  8.387e-01   2.515  0.01197 *  \nyearborn    -2.948e-02  1.135e-02  -2.597  0.00946 ** \nage          7.227e-02  1.212e-02   5.962 2.77e-09 ***\nelectric1   -3.135e-02  1.330e-02  -2.357  0.01851 *  \nradio1      -1.997e-02  9.531e-03  -2.096  0.03620 *  \neduc        -3.248e-03  1.270e-03  -2.558  0.01059 *  \nagefbrth    -2.072e-02  1.604e-03 -12.913  < 2e-16 ***\nchildren     2.550e-01  3.157e-03  80.773  < 2e-16 ***\nusemeth      4.749e-02  9.875e-03   4.809 1.59e-06 ***\nagesq       -1.312e-03  6.351e-05 -20.657  < 2e-16 ***\nurban1      -2.796e-02  9.072e-03  -3.083  0.00207 ** \nevermarr1    5.743e-02  9.546e-03   6.016 2.00e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2328 on 3106 degrees of freedom\nMultiple R-squared:  0.8895,    Adjusted R-squared:  0.8891 \nF-statistic:  2272 on 11 and 3106 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#model-evaluation",
    "href": "posts/Final Project 2_Kaushika Potluri.html#model-evaluation",
    "title": "Final Project Submission 2",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\nCode\npar(mfrow=c(2,2)) \n\nplot(model1)\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)) \n\nplot(model2)\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)) \n\nplot(model3)\n\n\n\n\n\n\n\nCode\npar(mfrow=c(2,2)) \n\nplot(model4)\n\n\n\n\n\nR^2 = 0.8895 and adjusted R^2 = 0.8891, F test value = 2272 p-value = 0.001. Under normal distribution assumption. According to Central limit teorem , every distribution approximated by a normal distribution. A normal distribution is approached very quickly as n increases, note that n is the sample size for each mean and not the number of samples If the null hypothesis is true, the above-mentioned F test statistic can be condensed (dramatically). The test statistic will be this sample variance ratio. If the null hypothesis is incorrect, we will disprove both our presumption that they were equal and the null hypothesis that the ratio was equal to 1.\n##Checking for Heteroskedasticity Breusch Pagan Test for Heteroskedasticity\nHo: the variance is constant\nH1: the variance is not constant\n\n\nCode\nbptest(ceb ~ ., data = Womendatacleaned)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ceb ~ .\nBP = 319.48, df = 94, p-value < 2.2e-16\n\n\nHo hypothesis is rejected since the variance is not constant.\nIn multiple regression two or more predictor variables might be correlated with each other and situation is referred as collinearity. Multicollinearity is where collinearity exists between three or more variables even if no pair of variables has a particularly high correlation. This means that there is redundancy between predictor variables.Multicollinearity can assessed by computing a score called the variance inflation factor (or VIF), which measures how much the variance of a regression coefficient is inflated due to multicollinearity in the model. The smallest possible value of VIF is one (absence of multicollinearity). A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\n\n\nCode\nvif(step.model)\n\n\n  yearborn        age   electric      radio       educ   agefbrth   children \n459.569426 524.025963   1.250912   1.102156   1.523564   1.430945   2.370392 \n   usemeth      agesq      urban   evermarr \n  1.210953  59.396420   1.183061   1.273500 \n\n\nAs a thumb rule, since we follow that a VIF value that exceeds 5 or 10 can be a problem. This leads to a simpler model without compromising the model accuracy, which is good. So now the new model will be without yearborn."
  },
  {
    "objectID": "posts/Final Project 2_Kaushika Potluri.html#without-multicollinearity",
    "href": "posts/Final Project 2_Kaushika Potluri.html#without-multicollinearity",
    "title": "Final Project Submission 2",
    "section": "Without Multicollinearity",
    "text": "Without Multicollinearity\n\n\nCode\nlibrary(MASS)\nmodel2<- lm(log(ceb)~ mnthborn + age +\n                            electric +\n                            children + knowmeth +\n                            usemeth  +\n                            urban + radio +\n                            tv + bicycle +\n                            I(as.factor(educ)) +\n                            idlnchld+urb_educ +\n                            protest, \n                            data = na.omit(Womendatacleaned))\n\nstep.model2 <- stepAIC(model2, \n                        direction = \"both\", \n                        trace = FALSE)\n\nvif(step.model2) # Variance Inflation Factor (or VIF)\n\n\n             GVIF Df GVIF^(1/(2*Df))\nage      3.298838 34        1.017707\nelectric 1.269286  1        1.126626\nchildren 3.353509 13        1.047639\nurban    2.365418  1        1.537992\nradio    1.065265  1        1.032117\nidlnchld 1.215614  1        1.102549\nurb_educ 2.779986  1        1.667329\n\n\nNow the new VIF values are all less than 5. This is good for our model. There is no Multicollinearity.\nWe expect from our study, if the level of education increases, the number of children is decreasing. Also, it should be same parameters negative corelation for example between education and number of children. We found that if the level of education increases, the number of children is decreasing. In addition, same parameters negative corelation for example between education and number of children or between birth control pill and children every born / children. Also, age and education level is negative corelation. Age and number of children are positive corelation.\n##References [[1]The effect of women’s schooling on fertility by W Sander · 1992 [2]The Impact of Women’s Schooling on Fertility and Contraceptive Use by M Ainsworth · 1996 [3]Fertility in Botswana: The Recent Decline and Future Prospects by Naomi Rutenberg and Ian Diamond"
  },
  {
    "objectID": "posts/Final Project Proposal.html",
    "href": "posts/Final Project Proposal.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "The research questions that I am looking to investigate involve the factors that increase university students’ GPA. These include the following:\n1) Does classroom engagement (i.e., taking notes, attending class, listening) result in a higher GPA in university students?\n2) Does reported studying (i.e., weekly study hours) result in a higher GPA in university students?\n3) Does collaboration between students (i.e., studying together, positive class discussions) result in a higher GPA in university students?\n\n\n\n\n\nFor the first research question, it is reasonable to hypothesize that classroom engagement will have a positive effect on students’ academic achievement. Previous research supports this hypothesis. For example, one study found that classroom engagement, as well as other related factors such as time management and autonomous motivation, are predictors of academic achievement (Fokkens-Bruinsma, et al., 2021). Another study found that attendance in higher education is a small, but still statistically significant, predictor of academic performance (Büchele, 2021). In this study, classroom engagement will be defined as “taking notes, attendance, and frequency of listening.” These measures will be reported by university students via survey.\n\n\n\nIn regards to the second research question, it is hypothesized that students who study more will have a higher GPA. There are many previous studies that support this claim. For instance, one study found that university freshmen who studied more than eight hours a week saw an average increase in GPA of 0.580 (Nelson, 2003). Research has also found that increasing study time leads to an increased GPA (Thibodeaux, et al., 2017). In this study, hours spent studying will be measured through students’ estimated range of hours studied, reported via survey.\n\n\n\nIn response to the third research question, it is hypothesized that student collaboration will have a positive effect on student GPA. There is some research literature that supports this statement. One study found that students who study with their peers achieve significantly higher homework scores (Vargas, et al., 2018). Another study found that university students who had a strong social network and exhibited collaborative behaviors tended to achieve higher grades (Ellis & Han, 2021). Effective student collaboration can also occur during class time, such as through small group discussions. Research has found that students who participate in small group discussions demonstrate an increase in resilience, which has shown to improve academic performance (Torrento-estimo, et al, 2012). In this study, student collaboration will be measured through students’ reported time spent studying with peers, and impact that their class discussions have.\n\n\n\n\nThe dataset used is one retrieved from Kaggle using the link here. The dataset is named, “Higher Education Student Performance Evaluation.” This dataset was used in a self-report survey study conducted by Yılmaz and Sekeroglu (2019).\n\n\nCode\nstudentsurvey <- read.csv(\"student_prediction.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'student_prediction.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nsummary(studentsurvey)\n\n\nError in summary(studentsurvey): object 'studentsurvey' not found\n\n\nCode\nlibrary(ggplot2)\n\n\nTo begin, it is important to examine the demographic variables through descriptive statistics to observe the sample.\n\n\nTo start, students’ reported gender (1 = female and 2 = male) is plotted in the bar graph below.\n\n\nCode\nggplot(studentsurvey, aes(x = GENDER)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = GENDER)): object 'studentsurvey' not found\n\n\nIn this sample, there are more males than females.\n\n\n\nThe bar graph below plots the students’ reported ages at the time of the survey (1 = 18-21, 2 = 22-25, 3 = 26 or above).\n\n\nCode\nggplot(studentsurvey, aes(x = AGE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = AGE)): object 'studentsurvey' not found\n\n\nThe majority of students are between the ages 18-25, with very few above the age of 26.\n\n\n\nThe bar graph below depicts what type of high school the university students graduated from (1= private, 2 = state, 3 = other).\n\n\nCode\nggplot(studentsurvey, aes(x = HS_TYPE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = HS_TYPE)): object 'studentsurvey' not found\n\n\nAccording to the graph, most students attended a state (public) high school.\n\n\n\nThe bar graph below demonstrates what percentage of their tuition was paid for by scholarship (1 = None, 2 = 25%, 3 = 50%, 4 = 75%, 5 = Full)\n\n\nCode\nggplot(studentsurvey, aes(x = SCHOLARSHIP)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = SCHOLARSHIP)): object 'studentsurvey' not found\n\n\nMost students have received at least 50% scholarship at this university.\n\n\n\nThe bar graph below depicts how many students work a job outside of their classes (1 = Yes, 2 = No)\n\n\nCode\nggplot(studentsurvey, aes(x = WORK)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = WORK)): object 'studentsurvey' not found\n\n\nMost students do not have a job while they are studying at university in this sample.\n\n\n\nThis sample may not be representative of the U.S. student population. There are more male than female students, which is not the case at most schools: there is about a 1:2 male to female ratio at U.S. colleges (Leukhina & Smaldone, 2022). The ages of students, however, do align with the ages of current university students: about a third of students in university are ages 24 and under (Hanson, 2022). Additionally, like in the sample, the vast majority of students attended public schools (Riser-Kositsky, 2022). In regards to scholarships, the students at this particular university receive scholarships at significantly higher rates than the rest of the U.S. Only about one in eight students receive a scholarship, and only 5% receive a full scholarship (Scholarship Statistics, 2021). While the enrollment statuses of the students were not given, if all students were full-time students, it would align with research that shows that less than half of full-time students (40%) in U.S. universities work while in school. While this sample may not be entirely representative of the U.S. college student population, analyses of this dataset conducted may provide some insight on factors that improve university students GPA.\n\n\n\n\nBüchele, S. (2021). Evaluating the link between attendance and performance in higher education: the role of classroom engagement dimensions. Assessment & Evaluation in Higher Education, 46(1), 132-150.\nEllis, R., & Han, F. (2021). Assessing university student collaboration in new ways. Assessment & Evaluation in Higher Education, 46(4), 509-524.\nFokkens-Bruinsma, M., Vermue, C., Deinumdataset, J. F., & van Rooij, E. (2021). First-year academic achievement: the role of academic self-efficacy, self-regulated learning and beyond classroom engagement. Assessment & Evaluation in Higher Education, 46(7), 1115-1126.\nHanson, M. (2022, July 26). College Enrollment & Student Demographic Statistics. EducationData.org. Retrieved from https://educationdata.org/college-enrollment-statistics.\nLeukhina, O., & Smaldone, A. (2022, March 14). Why do women outnumber men in college enrollment? Saint Louis Fed Eagle. Retrieved from https://www.stlouisfed.org/on-the-economy/2022/mar/why-women-outnumber-men-college-enrollment#:~:text=When%20the%20fall%20college%20enrollment,seen%20in%20U.S.%20college%20enrollment.\nNational Center for Education Statistics. (2022, May). College Student Employment. Coe - college student employment. Retrieved from https://nces.ed.gov/programs/coe/indicator/ssa/college-student-employment\nNelson, R. (2003). Student Efficiency: A study on the behavior and productive efficiency of college students and the determinants of GPA. Issues in Political Economy, 12, 32-43.\nRiser-Kositsky, M. (2022, August 2). Education statistics: Facts about American Schools. Education Week. Retrieved from https://www.edweek.org/leadership/education-statistics-facts-about-american-schools/2019/01.\nScholarship statistics. ThinkImpact.com. (2021, November 10). Retrieved from https://www.thinkimpact.com/scholarship-statistics/.\nThibodeaux, J., Deutsch, A., Kitsantas, A., & Winsler, A. (2017). First-year college students' time use: Relations with self-regulation and GPA. Journal of Advanced Academics, 28(1), 5-27.\nTorrento-estimo, E., Lourdes, C., & Evidente, L. G. (2012). Collaborative Learning in Small Group Discussions and Its Impact on Resilience Quotient and Academic Performance. JPAIR Multidisciplinary Research Journal, 7(1), 1-1.\nVargas, D. L., Bridgeman, A. M., Schmidt, D. R., Kohl, P. B., Wilcox, B. R., & Carr, L. D. (2018). Correlation between student collaboration network centrality and academic performance. Physical Review Physics Education Research, 14(2), 020112.\nYılmaz, N., & Sekeroglu, B. (2019, August). Student Performance Classification Using Artificial Intelligence Techniques. In International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions (pp. 596-603). Springer, Cham."
  },
  {
    "objectID": "posts/Final pt 1.html",
    "href": "posts/Final pt 1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/Final pt 1.html#part-1",
    "href": "posts/Final pt 1.html#part-1",
    "title": "Final Project Proposal",
    "section": "Part 1",
    "text": "Part 1\n\nResearch Question\nIn the United States, wage stagnation has become a hot-button issue for many people in various fields of employment. Graduate students have been at the center of this issue in recent years- strikes for wage increases and cost-of-living adjustments have taken place at multiple universities throughout the country. Because PhD students often do not have the time to earn extra income (and their contracts often prohibit them from pursuing work elsewhere), how much they will earn from their stipend is a huge factor in considering where to pursue their research (Powell, 2004; Soar et al., 2022). Knowing how much My research question is: What is the strongest predictor of the value of a PhD stipend?\n\n\nHypothesis\nH₀: Cost of living is not the strongest predictor of the value of a PhD stipend.\nH₁: Cost of living is the strongest predictor of the value of a PhD stipend.\n\n\nDataset\nThis dataset is comprised of self-reported survey data collected by PhDStipends.com. Respondents are asked their university, department, academic year, and year in the program. They are also asked whether they receive a 12-month or 9-month salary, gross pay, and required fees. PhDStipends automatically calculators the LW Ratio (living wage ratio), which is the stipend divided by the living wage of the country the university is located in. I will likely need to add additional information for my own analysis.\nThe variables of interest for me are the university, department, and program year.\n\n\nCode\nlibrary(readr)\ncsv <- read_csv(\"~/School/UMASS/DACSS 603/Final Project/csv.csv\")\n\n\nRows: 12160 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): University, Status, Department, Category, AcYear\ndbl (7): Pay, LW Ratio, ProgYear, 12 M Gross Pay, 9 M Gross Pay, 3 M Gross P...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nsummary(csv)\n\n\n  University           Status           Department          Category        \n Length:12160       Length:12160       Length:12160       Length:12160      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n      Pay           LW Ratio        AcYear             ProgYear   \n Min.   :    1   Min.   :0.000   Length:12160       Min.   :1.00  \n 1st Qu.:20000   1st Qu.:0.880   Class :character   1st Qu.:1.00  \n Median :26000   Median :1.130   Mode  :character   Median :1.00  \n Mean   :25765   Mean   :1.095                      Mean   :2.05  \n 3rd Qu.:31500   3rd Qu.:1.330                      3rd Qu.:3.00  \n Max.   :96000   Max.   :4.120                      Max.   :6.00  \n NA's   :47      NA's   :422                        NA's   :1221  \n 12 M Gross Pay   9 M Gross Pay   3 M Gross Pay        Fees      \n Min.   :     1   Min.   :   15   Min.   :    4   Min.   :    1  \n 1st Qu.: 24000   1st Qu.:16500   1st Qu.: 3000   1st Qu.:  500  \n Median : 29000   Median :20000   Median : 5000   Median : 1000  \n Mean   : 28474   Mean   :20128   Mean   : 5194   Mean   : 2030  \n 3rd Qu.: 33000   3rd Qu.:24000   3rd Qu.: 6204   3rd Qu.: 2000  \n Max.   :140000   Max.   :87467   Max.   :55816   Max.   :93725  \n NA's   :3632     NA's   :8551    NA's   :10951   NA's   :7404   \n\n\n\n\nCode\nprint(summarytools::dfSummary(csv,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ncsv\nDimensions: 12160 x 12\n  Duplicates: 339\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      University\n[character]\n      1. University of Wisconsin -2. Duke University (DU)3. University of North Carol4. University of California 5. University of California,6. University of Michigan - 7. University of Pennsylvani8. University of Southern Ca9. Pennsylvania State Univer10. University of Minnesota -[ 390 others ]\n      230(1.9%)208(1.7%)206(1.7%)205(1.7%)204(1.7%)195(1.6%)193(1.6%)191(1.6%)190(1.6%)179(1.5%)10159(83.5%)\n      \n      0\n(0.0%)\n    \n    \n      Status\n[character]\n      1. Private2. Public\n      4236(34.8%)7924(65.2%)\n      \n      0\n(0.0%)\n    \n    \n      Department\n[character]\n      1. Chemistry2. Psychology3. Sociology4. Computer Science5. Physics6. English7. Political Science8. Biology9. Economics10. Biomedical Engineering[ 2916 others ]\n      530(4.5%)391(3.3%)323(2.7%)322(2.7%)292(2.5%)289(2.4%)286(2.4%)266(2.2%)197(1.7%)196(1.7%)8747(73.9%)\n      \n      321\n(2.6%)\n    \n    \n      Category\n[character]\n      1. #N/A2. 03. Business/Policy4. Formal Science5. Humanities6. Natural Science7. Social Science\n      3310(27.2%)625(5.1%)211(1.7%)1658(13.6%)919(7.6%)3435(28.2%)2002(16.5%)\n      \n      0\n(0.0%)\n    \n    \n      Pay\n[numeric]\n      Mean (sd) : 25765.1 (9125.4)min ≤ med ≤ max:1 ≤ 26000 ≤ 96000IQR (CV) : 11500 (0.4)\n      3420 distinct values\n      \n      47\n(0.4%)\n    \n    \n      LW Ratio\n[numeric]\n      Mean (sd) : 1.1 (0.4)min ≤ med ≤ max:0 ≤ 1.1 ≤ 4.1IQR (CV) : 0.5 (0.3)\n      253 distinct values\n      \n      422\n(3.5%)\n    \n    \n      AcYear\n[character]\n      1. 2020-20212. 2016-20173. 2018-20194. 2019-20205. 2021-20226. 2017-20187. 2022-20238. 2014-20159. 2015-201610. 2013-2014[ 14 others ]\n      2657(21.9%)1959(16.1%)1708(14.0%)1347(11.1%)1194(9.8%)1111(9.1%)998(8.2%)524(4.3%)395(3.2%)90(0.7%)175(1.4%)\n      \n      2\n(0.0%)\n    \n    \n      ProgYear\n[numeric]\n      Mean (sd) : 2 (1.5)min ≤ med ≤ max:1 ≤ 1 ≤ 6IQR (CV) : 2 (0.7)\n      1:6185(56.5%)2:1518(13.9%)3:1191(10.9%)4:951(8.7%)5:740(6.8%)6:354(3.2%)\n      \n      1221\n(10.0%)\n    \n    \n      12 M Gross Pay\n[numeric]\n      Mean (sd) : 28473.9 (9013.8)min ≤ med ≤ max:1 ≤ 29000 ≤ 140000IQR (CV) : 9000 (0.3)\n      1608 distinct values\n      \n      3632\n(29.9%)\n    \n    \n      9 M Gross Pay\n[numeric]\n      Mean (sd) : 20128.2 (7100.4)min ≤ med ≤ max:15 ≤ 20000 ≤ 87467IQR (CV) : 7500 (0.4)\n      1046 distinct values\n      \n      8551\n(70.3%)\n    \n    \n      3 M Gross Pay\n[numeric]\n      Mean (sd) : 5194.4 (3370.8)min ≤ med ≤ max:4 ≤ 5000 ≤ 55816IQR (CV) : 3204 (0.6)\n      308 distinct values\n      \n      10951\n(90.1%)\n    \n    \n      Fees\n[numeric]\n      Mean (sd) : 2030.1 (4711.9)min ≤ med ≤ max:1 ≤ 1000 ≤ 93725IQR (CV) : 1500 (2.3)\n      985 distinct values\n      \n      7404\n(60.9%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-11-13\n\n\n\nBased on this summary, there are some extreme outliers in need of removal, particularly in the Overall Pay column. Interesting, the mean Overall Pay of $27549.4 does not seem unreasonable,."
  },
  {
    "objectID": "posts/Final pt 1.html#part-2",
    "href": "posts/Final pt 1.html#part-2",
    "title": "Final Project Proposal",
    "section": "Part 2",
    "text": "Part 2\n\nReferences\nLiving Wage Calculator. (n.d.). Retrieved October 10, 2022, from https://livingwage.mit.edu/\nPowell, K. Stipend survival. Nature 428, 102–103 (2004). https://doi.org/10.1038/nj6978-102a\nEmily Roberts & Kyle Roberts. (2022, October 10). PhD stipends Dataset. http://www.phdstipends.com/csv\nSoar, M., Stewart, L., Nissen, S. et al. Sweat Equity: Student Scholarships in Aotearoa New Zealand’s Universities. NZ J Educ Stud (2022). https://doi.org/10.1007/s40841-022-00244-5"
  },
  {
    "objectID": "posts/Final pt 2.html",
    "href": "posts/Final pt 2.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/Final pt 2.html#part-1",
    "href": "posts/Final pt 2.html#part-1",
    "title": "Final Project Proposal",
    "section": "Part 1",
    "text": "Part 1\n\nResearch Question\nIn the United States, wage stagnation has become a hot-button issue for many people in various fields of employment. Graduate students have been at the center of this issue in recent years- strikes for wage increases and cost-of-living adjustments have taken place at multiple universities throughout the country. Because PhD students often do not have the time to earn extra income (and their contracts often prohibit them from pursuing work elsewhere), how much they will earn from their stipend is a huge factor in considering where to pursue their research (Powell, 2004; Soar et al., 2022). Knowing how much My research question is: Is university ownership status (public vs. private) a predictor of the value of a PhD stipend?\n\n\nHypothesis\nH₀:University ownership status is a predictor of the value of a PhD stipend.\nH₁: University ownership status is not a predictor of the value of a PhD stipend.\n\n\nDataset\nThis dataset is comprised of self-reported survey data collected by PhDStipends.com. Respondents are asked their university, department, academic year, and year in the program. They are also asked whether they receive a 12-month or 9-month salary, gross pay, and required fees. PhDStipends automatically calculates the LW Ratio (living wage ratio), which is the stipend divided by the living wage of the county the university is located in.\nIn addition to this information, I also manually categorized universities by their ownership status as public or private, and assigned each program to 1 of five broader academic disciplines: Business/Policy, Social Science, Natural Science, Formal Science, and Humanities. Due to a computer issue much of my work was lost, so the dataset is currently incomplete. The analysis that follows is based on the information I was able to recover or reenter within a reasonable period of time.\nThe variables of interest for me are the ownership status, gross pay, program year, and academic discipline.\n\n\nCode\nlibrary(readr)\ncsv <- read_csv(\"~/School/UMASS/DACSS 603/Final Project/csv.csv\")\n\n\nRows: 12160 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): University, Status, Department, Category, AcYear\ndbl (7): Pay, LW Ratio, ProgYear, 12 M Gross Pay, 9 M Gross Pay, 3 M Gross P...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nsummary(csv)\n\n\n  University           Status           Department          Category        \n Length:12160       Length:12160       Length:12160       Length:12160      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n      Pay           LW Ratio        AcYear             ProgYear   \n Min.   :    1   Min.   :0.000   Length:12160       Min.   :1.00  \n 1st Qu.:20000   1st Qu.:0.880   Class :character   1st Qu.:1.00  \n Median :26000   Median :1.130   Mode  :character   Median :1.00  \n Mean   :25765   Mean   :1.095                      Mean   :2.05  \n 3rd Qu.:31500   3rd Qu.:1.330                      3rd Qu.:3.00  \n Max.   :96000   Max.   :4.120                      Max.   :6.00  \n NA's   :47      NA's   :422                        NA's   :1221  \n 12 M Gross Pay   9 M Gross Pay   3 M Gross Pay        Fees      \n Min.   :     1   Min.   :   15   Min.   :    4   Min.   :    1  \n 1st Qu.: 24000   1st Qu.:16500   1st Qu.: 3000   1st Qu.:  500  \n Median : 29000   Median :20000   Median : 5000   Median : 1000  \n Mean   : 28474   Mean   :20128   Mean   : 5194   Mean   : 2030  \n 3rd Qu.: 33000   3rd Qu.:24000   3rd Qu.: 6204   3rd Qu.: 2000  \n Max.   :140000   Max.   :87467   Max.   :55816   Max.   :93725  \n NA's   :3632     NA's   :8551    NA's   :10951   NA's   :7404   \n\n\n\n\nCode\nprint(summarytools::dfSummary(csv,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ncsv\nDimensions: 12160 x 12\n  Duplicates: 339\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      University\n[character]\n      1. University of Wisconsin -2. Duke University (DU)3. University of North Carol4. University of California 5. University of California,6. University of Michigan - 7. University of Pennsylvani8. University of Southern Ca9. Pennsylvania State Univer10. University of Minnesota -[ 390 others ]\n      230(1.9%)208(1.7%)206(1.7%)205(1.7%)204(1.7%)195(1.6%)193(1.6%)191(1.6%)190(1.6%)179(1.5%)10159(83.5%)\n      \n      0\n(0.0%)\n    \n    \n      Status\n[character]\n      1. Private2. Public\n      4236(34.8%)7924(65.2%)\n      \n      0\n(0.0%)\n    \n    \n      Department\n[character]\n      1. Chemistry2. Psychology3. Sociology4. Computer Science5. Physics6. English7. Political Science8. Biology9. Economics10. Biomedical Engineering[ 2916 others ]\n      530(4.5%)391(3.3%)323(2.7%)322(2.7%)292(2.5%)289(2.4%)286(2.4%)266(2.2%)197(1.7%)196(1.7%)8747(73.9%)\n      \n      321\n(2.6%)\n    \n    \n      Category\n[character]\n      1. #N/A2. 03. Business/Policy4. Formal Science5. Humanities6. Natural Science7. Social Science\n      3310(27.2%)625(5.1%)211(1.7%)1658(13.6%)919(7.6%)3435(28.2%)2002(16.5%)\n      \n      0\n(0.0%)\n    \n    \n      Pay\n[numeric]\n      Mean (sd) : 25765.1 (9125.4)min ≤ med ≤ max:1 ≤ 26000 ≤ 96000IQR (CV) : 11500 (0.4)\n      3420 distinct values\n      \n      47\n(0.4%)\n    \n    \n      LW Ratio\n[numeric]\n      Mean (sd) : 1.1 (0.4)min ≤ med ≤ max:0 ≤ 1.1 ≤ 4.1IQR (CV) : 0.5 (0.3)\n      253 distinct values\n      \n      422\n(3.5%)\n    \n    \n      AcYear\n[character]\n      1. 2020-20212. 2016-20173. 2018-20194. 2019-20205. 2021-20226. 2017-20187. 2022-20238. 2014-20159. 2015-201610. 2013-2014[ 14 others ]\n      2657(21.9%)1959(16.1%)1708(14.0%)1347(11.1%)1194(9.8%)1111(9.1%)998(8.2%)524(4.3%)395(3.2%)90(0.7%)175(1.4%)\n      \n      2\n(0.0%)\n    \n    \n      ProgYear\n[numeric]\n      Mean (sd) : 2 (1.5)min ≤ med ≤ max:1 ≤ 1 ≤ 6IQR (CV) : 2 (0.7)\n      1:6185(56.5%)2:1518(13.9%)3:1191(10.9%)4:951(8.7%)5:740(6.8%)6:354(3.2%)\n      \n      1221\n(10.0%)\n    \n    \n      12 M Gross Pay\n[numeric]\n      Mean (sd) : 28473.9 (9013.8)min ≤ med ≤ max:1 ≤ 29000 ≤ 140000IQR (CV) : 9000 (0.3)\n      1608 distinct values\n      \n      3632\n(29.9%)\n    \n    \n      9 M Gross Pay\n[numeric]\n      Mean (sd) : 20128.2 (7100.4)min ≤ med ≤ max:15 ≤ 20000 ≤ 87467IQR (CV) : 7500 (0.4)\n      1046 distinct values\n      \n      8551\n(70.3%)\n    \n    \n      3 M Gross Pay\n[numeric]\n      Mean (sd) : 5194.4 (3370.8)min ≤ med ≤ max:4 ≤ 5000 ≤ 55816IQR (CV) : 3204 (0.6)\n      308 distinct values\n      \n      10951\n(90.1%)\n    \n    \n      Fees\n[numeric]\n      Mean (sd) : 2030.1 (4711.9)min ≤ med ≤ max:1 ≤ 1000 ≤ 93725IQR (CV) : 1500 (2.3)\n      985 distinct values\n      \n      7404\n(60.9%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-11-13"
  },
  {
    "objectID": "posts/Final pt 2.html#part-2",
    "href": "posts/Final pt 2.html#part-2",
    "title": "Final Project Proposal",
    "section": "Part 2",
    "text": "Part 2\n\nVisualizations\nI’ll start with a histogram of all stipends, regardless of university ownership status.\n\n\nCode\nviz <- csv %>% filter(Status %in% c(\"Public\", \"Private\")) \n\nhist(viz$Pay, breaks = 10)\n\n\n\n\n\nThe distribution appears somewhat normal, with annual pay most frequently in the range of $20,000 to $30,000 annually.\nNext I will generate 2 boxplots: one for public universities, and one for private.\n\n\nCode\nviz %>%\n  ggplot(\n    aes(x=Status, y=Pay, fill=Status)) +\n    geom_boxplot()\n\n\nWarning: Removed 47 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nThere are quite a few outliers for both categories, but we can see that median pay is higher in private universities than in public universities. There are also significantly more outliers below the 1st quartile in private universities than in public.\n\n\nHypothesis Testing\n\nExplanatory Variable: Ownership Status (Status)\nResponse Variable: Gross Pay (Pay)\nControl Variable: Academic Discipline (Category), Program Year (ProgYear)\n\nFirst I will run a model for gross pay, using as.factor() to convert ownership status into dummy variables.\n\n\nCode\nfit1=lm(Pay ~ as.factor(Status), data = csv)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = Pay ~ as.factor(Status), data = csv)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-30328  -4729    668   4918  72671 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30332.2      130.8  231.81   <2e-16 ***\nas.factor(Status)Public  -7003.5      162.0  -43.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8494 on 12111 degrees of freedom\n  (47 observations deleted due to missingness)\nMultiple R-squared:  0.1336,    Adjusted R-squared:  0.1336 \nF-statistic:  1868 on 1 and 12111 DF,  p-value: < 2.2e-16\n\n\nBased on the p-values, it does seem that ownership status is statistically significant with regards to pay. Now I will plot this model.\nNext I will create a model adding the control variable “Category” (academic discipline).\n\n\nCode\nfit2=lm(Pay ~ as.factor(Status) + Category, data = csv)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = Pay ~ as.factor(Status) + Category, data = csv)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32473  -4293    375   4701  72535 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              30191.0      176.8 170.757  < 2e-16 ***\nas.factor(Status)Public  -7118.8      159.0 -44.770  < 2e-16 ***\nCategory0                  519.7      362.9   1.432 0.152228    \nCategoryBusiness/Policy   2089.8      592.7   3.526 0.000424 ***\nCategoryFormal Science     393.0      250.6   1.568 0.116838    \nCategoryHumanities       -3447.6      310.8 -11.093  < 2e-16 ***\nCategoryNatural Science   2371.5      202.7  11.697  < 2e-16 ***\nCategorySocial Science   -1892.2      236.0  -8.019 1.16e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8310 on 12105 degrees of freedom\n  (47 observations deleted due to missingness)\nMultiple R-squared:  0.1713,    Adjusted R-squared:  0.1708 \nF-statistic: 357.4 on 7 and 12105 DF,  p-value: < 2.2e-16\n\n\nFormal Science, Humanities, and Natural Science all appear to be statistically significant. However, “Category0” is likely skewing the data, as this includes degree programs I have yet to assign to a category. The R-squared value here is higher than the previous model; however, due to the incomplete data, I will take this with a grain of salt.\nNext I will create a model adding the control variable “ProgYear” (program year).\n\n\nCode\nfit3=lm(Pay ~ as.factor(Status) + ProgYear, data = csv)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = Pay ~ as.factor(Status) + ProgYear, data = csv)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-30564  -4621    469   4969  72741 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             30719.46     177.92 172.659   <2e-16 ***\nas.factor(Status)Public -7052.72     169.55 -41.597   <2e-16 ***\nProgYear                 -135.82      55.31  -2.456   0.0141 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8455 on 10896 degrees of freedom\n  (1261 observations deleted due to missingness)\nMultiple R-squared:  0.1374,    Adjusted R-squared:  0.1372 \nF-statistic: 867.5 on 2 and 10896 DF,  p-value: < 2.2e-16\n\n\nProgram year does appear to be statistically significant. R-squared is comparable to the original model.\nFinally, I will create a model using both control variables.\n\n\nCode\nfit4=lm(Pay ~ as.factor(Status) + Category + ProgYear, data = csv)\nsummary(fit4)\n\n\n\nCall:\nlm(formula = Pay ~ as.factor(Status) + Category + ProgYear, data = csv)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-31798  -4232    305   4704  72735 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             30664.55     217.30 141.118  < 2e-16 ***\nas.factor(Status)Public -7168.59     166.29 -43.110  < 2e-16 ***\nCategory0                 701.14     378.22   1.854  0.06380 .  \nCategoryBusiness/Policy  2311.60     644.93   3.584  0.00034 ***\nCategoryFormal Science    414.79     260.64   1.591  0.11154    \nCategoryHumanities      -3359.96     330.50 -10.166  < 2e-16 ***\nCategoryNatural Science  2514.91     212.68  11.825  < 2e-16 ***\nCategorySocial Science  -1869.95     247.92  -7.543 4.97e-14 ***\nProgYear                 -215.27      54.28  -3.966 7.35e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8265 on 10890 degrees of freedom\n  (1261 observations deleted due to missingness)\nMultiple R-squared:  0.1762,    Adjusted R-squared:  0.1756 \nF-statistic: 291.1 on 8 and 10890 DF,  p-value: < 2.2e-16\n\n\nIn this model, the disciplines of Business/Policy and Formal Science are the only ones which are not statistically significant.\n\n\nCode\npar(mfrow= c(2,3)); plot(fit1, which=1:6)\n\n\n\n\n\n\n\nCode\npar(mfrow= c(2,3)); plot(fit2, which=1:6)\n\n\n\n\n\n\n\nCode\npar(mfrow= c(2,3)); plot(fit3, which=1:6)\n\n\n\n\n\n\n\nCode\npar(mfrow= c(2,3)); plot(fit4, which=1:6)\n\n\n\n\n\nThe large number of categorical variables in my data makes plotting any model challenging, but from what I can see the fit is not great for any model. I am curious if a logit model would produce better results."
  },
  {
    "objectID": "posts/Final pt 2.html#summary",
    "href": "posts/Final pt 2.html#summary",
    "title": "Final Project Proposal",
    "section": "Summary",
    "text": "Summary\nI need to reevaluate some of my variables and data and see if I can come up with a way to transform the data so that the models can be improved. I may experiment with relevel() and see if that has any effect. I also have yet to try an F-test.\nAlso, as previously mentioned, my data is incomplete- finishing the categorization of each degree program may improve my results.\n\nReferences\nLiving Wage Calculator. (n.d.). Retrieved October 10, 2022, from https://livingwage.mit.edu/\nPowell, K. Stipend survival. Nature 428, 102–103 (2004). https://doi.org/10.1038/nj6978-102a\nEmily Roberts & Kyle Roberts. (2022, October 10). PhD stipends Dataset. http://www.phdstipends.com/csv\nSoar, M., Stewart, L., Nissen, S. et al. Sweat Equity: Student Scholarships in Aotearoa New Zealand’s Universities. NZ J Educ Stud (2022). https://doi.org/10.1007/s40841-022-00244-5"
  },
  {
    "objectID": "posts/finalpart1-1.html",
    "href": "posts/finalpart1-1.html",
    "title": "Final Project Part-1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(sqldf)\n\n\nWarning: package 'sqldf' was built under R version 4.2.2\n\n\nLoading required package: gsubfn\n\n\nWarning: package 'gsubfn' was built under R version 4.2.2\n\n\nLoading required package: proto\n\n\nWarning: package 'proto' was built under R version 4.2.2\n\n\nLoading required package: RSQLite\n\n\nWarning: package 'RSQLite' was built under R version 4.2.2\n\n\nCode\nlibrary(data.table)\n\n\nWarning: package 'data.table' was built under R version 4.2.2\n\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose"
  },
  {
    "objectID": "posts/finalpart1-1.html#backgroundmotivation",
    "href": "posts/finalpart1-1.html#backgroundmotivation",
    "title": "Final Project Part-1",
    "section": "Background/Motivation",
    "text": "Background/Motivation\nWhat is takes for a country or a continent to be happy? Is it the economy, life-expectancy, freedom or the trust in the government? What are the factors that affect a country’s or continents overall happiness? Can we predict the happiness score? The curiosity to find answers to these questions made me explore the world happiness data of 2022.\n“This year marks the 10th anniversary of the World Happiness Report, which uses global survey data to report how people evaluate their own lives in more than 150 countries worldwide. The World Happiness Report 2022 reveals a bright light in dark times. The pandemic brought not only pain and suffering but also an increase in social support and benevolence. As we battle the ills of disease and war, it is essential to remember the universal desire for happiness and the capacity of individuals to rally to each other’s support in times of great need.” - World Happiness Report 2022"
  },
  {
    "objectID": "posts/finalpart1-1.html#research-question",
    "href": "posts/finalpart1-1.html#research-question",
    "title": "Final Project Part-1",
    "section": "Research Question",
    "text": "Research Question\nThe World happiness data tries to measure the happiness of the populace of every country and comes up with a score which connotes the level of happiness of the populace.\nThe data set uses various variables to measure happiness such as the GDP per capita, Freedom to make choices, life expectancy, the perception of corruption, generosity and social support.\nIn this study, I aim to find out answers to the following research questions:\n\nWhat are the variables or factors that are affecting world’s happiness, with a focus on individual countries & continents. This includes analyzing the correlation between most effective variables.\nTo find out which model accurately predicts the happiness score."
  },
  {
    "objectID": "posts/finalpart1-1.html#hypothesis",
    "href": "posts/finalpart1-1.html#hypothesis",
    "title": "Final Project Part-1",
    "section": "Hypothesis",
    "text": "Hypothesis\nI wish to test the following hypothesis,\n\nBetter economy of a country would lead to happiness\nLonger life expectancy would lead to happiness\nHaving family/social support leads to happiness\nFreedom leads to happiness\nPeople’s trust in the Government leads to happiness\nGenerosity leads to happiness"
  },
  {
    "objectID": "posts/finalpart1-1.html#model",
    "href": "posts/finalpart1-1.html#model",
    "title": "Final Project Part-1",
    "section": "Model",
    "text": "Model\nLook at each variable individually, and put all the variables together."
  },
  {
    "objectID": "posts/finalpart1-1.html#data-preparation",
    "href": "posts/finalpart1-1.html#data-preparation",
    "title": "Final Project Part-1",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nReading the data set\n\n\nCode\nprimary <- read.csv(\"project datasets/2022.csv\")\nhead(primary)\n\n\n\n\n  \n\n\n\nCode\nstr(primary)\n\n\n'data.frame':   147 obs. of  15 variables:\n $ RANK                                      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Country                                   : chr  \"Finland\" \"Denmark\" \"Iceland\" \"Switzerland\" ...\n $ Happiness.score                           : num  7821 7636 7557 7512 7415 ...\n $ Whisker.high                              : num  7.89 7.71 7.65 7.59 7.47 7.5 7.45 7.44 7.43 7.28 ...\n $ Whisker.high.1                            : num  7886 7710 7651 7586 7471 ...\n $ Whisker.low                               : num  7.76 7.56 7.46 7.44 7.36 7.31 7.32 7.29 7.3 7.12 ...\n $ Whisker.low.1                             : num  7756 7563 7464 7437 7359 ...\n $ Dystopia..1.83....residual                : chr  \"2518.00\" \"2226.00\" \"2320.00\" \"2153.00\" ...\n $ Explained.by..GDP.per.capita              : chr  \"1892.00\" \"1953.00\" \"1936.00\" \"2026.00\" ...\n $ Explained.by..Social.support              : chr  \"1258.00\" \"1243.00\" \"1320.00\" \"1226.00\" ...\n $ Explained.by..Healthy.life.expectancy     : chr  \"0,775\" \"0,777\" \"0,803\" \"0,822\" ...\n $ Explained.by..Freedom.to.make.life.choices: chr  \"0,736\" \"0,719\" \"0,718\" \"0,677\" ...\n $ Explained.by..Generosity                  : chr  \"0,109\" \"0,188\" \"0,270\" \"0,147\" ...\n $ Explained.by..Perceptions.of.corruption   : chr  \"0,534\" \"0,532\" \"0,191\" \"0,461\" ...\n $ X                                         : num  0.78 NA NA NA NA NA NA NA NA NA ...\n\n\nCode\nstr(primary$Country)\n\n\n chr [1:147] \"Finland\" \"Denmark\" \"Iceland\" \"Switzerland\" \"Netherlands\" ...\n\n\nThe dataset that I have chosen is happiness 2022 dataset, one of Kaggle’s dataset. This dataset gives the happiness rank and happiness score of 147 countries around the world based on 8 factors including GDP per capita, Social support, Health life expectancy, freedom to make life choices, Generosity, Perceptions of corruption and dystopia residual. The higher value of each of these 8 factors means the level of happiness is higher. Dystopia is the opposite of utopia and has the lowest happiness level. Dystopia will be considered as a reference for other countries to show how far they are from being the poorest country regarding happiness level.\nSource of the data: World Happiness Report 2022 use data from the Gallup World Poll surveys from 2019 to 2021. They are based on answers to the main life evaluation question asked in the poll.\nSome of the variable names are not clear enough and I decided to change the name of several of them a little bit. Also, I will remove whisker low and whisker high variables from my dataset because these variables give only the lower and upper confidence interval of happiness score and there is no need to use them for visualization and prediction.\nThe next step is adding another column to the dataset which is continent. I want to work on different continents to discover whether there are different trends for them regarding which factors play a significant role in gaining higher happiness score. Asia, Africa, North America, South America, Europe, and Australia are our six continents in this dataset. Then I moved the position of the continent column to the second column because I think with this position arrange, dataset looks better. Finally, I changed the type of continent variable to factor to be able to work with it easily for visualization."
  },
  {
    "objectID": "posts/finalpart1-1.html#preparation-of-the-data",
    "href": "posts/finalpart1-1.html#preparation-of-the-data",
    "title": "Final Project Part-1",
    "section": "Preparation of the data",
    "text": "Preparation of the data\n\n\nCode\n# Changing the name of columns\ncolnames (primary) <- c(\"Country\", \"Happiness.Rank\", \"Happiness.Score\",\n                          \"Whisker.High\", \"Whisker.Low\", \"Economy\", \"Family\",\n                          \"Life.Expectancy\", \"Freedom\", \"Generosity\",\n                          \"Trust\", \"Dystopia.Residual\")\n\n\n# Country: Name of countries\n# Happiness.Rank: Rank of the country based on the Happiness Score\n# Happiness.Score: Happiness measurement on a scale of 0 to 10\n# Whisker.High: Upper confidence interval of happiness score\n# Whisker.Low: Lower confidence interval of happiness score\n# Economy: The value of all final goods and services produced within a nation in a given year\n# Family: Importance of having a family\n# Life.Expectancy: Importance of health and amount of time prople expect to live\n# Freedom: Importance of freedom in each country\n# Generosity: The quality of being kind and generous\n# Trust: Perception of corruption in a government\n# Dystopia.Residual: Plays as a reference\n\n# Deleting unnecessary columns (Whisker.high and Whisker.low)\n\nprimary <- primary[, -c(4,5)]\n\n\n\n\nCode\nprimary$Continent <- NA\n\nprimary$Continent[which(primary$Country %in% c(\"Israel\", \"United Arab Emirates\", \"Singapore\", \"Thailand\", \"Taiwan Province of China\",\n                                   \"Qatar\", \"Saudi Arabia\", \"Kuwait\", \"Bahrain\", \"Malaysia\", \"Uzbekistan\", \"Japan\",\n                                   \"South Korea\", \"Turkmenistan\", \"Kazakhstan\", \"Turkey\", \"Hong Kong S.A.R., China\", \"Philippines\",\n                                   \"Jordan\", \"China\", \"Pakistan\", \"Indonesia\", \"Azerbaijan\", \"Lebanon\", \"Vietnam\",\n                                   \"Tajikistan\", \"Bhutan\", \"Kyrgyzstan\", \"Nepal\", \"Mongolia\", \"Palestinian Territories\",\n                                   \"Iran\", \"Bangladesh\", \"Myanmar\", \"Iraq\", \"Sri Lanka\", \"Armenia\", \"India\", \"Georgia\",\n                                   \"Cambodia\", \"Afghanistan\", \"Yemen\", \"Syria\"))] <- \"Asia\"\nprimary$Continent[which(primary$Country %in% c(\"Norway\", \"Denmark\", \"Iceland\", \"Switzerland\", \"Finland\",\n                                   \"Netherlands\", \"Sweden\", \"Austria\", \"Ireland\", \"Germany\",\n                                   \"Belgium\", \"Luxembourg\", \"United Kingdom\", \"Czech Republic\",\n                                   \"Malta\", \"France\", \"Spain\", \"Slovakia\", \"Poland\", \"Italy\",\n                                   \"Russia\", \"Lithuania\", \"Latvia\", \"Moldova\", \"Romania\",\n                                   \"Slovenia\", \"North Cyprus\", \"Cyprus\", \"Estonia\", \"Belarus\",\n                                   \"Serbia\", \"Hungary\", \"Croatia\", \"Kosovo\", \"Montenegro\",\n                                   \"Greece\", \"Portugal\", \"Bosnia and Herzegovina\", \"Macedonia\",\n                                   \"Bulgaria\", \"Albania\", \"Ukraine\"))] <- \"Europe\"\nprimary$Continent[which(primary$Country %in% c(\"Canada\", \"Costa Rica\", \"United States\", \"Mexico\",  \n                                   \"Panama\",\"Trinidad and Tobago\", \"El Salvador\", \"Belize\", \"Guatemala\",\n                                   \"Jamaica\", \"Nicaragua\", \"Dominican Republic\", \"Honduras\",\n                                   \"Haiti\"))] <- \"North America\"\nprimary$Continent[which(primary$Country %in% c(\"Chile\", \"Brazil\", \"Argentina\", \"Uruguay\",\n                                   \"Colombia\", \"Ecuador\", \"Bolivia\", \"Peru\",\n                                   \"Paraguay\", \"Venezuela\"))] <- \"South America\"\nprimary$Continent[which(primary$Country %in% c(\"New Zealand\", \"Australia\"))] <- \"Australia\"\nprimary$Continent[which(is.na(primary$Continent))] <- \"Africa\"\n\nview(primary)\n\n# Moving the continent column's position in the dataset to the second column\n\nprimary <- primary %>% select(Country,Continent, everything())\n\n\nError in `select()`:\n! Names repair functions can't return `NA` values.\n\n\nCode\n#Renaming the final dataframe to happy\n\nhappy <- primary\nstr(happy)\n\n\n'data.frame':   147 obs. of  14 variables:\n $ Country          : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Happiness.Rank   : chr  \"Finland\" \"Denmark\" \"Iceland\" \"Switzerland\" ...\n $ Happiness.Score  : num  7821 7636 7557 7512 7415 ...\n $ Economy          : num  7.76 7.56 7.46 7.44 7.36 7.31 7.32 7.29 7.3 7.12 ...\n $ Family           : num  7756 7563 7464 7437 7359 ...\n $ Life.Expectancy  : chr  \"2518.00\" \"2226.00\" \"2320.00\" \"2153.00\" ...\n $ Freedom          : chr  \"1892.00\" \"1953.00\" \"1936.00\" \"2026.00\" ...\n $ Generosity       : chr  \"1258.00\" \"1243.00\" \"1320.00\" \"1226.00\" ...\n $ Trust            : chr  \"0,775\" \"0,777\" \"0,803\" \"0,822\" ...\n $ Dystopia.Residual: chr  \"0,736\" \"0,719\" \"0,718\" \"0,677\" ...\n $ NA               : chr  \"0,109\" \"0,188\" \"0,270\" \"0,147\" ...\n $ NA.1             : chr  \"0,534\" \"0,532\" \"0,191\" \"0,461\" ...\n $ NA.2             : num  0.78 NA NA NA NA NA NA NA NA NA ...\n $ Continent        : chr  \"Africa\" \"Africa\" \"Africa\" \"Africa\" ..."
  },
  {
    "objectID": "posts/FinalPart1.html",
    "href": "posts/FinalPart1.html",
    "title": "finalpart1",
    "section": "",
    "text": "Are Women and Racial minorities underrepresented in STEM fields (Study & Career)? A predictive analysis of the likelihood of STEM careers.\n\n\nWomen are significantly underrepresented in STEM (science, technology, engineering, and mathematics) fields in the USA, making up less than a quarter of those working in STEM occupations (Noonan, [2017](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR13 “Noonan, R. Women in STEM: 2017 update (ESA Issue Brief #06-17). Office of the Chief Economist, Economics and Statistics Administration, U.S. Department of Commerce (November 13, 2017). Retrieved from https://www.commerce.gov/news/fact-sheets/2017/11/women-stem-2017-update\n“); Ong, Smith, & Ko, [2018](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR15”Ong, M., Smith, J. M., & Ko, L. T. (2018). Counterspaces for women of color in STEM higher education: marginal and central spaces for persistence and success. Journal of Research in Science Teaching, 55(2), 206–245. https://doi.org/10.1002/tea.21417\n.”)).\nRepresentation of women of color is even lower, with Hispanic, Asian, and African American women each receiving less than 5% of STEM bachelor's degrees in the USA (Ong et al., [2018](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR15 “Ong, M., Smith, J. M., & Ko, L. T. (2018). Counterspaces for women of color in STEM higher education: marginal and central spaces for persistence and success. Journal of Research in Science Teaching, 55(2), 206–245. https://doi.org/10.1002/tea.21417\n.”);\nBy the time students reach college, women are significantly underrepresented in STEM majors — for instance, only around 21% of engineering majors are women and only around 19% of computer and information science majors are women.https://www.aauw.org/resources/research/the-stem-gap/\nThe fact that women and racial minorities are still discriminated and underrepresented in the STEM in the 21st century while mankind is stepping foot on other planets is a topic to be given a serious thought.\nThe above mentioned articles are my motivation to perform this analysis in addition to the 2011 survey by US Department of Commerce showing that women and racial minorities are underrpresented in stem fields in two ways: They represent a disproportionatly small percentage of STEM degree holders, as well as STEM workers. These reports are linked below:\n\n“Women in STEM: A Gender Gap to Innovation”\n“Education Supports Racial and Ethnic Equality in STEM”\n\nThe goal of this project is to build a model to predict likelihood of working in a STEM (Science, Technology, Engineering, and Math) career based on basic demographics: Age, sex, race, state of origin."
  },
  {
    "objectID": "posts/FinalPart1.html#hypothesis",
    "href": "posts/FinalPart1.html#hypothesis",
    "title": "finalpart1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\nMy hypothesis: Women and Racial minorities are underrepresented in STEM fields.\nThe above mentioned hypothesis has been tested and proved by many researchers and government survey analysis already. Bus i wish to perform this study again by modifying it by developing regression models to resume the likelihood of STEM careers.\n---\n\nLoading the libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\nReading the raw data\n\n\nCode\npop <- read.csv(\"C:/Users/91955/Desktop/603_Fall_2022/ss13pusa.csv\")\nhead(pop)"
  },
  {
    "objectID": "posts/FinalPart1.html#descriptive-statistics",
    "href": "posts/FinalPart1.html#descriptive-statistics",
    "title": "finalpart1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nsummary(pop)\n\n\n      RT               SERIALNO          SPORDER            PUMA      \n Length:1613672     Min.   :      1   Min.   : 1.000   Min.   :  100  \n Class :character   1st Qu.: 372368   1st Qu.: 1.000   1st Qu.:  802  \n Mode  :character   Median : 745780   Median : 2.000   Median : 2000  \n                    Mean   : 746221   Mean   : 2.111   Mean   : 3075  \n                    3rd Qu.:1119428   3rd Qu.: 3.000   3rd Qu.: 3762  \n                    Max.   :1492843   Max.   :20.000   Max.   :12704  \n                                                                      \n       ST            ADJINC            PWGTP             AGEP      \n Min.   : 1.00   Min.   :1007549   Min.   :   1.0   Min.   : 0.00  \n 1st Qu.: 6.00   1st Qu.:1007549   1st Qu.:  54.0   1st Qu.:20.00  \n Median :12.00   Median :1007549   Median :  78.0   Median :41.00  \n Mean   :13.91   Mean   :1007549   Mean   : 101.1   Mean   :40.51  \n 3rd Qu.:21.00   3rd Qu.:1007549   3rd Qu.: 122.0   3rd Qu.:59.00  \n Max.   :28.00   Max.   :1007549   Max.   :1830.0   Max.   :95.00  \n                                                                   \n      CIT            CITWP              COW              DDRS      \n Min.   :1.000   Min.   :1928      Min.   :1.0      Min.   :1.00   \n 1st Qu.:1.000   1st Qu.:1987      1st Qu.:1.0      1st Qu.:2.00   \n Median :1.000   Median :1999      Median :1.0      Median :2.00   \n Mean   :1.474   Mean   :1995      Mean   :2.2      Mean   :1.96   \n 3rd Qu.:1.000   3rd Qu.:2007      3rd Qu.:3.0      3rd Qu.:2.00   \n Max.   :5.000   Max.   :2013      Max.   :9.0      Max.   :2.00   \n                 NA's   :1503773   NA's   :669251   NA's   :85453  \n      DEAR            DEYE            DOUT             DPHY      \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :1.00   \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.00     1st Qu.:2.00   \n Median :2.000   Median :2.000   Median :2.00     Median :2.00   \n Mean   :1.958   Mean   :1.973   Mean   :1.93     Mean   :1.92   \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.00     3rd Qu.:2.00   \n Max.   :2.000   Max.   :2.000   Max.   :2.00     Max.   :2.00   \n                                 NA's   :281475   NA's   :85453  \n      DRAT             DRATX              DREM            ENG         \n Min.   :1.0       Min.   :1.0       Min.   :1.00    Min.   :1.0      \n 1st Qu.:2.0       1st Qu.:2.0       1st Qu.:2.00    1st Qu.:1.0      \n Median :3.0       Median :2.0       Median :2.00    Median :1.0      \n Mean   :3.3       Mean   :1.8       Mean   :1.94    Mean   :1.7      \n 3rd Qu.:5.0       3rd Qu.:2.0       3rd Qu.:2.00    3rd Qu.:2.0      \n Max.   :6.0       Max.   :2.0       Max.   :2.00    Max.   :4.0      \n NA's   :1592578   NA's   :1477285   NA's   :85453   NA's   :1307558  \n      FER               GCL              GCM               GCR         \n Min.   :1.0       Min.   :1        Min.   :1.0       Min.   :1.0      \n 1st Qu.:2.0       1st Qu.:2        1st Qu.:3.0       1st Qu.:1.0      \n Median :2.0       Median :2        Median :4.0       Median :2.0      \n Mean   :1.9       Mean   :2        Mean   :3.7       Mean   :1.6      \n 3rd Qu.:2.0       3rd Qu.:2        3rd Qu.:5.0       3rd Qu.:2.0      \n Max.   :2.0       Max.   :2        Max.   :5.0       Max.   :2.0      \n NA's   :1251923   NA's   :584499   NA's   :1599288   NA's   :1576461  \n     HINS1           HINS2           HINS3           HINS4      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :1.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.466   Mean   :1.861   Mean   :1.805   Mean   :1.822  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :2.000  \n                                                                \n     HINS5           HINS6           HINS7            INTP       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : -6300  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:     0  \n Median :2.000   Median :2.000   Median :2.000   Median :     0  \n Mean   :1.969   Mean   :1.975   Mean   :1.994   Mean   :  2179  \n 3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:     0  \n Max.   :2.000   Max.   :2.000   Max.   :2.000   Max.   :300000  \n                                                 NA's   :281475  \n     JWMNP            JWRIP              JWTR             LANX      \n Min.   :  1.0    Min.   : 1.0      Min.   : 1       Min.   :1.0    \n 1st Qu.: 10.0    1st Qu.: 1.0      1st Qu.: 1       1st Qu.:2.0    \n Median : 20.0    Median : 1.0      Median : 1       Median :2.0    \n Mean   : 26.2    Mean   : 1.2      Mean   : 2       Mean   :1.8    \n 3rd Qu.: 30.0    3rd Qu.: 1.0      3rd Qu.: 1       3rd Qu.:2.0    \n Max.   :167.0    Max.   :10.0      Max.   :12       Max.   :2.0    \n NA's   :943025   NA's   :1004116   NA's   :908560   NA's   :85453  \n      MAR            MARHD            MARHM            MARHT       \n Min.   :1.000   Min.   :1        Min.   :1        Min.   :1.0     \n 1st Qu.:1.000   1st Qu.:2        1st Qu.:2        1st Qu.:1.0     \n Median :3.000   Median :2        Median :2        Median :1.0     \n Mean   :2.961   Mean   :2        Mean   :2        Mean   :1.3     \n 3rd Qu.:5.000   3rd Qu.:2        3rd Qu.:2        3rd Qu.:2.0     \n Max.   :5.000   Max.   :2        Max.   :2        Max.   :3.0     \n                 NA's   :679081   NA's   :679081   NA's   :679081  \n     MARHW            MARHYP            MIG             MIL        \n Min.   :1        Min.   :1933     Min.   :1.000   Min.   :1.0     \n 1st Qu.:2        1st Qu.:1974     1st Qu.:1.000   1st Qu.:4.0     \n Median :2        Median :1989     Median :1.000   Median :4.0     \n Mean   :2        Mean   :1987     Mean   :1.274   Mean   :3.8     \n 3rd Qu.:2        3rd Qu.:2001     3rd Qu.:1.000   3rd Qu.:4.0     \n Max.   :2        Max.   :2013     Max.   :3.000   Max.   :4.0     \n NA's   :679081   NA's   :679081   NA's   :16101   NA's   :323518  \n      MLPA              MLPB             MLPCD              MLPE        \n Min.   :0.0       Min.   :0.0       Min.   :0.0       Min.   :0.0      \n 1st Qu.:0.0       1st Qu.:0.0       1st Qu.:0.0       1st Qu.:0.0      \n Median :0.0       Median :0.0       Median :0.0       Median :0.0      \n Mean   :0.1       Mean   :0.2       Mean   :0.2       Mean   :0.4      \n 3rd Qu.:0.0       3rd Qu.:0.0       3rd Qu.:0.0       3rd Qu.:1.0      \n Max.   :1.0       Max.   :1.0       Max.   :1.0       Max.   :1.0      \n NA's   :1496195   NA's   :1496195   NA's   :1496195   NA's   :1496195  \n     MLPFG              MLPH              MLPI              MLPJ        \n Min.   :0.0       Min.   :0.0       Min.   :0         Min.   :0.0      \n 1st Qu.:0.0       1st Qu.:0.0       1st Qu.:0         1st Qu.:0.0      \n Median :0.0       Median :0.0       Median :0         Median :0.0      \n Mean   :0.2       Mean   :0.1       Mean   :0         Mean   :0.1      \n 3rd Qu.:0.0       3rd Qu.:0.0       3rd Qu.:0         3rd Qu.:0.0      \n Max.   :1.0       Max.   :1.0       Max.   :1         Max.   :1.0      \n NA's   :1496195   NA's   :1496195   NA's   :1496195   NA's   :1496195  \n      MLPK              NWAB             NWAV             NWLA       \n Min.   :0         Min.   :1.00     Min.   :1.00     Min.   :1.00    \n 1st Qu.:0         1st Qu.:2.00     1st Qu.:5.00     1st Qu.:2.00    \n Median :0         Median :3.00     Median :5.00     Median :3.00    \n Mean   :0         Mean   :2.57     Mean   :4.65     Mean   :2.55    \n 3rd Qu.:0         3rd Qu.:3.00     3rd Qu.:5.00     3rd Qu.:3.00    \n Max.   :1         Max.   :3.00     Max.   :5.00     Max.   :3.00    \n NA's   :1496195   NA's   :302343   NA's   :302343   NA's   :302343  \n      NWLK             NWRE             OIP               PAP          \n Min.   :1.00     Min.   :1.00     Min.   :    0.0   Min.   :    0.00  \n 1st Qu.:2.00     1st Qu.:3.00     1st Qu.:    0.0   1st Qu.:    0.00  \n Median :3.00     Median :3.00     Median :    0.0   Median :    0.00  \n Mean   :2.53     Mean   :2.93     Mean   :  674.6   Mean   :   51.37  \n 3rd Qu.:3.00     3rd Qu.:3.00     3rd Qu.:    0.0   3rd Qu.:    0.00  \n Max.   :3.00     Max.   :3.00     Max.   :83000.0   Max.   :30000.00  \n NA's   :302343   NA's   :302343   NA's   :281475    NA's   :281475    \n      RELP             RETP             SCH             SCHG        \n Min.   : 0.000   Min.   :     0   Min.   :1.0     Min.   : 1.0     \n 1st Qu.: 0.000   1st Qu.:     0   1st Qu.:1.0     1st Qu.: 6.0     \n Median : 1.000   Median :     0   Median :1.0     Median :11.0     \n Mean   : 2.605   Mean   :  2383   Mean   :1.3     Mean   : 9.8     \n 3rd Qu.: 2.000   3rd Qu.:     0   3rd Qu.:2.0     3rd Qu.:15.0     \n Max.   :17.000   Max.   :178000   Max.   :3.0     Max.   :16.0     \n                  NA's   :281475   NA's   :49494   NA's   :1211500  \n      SCHL            SEMP             SEX             SSIP        \n Min.   : 1.00   Min.   : -7500   Min.   :1.000   Min.   :    0.0  \n 1st Qu.:14.00   1st Qu.:     0   1st Qu.:1.000   1st Qu.:    0.0  \n Median :17.00   Median :     0   Median :2.000   Median :    0.0  \n Mean   :15.85   Mean   :  1810   Mean   :1.511   Mean   :  273.1  \n 3rd Qu.:20.00   3rd Qu.:     0   3rd Qu.:2.000   3rd Qu.:    0.0  \n Max.   :24.00   Max.   :525000   Max.   :2.000   Max.   :30000.0  \n NA's   :49494   NA's   :281475                   NA's   :281475   \n      SSP              WAGP             WKHP             WKL        \n Min.   :    0    Min.   :     0   Min.   : 1.0     Min.   :1.00    \n 1st Qu.:    0    1st Qu.:     0   1st Qu.:32.0     1st Qu.:1.00    \n Median :    0    Median :  5000   Median :40.0     Median :1.00    \n Mean   : 2925    Mean   : 25569   Mean   :37.8     Mean   :1.67    \n 3rd Qu.:    0    3rd Qu.: 36000   3rd Qu.:41.0     3rd Qu.:3.00    \n Max.   :50000    Max.   :660000   Max.   :99.0     Max.   :3.00    \n NA's   :281475   NA's   :281475   NA's   :802274   NA's   :302343  \n      WKW              WRK              YOEP              ANC      \n Min.   :1.0      Min.   :1.0      Min.   :1921      Min.   :1.00  \n 1st Qu.:1.0      1st Qu.:1.0      1st Qu.:1980      1st Qu.:1.00  \n Median :1.0      Median :1.0      Median :1992      Median :1.00  \n Mean   :1.9      Mean   :1.4      Mean   :1990      Mean   :1.71  \n 3rd Qu.:3.0      3rd Qu.:2.0      3rd Qu.:2002      3rd Qu.:2.00  \n Max.   :6.0      Max.   :2.0      Max.   :2013      Max.   :4.00  \n NA's   :802274   NA's   :431414   NA's   :1382392                 \n     ANC1P           ANC2P           DECADE             DIS       \n Min.   :  1.0   Min.   :  1.0   Min.   :1.0       Min.   :1.000  \n 1st Qu.: 50.0   1st Qu.:939.0   1st Qu.:5.0       1st Qu.:2.000  \n Median :226.0   Median :999.0   Median :6.0       Median :2.000  \n Mean   :465.3   Mean   :796.2   Mean   :5.5       Mean   :1.853  \n 3rd Qu.:924.0   3rd Qu.:999.0   3rd Qu.:7.0       3rd Qu.:2.000  \n Max.   :999.0   Max.   :999.0   Max.   :7.0       Max.   :2.000  \n                                 NA's   :1382392                  \n    DRIVESP             ESP               ESR             FOD1P        \n Min.   :1.0       Min.   :1.0       Min.   :1.00     Min.   :1100     \n 1st Qu.:1.0       1st Qu.:1.0       1st Qu.:1.00     1st Qu.:2405     \n Median :1.0       Median :2.0       Median :1.00     Median :5007     \n Mean   :1.2       Mean   :3.1       Mean   :3.12     Mean   :4320     \n 3rd Qu.:1.0       3rd Qu.:6.0       3rd Qu.:6.00     3rd Qu.:6107     \n Max.   :6.0       Max.   :8.0       Max.   :6.00     Max.   :6403     \n NA's   :1004116   NA's   :1288953   NA's   :302343   NA's   :1257880  \n     FOD2P             HICOV           HISP             INDP       \n Min.   :1100      Min.   :1.00   Min.   : 1.000   Min.   : 170    \n 1st Qu.:2409      1st Qu.:1.00   1st Qu.: 1.000   1st Qu.:4970    \n Median :5007      Median :1.00   Median : 1.000   Median :7390    \n Mean   :4344      Mean   :1.13   Mean   : 1.576   Mean   :6411    \n 3rd Qu.:6006      3rd Qu.:1.00   3rd Qu.: 1.000   3rd Qu.:8270    \n Max.   :6403      Max.   :2.00   Max.   :24.000   Max.   :9920    \n NA's   :1577632                                   NA's   :669251  \n      JWAP             JWDP             LANP            MIGPUMA       \n Min.   :  1.0    Min.   :  1.0    Min.   :601.0     Min.   :    1    \n 1st Qu.: 81.0    1st Qu.: 37.0    1st Qu.:625.0     1st Qu.:  500    \n Median : 92.0    Median : 49.0    Median :625.0     Median : 1800    \n Mean   :103.6    Mean   : 54.7    Mean   :655.8     Mean   : 3231    \n 3rd Qu.:107.0    3rd Qu.: 63.0    3rd Qu.:671.0     3rd Qu.: 3700    \n Max.   :285.0    Max.   :150.0    Max.   :994.0     Max.   :70100    \n NA's   :943025   NA's   :943025   NA's   :1307558   NA's   :1389943  \n     MIGSP              MSP            NAICSP             NATIVITY   \n Min.   :  1.0     Min.   :1.00     Length:1613672     Min.   :1.00  \n 1st Qu.:  6.0     1st Qu.:1.00     Class :character   1st Qu.:1.00  \n Median : 13.0     Median :2.00     Mode  :character   Median :1.00  \n Mean   : 25.9     Mean   :3.05                        Mean   :1.13  \n 3rd Qu.: 25.0     3rd Qu.:6.00                        3rd Qu.:1.00  \n Max.   :555.0     Max.   :6.00                        Max.   :2.00  \n NA's   :1389943   NA's   :281475                                    \n      NOP                OC              OCCP             PAOC       \n Min.   :1.0       Min.   :0.0000   Min.   :  10     Min.   :1.0     \n 1st Qu.:1.0       1st Qu.:0.0000   1st Qu.:2310     1st Qu.:3.0     \n Median :2.0       Median :0.0000   Median :4500     Median :4.0     \n Mean   :3.3       Mean   :0.1844   Mean   :4345     Mean   :3.5     \n 3rd Qu.:6.0       3rd Qu.:0.0000   3rd Qu.:5700     3rd Qu.:4.0     \n Max.   :8.0       Max.   :1.0000   Max.   :9920     Max.   :4.0     \n NA's   :1289567                    NA's   :669251   NA's   :965233  \n     PERNP             PINCP              POBP            POVPIP     \n Min.   :  -7500   Min.   : -11600   Min.   :  1.00   Min.   :  0.0  \n 1st Qu.:      0   1st Qu.:   6400   1st Qu.: 12.00   1st Qu.:157.0  \n Median :   9200   Median :  20300   Median : 21.00   Median :306.0  \n Mean   :  27814   Mean   :  35865   Mean   : 53.83   Mean   :303.8  \n 3rd Qu.:  39000   3rd Qu.:  45300   3rd Qu.: 36.00   3rd Qu.:501.0  \n Max.   :1019000   Max.   :1272000   Max.   :554.00   Max.   :501.0  \n NA's   :302343    NA's   :281475                     NA's   :66398  \n    POWPUMA           POWSP           PRIVCOV          PUBCOV     \n Min.   :    1    Min.   :  1.0    Min.   :1.000   Min.   :1.000  \n 1st Qu.:  600    1st Qu.:  6.0    1st Qu.:1.000   1st Qu.:1.000  \n Median : 1900    Median : 13.0    Median :1.000   Median :2.000  \n Mean   : 3052    Mean   : 14.7    Mean   :1.338   Mean   :1.659  \n 3rd Qu.: 3700    3rd Qu.: 21.0    3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :70100    Max.   :555.0    Max.   :2.000   Max.   :2.000  \n NA's   :908560   NA's   :908560                                  \n     QTRBIR          RAC1P           RAC2P            RAC3P        \n Min.   :1.000   Min.   :1.000   Min.   : 1.000   Min.   :  1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:  1.000  \n Median :3.000   Median :1.000   Median : 1.000   Median :  1.000  \n Mean   :2.517   Mean   :1.975   Mean   : 8.877   Mean   :  3.022  \n 3rd Qu.:4.000   3rd Qu.:2.000   3rd Qu.: 2.000   3rd Qu.:  2.000  \n Max.   :4.000   Max.   :9.000   Max.   :68.000   Max.   :100.000  \n                                                                   \n    RACAIAN            RACASN            RACBLK           RACNH         \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000000  \n Median :0.00000   Median :0.00000   Median :0.0000   Median :0.000000  \n Mean   :0.01852   Mean   :0.06935   Mean   :0.1211   Mean   :0.002961  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.000000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.0000   Max.   :1.000000  \n                                                                        \n     RACNUM          RACPI              RACSOR            RACWHT      \n Min.   :1.000   Min.   :0.000000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:1.000   1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:1.0000  \n Median :1.000   Median :0.000000   Median :0.00000   Median :1.0000  \n Mean   :1.033   Mean   :0.002374   Mean   :0.04773   Mean   :0.7712  \n 3rd Qu.:1.000   3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :5.000   Max.   :1.000000   Max.   :1.00000   Max.   :1.0000  \n                                                                      \n       RC            SCIENGP          SCIENGRLP            SFN         \n Min.   :0.0000   Min.   :1.0       Min.   :1.0       Min.   :1        \n 1st Qu.:0.0000   1st Qu.:1.0       1st Qu.:2.0       1st Qu.:1        \n Median :0.0000   Median :2.0       Median :2.0       Median :1        \n Mean   :0.2079   Mean   :1.6       Mean   :1.9       Mean   :1        \n 3rd Qu.:0.0000   3rd Qu.:2.0       3rd Qu.:2.0       3rd Qu.:1        \n Max.   :1.0000   Max.   :2.0       Max.   :2.0       Max.   :3        \n                  NA's   :1257880   NA's   :1257880   NA's   :1558638  \n      SFR              SOCP                VPS               WAOB      \n Min.   :1.0       Length:1613672     Min.   : 1.0      Min.   :1.000  \n 1st Qu.:3.0       Class :character   1st Qu.: 5.0      1st Qu.:1.000  \n Median :3.0       Mode  :character   Median : 6.0      Median :1.000  \n Mean   :3.6                          Mean   : 7.3      Mean   :1.401  \n 3rd Qu.:5.0                          3rd Qu.:11.0      3rd Qu.:1.000  \n Max.   :6.0                          Max.   :15.0      Max.   :8.000  \n NA's   :1558638                      NA's   :1496195                  \n     FAGEP             FANCP       FCITP             FCITWP       \n Min.   :0.00000   Min.   :0   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0   Median :0.00000   Median :0.00000  \n Mean   :0.01298   Mean   :0   Mean   :0.05729   Mean   :0.01474  \n 3rd Qu.:0.00000   3rd Qu.:0   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :0   Max.   :1.00000   Max.   :1.00000  \n                                                                  \n     FCOWP             FDDRSP            FDEARP            FDEYEP       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.06808   Mean   :0.07084   Mean   :0.06455   Mean   :0.06804  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n                                                                        \n     FDISP             FDOUTP            FDPHYP            FDRATP         \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.0000000  \n Mean   :0.09347   Mean   :0.05903   Mean   :0.07067   Mean   :0.0002504  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000000  \n                                                                          \n    FDRATXP            FDREMP            FENGP             FESRP        \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.00573   Mean   :0.07042   Mean   :0.01378   Mean   :0.07072  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n                                                                        \n     FFERP             FFODP             FGCLP              FGCMP         \n Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.000000  \n Median :0.00000   Median :0.00000   Median :0.000000   Median :0.000000  \n Mean   :0.01653   Mean   :0.02661   Mean   :0.006617   Mean   :0.001508  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.000000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.000000   Max.   :1.000000  \n                                                                          \n     FGCRP             FHINS1P          FHINS2P          FHINS3C       \n Min.   :0.000000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0      \n 1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0      \n Median :0.000000   Median :0.0000   Median :0.0000   Median :0.0      \n Mean   :0.003639   Mean   :0.1018   Mean   :0.1111   Mean   :0.1      \n 3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0      \n Max.   :1.000000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0      \n                                                      NA's   :1299053  \n    FHINS3P           FHINS4C           FHINS4P          FHINS5C       \n Min.   :0.00000   Min.   :0.0       Min.   :0.0000   Min.   :0        \n 1st Qu.:0.00000   1st Qu.:0.0       1st Qu.:0.0000   1st Qu.:0        \n Median :0.00000   Median :0.0       Median :0.0000   Median :0        \n Mean   :0.08933   Mean   :0.1       Mean   :0.1216   Mean   :0        \n 3rd Qu.:0.00000   3rd Qu.:0.0       3rd Qu.:0.0000   3rd Qu.:0        \n Max.   :1.00000   Max.   :1.0       Max.   :1.0000   Max.   :1        \n                   NA's   :1326192                    NA's   :1563658  \n    FHINS5P          FHINS6P          FHINS7P           FHISP        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.1258   Mean   :0.1247   Mean   :0.1308   Mean   :0.02567  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n                                                                     \n     FINDP             FINTP           FJWDP             FJWMNP       \n Min.   :0.00000   Min.   :0.000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.000   Median :0.00000   Median :0.00000  \n Mean   :0.07119   Mean   :0.109   Mean   :0.07863   Mean   :0.05503  \n 3rd Qu.:0.00000   3rd Qu.:0.000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.000   Max.   :1.00000   Max.   :1.00000  \n                                                                      \n     FJWRIP            FJWTRP            FLANP             FLANXP       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.04005   Mean   :0.04065   Mean   :0.01637   Mean   :0.06223  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n                                                                        \n    FMARHDP          FMARHMP          FMARHTP          FMARHWP       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.0436   Mean   :0.0381   Mean   :0.0464   Mean   :0.04388  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n                                                                     \n    FMARHYP            FMARP             FMIGP            FMIGSP       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.0000   Median :0.00000  \n Mean   :0.07043   Mean   :0.04516   Mean   :0.0734   Mean   :0.01839  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.0000   Max.   :1.00000  \n                                                                       \n     FMILPP             FMILSP            FOCCP              FOIP        \n Min.   :0.000000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.000000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.000000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.006952   Mean   :0.05872   Mean   :0.07313   Mean   :0.09377  \n 3rd Qu.:0.000000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.000000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n                                                                         \n      FPAP            FPERNP           FPINCP           FPOBP        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.0919   Mean   :0.1476   Mean   :0.1964   Mean   :0.09454  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n                                                                     \n     FPOWSP          FPRIVCOVP         FPUBCOVP          FRACP        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.0000   Median :0.00000  \n Mean   :0.04919   Mean   :0.1363   Mean   :0.1382   Mean   :0.01852  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n                                                                      \n     FRELP             FRETP             FSCHGP            FSCHLP      \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.0000  \n Mean   :0.01029   Mean   :0.09726   Mean   :0.02562   Mean   :0.0822  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  \n                                                                       \n     FSCHP             FSEMP            FSEXP               FSSIP        \n Min.   :0.00000   Min.   :0.0000   Min.   :0.0000000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.0000000   Median :0.00000  \n Mean   :0.06372   Mean   :0.0821   Mean   :0.0006203   Mean   :0.09111  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.0000   Max.   :1.0000000   Max.   :1.00000  \n                                                                         \n      FSSP            FWAGP            FWKHP             FWKLP        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.0000   Median :0.00000   Median :0.00000  \n Mean   :0.1075   Mean   :0.1423   Mean   :0.05616   Mean   :0.08091  \n 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.00000  \n                                                                      \n     FWKWP             FWRKP              FYOEP            pwgtp1      \n Min.   :0.00000   Min.   :0.000000   Min.   :0.0000   Min.   : -38.0  \n 1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:  34.0  \n Median :0.00000   Median :0.000000   Median :0.0000   Median :  73.0  \n Mean   :0.05191   Mean   :0.001824   Mean   :0.0186   Mean   : 101.1  \n 3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.: 129.0  \n Max.   :1.00000   Max.   :1.000000   Max.   :1.0000   Max.   :2514.0  \n                                                                       \n     pwgtp2           pwgtp3           pwgtp4           pwgtp5      \n Min.   :  -8.0   Min.   : -46.0   Min.   : -96.0   Min.   :-257.0  \n 1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2420.0   Max.   :2366.0   Max.   :3026.0   Max.   :2024.0  \n                                                                    \n     pwgtp6           pwgtp7           pwgtp8           pwgtp9      \n Min.   : -20.0   Min.   : -74.0   Min.   : -49.0   Min.   : -95.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  33.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  72.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2148.0   Max.   :2468.0   Max.   :3107.0   Max.   :3101.0  \n                                                                    \n    pwgtp10          pwgtp11          pwgtp12          pwgtp13      \n Min.   :-112.0   Min.   : -18.0   Min.   :  -4.0   Min.   : -40.0  \n 1st Qu.:  33.0   1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  36.0  \n Median :  72.0   Median :  73.0   Median :  73.0   Median :  74.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2987.0   Max.   :2372.0   Max.   :2854.0   Max.   :2349.0  \n                                                                    \n    pwgtp14          pwgtp15          pwgtp16          pwgtp17      \n Min.   : -49.0   Min.   :   0.0   Min.   : -23.0   Min.   :-248.0  \n 1st Qu.:  35.0   1st Qu.:  36.0   1st Qu.:  35.0   1st Qu.:  35.0  \n Median :  73.0   Median :  74.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 128.0   3rd Qu.: 128.0  \n Max.   :3003.0   Max.   :2838.0   Max.   :2471.0   Max.   :2502.0  \n                                                                    \n    pwgtp18          pwgtp19          pwgtp20          pwgtp21      \n Min.   :-107.0   Min.   : -45.0   Min.   :-132.0   Min.   :   0.0  \n 1st Qu.:  35.0   1st Qu.:  37.0   1st Qu.:  34.0   1st Qu.:  36.0  \n Median :  73.0   Median :  74.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2462.0   Max.   :2198.0   Max.   :3075.0   Max.   :2967.0  \n                                                                    \n    pwgtp22          pwgtp23          pwgtp24          pwgtp25      \n Min.   : -90.0   Min.   :-244.0   Min.   : -73.0   Min.   : -33.0  \n 1st Qu.:  35.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  35.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2782.0   Max.   :2258.0   Max.   :2640.0   Max.   :2452.0  \n                                                                    \n    pwgtp26          pwgtp27          pwgtp28          pwgtp29      \n Min.   : -71.0   Min.   : -50.0   Min.   : -85.0   Min.   :   0.0  \n 1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2703.0   Max.   :3022.0   Max.   :2164.0   Max.   :2397.0  \n                                                                    \n    pwgtp30          pwgtp31          pwgtp32          pwgtp33      \n Min.   : -24.0   Min.   :-261.0   Min.   :  -7.0   Min.   : -40.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  35.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2378.0   Max.   :2440.0   Max.   :3069.0   Max.   :2354.0  \n                                                                    \n    pwgtp34          pwgtp35          pwgtp36          pwgtp37      \n Min.   : -17.0   Min.   : -28.0   Min.   :  -5.0   Min.   : -38.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2810.0   Max.   :2226.0   Max.   :2541.0   Max.   :2794.0  \n                                                                    \n    pwgtp38          pwgtp39          pwgtp40          pwgtp41      \n Min.   :-341.0   Min.   : -52.0   Min.   : -66.0   Min.   :-196.0  \n 1st Qu.:  35.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2278.0   Max.   :3088.0   Max.   :2486.0   Max.   :2237.0  \n                                                                    \n    pwgtp42          pwgtp43          pwgtp44          pwgtp45      \n Min.   : -12.0   Min.   : -47.0   Min.   : -31.0   Min.   : -27.0  \n 1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2414.0   Max.   :2551.0   Max.   :3238.0   Max.   :2381.0  \n                                                                    \n    pwgtp46          pwgtp47          pwgtp48          pwgtp49      \n Min.   :   0.0   Min.   :-358.0   Min.   :-293.0   Min.   :-119.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2782.0   Max.   :2680.0   Max.   :3055.0   Max.   :2930.0  \n                                                                    \n    pwgtp50          pwgtp51          pwgtp52          pwgtp53      \n Min.   : -73.0   Min.   : -22.0   Min.   : -79.0   Min.   : -35.0  \n 1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  36.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2903.0   Max.   :2340.0   Max.   :2428.0   Max.   :2485.0  \n                                                                    \n    pwgtp54          pwgtp55          pwgtp56          pwgtp57      \n Min.   :-202.0   Min.   :  -7.0   Min.   : -21.0   Min.   : -31.0  \n 1st Qu.:  35.0   1st Qu.:  36.0   1st Qu.:  35.0   1st Qu.:  35.0  \n Median :  73.0   Median :  74.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 128.0   3rd Qu.: 128.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :3061.0   Max.   :2507.0   Max.   :2697.0   Max.   :2467.0  \n                                                                    \n    pwgtp58          pwgtp59          pwgtp60          pwgtp61      \n Min.   :  -3.0   Min.   : -37.0   Min.   :  -5.0   Min.   :   0.0  \n 1st Qu.:  35.0   1st Qu.:  36.0   1st Qu.:  34.0   1st Qu.:  36.0  \n Median :  73.0   Median :  74.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2441.0   Max.   :2419.0   Max.   :3032.0   Max.   :3263.0  \n                                                                    \n    pwgtp62          pwgtp63          pwgtp64          pwgtp65      \n Min.   : -83.0   Min.   : -49.0   Min.   :-116.0   Min.   : -21.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  35.0   1st Qu.:  35.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2428.0   Max.   :2719.0   Max.   :2468.0   Max.   :2959.0  \n                                                                    \n    pwgtp66          pwgtp67          pwgtp68          pwgtp69      \n Min.   :-163.0   Min.   : -13.0   Min.   :   0.0   Min.   : -19.0  \n 1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0   1st Qu.:  34.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2339.0   Max.   :2935.0   Max.   :2695.0   Max.   :2310.0  \n                                                                    \n    pwgtp70          pwgtp71          pwgtp72          pwgtp73      \n Min.   :  -7.0   Min.   : -20.0   Min.   :-494.0   Min.   : -14.0  \n 1st Qu.:  33.0   1st Qu.:  34.0   1st Qu.:  33.0   1st Qu.:  35.0  \n Median :  72.0   Median :  73.0   Median :  72.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 128.0  \n Max.   :2442.0   Max.   :3132.0   Max.   :3020.0   Max.   :2374.0  \n                                                                    \n    pwgtp74          pwgtp75          pwgtp76          pwgtp77      \n Min.   : -93.0   Min.   : -10.0   Min.   :   0.0   Min.   :  -4.0  \n 1st Qu.:  34.0   1st Qu.:  35.0   1st Qu.:  35.0   1st Qu.:  35.0  \n Median :  73.0   Median :  73.0   Median :  73.0   Median :  73.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0   3rd Qu.: 129.0  \n Max.   :2347.0   Max.   :2773.0   Max.   :2499.0   Max.   :2313.0  \n                                                                    \n    pwgtp78          pwgtp79          pwgtp80      \n Min.   : -26.0   Min.   :   0.0   Min.   : -22.0  \n 1st Qu.:  35.0   1st Qu.:  36.0   1st Qu.:  33.0  \n Median :  73.0   Median :  73.0   Median :  72.0  \n Mean   : 101.1   Mean   : 101.1   Mean   : 101.1  \n 3rd Qu.: 129.0   3rd Qu.: 128.0   3rd Qu.: 129.0  \n Max.   :2795.0   Max.   :2959.0   Max.   :3018.0  \n                                                   \n\n\nCode\nnrow(pop)\n\n\n[1] 1613672\n\n\nCode\nncol(pop)\n\n\n[1] 283\n\n\nCode\nglimpse(pop)\n\n\nRows: 1,613,672\nColumns: 283\n$ RT        <chr> \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", …\n$ SERIALNO  <int> 84, 154, 154, 154, 154, 156, 160, 160, 160, 231, 286, 312, 3…\n$ SPORDER   <int> 1, 1, 2, 3, 4, 1, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, …\n$ PUMA      <int> 2600, 2500, 2500, 2500, 2500, 1700, 2200, 2200, 2200, 2400, …\n$ ST        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ADJINC    <int> 1007549, 1007549, 1007549, 1007549, 1007549, 1007549, 100754…\n$ PWGTP     <int> 65, 51, 62, 232, 97, 449, 16, 30, 7, 52, 77, 46, 33, 37, 42,…\n$ AGEP      <int> 19, 55, 56, 21, 21, 63, 61, 20, 12, 78, 81, 59, 56, 70, 71, …\n$ CIT       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, …\n$ CITWP     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1996, 19…\n$ COW       <int> NA, 1, 6, NA, NA, 3, NA, 1, NA, 2, NA, 1, 1, NA, NA, NA, 4, …\n$ DDRS      <int> 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ DEAR      <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ DEYE      <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ DOUT      <int> 2, 2, 2, 2, 1, 2, 1, 2, NA, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ DPHY      <int> 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ DRAT      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ DRATX     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ DREM      <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ENG       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ FER       <int> 2, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ GCL       <int> NA, 2, 2, NA, NA, 2, 1, NA, NA, 2, 2, 2, 2, 2, 2, 2, 2, 2, N…\n$ GCM       <int> NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ GCR       <int> NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HINS1     <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, …\n$ HINS2     <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, …\n$ HINS3     <int> 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ HINS4     <int> 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ HINS5     <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ HINS6     <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ HINS7     <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ INTP      <int> 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ JWMNP     <int> NA, 30, NA, NA, NA, 15, NA, NA, NA, NA, NA, 10, 45, NA, NA, …\n$ JWRIP     <int> NA, 1, NA, NA, NA, 1, NA, NA, NA, NA, NA, 2, 2, NA, NA, NA, …\n$ JWTR      <int> NA, 1, 11, NA, NA, 1, NA, NA, NA, NA, NA, 1, 1, NA, NA, NA, …\n$ LANX      <int> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ MAR       <int> 5, 1, 1, 5, 5, 3, 4, 5, 5, 2, 2, 1, 1, 1, 1, 5, 1, 1, 5, 1, …\n$ MARHD     <int> NA, 2, 2, NA, NA, 2, 2, NA, NA, 2, 2, 2, 2, 2, 2, NA, 2, 2, …\n$ MARHM     <int> NA, 2, 2, NA, NA, 2, 2, NA, NA, 2, 2, 2, 2, 2, 2, NA, 2, 2, …\n$ MARHT     <int> NA, 1, 1, NA, NA, 1, 2, NA, NA, 2, 1, 2, 2, 1, 1, NA, 2, 2, …\n$ MARHW     <int> NA, 2, 2, NA, NA, 2, 2, NA, NA, 2, 2, 2, 2, 2, 2, NA, 2, 2, …\n$ MARHYP    <int> NA, 1990, 1990, NA, NA, 1972, 1975, NA, NA, 1970, 1950, 1985…\n$ MIG       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, …\n$ MIL       <int> 4, 4, 4, 4, 4, 4, 4, 4, NA, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ MLPA      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPB      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPCD     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPE      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPFG     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPH      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPI      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPJ      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MLPK      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ NWAB      <int> 2, 3, 3, 2, 2, 3, 2, 2, NA, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2,…\n$ NWAV      <int> 5, 5, 5, 5, 5, 5, 5, 1, NA, 3, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ NWLA      <int> 2, 3, 3, 2, 2, 3, 2, 2, NA, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2,…\n$ NWLK      <int> 2, 3, 3, 2, 2, 3, 2, 1, NA, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2,…\n$ NWRE      <int> 3, 3, 3, 3, 3, 3, 3, 3, NA, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ OIP       <int> 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PAP       <int> 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ RELP      <int> 17, 0, 1, 2, 2, 0, 0, 7, 7, 0, 0, 0, 1, 0, 1, 0, 0, 1, 17, 0…\n$ RETP      <int> 0, 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 20900, 4300, 103000,…\n$ SCH       <int> 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, …\n$ SCHG      <int> 15, NA, NA, 15, 15, NA, NA, NA, 9, NA, NA, NA, NA, NA, NA, N…\n$ SCHL      <int> 19, 20, 16, 19, 19, 21, 14, 16, 9, 1, 10, 16, 18, 17, 17, 19…\n$ SEMP      <int> 0, 0, 99000, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ SEX       <int> 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, …\n$ SSIP      <int> 0, 0, 0, 0, 0, 0, 0, 0, NA, 3900, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ SSP       <int> 0, 0, 0, 0, 0, 930, 10300, 0, NA, 0, 5400, 0, 0, 18600, 8800…\n$ WAGP      <int> 0, 52000, 0, 0, 0, 39000, 0, 1100, NA, 0, 0, 90000, 46000, 0…\n$ WKHP      <int> NA, 40, 40, NA, NA, 40, NA, 15, NA, NA, NA, 48, 40, NA, NA, …\n$ WKL       <int> 3, 1, 1, 3, 3, 1, 3, 1, NA, 2, 3, 1, 1, 3, 3, 3, 1, 1, 1, 3,…\n$ WKW       <int> NA, 1, 1, NA, NA, 1, NA, 6, NA, NA, NA, 1, 1, NA, NA, NA, 1,…\n$ WRK       <int> 2, 1, 1, 2, 2, 1, 2, 2, NA, 2, NA, 1, 1, 2, 2, 2, 1, 1, 1, 2…\n$ YOEP      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1973, 19…\n$ ANC       <int> 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ ANC1P     <int> 999, 902, 902, 902, 902, 902, 917, 917, 917, 902, 999, 924, …\n$ ANC2P     <int> 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, …\n$ DECADE    <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4, 4, NA…\n$ DIS       <int> 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ DRIVESP   <int> NA, 1, NA, NA, NA, 1, NA, NA, NA, NA, NA, 2, 2, NA, NA, NA, …\n$ ESP       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ESR       <int> 6, 1, 1, 6, 6, 1, 6, 3, NA, 6, 6, 1, 1, 6, 6, 6, 1, 1, 1, 6,…\n$ FOD1P     <int> NA, NA, NA, NA, NA, 6107, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ FOD2P     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ HICOV     <int> 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ HISP      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ INDP      <int> NA, 5380, 8880, NA, NA, 7860, NA, 770, NA, 1880, NA, 6380, 7…\n$ JWAP      <int> NA, 100, NA, NA, NA, 109, NA, NA, NA, NA, NA, 72, 79, NA, NA…\n$ JWDP      <int> NA, 55, NA, NA, NA, 67, NA, NA, NA, NA, NA, 31, 31, NA, NA, …\n$ LANP      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MIGPUMA   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MIGSP     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MSP       <int> 6, 1, 1, 6, 6, 4, 5, 6, NA, 3, 3, 1, 1, 1, 1, 6, 1, 1, 6, 1,…\n$ NAICSP    <chr> \"\", \"45211\", \"8114\", \"\", \"\", \"6111\", \"\", \"23\", \"\", \"32221\", …\n$ NATIVITY  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, …\n$ NOP       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ OC        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ OCCP      <int> NA, 4700, 7240, NA, NA, 2310, NA, 6260, NA, 8740, NA, 5510, …\n$ PAOC      <int> NA, 4, NA, NA, 4, 4, 4, NA, NA, 4, 4, NA, 4, NA, 4, 4, NA, 4…\n$ PERNP     <int> 0, 52000, 99000, 0, 0, 39000, 0, 1100, NA, 0, 0, 90000, 4600…\n$ PINCP     <int> 0, 52000, 99000, 0, 0, 39930, 10300, 1100, NA, 3900, 5400, 9…\n$ POBP      <int> 28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47, 21, 139, 139, 1, 1, 17…\n$ POVPIP    <int> NA, 501, 501, 501, 501, 330, 61, 61, 61, 35, 48, 501, 501, 3…\n$ POWPUMA   <int> NA, 2500, 2500, NA, NA, 1700, NA, NA, NA, NA, NA, 1300, 1300…\n$ POWSP     <int> NA, 1, 1, NA, NA, 1, NA, NA, NA, NA, NA, 1, 1, NA, NA, NA, 1…\n$ PRIVCOV   <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, …\n$ PUBCOV    <int> 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, …\n$ QTRBIR    <int> 1, 1, 4, 4, 4, 3, 1, 4, 3, 1, 4, 2, 3, 2, 2, 4, 1, 2, 3, 4, …\n$ RAC1P     <int> 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, …\n$ RAC2P     <int> 1, 2, 2, 2, 2, 2, 9, 9, 9, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, …\n$ RAC3P     <int> 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, …\n$ RACAIAN   <int> 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ RACASN    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ RACBLK    <int> 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, …\n$ RACNH     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ RACNUM    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ RACPI     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ RACSOR    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ RACWHT    <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, …\n$ RC        <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ SCIENGP   <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SCIENGRLP <int> NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SFN       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ SFR       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ SOCP      <chr> \"\", \"411011\", \"493050\", \"\", \"\", \"252020\", \"\", \"472061\", \"\", …\n$ VPS       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ WAOB      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, …\n$ FAGEP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FANCP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FCITP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FCITWP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FCOWP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDDRSP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDEARP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDEYEP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDISP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDOUTP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDPHYP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDRATP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDRATXP   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FDREMP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FENGP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FESRP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FFERP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FFODP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FGCLP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FGCMP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FGCRP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FHINS1P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FHINS2P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FHINS3C   <int> NA, NA, NA, NA, NA, NA, 0, NA, NA, 0, 0, NA, NA, 0, 0, 0, 0,…\n$ FHINS3P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FHINS4C   <int> NA, NA, NA, NA, NA, NA, NA, NA, 0, 1, NA, NA, NA, NA, NA, NA…\n$ FHINS4P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FHINS5C   <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ FHINS5P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FHINS6P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FHINS7P   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FHISP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FINDP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FINTP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FJWDP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ FJWMNP    <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ FJWRIP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ FJWTRP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ FLANP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FLANXP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARHDP   <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARHMP   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARHTP   <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARHWP   <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARHYP   <int> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMARP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMIGP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMIGSP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMILPP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FMILSP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FOCCP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FOIP      <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FPAP      <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FPERNP    <int> 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ FPINCP    <int> 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ FPOBP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FPOWSP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ FPRIVCOVP <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FPUBCOVP  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ FRACP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ FRELP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FRETP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSCHGP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSCHLP    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSCHP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSEMP     <int> 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSEXP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSSIP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FSSP      <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FWAGP     <int> 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ FWKHP     <int> 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FWKLP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FWKWP     <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FWRKP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FYOEP     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ pwgtp1    <int> 5, 45, 66, 244, 106, 481, 21, 37, 8, 74, 84, 83, 69, 10, 9, …\n$ pwgtp2    <int> 127, 51, 67, 211, 97, 575, 15, 29, 5, 49, 69, 58, 49, 13, 13…\n$ pwgtp3    <int> 65, 53, 60, 224, 87, 807, 6, 10, 2, 44, 20, 16, 12, 34, 44, …\n$ pwgtp4    <int> 66, 50, 66, 187, 92, 739, 20, 37, 8, 13, 119, 79, 63, 65, 80…\n$ pwgtp5    <int> 69, 100, 108, 368, 162, 153, 29, 69, 20, 13, 91, 13, 9, 15, …\n$ pwgtp6    <int> 5, 79, 106, 324, 146, 173, 16, 27, 7, 102, 78, 43, 29, 86, 8…\n$ pwgtp7    <int> 120, 78, 102, 335, 184, 722, 5, 7, 1, 80, 20, 48, 40, 34, 41…\n$ pwgtp8    <int> 66, 50, 63, 239, 110, 455, 24, 21, 11, 20, 76, 49, 39, 32, 3…\n$ pwgtp9    <int> 63, 19, 21, 63, 30, 158, 4, 12, 1, 59, 73, 52, 40, 45, 43, 8…\n$ pwgtp10   <int> 5, 57, 72, 321, 99, 671, 27, 56, 19, 97, 180, 37, 29, 41, 42…\n$ pwgtp11   <int> 67, 89, 107, 299, 127, 681, 10, 27, 4, 17, 68, 42, 31, 38, 3…\n$ pwgtp12   <int> 6, 46, 53, 211, 90, 469, 27, 36, 8, 18, 79, 62, 50, 38, 35, …\n$ pwgtp13   <int> 5, 67, 74, 209, 110, 402, 14, 34, 6, 52, 24, 15, 10, 11, 14,…\n$ pwgtp14   <int> 137, 109, 133, 412, 177, 118, 15, 42, 5, 44, 131, 48, 40, 58…\n$ pwgtp15   <int> 124, 50, 61, 230, 113, 339, 27, 47, 8, 96, 136, 48, 33, 32, …\n$ pwgtp16   <int> 65, 18, 19, 63, 29, 436, 41, 77, 17, 56, 73, 13, 10, 30, 28,…\n$ pwgtp17   <int> 66, 17, 22, 93, 29, 443, 32, 62, 14, 46, 142, 12, 10, 66, 62…\n$ pwgtp18   <int> 64, 18, 17, 75, 26, 393, 4, 7, 1, 75, 82, 43, 33, 73, 86, 12…\n$ pwgtp19   <int> 63, 46, 54, 279, 95, 161, 17, 21, 5, 47, 26, 68, 51, 33, 39,…\n$ pwgtp20   <int> 6, 88, 102, 308, 187, 411, 20, 38, 10, 54, 67, 42, 29, 38, 4…\n$ pwgtp21   <int> 131, 49, 57, 206, 102, 467, 8, 20, 4, 16, 21, 51, 38, 31, 40…\n$ pwgtp22   <int> 6, 50, 55, 309, 110, 387, 25, 37, 8, 42, 27, 12, 11, 31, 34,…\n$ pwgtp23   <int> 62, 49, 54, 230, 78, 127, 39, 80, 17, 57, 63, 45, 36, 51, 62…\n$ pwgtp24   <int> 63, 65, 69, 281, 107, 150, 32, 72, 11, 122, 74, 54, 38, 25, …\n$ pwgtp25   <int> 60, 13, 19, 66, 29, 553, 39, 53, 17, 93, 24, 44, 27, 36, 36,…\n$ pwgtp26   <int> 126, 16, 18, 76, 31, 514, 9, 12, 2, 17, 119, 11, 8, 40, 51, …\n$ pwgtp27   <int> 6, 16, 20, 128, 35, 133, 15, 50, 6, 14, 86, 80, 60, 66, 64, …\n$ pwgtp28   <int> 66, 54, 69, 300, 109, 613, 7, 14, 4, 61, 126, 92, 62, 12, 13…\n$ pwgtp29   <int> 68, 85, 101, 421, 145, 562, 19, 30, 7, 58, 131, 82, 56, 9, 1…\n$ pwgtp30   <int> 120, 48, 66, 167, 114, 151, 22, 49, 13, 23, 67, 85, 56, 12, …\n$ pwgtp31   <int> 66, 14, 20, 68, 29, 196, 5, 5, 1, 83, 130, 13, 12, 11, 17, 3…\n$ pwgtp32   <int> 126, 49, 60, 296, 99, 431, 16, 19, 4, 69, 29, 48, 33, 61, 11…\n$ pwgtp33   <int> 115, 50, 58, 221, 83, 576, 4, 11, 2, 49, 78, 52, 35, 34, 39,…\n$ pwgtp34   <int> 5, 11, 17, 71, 37, 710, 27, 43, 10, 55, 80, 87, 61, 36, 39, …\n$ pwgtp35   <int> 5, 56, 58, 264, 101, 378, 21, 34, 8, 20, 71, 14, 10, 11, 13,…\n$ pwgtp36   <int> 63, 83, 107, 393, 154, 481, 33, 47, 12, 59, 20, 45, 30, 55, …\n$ pwgtp37   <int> 66, 71, 91, 294, 155, 504, 14, 31, 5, 62, 93, 51, 34, 33, 48…\n$ pwgtp38   <int> 66, 108, 114, 305, 132, 530, 11, 23, 3, 43, 119, 14, 9, 41, …\n$ pwgtp39   <int> 62, 54, 64, 193, 109, 760, 6, 13, 2, 38, 66, 45, 30, 76, 95,…\n$ pwgtp40   <int> 5, 81, 104, 372, 161, 409, 31, 62, 13, 40, 23, 83, 59, 12, 1…\n$ pwgtp41   <int> 118, 43, 70, 274, 115, 560, 10, 18, 4, 16, 84, 66, 46, 10, 1…\n$ pwgtp42   <int> 6, 52, 66, 222, 114, 403, 20, 33, 11, 47, 82, 45, 28, 9, 11,…\n$ pwgtp43   <int> 64, 50, 58, 261, 83, 638, 22, 43, 8, 40, 23, 11, 8, 36, 34, …\n$ pwgtp44   <int> 66, 63, 72, 236, 125, 516, 11, 27, 4, 108, 112, 84, 59, 54, …\n$ pwgtp45   <int> 62, 17, 19, 110, 32, 100, 9, 14, 3, 88, 69, 13, 11, 11, 11, …\n$ pwgtp46   <int> 124, 15, 16, 65, 22, 150, 23, 49, 8, 15, 73, 43, 32, 82, 84,…\n$ pwgtp47   <int> 6, 15, 21, 63, 39, 660, 27, 63, 9, 14, 22, 42, 30, 47, 44, 9…\n$ pwgtp48   <int> 63, 47, 68, 235, 107, 454, 16, 37, 5, 86, 86, 55, 34, 40, 50…\n$ pwgtp49   <int> 68, 88, 111, 343, 133, 179, 29, 39, 12, 59, 69, 53, 35, 35, …\n$ pwgtp50   <int> 132, 47, 58, 246, 129, 793, 6, 7, 1, 16, 124, 42, 32, 31, 46…\n$ pwgtp51   <int> 62, 13, 19, 77, 32, 723, 28, 47, 17, 92, 92, 42, 31, 31, 37,…\n$ pwgtp52   <int> 119, 51, 59, 197, 98, 450, 10, 18, 4, 112, 74, 99, 71, 31, 3…\n$ pwgtp53   <int> 126, 52, 57, 228, 93, 528, 17, 28, 4, 60, 29, 14, 7, 10, 10,…\n$ pwgtp54   <int> 6, 20, 18, 73, 40, 168, 12, 21, 5, 64, 138, 49, 30, 59, 70, …\n$ pwgtp55   <int> 5, 53, 63, 228, 93, 355, 24, 29, 6, 13, 99, 46, 35, 42, 52, …\n$ pwgtp56   <int> 68, 84, 97, 339, 145, 445, 3, 10, 3, 73, 69, 13, 12, 36, 50,…\n$ pwgtp57   <int> 72, 78, 87, 349, 150, 465, 5, 11, 2, 48, 121, 14, 9, 60, 77,…\n$ pwgtp58   <int> 65, 75, 93, 359, 126, 426, 38, 86, 24, 43, 87, 42, 28, 66, 5…\n$ pwgtp59   <int> 66, 53, 68, 262, 129, 228, 14, 37, 5, 53, 23, 84, 66, 34, 39…\n$ pwgtp60   <int> 125, 15, 19, 68, 38, 575, 17, 58, 6, 51, 67, 47, 39, 34, 31,…\n$ pwgtp61   <int> 5, 55, 59, 193, 102, 386, 32, 57, 12, 83, 18, 46, 35, 44, 37…\n$ pwgtp62   <int> 123, 55, 71, 283, 106, 433, 8, 14, 4, 48, 24, 13, 10, 47, 50…\n$ pwgtp63   <int> 62, 58, 65, 230, 95, 166, 10, 13, 4, 51, 77, 43, 31, 61, 68,…\n$ pwgtp64   <int> 64, 65, 73, 230, 120, 230, 7, 11, 2, 16, 62, 43, 36, 39, 43,…\n$ pwgtp65   <int> 65, 110, 126, 313, 141, 797, 7, 10, 2, 15, 28, 46, 39, 34, 3…\n$ pwgtp66   <int> 6, 82, 94, 355, 131, 571, 21, 37, 8, 92, 125, 13, 11, 30, 40…\n$ pwgtp67   <int> 123, 90, 102, 441, 174, 155, 22, 27, 7, 95, 72, 68, 49, 52, …\n$ pwgtp68   <int> 65, 50, 61, 278, 104, 556, 19, 21, 7, 15, 121, 84, 62, 15, 1…\n$ pwgtp69   <int> 67, 12, 20, 76, 31, 596, 12, 32, 5, 47, 134, 103, 74, 13, 17…\n$ pwgtp70   <int> 6, 55, 60, 151, 123, 170, 11, 21, 4, 106, 100, 86, 75, 10, 1…\n$ pwgtp71   <int> 68, 87, 96, 346, 186, 162, 31, 40, 15, 21, 128, 14, 8, 9, 11…\n$ pwgtp72   <int> 6, 52, 57, 259, 84, 530, 24, 62, 10, 17, 25, 56, 42, 58, 72,…\n$ pwgtp73   <int> 5, 60, 70, 248, 87, 602, 34, 55, 11, 37, 68, 40, 31, 39, 40,…\n$ pwgtp74   <int> 127, 84, 116, 422, 180, 579, 7, 15, 2, 50, 66, 75, 56, 36, 5…\n$ pwgtp75   <int> 119, 50, 60, 284, 88, 341, 7, 8, 3, 103, 80, 13, 9, 9, 10, 9…\n$ pwgtp76   <int> 63, 15, 17, 91, 25, 378, 14, 15, 5, 38, 27, 42, 32, 63, 66, …\n$ pwgtp77   <int> 68, 15, 21, 68, 25, 387, 19, 37, 8, 49, 66, 47, 38, 46, 41, …\n$ pwgtp78   <int> 67, 20, 19, 72, 23, 421, 22, 43, 14, 51, 165, 13, 9, 40, 55,…\n$ pwgtp79   <int> 64, 50, 58, 195, 101, 621, 23, 45, 12, 46, 89, 61, 48, 52, 6…\n$ pwgtp80   <int> 122, 16, 18, 94, 36, 486, 5, 10, 2, 47, 24, 74, 59, 9, 13, 1…\n\n\n\n\nThe data has 1613672 rows and 238 columns. The variables I am interested in are AGEP, SEX, HISP, POBP, RAC1P, SCIENGP, SOCP.\nThe data needs cleaning and rearranging the columns and rows."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html",
    "href": "posts/FinalPart1_CalebHill.html",
    "title": "Final Part 1",
    "section": "",
    "text": "Multiple research reports state that there is a relationship between re-hospitalization rates and social characteristics, such as demographic and economic identifiers, (Barnett, Hsu & McWilliams, 2015; Murray, Allen, Clark, Daly & Jacobs, 2021). Specifically, racial characteristics play a large role in predicting re-hospitalization in a population (Li, Cai & Glance, 2015). While some articles examine economic and health factors contributing to these disparities, very few dig deep into environmental factors that influence this phenomenon, (Spatz, Bernheim, Horwitz & Herrin, 2020). With your zipcode affecting up to 60% of your health outcomes, this research is relevant to better improving one of our most costly health expenditures: hospitalization.\nThis paper aims to explore how different environmental variables impact re-hospitalization rates on a county-by-county level, controlling for racial, ethnic, and sex variables (maybe). These environmental factors will include both common environmental concerns, such as heat index, average temperature, precipitation, and natural disasters, along with the built environment, population density.\nThe data-set chosen for this analysis is taken from the Agency for Healthcare Research and Quality, Social Determinants of Health (SDOH) Database. This data-set has over 300 variables to explore each SDOH domain: social context, economic context, education, healthcare, and the environment. We shall pull data from three of these five domains: social, economic, and environmental.\nTo further reduce data bloat, we shall limit the geographic review to Texas counties – my home state! That should provide us with 200+ observations.\nThe hypothesis for this research report is: *Environmental factors increase rates of re-hospitalization in Texas counties.\nTherefore, the null hypothesis is: *Environmental factors do not increase rates of re-hospitalization in Texas counties.\nMultiple regression analyses shall be employed to determine the relationship – or lack thereof – between these variables.\nFirst I’ll import the relevant libraries.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(corrplot)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\nThen I’ll import the dataset and view the first six rows.\n\n\nCode\ndf <- SDOH_2020_COUNTY_1_0 <- read_excel(\"_data/SDOH_2020_COUNTY_1_0.xlsx\", sheet = \"Data\")\n\n\nWarning: Expecting logical in OA1673 / R1673C391: got '46123'\n\n\nWarning: Expecting logical in OA1765 / R1765C391: got '32510'\n\n\nWarning: Expecting logical in OB1765 / R1765C392: got '41025'\n\n\nWarning: Expecting logical in OC1765 / R1765C393: got '41037'\n\n\nWarning: Expecting logical in OA2799 / R2799C391: got '49017'\n\n\nWarning: Expecting logical in OB2799 / R2799C392: got '49019'\n\n\nWarning: Expecting logical in OC2799 / R2799C393: got '49025'\n\n\nWarning: Expecting logical in OD2799 / R2799C394: got '49055'\n\n\nWarning: Expecting logical in OA2844 / R2844C391: got '51760'\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 685\n   YEAR COUNTYFIPS STATEFIPS STATE COUNTY REGION TERRI…¹ ACS_T…² ACS_T…³ ACS_T…⁴\n  <dbl> <chr>      <chr>     <chr> <chr>  <chr>    <dbl>   <dbl>   <dbl>   <dbl>\n1  2020 01001      01        Alab… Autau… South        0   55639   54929   52404\n2  2020 01003      01        Alab… Baldw… South        0  218289  216518  206329\n3  2020 01005      01        Alab… Barbo… South        0   25026   24792   23694\n4  2020 01007      01        Alab… Bibb … South        0   22374   22073   21121\n5  2020 01009      01        Alab… Bloun… South        0   57755   57164   54250\n6  2020 01011      01        Alab… Bullo… South        0   10173   10143    9579\n# … with 675 more variables: ACS_TOT_POP_ABOVE15 <dbl>,\n#   ACS_TOT_POP_ABOVE16 <dbl>, ACS_TOT_POP_16_19 <dbl>,\n#   ACS_TOT_POP_ABOVE25 <dbl>, ACS_TOT_CIVIL_POP_ABOVE18 <dbl>,\n#   ACS_TOT_CIVIL_VET_POP_ABOVE25 <dbl>, ACS_TOT_OWN_CHILD_BELOW17 <dbl>,\n#   ACS_TOT_WORKER_NWFH <dbl>, ACS_TOT_WORKER_HH <dbl>,\n#   ACS_TOT_CIVILIAN_LABOR <dbl>, ACS_TOT_CIVIL_EMPLOY_POP <dbl>,\n#   ACS_TOT_POP_POV <dbl>, ACS_TOT_CIVIL_NONINST_POP_POV <dbl>, …\n\n\nNext I want to verify the class is a dataframe. Otherwise, I’ll need to transform the data to make it easier to work with.\n\n\nCode\nclass(df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nAll good here.\nNow on to data transformation. We will need to select only the relevant columns for this analysis and filter by Texas, bringing the observations (rows) down to 254.\n\n\nCode\ndf_new <- df %>%\n  select(COUNTYFIPS,\n         STATE,\n         COUNTY,\n         ACS_TOT_POP_WT,\n         ACS_PCT_MALE,\n         ACS_PCT_FEMALE,\n         ACS_PCT_AIAN,\n         ACS_PCT_ASIAN,\n         ACS_PCT_BLACK,\n         ACS_PCT_HISPANIC,\n         ACS_PCT_MULT_RACE,\n         ACS_PCT_NHPI,\n         ACS_PCT_OTHER_RACE,\n         ACS_PCT_WHITE,\n         CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  filter(STATE == \"Texas\")\nhead(df_new)\n\n\n# A tibble: 6 × 22\n  COUNTYF…¹ STATE COUNTY ACS_T…² ACS_P…³ ACS_P…⁴ ACS_P…⁵ ACS_P…⁶ ACS_P…⁷ ACS_P…⁸\n  <chr>     <chr> <chr>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 48001     Texas Ander…   57917    61.2    38.8    0.41    0.6    20.9    17.9 \n2 48003     Texas Andre…   18227    49.6    50.4    0       0.31    2.29   56.9 \n3 48005     Texas Angel…   87119    48.9    51.1    0.31    1.11   15.1    22.3 \n4 48007     Texas Arans…   24220    49.5    50.5    0.9     1.14    0.33   28.0 \n5 48009     Texas Arche…    8754    50.2    49.8    1.46    0.17    1.1     8.21\n6 48011     Texas Armst…    1950    45.7    54.3    0.77    0       0.72    8.46\n# … with 12 more variables: ACS_PCT_MULT_RACE <dbl>, ACS_PCT_NHPI <dbl>,\n#   ACS_PCT_OTHER_RACE <dbl>, ACS_PCT_WHITE <dbl>, CEN_POPDENSITY_COUNTY <dbl>,\n#   NEPHTN_HEATIND_105 <dbl>, NOAAC_AVG_TEMP_YEARLY <dbl>,\n#   NOAAC_PRECIPITATION_AVG_YEARLY <dbl>, NOAAS_TOT_NATURAL_DISASTERS <dbl>,\n#   SAIPE_MEDIAN_HH_INCOME <dbl>, SAIPE_PCT_POV <dbl>,\n#   LTC_AVG_OBS_REHOSP_RATE <dbl>, and abbreviated variable names ¹​COUNTYFIPS,\n#   ²​ACS_TOT_POP_WT, ³​ACS_PCT_MALE, ⁴​ACS_PCT_FEMALE, ⁵​ACS_PCT_AIAN, …\n\n\nOut of 300+ variables, we’ve whittled them down to 22. Of those 22, we have three (3) that are unique identifiers (FIPS, State, and County), 11 that are potential control variables (population, gender, and race / ethnicity), and eight (8) that we can explore (Population Density to Re-hospitalization Rate).\nBefore we launch into exploring these eight variables via descriptive statistics, first we need to determine where the NAs are and see if any of the variables will have a substantial amount of missing data.\n\n\nCode\ncolSums(is.na(df_new))\n\n\n                    COUNTYFIPS                          STATE \n                             0                              0 \n                        COUNTY                 ACS_TOT_POP_WT \n                             0                              0 \n                  ACS_PCT_MALE                 ACS_PCT_FEMALE \n                             0                              0 \n                  ACS_PCT_AIAN                  ACS_PCT_ASIAN \n                             0                              0 \n                 ACS_PCT_BLACK               ACS_PCT_HISPANIC \n                             0                              0 \n             ACS_PCT_MULT_RACE                   ACS_PCT_NHPI \n                             0                              0 \n            ACS_PCT_OTHER_RACE                  ACS_PCT_WHITE \n                             0                              0 \n         CEN_POPDENSITY_COUNTY             NEPHTN_HEATIND_105 \n                             0                              0 \n         NOAAC_AVG_TEMP_YEARLY NOAAC_PRECIPITATION_AVG_YEARLY \n                             0                              0 \n   NOAAS_TOT_NATURAL_DISASTERS         SAIPE_MEDIAN_HH_INCOME \n                             0                              0 \n                 SAIPE_PCT_POV        LTC_AVG_OBS_REHOSP_RATE \n                             0                             44 \n\n\nThis is not ideal, as that’s our dependent variable. However, 44 / 254 is not bad. That still leaves us with plenty of counties to review.\n\n\nCode\ndf_new %>%\n  drop_na() %>%\n  print(nrow(df_new))\n\n\n# A tibble: 210 × 22\n   COUNTYFIPS STATE COUNTY           ACS_TOT_POP_WT ACS_PCT_MALE ACS_PCT_FEMALE\n   <chr>      <chr> <chr>                     <dbl>        <dbl>          <dbl>\n 1 48001      Texas Anderson County           57917         61.2           38.8\n 2 48003      Texas Andrews County            18227         49.6           50.4\n 3 48005      Texas Angelina County           87119         48.9           51.1\n 4 48007      Texas Aransas County            24220         49.5           50.5\n 5 48011      Texas Armstrong County           1950         45.7           54.3\n 6 48013      Texas Atascosa County           50194         50.2           49.8\n 7 48015      Texas Austin County             29892         49.9           50.1\n 8 48017      Texas Bailey County              6916         50.0           50.0\n 9 48019      Texas Bandera County            22770         49.8           50.2\n10 48021      Texas Bastrop County            86839         50.8           49.2\n   ACS_PCT_AIAN ACS_PCT_ASIAN ACS_PCT_BLACK ACS_PCT_HISPANIC ACS_PCT_MULT_RACE\n          <dbl>         <dbl>         <dbl>            <dbl>             <dbl>\n 1         0.41          0.6          20.9             17.9               4.46\n 2         0             0.31          2.29            56.9               5.76\n 3         0.31          1.11         15.1             22.3               3.21\n 4         0.9           1.14          0.33            28.0               6.2 \n 5         0.77          0             0.72             8.46              5.33\n 6         0.08          0.5           1.08            64.7              11.4 \n 7         0.14          0.55          8.77            27.2               2.96\n 8         1             0.68          0.29            65.8               0.49\n 9         1.2           0.34          0.75            19.3               5.97\n10         0.53          0.84          7.83            38.8               6.98\n   ACS_PCT_NHPI ACS_PCT_OTHER_RACE ACS_PCT_WHITE CEN_POPDENSITY_COUNTY\n          <dbl>              <dbl>         <dbl>                 <dbl>\n 1         0.02               2.35          71.2                 54.5 \n 2         0.14              10.2           81.2                 12.2 \n 3         0.01               2.78          77.4                109.  \n 4         0                  3.57          87.9                 96.1 \n 5         0                  1.59          91.6                  2.14\n 6         0                  2.2           84.8                 41.2 \n 7         0                 12.1           75.5                 46.2 \n 8         0                  4.38          93.2                  8.36\n 9         0                  2.7           89.0                 28.8 \n10         0                 18.4           65.4                 97.8 \n   NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_Y…¹ NOAAC…² NOAAS…³ SAIPE…⁴ SAIPE…⁵ LTC_A…⁶\n                <dbl>              <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1                 28               66.5   4.36       46   50879    20.9    0.21\n 2                  0               64.6   0.595      26   76600     9.2    0.2 \n 3                 26               67.6   4.05       15   49943    17      0.18\n 4                  7               73.2   2.24       36   51461    17.1    0.09\n 5                  0               60.6   1.09       90   62256     9.3    0.33\n 6                 47               71.8   2.09       38   60594    14.9    0.16\n 7                 33               70.4   3.41       19   60593    11.4    0.08\n 8                  0               59.6   0.692      38   48259    14.4    0   \n 9                  7               68.2   2.03       29   64389    11      0.08\n10                 44               70.0   2.89       30   74612    10.8    0.14\n# … with 200 more rows, and abbreviated variable names ¹​NOAAC_AVG_TEMP_YEARLY, ²​NOAAC_PRECIPITATION_AVG_YEARLY, ³​NOAAS_TOT_NATURAL_DISASTERS, ⁴​SAIPE_MEDIAN_HH_INCOME, ⁵​SAIPE_PCT_POV, ⁶​LTC_AVG_OBS_REHOSP_RATE\n\n\n210 x 22 is a good place to start. We’ll need to re-do this step for the descriptive statistics section, but we can carry over this object when we fit the linear models."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "href": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "title": "Final Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nFor our preliminary analysis, we’re going to provide summary statistics analyzing the 8 variables relevant to our research question, from Population Density to the end of the data-set, and a visualization for each. Re-hospitalization rates will be the dependent variable in future models, with the 11 demographic variables as potential controls for the regression(s).\n\n\nCode\ndata <- df_new %>%\n  select(CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  drop_na()\nsummary(data)\n\n\n CEN_POPDENSITY_COUNTY NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_YEARLY\n Min.   :   0.78       Min.   : 0.00      Min.   :56.52        \n 1st Qu.:  12.53       1st Qu.: 7.00      1st Qu.:64.89        \n Median :  30.48       Median :24.00      Median :66.53        \n Mean   : 143.53       Mean   :22.14      Mean   :67.06        \n 3rd Qu.:  78.46       3rd Qu.:34.00      3rd Qu.:69.71        \n Max.   :3003.99       Max.   :59.00      Max.   :76.47        \n NOAAC_PRECIPITATION_AVG_YEARLY NOAAS_TOT_NATURAL_DISASTERS\n Min.   :0.4583                 Min.   :  0.00             \n 1st Qu.:1.6135                 1st Qu.: 14.25             \n Median :2.5933                 Median : 28.50             \n Mean   :2.7027                 Mean   : 32.75             \n 3rd Qu.:3.8808                 3rd Qu.: 40.00             \n Max.   :5.4558                 Max.   :186.00             \n SAIPE_MEDIAN_HH_INCOME SAIPE_PCT_POV   LTC_AVG_OBS_REHOSP_RATE\n Min.   : 33513         Min.   : 4.80   Min.   :0.0000         \n 1st Qu.: 48455         1st Qu.:11.45   1st Qu.:0.1100         \n Median : 54536         Median :14.50   Median :0.1500         \n Mean   : 57028         Mean   :14.75   Mean   :0.1528         \n 3rd Qu.: 61901         3rd Qu.:17.40   3rd Qu.:0.2000         \n Max.   :106225         Max.   :28.70   Max.   :1.0000         \n\n\n\nPopulation Density\n\n\nCode\nggplot(data, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\nWe see quite a number of counties have a low population density. This is no surprise, as over 80% of counties in Texas are labeled as “rural” by multiple federal agencies – dependent upon low population density.\nThis is further attested and we see a wide range between this variable’s median (21.8) and mean (119.4). Lots of out-liers. If we had a urban/rural classification code, we could filter on only rural counties to help mitigate this spread. I may need to merge a data-set due to this wide range.\n\n\nHeat Index Over 105F\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_boxplot()\n\n\n\n\n\nTexas is a hot state, and this visualization is evidence of that. The median number of days Texas’ counties experience a heat index of over 105F each year is 20 days per year. One county even reached 59 days!\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe data-set has a very normal distribution, centered around the 25/30 mark – if the number of counties at 0 were removed. Yet because that’s not so, this variable has a sharp bimodal distribution. We may have to separate the data into two bins: those with less than 10 days over 105F and those with more than 10 days over 105F. That’s yet to be determined.\n\n\nAverage Yearly Temperature\n\n\nCode\nggplot(data, aes(NOAAC_AVG_TEMP_YEARLY)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere’s a good distribution. Average temperature each month is between 65 to 67 for most of the counties. The range (20) is also fairly small for a state with such a large area and multiple climates within its borders.\n\n\nAverage Yearly Precipitation\n\n\nCode\nggplot(data, aes(NOAAC_PRECIPITATION_AVG_YEARLY)) +\n  geom_boxplot()\n\n\n\n\n\nAverage precipitation each month is fairly uniform, with the mean at 2.5 inches of rain, on average, each month. This variable will most likely provide less variation in the analysis compared to others, such as population density and heat index. This can be both a good and a bad thing, as variations in precipitation was one of the variables I was most interested in exploring for this project. Oh well.\n\n\nTotal Natural Disasters\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_boxplot()\n\n\n\n\n\nMany high out-liers over 75. Let’s plot a histogram to get a better look at the data’s distribution.\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA right skewed variable, with observations dropping off dramatically once we reach 50 total recorded natural disasters.\n\n\nMedian Household Income\n\n\nCode\nggplot(data, aes(SAIPE_MEDIAN_HH_INCOME)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA couple of high out-liers, hovering around $90,000+ in median household income, but the mean holds at $57,291.\n\n\nPercent in Poverty\n\n\nCode\nggplot(data, aes(SAIPE_PCT_POV)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother close to normal distribution. Most counties have poverty rates ranging from 10% to 20%. There are of course out-liers, especially a good number below 10%, but those are rare.\n\n\nRe-hospitalization Rate\n\n\nCode\nggplot(data, aes(LTC_AVG_OBS_REHOSP_RATE)) +   geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother right skewed variable. Lots of counties with 0.00 rates of re-hospitalization, and few, if any, above 0.25 per 100,000 people. From a health perspective, this is good news! From a research perspective, that’s going to make analysis a little trickier. However, the somewhat normal and/or bimodal distribution should be fairly easy to work with, needing little to no transformation for a linear regression.\n\n\nCorrelation\nFinally, let’s plot a brief correlation matrix to see if there’s any relationships we can explore as a simple linear regression in the next section.\n\n\nCode\ndata %>%\n  cor(data) %>%\n  corrplot(is.corr = FALSE, method=\"number\", tl.cex = .4)\n\n\n\n\n\nThe closer a box is to 1, the higher the correlation. Not particularly exciting news, as it shows there’s not a high correlation between re-hospitalization rates and any of the explanatory variables. This may throw a kink in our analysis – and explain why others haven’t delved deeply into this research!\nPerhaps this step should have been completed first, but nonetheless, we shall continue on with the report. I may pull two more environmental variables, to see if we can find a correlation somewhere. Even so, the sum total of all environmental variables might contribute to re-hospitalization rates as well. I’m just not sure if that – along with control variables – is outside the scope of this report.\nFor Part 2, I’d like to rename the variables to more digestible phrases, and I would like to overhaul the code outputs, to make the tables and visualizations a little easier on the eyes. That’s just polish work, though, and won’t affect the analysis.\nLooking over the Spatz et. al. (2020) article again, the two most significant Built Environment variables (with the highest R2 value) are 1) Long Commute, Driving Alone and 2) Severe Housing Problems. I’m going to scour the SDOH data-set to see what relevant variables match these two and add them into Part 2."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#references",
    "href": "posts/FinalPart1_CalebHill.html#references",
    "title": "Final Part 1",
    "section": "References",
    "text": "References\nBarnett, M., Hsu, J. & McWilliams, M. (2015). “Patient Characteristics and Differences in Hospital Readmission Rates.” JAMA Intern Med., 175(11): 1803-1812.\nLi, Y., Cai, X. & Glance, L. (2015). “Disparities in 30-day rehospitalization rates among Medicare skilled nursing facility residents by race and site of care.” Med Care, 53(12): 1058-1065.\nMurray, F., Allen, M., Clark, C., Daly, C. & Jacobs, D. (2021). “Socio-demographic and -economic factors associated with 30-day readmission for conditions targeted by the hospital readmissions reduction program: a population-based study.” BMC Public Health, 21.\nSpatz, E., Bernheim, S., Horwitz, L. & Herrin, J. (2020). Community factors and hospital wide readmission rates: Does context matter? PLoS One, 15(10)."
  },
  {
    "objectID": "posts/FinalPart1_StephRoberts.html",
    "href": "posts/FinalPart1_StephRoberts.html",
    "title": "Final Project: Diabetes Prediction",
    "section": "",
    "text": "####Diabetes risk factors\nAccording to the World Health Organization (WHO), an estimated 537 million people worldwide are living with diabetes. It is a leading cause of health complications and even death. The WHO states close to 1.5 million people died due to diabetes and its complication in 2019 alone. It is a growing problem that requires dedicated research to aim at the slowdown and prevention of future cases.\n###Research Questions 1. What risk factors are most predictive of diabetes?\nResearch on Diabetes is ongoing and in-depth within the medical field. The prevalence and incidence of diabetes mellitus type 2 (DQ2) have increased consistently for decades, giving way to an increase in mortality related to diabetes. Commonly in the medical field, many risk factors are used to measure a patient’s risk of developing DM2, such as obesity, family history, hypertension and changes in fasting blood sugar levels. Moreno et al. (2018) studied risk parameters for diabetes and concluded “risk of being diabetic rises in patients whose father has suffered an acute myocardial infarction, in those whose mother or father is diabetic and in patients with a high waist perimeter.” Their focus on family history leaves room for studies more focused on individual medical factors, such as blood pressure, BMI, number of pregnancies, etc. That is the aim of this project.\nM. L. M. V. J. A. (2018, June 11). Predictive risk model for the diagnosis of diabetes mellitus type 2 in a follow-up study 15 Years on: Prodi2 study. European journal of public health. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/29897477/\n\nCan the use of regression analysis help predict risk of diabetes based on several medical variables?\n\nOther research, such as a Edlitz & Segal (2022) study titled Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards, does focus on using individual medical factors to predict risk of diabetes through regression. This project aims to conduct similar analysis on different data.\nE. Y. S. E. (2022, June 22). Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards. eLife. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/35731045/\n###Hypothesis\n\nBody mass index (BMI), glucose, and age are significant predictors of diabetes mellitus type 2.\nRegression analysis can help predict the risk of developing diabetes mellitus type 2.\n\nBoth hypothesis have been tested in the above mentioned studies. The contribution from this project will be the additional support for or against the hypotheses from the analysis of different data.\n###Descriptive Statistics\nThe data was collected by the “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database (PIDD). A total of 768 cases are available in PIDD. However, 5 patients had a glucose of 0, 11 patients had a body mass index of 0, 28 others had a diastolic blood pressure of 0, 192 others had skin fold thickness readings of 0, and 140 others had serum insulin levels of 0. After cleaning the data by removing the cases with numbers that are incompatible with life, 392 cases remained. All patients belong to the Pima Indian heritage (subgroup of Native Americans), and are females aged 21 years and above.\nThe datasets consists of 9 medical predictor (independent) variables and one target (dependent) variable, outcome.\nPregnancies: Number of times a woman has been pregnant Glucose: Plasma Glucose concentration of 2 hours in an oral glucose tolerance test BloodPressure: Diastollic Blood Pressure (mm hg) SkinThickness: Triceps skin fold thickness(mm) Insulin: 2 hour serum insulin(mu U/ml) BMI: Body Mass Index ((weight in kg/height in m)^2) Age: Age(years) DiabetesPedigreeFunction: scores likelihood of diabetes based on family history) Outcome: 0(doesn’t have diabetes) or 1 (has diabetes)\n\n\nCode\ndf<- read_csv(\"diabetes2.csv\")\n\n\nError: 'diabetes2.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/603_Fall_2022/posts').\n\n\nCode\ndim(df)\n\n\nNULL\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\nCode\nhead(df)\n\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\n\n\nCode\n#check for null entries\nis.null(df)\n\n\n[1] FALSE\n\n\n\n\nCode\n#Check number of 0s in each column\ncolSums(df==0)\n\n\nError in df == 0: comparison (1) is possible only for atomic and list types\n\n\nGlucose, blood pressure, skin thickness, insulin, BMI and Age are not variables that should logically have 0s. Those values, if true, are likely incompatible with life. We will remove those cases from analysis.\n\n\nCode\n#Remove rows with 0 in respective columns\ndfc <- df[apply(df[c(2:6, 8)],1,function(z) !any(z==0)),] \n\n\nError in df[c(2:6, 8)]: object of type 'closure' is not subsettable\n\n\nCode\n#Verify 0s are gone in selected rows\ncolSums(dfc==0)\n\n\nError in is.data.frame(x): object 'dfc' not found\n\n\n\n\nCode\n#Check cleaned data frame\nglimpse(dfc)\n\n\nError in glimpse(dfc): object 'dfc' not found\n\n\nThe cleaned data frame includes 392 observations, or cases, along the original 9 variables.\n\n\nCode\n#Summarize df\nsummary(dfc)\n\n\nError in summary(dfc): object 'dfc' not found\n\n\nAt a glance, this summary is more fitting after having cleaned our data. An average of 3 pregnancies, considering our 21+ female population, makes sense. A mean glucose of 122, blood pressure of 70.66, a BMI of 33, and age of 30.86 are reasonable accurate of our population. The data is clean and ready for analysis.\n###Analysis\n###Hypothesis Testing\n###Model Comparisons\n###Diagnostics"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html",
    "href": "posts/FinalPart1_ToryBarteloni.html",
    "title": "DACSS 603: Final Part 1",
    "section": "",
    "text": "The concept of political trust has been researched in great depth for decades. That research indicates that a number of factors have at least some impact on a group’s level of trust or confidence in their government. Most of the factors studied are related to the public’s perception of government performance including control over crime, the economy, and the appearance of corruption and scandal. To this point there has been no consensus or holistic model that produces a satisfactory answer to the question why do groups trust and have confidence in their government?. In this project we will try to bring us one step closer by examining a model that takes into account several factors."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#setup",
    "href": "posts/FinalPart1_ToryBarteloni.html#setup",
    "title": "DACSS 603: Final Part 1",
    "section": "Setup",
    "text": "Setup\nLoading packages and reading in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_final <- read.csv(\"_data/FinalPart1_ToryBartelloni_data.csv\")"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "href": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "title": "DACSS 603: Final Part 1",
    "section": "Summary of Data",
    "text": "Summary of Data\nFirst I will include a brief look at the data set and then we will look at the specifics.\n\n\nCode\nstr(data_final)\n\n\n'data.frame':   91 obs. of  27 variables:\n $ X                           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Power_Distance              : int  NA NA 49 NA 38 11 NA 80 NA NA ...\n $ Individualism               : int  NA NA 46 NA 90 55 NA 20 NA NA ...\n $ Masculinity                 : int  NA NA 56 NA 61 79 NA 55 NA NA ...\n $ Uncertainty_Avoidance       : int  NA NA 86 NA 51 70 NA 60 NA NA ...\n $ Time_Perspective            : int  61 NA 20 61 21 60 61 47 81 70 ...\n $ Indulgence                  : int  15 65 62 NA 71 63 22 20 15 44 ...\n $ Country_Final               : chr  \"Albania\" \"Andorra\" \"Argentina\" \"Armenia\" ...\n $ CPI.Score.2018              : int  36 NA 40 35 77 76 25 26 44 38 ...\n $ GDP_per_Capita              : num  13653 NA 22066 13654 49309 ...\n $ Homicides_per_100K          : num  2.256 NA 5.143 2.468 0.893 ...\n $ Gov_Exp_Employees           : num  9.35e+10 NA 5.52e+11 3.01e+11 5.37e+10 ...\n $ Gov_Exp_GoodsAndServices    : num  3.66e+10 NA 1.74e+11 2.06e+11 5.37e+10 ...\n $ Gov_Exp_Total               : num  3.92e+11 NA 4.75e+12 1.42e+12 5.09e+11 ...\n $ Gov_Exp_Interest            : num  3.50e+10 NA 7.50e+11 1.58e+11 1.80e+10 ...\n $ Gov_Exp_Subsidies           : num  2.09e+11 NA 3.05e+12 5.44e+11 3.41e+11 ...\n $ Gov_Exp_Military            : num  2.17e+10 NA 1.51e+11 3.24e+11 3.73e+10 ...\n $ Wage_Workers                : num  45.7 NA 73.5 66 83.4 ...\n $ Vulnerable_Employment       : num  51.2 NA 22.7 33.1 10.6 ...\n $ WGI_Control_Corruption      : num  -0.5434 1.231 -0.0837 -0.2038 1.8221 ...\n $ WGI_Government_Effectiveness: num  -0.0333 1.901 -0.0965 -0.1975 1.5649 ...\n $ WGI_Political_Stability     : num  0.1112 1.6022 -0.0914 -0.4134 0.9117 ...\n $ WGI_Regulatory_Quality      : num  0.286 1.227 -0.437 0.256 1.872 ...\n $ WGI_Rule_of_Law             : num  -0.403 1.572 -0.408 -0.157 1.726 ...\n $ WGI_Voice_Accountability    : num  0.1427 1.1101 0.5724 0.0555 1.2674 ...\n $ Gov_Confidence              : num  0.148 0.491 0.314 0.308 0.313 ...\n $ Gov_Confidence_Mean         : num  3.39 2.56 2.94 2.97 2.82 ...\n\n\nCode\nsummary(data_final)\n\n\n       X        Power_Distance   Individualism    Masculinity    \n Min.   : 1.0   Min.   : 11.00   Min.   : 6.00   Min.   :  5.00  \n 1st Qu.:23.5   1st Qu.: 41.00   1st Qu.:24.00   1st Qu.: 39.50  \n Median :46.0   Median : 63.00   Median :41.00   Median : 49.00  \n Mean   :46.0   Mean   : 59.61   Mean   :45.32   Mean   : 49.31  \n 3rd Qu.:68.5   3rd Qu.: 72.00   3rd Qu.:68.50   3rd Qu.: 63.50  \n Max.   :91.0   Max.   :104.00   Max.   :91.00   Max.   :110.00  \n                NA's   :32       NA's   :32      NA's   :32      \n Uncertainty_Avoidance Time_Perspective   Indulgence     Country_Final     \n Min.   :  8.00        Min.   :  0.00   Min.   :  0.00   Length:91         \n 1st Qu.: 51.00        1st Qu.: 31.25   1st Qu.: 28.00   Class :character  \n Median : 68.00        Median : 51.50   Median : 42.50   Mode  :character  \n Mean   : 66.46        Mean   : 50.46   Mean   : 44.42                     \n 3rd Qu.: 85.00        3rd Qu.: 69.00   3rd Qu.: 63.50                     \n Max.   :112.00        Max.   :100.00   Max.   :100.00                     \n NA's   :32            NA's   :21       NA's   :19                         \n CPI.Score.2018  GDP_per_Capita   Homicides_per_100K Gov_Exp_Employees  \n Min.   :17.00   Min.   :  2221   Min.   : 0.2067    Min.   :1.635e+09  \n 1st Qu.:33.00   1st Qu.: 12845   1st Qu.: 0.7453    1st Qu.:1.830e+10  \n Median :44.50   Median : 25641   Median : 1.3927    Median :7.092e+10  \n Mean   :50.14   Mean   : 30218   Mean   : 3.8824    Mean   :7.315e+12  \n 3rd Qu.:71.25   3rd Qu.: 42847   3rd Qu.: 3.7136    3rd Qu.:4.237e+11  \n Max.   :88.00   Max.   :127273   Max.   :28.7367    Max.   :3.726e+14  \n NA's   :3       NA's   :3        NA's   :20         NA's   :17         \n Gov_Exp_GoodsAndServices Gov_Exp_Total       Gov_Exp_Interest    \n Min.   :7.335e+08        Min.   :8.294e+09   Min.   :-1.270e+09  \n 1st Qu.:8.305e+09        1st Qu.:8.688e+10   1st Qu.: 3.377e+09  \n Median :3.493e+10        Median :4.187e+11   Median : 1.754e+10  \n Mean   :5.153e+12        Mean   :4.773e+13   Mean   : 4.768e+12  \n 3rd Qu.:2.275e+11        3rd Qu.:2.338e+12   3rd Qu.: 2.648e+11  \n Max.   :2.510e+14        Max.   :2.295e+15   Max.   : 2.751e+14  \n NA's   :17               NA's   :17          NA's   :16          \n Gov_Exp_Subsidies   Gov_Exp_Military     Wage_Workers   Vulnerable_Employment\n Min.   :2.170e+09   Min.   :0.000e+00   Min.   :15.85   Min.   : 3.30        \n 1st Qu.:4.227e+10   1st Qu.:3.952e+09   1st Qu.:57.30   1st Qu.: 9.09        \n Median :2.930e+11   Median :2.669e+10   Median :77.26   Median :18.87        \n Mean   :2.540e+13   Mean   :9.487e+12   Mean   :71.64   Mean   :24.75        \n 3rd Qu.:9.952e+11   3rd Qu.:1.584e+11   3rd Qu.:86.35   3rd Qu.:36.96        \n Max.   :1.138e+15   Max.   :5.314e+14   Max.   :95.73   Max.   :83.70        \n NA's   :17          NA's   :11          NA's   :2       NA's   :2            \n WGI_Control_Corruption WGI_Government_Effectiveness WGI_Political_Stability\n Min.   :-1.560314      Min.   :-1.7516              Min.   :-2.603781      \n 1st Qu.:-0.533848      1st Qu.:-0.1993              1st Qu.:-0.565488      \n Median : 0.009211      Median : 0.2023              Median : 0.111169      \n Mean   : 0.272002      Mean   : 0.4396              Mean   : 0.007762      \n 3rd Qu.: 1.221506      3rd Qu.: 1.3999              3rd Qu.: 0.775985      \n Max.   : 2.167130      Max.   : 2.2127              Max.   : 1.639301      \n                                                                            \n WGI_Regulatory_Quality WGI_Rule_of_Law   WGI_Voice_Accountability\n Min.   :-2.3622        Min.   :-2.2536   Min.   :-1.7968         \n 1st Qu.:-0.3226        1st Qu.:-0.4966   1st Qu.:-0.4587         \n Median : 0.5280        Median : 0.1570   Median : 0.2624         \n Mean   : 0.4615        Mean   : 0.3214   Mean   : 0.2182         \n 3rd Qu.: 1.3576        3rd Qu.: 1.3373   3rd Qu.: 1.0170         \n Max.   : 2.1601        Max.   : 2.0488   Max.   : 1.6552         \n                                                                  \n Gov_Confidence    Gov_Confidence_Mean\n Min.   :0.08744   Min.   :1.561      \n 1st Qu.:0.24726   1st Qu.:2.502      \n Median :0.39372   Median :2.711      \n Mean   :0.41976   Mean   :2.708      \n 3rd Qu.:0.53634   3rd Qu.:3.020      \n Max.   :0.95441   Max.   :3.448      \n                                      \n\n\nWe can see that the data was 91 observations fo 27 variables. Each observation in the data is a country and there are observations for a number of potentially useful variables. Choosing the best variables and assessing the power of our test will be important due to the noticeable number of NA values for some of the variables."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "href": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "title": "DACSS 603: Final Part 1",
    "section": "Transparency International",
    "text": "Transparency International\nThe Corruption Perceptions Index (CPI) is created by Transparency International by taking a combination of 13 different data sources including assessments and surveys. These sources are largely comprised of experts and business interests so are not a direct reflection of the general public. The scores from each of the sources are standardized, averaged, and then scaled to provide a score for each of the countries in the data sources. What we end up with is Corruption Perception score between 1-100 for each of the countries.\n\n\nCode\ndata_final %>% ggplot(aes(x=CPI.Score.2018)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(CPI.Score.2018,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(CPI.Score.2018,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(CPI.Score.2018,na.rm=TRUE)+\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(CPI.Score.2018,na.rm=TRUE)-\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Corruption Perceptions Index\",\n       subtitle=\"Distribution of CPI 2018\",\n       x=\"CPI Score\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-government-indicators",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-government-indicators",
    "title": "DACSS 603: Final Part 1",
    "section": "World Bank Development and Government Indicators",
    "text": "World Bank Development and Government Indicators\nThe World Bank collects data from many different sources to obtain indicators for world development as well as the World Governance Indicators project.\nThe Development Indicators are taken from a wide variety of sources. We will be using two primary indicators: GDP per Capita and Intentional Homicides per 100K people. GDP per capita is derived from the World Bank and OECD National Accounts data while Inentional Homicides are taken from the UN Office on Drugs and Crime’s International Homicide Statistics database.\n\n\nCode\ndata_final %>% ggplot(aes(x=GDP_per_Capita)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(GDP_per_Capita,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(GDP_per_Capita,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(GDP_per_Capita,na.rm=TRUE)+\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(GDP_per_Capita,na.rm=TRUE)-\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Gross Domestic Product per Capita\",\n       subtitle=\"Distribution of GDP per capita 2019\",\n       x=\"GDP per Capita\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\ndata_final %>% ggplot(aes(x=Homicides_per_100K)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Homicides_per_100K,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Homicides_per_100K,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Homicides_per_100K,na.rm=TRUE)+\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Homicides_per_100K,na.rm=TRUE)-\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Intentional Homicides per 100K Residents\",\n       subtitle=\"Distribution of homicides per 100K 2019\",\n       x=\"Homicides per 100K\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\nThe World Governance Indicators are a combination of enterprise, citizen, and expert survey respondents from around the world. They use more than 30 surveys to create their six indicators with each indicator using differnt surveys and different data from each survey to aggregate to the final indicator.\n\n\nCode\ndata_final %>% ggplot(aes(x=WGI_Voice_Accountability)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(WGI_Voice_Accountability,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(WGI_Voice_Accountability,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(WGI_Voice_Accountability,na.rm=TRUE)+\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(WGI_Voice_Accountability,na.rm=TRUE)-\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"World Governance Indicators - Voice and Accountability\",\n       subtitle=\"Distribution of Voice and Accountability 2019\",\n       x=\"Voice and Accountability\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "title": "DACSS 603: Final Part 1",
    "section": "World Values Survey and European Values Survey",
    "text": "World Values Survey and European Values Survey\nThe World Values Survey and European Values survey collect data by conducting representative surveys in around 100 countries every five years. Their surveys are specifically designed to gather opinions on values ranging from political to religious to social. One of the questions they consistently ask is for respondents to indicate what level of confidence they have in their government. This will be our dependent variable of interest, Confidence in Governance.\n\n\nCode\ndata_final %>% ggplot(aes(x=Gov_Confidence)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Gov_Confidence,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Gov_Confidence,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Gov_Confidence,na.rm=TRUE)+\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Gov_Confidence,na.rm=TRUE)-\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Proportion of Population that has Confidence in Government\",\n       subtitle=\"Distribution of Confidence in Government 2017-2020\",\n       x=\"Confidence in Government\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html",
    "href": "posts/FinalPart2_CalebHill.html",
    "title": "Final Part 2",
    "section": "",
    "text": "Multiple research reports state that there is a relationship between re-hospitalization rates and social characteristics, such as demographic and economic identifiers, (Barnett, Hsu & McWilliams, 2015; Murray, Allen, Clark, Daly & Jacobs, 2021). Specifically, racial characteristics play a large role in predicting re-hospitalization in a population (Li, Cai & Glance, 2015). While some articles examine economic and health factors contributing to these disparities, very few dig deep into environmental factors that influence this phenomenon, (Spatz, Bernheim, Horwitz & Herrin, 2020). With your zipcode affecting up to 60% of your health outcomes, this research is relevant to better improving one of our most costly health expenditures: hospitalization.\nRe-hospitalization is a substantially costlier expenditure, as readmitting a patient further increases costs – especially if the diagnosis was untreated, poorly treated, or incorrectly treated. Most inpatient episodes characterized as a re-hospitalization when the patient is readmitted to the hospital 60 days after discharge. If the cause is different, sometimes that is counted as a re-hospitalization; other times, not so much, (Bhosale, K., Nath, R., Pandit, N., Agarwal, P., Khairnar, S., Yadav, B. & Chandrakar, S., 2020).\n\n\nThis paper aims to explore how different environmental variables impact re-hospitalization rates on a county-by-county level. Due to the nature of this project, we will not be controlling for racial, ethnic, and sex variables. These environmental factors will include both common environmental concerns, such as heat index, average temperature, precipitation, and natural disasters, along with the built environment, mean travel time to work, renter burden, and population density. We will also stratify by rural/urban classification, to determine if counties above or below 250,000 population experience differences in re-hospitalization rates, dependent upon these explanatory variables.\nThe data-set chosen for this analysis is taken from the Agency for Healthcare Research and Quality, Social Determinants of Health (SDOH) Database. This data-set has over 300 variables to explore each SDOH domain: social context, economic context, education, healthcare, and the environment. We shall pull data from three of these five domains: social, economic, and environmental.\nHow re-hospitalization is measured is not clarified per this data-set’s codebook. However, the Center for Medicare and Medicaid (CMS) 30-day Risk-Standardized Readmission Rate (RSRR) measures re-hospitalization as an unplanned readmission to inpatient services. It does stratify and specify based upon diagnosis. As the AHRQ is a federal agency alongside CMS, it is likely that they are pulling from CMS for this measure and aggregating various diagnoses into one county rate.\n\n\n\nThe hypothesis for this research report is:\n\nEnvironmental factors increase rates of re-hospitalization in the United States.\n\nTherefore, the null hypothesis is:\n\nEnvironmental factors do not increase rates of re-hospitalization in the United States.\n\nVarious regression analyses shall be employed to determine the relationship – or lack thereof – between these variables.\nFirst I’ll import the relevant libraries.\nThen I’ll import the dataset and view the first six rows.\n\n\nCode\ndf <- SDOH_2020_COUNTY_1_0 <- read_excel(\"_data/SDOH_2020_COUNTY_1_0.xlsx\", \n                                         sheet = \"Data\")\n\n\nWarning: Expecting logical in OA1673 / R1673C391: got '46123'\n\n\nWarning: Expecting logical in OA1765 / R1765C391: got '32510'\n\n\nWarning: Expecting logical in OB1765 / R1765C392: got '41025'\n\n\nWarning: Expecting logical in OC1765 / R1765C393: got '41037'\n\n\nWarning: Expecting logical in OA2799 / R2799C391: got '49017'\n\n\nWarning: Expecting logical in OB2799 / R2799C392: got '49019'\n\n\nWarning: Expecting logical in OC2799 / R2799C393: got '49025'\n\n\nWarning: Expecting logical in OD2799 / R2799C394: got '49055'\n\n\nWarning: Expecting logical in OA2844 / R2844C391: got '51760'\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 685\n   YEAR COUNTYFIPS STATEFIPS STATE COUNTY REGION TERRI…¹ ACS_T…² ACS_T…³ ACS_T…⁴\n  <dbl> <chr>      <chr>     <chr> <chr>  <chr>    <dbl>   <dbl>   <dbl>   <dbl>\n1  2020 01001      01        Alab… Autau… South        0   55639   54929   52404\n2  2020 01003      01        Alab… Baldw… South        0  218289  216518  206329\n3  2020 01005      01        Alab… Barbo… South        0   25026   24792   23694\n4  2020 01007      01        Alab… Bibb … South        0   22374   22073   21121\n5  2020 01009      01        Alab… Bloun… South        0   57755   57164   54250\n6  2020 01011      01        Alab… Bullo… South        0   10173   10143    9579\n# … with 675 more variables: ACS_TOT_POP_ABOVE15 <dbl>,\n#   ACS_TOT_POP_ABOVE16 <dbl>, ACS_TOT_POP_16_19 <dbl>,\n#   ACS_TOT_POP_ABOVE25 <dbl>, ACS_TOT_CIVIL_POP_ABOVE18 <dbl>,\n#   ACS_TOT_CIVIL_VET_POP_ABOVE25 <dbl>, ACS_TOT_OWN_CHILD_BELOW17 <dbl>,\n#   ACS_TOT_WORKER_NWFH <dbl>, ACS_TOT_WORKER_HH <dbl>,\n#   ACS_TOT_CIVILIAN_LABOR <dbl>, ACS_TOT_CIVIL_EMPLOY_POP <dbl>,\n#   ACS_TOT_POP_POV <dbl>, ACS_TOT_CIVIL_NONINST_POP_POV <dbl>, …\n\n\nNext I want to verify the class is a dataframe. Otherwise, I’ll need to transform the data to make it easier to work with.\n\n\nCode\nclass(df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nAll good here.\nNow on to data transformation. We will need to select only the relevant columns for this analysis.\n\n\nCode\ndf_new <- df %>%\n  select(COUNTYFIPS,\n         STATE,\n         COUNTY,\n         AHRF_USDA_RUCC_2013,\n         CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         ACS_PCT_COMMT_60MINUP,\n         ACS_PCT_RENTER_HU_COST_50PCT,\n         LTC_AVG_OBS_REHOSP_RATE) \nnrow(df_new)\n\n\n[1] 3229\n\n\nCode\nhead(df_new)\n\n\n# A tibble: 6 × 14\n  COUNTYF…¹ STATE COUNTY AHRF_…² CEN_P…³ NEPHT…⁴ NOAAC…⁵ NOAAC…⁶ NOAAS…⁷ SAIPE…⁸\n  <chr>     <chr> <chr>  <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 01001     Alab… Autau… 2          93.6       4    66.2    5.72      17   67565\n2 01003     Alab… Baldw… 3         137.        0    68.9    5.27      58   71135\n3 01005     Alab… Barbo… 6          28.3       3    66.4    5.75      24   38866\n4 01007     Alab… Bibb … 1          35.9       0    64.1    5.61      23   50907\n5 01009     Alab… Bloun… 1          89.6       0    62.7    5.96      45   55203\n6 01011     Alab… Bullo… 6          16.3       2    66.2    5.40      12   33124\n# … with 4 more variables: SAIPE_PCT_POV <dbl>, ACS_PCT_COMMT_60MINUP <dbl>,\n#   ACS_PCT_RENTER_HU_COST_50PCT <dbl>, LTC_AVG_OBS_REHOSP_RATE <dbl>, and\n#   abbreviated variable names ¹​COUNTYFIPS, ²​AHRF_USDA_RUCC_2013,\n#   ³​CEN_POPDENSITY_COUNTY, ⁴​NEPHTN_HEATIND_105, ⁵​NOAAC_AVG_TEMP_YEARLY,\n#   ⁶​NOAAC_PRECIPITATION_AVG_YEARLY, ⁷​NOAAS_TOT_NATURAL_DISASTERS,\n#   ⁸​SAIPE_MEDIAN_HH_INCOME\n\n\nOut of 1400+ variables, we’ve whittled them down to 14. Of those 14, we have four (4) that are unique identifiers (FIPS, State, County, and Rural-Urban Continuation Code), four (4) environmental, two (2) economic, one (1) housing, two (2) built-enviornment, and one (1) healthcare outcome.\nBefore we launch into exploring these variables via descriptive statistics, first we need to determine where the NAs are and see if any of the variables will have a substantial amount of missing data.\n\n\nCode\nkable(colSums(is.na(df_new)))\n\n\n\n\n\n\nx\n\n\n\n\nCOUNTYFIPS\n0\n\n\nSTATE\n0\n\n\nCOUNTY\n0\n\n\nAHRF_USDA_RUCC_2013\n9\n\n\nCEN_POPDENSITY_COUNTY\n8\n\n\nNEPHTN_HEATIND_105\n121\n\n\nNOAAC_AVG_TEMP_YEARLY\n123\n\n\nNOAAC_PRECIPITATION_AVG_YEARLY\n123\n\n\nNOAAS_TOT_NATURAL_DISASTERS\n0\n\n\nSAIPE_MEDIAN_HH_INCOME\n87\n\n\nSAIPE_PCT_POV\n87\n\n\nACS_PCT_COMMT_60MINUP\n8\n\n\nACS_PCT_RENTER_HU_COST_50PCT\n8\n\n\nLTC_AVG_OBS_REHOSP_RATE\n410\n\n\n\n\n\nPlenty of variables with missing data. Some are minor, such as population density, housing cost, and commute time variables with 8. Some are concerning, such as Heat Index, Average Yearly Temperature, and Average Yearly Precipitation, all around 120+.\nThe most concerning is – of course – our outcome variable, Re-Hospitalization Rates. This is not ideal. However, 410 / 3229 (12.6%) is not bad. That still leaves us with plenty of counties to review.\n\n\nCode\ndf_new <- df_new %>%\n  drop_na() %>%\n  print(nrow(df_new))\n\n\n# A tibble: 2,814 × 14\n   COUNTYFIPS STATE   COUNTY          AHRF_USDA_RUCC_2013 CEN_POPDENSITY_COUNTY\n   <chr>      <chr>   <chr>           <chr>                               <dbl>\n 1 01001      Alabama Autauga County  2                                    93.6\n 2 01003      Alabama Baldwin County  3                                   137. \n 3 01005      Alabama Barbour County  6                                    28.3\n 4 01007      Alabama Bibb County     1                                    35.9\n 5 01009      Alabama Blount County   1                                    89.6\n 6 01011      Alabama Bullock County  6                                    16.3\n 7 01013      Alabama Butler County   6                                    25.4\n 8 01015      Alabama Calhoun County  3                                   189. \n 9 01017      Alabama Chambers County 6                                    56.0\n10 01019      Alabama Cherokee County 6                                    47.0\n   NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_YEARLY NOAAC_PRECIPITATION_AVG_YEARLY\n                <dbl>                 <dbl>                          <dbl>\n 1                  4                  66.2                           5.72\n 2                  0                  68.9                           5.27\n 3                  3                  66.4                           5.75\n 4                  0                  64.1                           5.61\n 5                  0                  62.7                           5.96\n 6                  2                  66.2                           5.40\n 7                  0                  67.1                           5.37\n 8                  0                  63.2                           5.74\n 9                  0                  63.3                           5.93\n10                  0                  62.3                           5.85\n   NOAAS_TOT_NATURAL_DISASTERS SAIPE_MEDIAN_HH_INCOME SAIPE_PCT_POV\n                         <dbl>                  <dbl>         <dbl>\n 1                          17                  67565          11.2\n 2                          58                  71135           8.9\n 3                          24                  38866          25.5\n 4                          23                  50907          17.8\n 5                          45                  55203          13.1\n 6                          12                  33124          30.8\n 7                          18                  42268          20.6\n 8                          14                  50259          14.5\n 9                          18                  39318          16.3\n10                          38                  50388          14.7\n   ACS_PCT_COMMT_60MINUP ACS_PCT_RENTER_HU_COST_50PCT LTC_AVG_OBS_REHOSP_RATE\n                   <dbl>                        <dbl>                   <dbl>\n 1                  6.06                         26.6                    0.14\n 2                  7.53                         20.8                    0.14\n 3                 11.8                          22.4                    0.22\n 4                 10.4                          27.4                    0.16\n 5                 18.6                          22.6                    0.14\n 6                 12.7                          34                      0.05\n 7                  9.14                         29.5                    0.11\n 8                  7.21                         20.5                    0.13\n 9                  5.93                         13.0                    0.15\n10                 10.2                          12.2                    0.19\n# … with 2,804 more rows\n\n\n2,814 x 14 is a good place to start."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#descriptive-statistics",
    "href": "posts/FinalPart2_CalebHill.html#descriptive-statistics",
    "title": "Final Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nFor our preliminary analysis, we’re going to provide summary statistics analyzing the 10 variables relevant to our research question, from Population Density to the end of the data-set, and a visualization for each.\n\n\nCode\nkable(describe(df_new))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\n\nCOUNTYFIPS*\n1\n2814\n1.407500e+03\n8.124762e+02\n1407.50000\n1.407500e+03\n1043.009100\n1.000000e+00\n2814.00000\n2.813000e+03\n0.0000000\n-1.2012794\n15.3161135\n\n\nSTATE*\n2\n2814\n2.442928e+01\n1.359471e+01\n23.00000\n2.441874e+01\n16.308600\n1.000000e+00\n48.00000\n4.700000e+01\n0.0584273\n-1.2662366\n0.2562759\n\n\nCOUNTY*\n3\n2814\n8.369058e+02\n4.694846e+02\n831.50000\n8.339405e+02\n584.885700\n1.000000e+00\n1674.00000\n1.673000e+03\n0.0526568\n-1.1127220\n8.8503264\n\n\nAHRF_USDA_RUCC_2013*\n4\n2814\n4.772210e+00\n2.619826e+00\n6.00000\n4.717140e+00\n2.965200\n1.000000e+00\n9.00000\n8.000000e+00\n0.0039932\n-1.2985960\n0.0493867\n\n\nCEN_POPDENSITY_COUNTY\n5\n2814\n2.893180e+02\n1.877174e+03\n50.74500\n8.620448e+01\n55.404762\n5.000000e-01\n71895.54000\n7.189504e+04\n25.8518141\n850.3767977\n35.3868910\n\n\nNEPHTN_HEATIND_105\n6\n2814\n4.098792e+00\n8.186776e+00\n0.00000\n1.963144e+00\n0.000000\n0.000000e+00\n59.00000\n5.900000e+01\n2.9358498\n9.6132423\n0.1543302\n\n\nNOAAC_AVG_TEMP_YEARLY\n7\n2814\n5.627572e+01\n8.030342e+00\n55.95833\n5.617415e+01\n8.784405\n3.541667e+01\n78.49167\n4.307500e+01\n0.1339838\n-0.5932113\n0.1513812\n\n\nNOAAC_PRECIPITATION_AVG_YEARLY\n8\n2814\n3.611196e+00\n1.628908e+00\n3.62375\n3.638924e+00\n1.887844\n2.241667e-01\n9.74500\n9.520833e+00\n-0.0899446\n-0.7490480\n0.0307068\n\n\nNOAAS_TOT_NATURAL_DISASTERS\n9\n2814\n3.678074e+01\n4.541085e+01\n25.00000\n2.851510e+01\n19.273800\n0.000000e+00\n662.00000\n6.620000e+02\n5.3277337\n46.7140570\n0.8560470\n\n\nSAIPE_MEDIAN_HH_INCOME\n10\n2814\n5.738628e+04\n1.442985e+04\n55107.00000\n5.584910e+04\n11735.520300\n2.599700e+04\n155362.00000\n1.293650e+05\n1.4172637\n3.6789671\n272.0193680\n\n\nSAIPE_PCT_POV\n11\n2814\n1.371606e+01\n5.320367e+00\n12.80000\n1.323637e+01\n4.744320\n3.000000e+00\n39.60000\n3.660000e+01\n1.0397109\n1.6688697\n0.1002951\n\n\nACS_PCT_COMMT_60MINUP\n12\n2814\n8.155924e+00\n4.864799e+00\n6.85500\n7.487016e+00\n3.810282\n0.000000e+00\n35.91000\n3.591000e+01\n1.5015546\n3.0016425\n0.0917071\n\n\nACS_PCT_RENTER_HU_COST_50PCT\n13\n2814\n2.060974e+01\n6.649144e+00\n20.78000\n2.057608e+01\n6.093486\n0.000000e+00\n49.26000\n4.926000e+01\n0.1136840\n0.5492796\n0.1253440\n\n\nLTC_AVG_OBS_REHOSP_RATE\n14\n2814\n1.449645e-01\n8.432610e-02\n0.14000\n1.422425e-01\n0.059304\n0.000000e+00\n1.00000\n1.000000e+00\n1.6404370\n12.5443704\n0.0015896\n\n\n\n\n\nWe should note for many of these analyses that the Urban / Rural Continuum Code runs from 1.00 to 9.00. Anything 4.00 or higher would be classified as Rural, with an urban population of less than 250,000.\n\nPopulation Density\n\n\nCode\npop_den <- df_new %>%\n  filter(CEN_POPDENSITY_COUNTY < 5000)\n\nggplot(pop_den, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\nWe’ve surely got some out-liers. The mean is 291, but the median is 46. The max is 70,000. We’ve filtered those out for this visualization and set the bins close to the median. A left-skewed variable is expected, as the majority of counties in the United States would be classified as rural and therefore have low population densities.\nLet’s plot a facet-grid on these codes.\n\n\nCode\nggplot(pop_den, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50) +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n\n\n\nAs expected, a large part of the distribution is 4.0 or higher. 1.0 also has a high variation of population density, which may cause issues with the regression.\n\n\nHeat Index Over 105F\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_boxplot()\n\n\n\n\n\nDue to the wide range in climate for the United States, it’s not surprising that there’s a large variety of out-liers. The median number of days a county experienceed a heat index of over 105F each year is 4 days per year. One county even reached 59 days – a Texas county!\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe data-set has a very left skewed distribution, similar to Population Density. Most counties experience under 10 days with a Heat Index over 105.\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis distribution stays fairly constant, regardless of UR classification. 2.00 and 6.00 may have interesting insights, as their right tails are more pronounced, but that would be better suited to a map for quick reference. That is outside the scope of this project.\n\n\nAverage Yearly Temperature\n\n\nCode\nggplot(df_new, aes(NOAAC_AVG_TEMP_YEARLY)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere’s a good distribution. Average temperature each month is between 50 to 60 for most of the counties. The range (45) is also fairly large and shows the multiple climates within its borders.\n\n\nAverage Yearly Precipitation\n\n\nCode\nggplot(df_new, aes(NOAAC_PRECIPITATION_AVG_YEARLY)) +\n  geom_boxplot()\n\n\n\n\n\nAverage precipitation each month is fairly uniform, with the mean at 3.49 inches of rain, on average, each month. This variable will most likely provide less variation in the analysis compared to others, such as population density and heat index. This can be both a good and a bad thing, as variations in precipitation was one of the variables I was most interested in exploring for this project. Oh well.\n\n\nTotal Natural Disasters\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_boxplot()\n\n\n\n\n\nMany high out-liers over 100; some even reaching over 600. Let’s plot a histogram to get a better look at the data’s distribution.\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA left skewed variable, with observations dropping off dramatically once we reach 50 total recorded natural disasters.\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLittle to no difference in UR classification for natural disaster out-liers.\n\n\nMedian Household Income\n\n\nCode\nggplot(df_new, aes(SAIPE_MEDIAN_HH_INCOME)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVery close to a normal distribution, if barely left-skewed. A couple of high out-liers, hovering around $90,000+ in median household income, but the mean holds at $57,465.\n\n\nPercent in Poverty\n\n\nCode\nggplot(df_new, aes(SAIPE_PCT_POV)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother close to normal distribution. Most counties have poverty rates ranging from 10% to 20%. There are of course out-liers, especially a good number below 10%, but those are rare.\n\n\nCode\nggplot(df_new, aes(SAIPE_PCT_POV)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHighly urbanized counties (1.00) have substantially less percentage poverty compared to their rural counterparts. 7.00, 8.00, and 9.00 have the highest spread, with some counties reaching 40% poverty rates! We barely see the urban areas (1.00 - 3.00) reach 30% poverty.\n\n\nPercent Commuting Alone, Over 60 Minutes\n\n\nCode\nggplot(df_new, aes(ACS_PCT_COMMT_60MINUP)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe majority of counties fall below 10% of their population commuting up to and more than 60 minutes for work. Let’s do another facet grid to see if there’s a relationship between UR codes.\n\n\nCode\nggplot(df_new, aes(ACS_PCT_COMMT_60MINUP)) +\n  geom_histogram() +\n  facet_wrap('AHRF_USDA_RUCC_2013')\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNot particularly. The only codes that appear different than the rest include 1.00 (highly urban, over 1 million population) and 8.00 (completely rural, fewer than 2,500 population). 7.00 and higher is surprising, as these are counties with very little population and often not adjacent to metro areas. Therefore, populations are most likely condensed around “urban” centers for economic purposes.\n\n\nPercent Renter Housing Costs Over 50 Percent of Income\n\n\nCode\nggplot(df_new, aes(ACS_PCT_RENTER_HU_COST_50PCT)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is a startling figure. On average, 20% of counties have renters where 50% or more of their income goes toward housing costs. These leaves little to no room for other expenses and drives economic instability. The data is normally distributed and barely left-skewed – but still an item to consider with further analysis.\n\n\nRe-hospitalization Rate\n\n\nCode\nggplot(df_new, aes(LTC_AVG_OBS_REHOSP_RATE)) +   \n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother right skewed variable. Lots of counties with 0.00 rates of re-hospitalization, and few, if any, above 0.50 per 100,000 people. From a health perspective, this is good news! From a research perspective, that’s going to make analysis a little trickier. However, the somewhat normal and/or bimodal distribution should be fairly easy to work with. While needing some transformation for a linear regression, we can test multiple models per each variable to determine which amendment provides the most robust inference."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#analysis",
    "href": "posts/FinalPart2_CalebHill.html#analysis",
    "title": "Final Part 2",
    "section": "Analysis",
    "text": "Analysis\n\nHypothesis Testing\nRemember that the hypothesis for this research report is:\n\nEnvironmental factors increase rates of re-hospitalization in the United States.\n\nWe have nine (9) explanatory variables to work with, so we can run different regressions to determine what variables influence re-hospitalization rates the most – if at all – and how they interact with other variables.\nReminder that the nine (9) explanatory variable are broken down into three domains: environmental, economic, and built environment.\nEnvironmental entails:\n\nDay with Heat Index over 105F\nAverage Annual Precipitation\nAverage Annual Precipitation\nTotal Natural Disasters Per Year\n\nEconomic is:\n\nMedian Household Income\nPercent Poverty\n\nAnd the Built Environment includes:\n\nPopulation Density\nPercent Rental Housing Cost, over 50%\nPercent Commuting Alone, over 60 minutes\n\nWe will run four models to test the hypothesis. They shall examine each environmental variable’s impact on the dependent variable, re-hospitalization rates. The control variables will be the economic and built environment variables, five (5) in total.\n\n\nCode\nmodel1 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NEPHTN_HEATIND_105 +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n\nmodel2 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAC_AVG_TEMP_YEARLY +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n  \nmodel3 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAC_PRECIPITATION_AVG_YEARLY +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n  \nmodel4 <- lm(LTC_AVG_OBS_REHOSP_RATE ~ \n               NOAAS_TOT_NATURAL_DISASTERS +\n               CEN_POPDENSITY_COUNTY +\n               SAIPE_MEDIAN_HH_INCOME +\n               SAIPE_PCT_POV +\n               ACS_PCT_COMMT_60MINUP +\n               ACS_PCT_RENTER_HU_COST_50PCT,\n             df_new)\n\n\nLet’s plot these regressions, removing the control variables to get a better visualization. For two of the models, we will employ log transformations for data that is skewed. These variables were identified during the Descriptive Statistic section. This shall include a log transformation for the response variable in particular.\n\n\nCode\nggplot(df_new, aes(NEPHTN_HEATIND_105,\n       log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 1\",\n       x = \"Heat Index Over 105F\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (stat_smooth).\n\n\n\n\n\nCode\nggplot(df_new, aes(log(NOAAC_AVG_TEMP_YEARLY),\n       log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 2\",\n       x = \"Average Annual Temperature\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (stat_smooth).\n\n\n\n\n\nCode\nggplot(df_new, aes(NOAAC_PRECIPITATION_AVG_YEARLY,\n                   log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 3\",\n       x = \"Average Annual Precipitation\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (stat_smooth).\n\n\n\n\n\nCode\nggplot(df_new, aes(NOAAS_TOT_NATURAL_DISASTERS,\n                   log(LTC_AVG_OBS_REHOSP_RATE))) +\n  geom_point() +\n  geom_smooth(method = lm,\n              se = FALSE,\n              fullrange = TRUE) +\n  labs(title = \"Model 4\",\n       x = \"Total Natural Disasters\",\n       y = \"Re-Hospitalization Rates\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 235 rows containing non-finite values (stat_smooth).\n\n\n\n\n\nOf these four, it looks like temperature and precipitation have the best fit. All but Total Natural Disasters have a positive relationship with the response variable, so that helps us in determining if we should accept or reject the null hypothesis.\nWe will reject the null hypothesis. While it looks like, at first glance, that there is little positive relationship, we can at least note that there is some positive relationship. In the next two sections, we will dig deeper into each model, examining the p-value and R-Squared value, to see what level of relationship is present.\n\n\nModel Comparisons\nNow we will compare the four (4) models in more depth.\n\n\nCode\nsummary(model1)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NEPHTN_HEATIND_105 + CEN_POPDENSITY_COUNTY + \n    SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + ACS_PCT_COMMT_60MINUP + \n    ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24905 -0.04273 -0.00201  0.03850  0.86257 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   5.582e-02  1.553e-02   3.595 0.000330 ***\nNEPHTN_HEATIND_105            6.909e-04  1.959e-04   3.526 0.000428 ***\nCEN_POPDENSITY_COUNTY         6.250e-07  8.575e-07   0.729 0.466180    \nSAIPE_MEDIAN_HH_INCOME        4.223e-07  1.825e-07   2.313 0.020777 *  \nSAIPE_PCT_POV                 3.331e-03  5.166e-04   6.449 1.32e-10 ***\nACS_PCT_COMMT_60MINUP        -4.504e-05  3.321e-04  -0.136 0.892129    \nACS_PCT_RENTER_HU_COST_50PCT  8.043e-04  2.517e-04   3.195 0.001413 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0824 on 2807 degrees of freedom\nMultiple R-squared:  0.04725,   Adjusted R-squared:  0.04521 \nF-statistic:  23.2 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model2)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAC_AVG_TEMP_YEARLY + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22744 -0.04216 -0.00227  0.03810  0.87918 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  -3.293e-03  1.691e-02  -0.195 0.845614    \nNOAAC_AVG_TEMP_YEARLY         1.721e-03  2.236e-04   7.697 1.91e-14 ***\nCEN_POPDENSITY_COUNTY         6.974e-07  8.498e-07   0.821 0.411866    \nSAIPE_MEDIAN_HH_INCOME        2.163e-07  1.835e-07   1.178 0.238760    \nSAIPE_PCT_POV                 2.082e-03  5.440e-04   3.827 0.000132 ***\nACS_PCT_COMMT_60MINUP        -4.280e-04  3.338e-04  -1.282 0.199842    \nACS_PCT_RENTER_HU_COST_50PCT  6.661e-04  2.498e-04   2.667 0.007701 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08172 on 2807 degrees of freedom\nMultiple R-squared:  0.06281,   Adjusted R-squared:  0.06081 \nF-statistic: 31.35 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model3)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAC_PRECIPITATION_AVG_YEARLY + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24203 -0.04423 -0.00280  0.03765  0.86494 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     3.851e-02  1.539e-02   2.502  0.01240 *  \nNOAAC_PRECIPITATION_AVG_YEARLY  8.861e-03  1.046e-03   8.470  < 2e-16 ***\nCEN_POPDENSITY_COUNTY           4.913e-07  8.473e-07   0.580  0.56206    \nSAIPE_MEDIAN_HH_INCOME          5.067e-07  1.801e-07   2.813  0.00495 ** \nSAIPE_PCT_POV                   2.954e-03  5.089e-04   5.805 7.15e-09 ***\nACS_PCT_COMMT_60MINUP          -5.921e-04  3.358e-04  -1.763  0.07794 .  \nACS_PCT_RENTER_HU_COST_50PCT    4.636e-04  2.514e-04   1.844  0.06534 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08154 on 2807 degrees of freedom\nMultiple R-squared:  0.06688,   Adjusted R-squared:  0.06489 \nF-statistic: 33.53 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(model4)\n\n\n\nCall:\nlm(formula = LTC_AVG_OBS_REHOSP_RATE ~ NOAAS_TOT_NATURAL_DISASTERS + \n    CEN_POPDENSITY_COUNTY + SAIPE_MEDIAN_HH_INCOME + SAIPE_PCT_POV + \n    ACS_PCT_COMMT_60MINUP + ACS_PCT_RENTER_HU_COST_50PCT, data = df_new)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24509 -0.04423 -0.00198  0.03901  0.85940 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   4.845e-02  1.564e-02   3.099  0.00196 ** \nNOAAS_TOT_NATURAL_DISASTERS  -5.117e-05  3.596e-05  -1.423  0.15490    \nCEN_POPDENSITY_COUNTY         4.512e-07  8.578e-07   0.526  0.59891    \nSAIPE_MEDIAN_HH_INCOME        5.359e-07  1.876e-07   2.856  0.00432 ** \nSAIPE_PCT_POV                 3.750e-03  5.097e-04   7.358 2.44e-13 ***\nACS_PCT_COMMT_60MINUP        -2.872e-05  3.331e-04  -0.086  0.93131    \nACS_PCT_RENTER_HU_COST_50PCT  7.913e-04  2.526e-04   3.132  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08255 on 2807 degrees of freedom\nMultiple R-squared:  0.04372,   Adjusted R-squared:  0.04168 \nF-statistic: 21.39 on 6 and 2807 DF,  p-value: < 2.2e-16\n\n\nThree of the four explanatory variables meet the statistical significance threshold (0.001). Total Natural Disasters Per Year do not. This helps in finalizing whether to accept or reject the null hypothesis. The p-value is most significant for precipitation, model 3.\nFor the model fit, the adjusted R squared ranges from 0.04 to 0.06. The highest is for precipitation, model 3.\nPrecipitation also has the lowest residual standard error, at 0.081.\nFinally, we’re going to calculate the PRESS statistic (Predicted Residual Sum of Squares) to best determine which model can predict the response variable based upon the explanatory variables.\n\n\nCode\nPRESS <- function(model) {\n  i <- residuals(model)/(1 - lm.influence(model)$hat)\n  sum(i^2)\n}\n\nPRESS(model1)\n\n\n[1] 19.17066\n\n\nCode\nPRESS(model2)\n\n\n[1] 18.86294\n\n\nCode\nPRESS(model3)\n\n\n[1] 18.77811\n\n\nCode\nPRESS(model4)\n\n\n[1] 19.23593\n\n\nModel 3 has the lowest PRESS score.\nDue to a strong p-value, PRESS score, and model fit compared to the other three models, model 3 will be chosen as the final model for the diagnostic exploration. While the adjusted R squared value is not strong when controlling for economic and built environment factors, there is still a positive relationship, and therefore some influence on the dependent variable.\n\n\nDiagnostics\nFinally, we’ll plot the diagnostics to best understand the model.\n\n\nCode\npar(mfrow = c(2,3));\nplot(model3, \n     which = 1:6)\n\n\n\n\n\nOf the six plots, Cook’s distance is the most striking and relevant, as there are three out-liers: 1679, 2189, and 2455.\n1679 is New York County, New York.\n2189 is Mellette County, South Dakota.\n2455 is Real County, Texas.\nOtherwise, the plot looks good.\nNormal Q-Q violates this test, as the points at the right tail of the plot do not generally fall along the line. This is very apparent for our three out-liers. The remaining plots do not violate their tested assumptions and further cement the model’s reliability."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#conclusion",
    "href": "posts/FinalPart2_CalebHill.html#conclusion",
    "title": "Final Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper explored the relationship between re-hospitalization rates and four environmental variables, when controlling for common variables that regularly influence the dependent variable. These four variables included days with a heat index over 105F, average annual temperature, average annual precipitation, and total natural disasters.\nFour models were selected, one for each variable, to best determine which measure best impacted re-hospitalization rates. Three of the four variables were statistically significant and two had a larger adjusted R squared value than the others. Precipitation was selected as the variable with the best model fit to explain re-hospitalization rate impact. While the adjusted R-squared value is negligible (0.06), there is a positive relationship that is statistically significant. Therein we see some form of an influence large amounts of annual precipitation has on re-hospitalization rates.\nI would have liked to tighten the analysis further instead of including multiple (10) variables, by focusing on some key measurements: Precipitation as the explanatory, Poverty as the control, and filtering by Rurality to determine the relationship with re-hospitalization rates. I could then fit different models (Simple Linear, Poisson, Polynomial, etc.) to see which worked best. I may not have time to do so for the poster presentation, nor would that perhaps be within the scope of this project. Either way, I have something for future classes, perhaps via time series analysis or machine learning (prediction over inference). Either way, this helped me better understand the robustness of linear regression models."
  },
  {
    "objectID": "posts/FinalPart2_CalebHill.html#references",
    "href": "posts/FinalPart2_CalebHill.html#references",
    "title": "Final Part 2",
    "section": "References",
    "text": "References\nBarnett, M., Hsu, J. & McWilliams, M. (2015). “Patient Characteristics and Differences in Hospital Readmission Rates.” JAMA Intern Med., 175(11): 1803-1812.\nBhosale KH, Nath RK, Pandit N, Agarwal P, Khairnar S, Yadav B, & Chandrakar S. (2020). “Rate of Rehospitalization in 60 Days of Discharge and It’s Determinants in Patients with Heart Failure with Reduced Ejection Fraction in a Tertiary Care Centre in India.” Int J Heart Fail. 21;2(2):131-144.\nLi, Y., Cai, X. & Glance, L. (2015). “Disparities in 30-day rehospitalization rates among Medicare skilled nursing facility residents by race and site of care.” Med Care, 53(12): 1058-1065.\nMurray, F., Allen, M., Clark, C., Daly, C. & Jacobs, D. (2021). “Socio-demographic and -economic factors associated with 30-day readmission for conditions targeted by the hospital readmissions reduction program: a population-based study.” BMC Public Health, 21.\nSpatz, E., Bernheim, S., Horwitz, L. & Herrin, J. (2020). Community factors and hospital wide readmission rates: Does context matter? PLoS One, 15(10)."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html",
    "href": "posts/FinalPart2_ToryBarteloni.html",
    "title": "DACSS 603: Final Part 2",
    "section": "",
    "text": "One of the foundational building blocks of a stable society is its government. Perhaps especially in modern times where populations are often large in size, spread over vast distances, and societies are interconnected in ways never before observed in the historical record. It is thus governments that play a pivotal role in mediating relationships within the state and between states. When viewed as part of a social ecosystem, a financially healthy and legitimate government is one core pillar in a stable society (Turchin 2013).\nThe concept of political trust can be succinctly understood as the condition when a people or person expect their government will act in their interest and do the right thing in the absence of constant scrutiny (Miller 1990). Given that this is the case, it is not a far leap to say that political trust is one mechanism by which a government can gain and retain legitimacy. Furthermore, research has shown that trust in government is critical for governments to be able to operate without coercive techniques and be able to collect and allocate resources for collective projects (Hetherington 2006). With this in mind we find it reasonable to seek an understanding of why a person or group of people would trust and have confidence in their government.\nResearch indicates that at least three types of factors have an impact on a group’s level of trust in their government. The first set of factors are related to the public’s perception of government performance. Multiple studies have observed that the approval of government action or responsiveness, control over crime and the economy, and the perception of corruption all had significant effects on trust in government (Keele 2007; Catterberg 2006).\nThe second set of factors relate to the economy. Perceptions of both micro and macro economic health have been observed to have significant positive effects on trust in government (Keele 2007; Catterberg 2006). While other economic factors such as income inequality, economic growth rates, and structural unemployment have shown ties to trust in government, but not as strong as the perceptions of economic health mentioned previously (Lawrence 1997; Foster and Frieden 2017).\nThe last set of factors to consider are cultural factors. We see this set of factors in two differing settings. The first is observed when values within a society change. One such instance is the shift toward individualism and postmaterial values that caused a decline in respect for authority (Inglehart 1999) or where a decline in social capital was shown to have significant negative effects on trust in government (Keele 2007). The second setting is between societies. Francis Fukuyama describes this phenomenon well, showing how trust operates differently in different cultures with the implication that reasons for trust and the mode of trust differs between groups (1996).\nTo this point there has been no consensus or holistic model that produces a satisfactory answer to the question why do groups trust their government? In this study I will try to bring us one step closer by examining a model that takes into account aspects from each of the sets of factors just described."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#the-dependent-variable",
    "href": "posts/FinalPart2_ToryBarteloni.html#the-dependent-variable",
    "title": "DACSS 603: Final Part 2",
    "section": "The Dependent Variable",
    "text": "The Dependent Variable\nIn order to do this we must establish a measurement for political trust. I will start by acknowledging that a consistent, trusted measure of trust in government on an international scale is not easily available. In this study we will use a survey measurement from the World Values Survey and European Values Study joint data set that asks about personal confidence in government. We do this because we have a high degree of trust in the measurement and because we can draw strong correlations between confidence and trust in government measures.\nThe World Values Survey and European Values Study collect data by conducting representative surveys in around 100 countries every five years (note: the number of countries has expanded over their active years). Their surveys are specifically designed to gather public opinions on values ranging from political, religious, and social. The question we will use is “…for each item listed, how much confidence you have in them…” and responses to the sub-item “Government”. The possible answers are on a 4 point scale ranging from “A great deal” to “None at all”. In the latest iteration of their survey we are able to get measurement of confidence in government from 87 countries. Our question focuses on groups so in this study we aggregate the survey data to the country level."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#the-independent-variables",
    "href": "posts/FinalPart2_ToryBarteloni.html#the-independent-variables",
    "title": "DACSS 603: Final Part 2",
    "section": "The Independent Variables",
    "text": "The Independent Variables\nThe independent or explanatory variables we use vary in their sources and formats so I will spend some time discussing each source and the variables used from each.\n\nWorld Governance Indicators Project\nThe World Governance Indicators are a combination of enterprise, citizen, and expert survey respondents from around the world. They use more than 30 surveys to create their six indicators with each indicator using different surveys and data from the surveys to aggregate to the final indicator. The one I am most interested in is Voice and Accountability which attempts to measure the level of political freedom (i.e. freedom of speech, press, etc.) and access to participation in governance by the public (i.e. free and fair elections). This will be the explanatory variable we will focus on for understanding effects, while the remaining independent variables will be used to control for effects.\n\n\nWorld Bank Development Indicators\nThe World Bank Development Indicators are taken from a wide variety of sources. We will be using two primary indicators: GDP per capita and Intentional Homicides per 100K people. GDP per capita is derived from the World Bank and OECD National Accounts data while Intentional Homicides are taken from the UN Office on Drugs and Crime’s International Homicide Statistics database.\n\n\nWorld Values Survey and European Values Study\nIn addition to our dependent variable being from this data set we will also be using the Inglehart Welzel Cultural Map that is derived from the World Values Survey. This map is derived through factor analysis of 10 individual indicators included in the survey and is designed to express broad underlying dimensions of cross-culture variation.The two major dimensions that comprise the categories are Traditional vs. Secular-rational values and Survival vs. Self-expression values. We get indicators for each of these dimensions as well as a categorical assignment for each country from those indicators. The category assignments are somewhat more subjective, but does well to take into account other, broader cultural factors not explicit in the data.\n\n\nTransparency International\nThe Corruption Perceptions Index (CPI) is created by Transparency International by taking a combination of 13 different data sources including assessments and surveys. These sources are largely comprised of experts and business interests so are not a direct reflection of the general public but a broader sense of the corruption perceived to be inherit in the country. The scores from each of the sources are standardized, averaged, and then scaled to provide a score for each of the countries in the data sources. What we end up with is Corruption Perception Score between 1-100 for each of the countries."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#summary-of-data",
    "href": "posts/FinalPart2_ToryBarteloni.html#summary-of-data",
    "title": "DACSS 603: Final Part 2",
    "section": "Summary of Data",
    "text": "Summary of Data\nFirst things first, I will include a brief look at the data set and then we will look at the specifics.\n\n\nCode\nknitr::kable(str(data_final))\n\n\n'data.frame':   81 obs. of  13 variables:\n $ Country_Final                 : chr  \"Denmark\" \"New Zealand\" \"Finland\" \"Singapore\" ...\n $ WGI_Voice_Accountability      : num  1.545 1.535 1.565 -0.206 1.561 ...\n $ GDP_per_Capita_PP             : num  57162 43110 48583 98283 52851 ...\n $ GDP_per_Capita                : num  59776 42865 48629 65831 51939 ...\n $ Homicides_per_100K            : num  1.143 2.634 1.591 0.207 1.106 ...\n $ Unemployment                  : num  5.02 4.11 6.69 3.1 6.83 ...\n $ CPI.Score.2018                : int  88 87 85 85 85 85 84 82 81 80 ...\n $ Gov_Confidence                : num  0.408 0.539 0.453 0.819 0.539 ...\n $ Gov_Confidence_Mean           : num  2.69 2.45 2.63 1.99 2.5 ...\n $ Survival                      : num  2.9177 2.8837 2.4782 -0.0969 3.1443 ...\n $ Traditional                   : num  1.0599 0.5741 0.8324 0.0998 1.1486 ...\n $ Cultural_Map                  : Factor w/ 8 levels \"African-Islamic\",..: 7 4 7 8 7 7 7 7 4 7 ...\n $ WGI_Voice_Accountability_Poly2: num  2.3876 2.357 2.4488 0.0424 2.4365 ...\n\n\n|| || || ||\n\n\nCode\nknitr::kable(summary(data_final))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Final\nWGI_Voice_Accountability\nGDP_per_Capita_PP\nGDP_per_Capita\nHomicides_per_100K\nUnemployment\nCPI.Score.2018\nGov_Confidence\nGov_Confidence_Mean\nSurvival\nTraditional\nCultural_Map\nWGI_Voice_Accountability_Poly2\n\n\n\n\n\nLength:81\nMin. :-1.7968\nMin. : 2221\nMin. : 855.8\nMin. : 0.2067\nMin. : 0.50\nMin. :17.00\nMin. :0.08744\nMin. :1.561\nMin. :-1.84730\nMin. :-1.76002\nAfrican-Islamic :22\nMin. :0.000042\n\n\n\nClass :character\n1st Qu.:-0.5154\n1st Qu.:12486\n1st Qu.: 4604.6\n1st Qu.: 0.7857\n1st Qu.: 3.67\n1st Qu.:33.00\n1st Qu.:0.24247\n1st Qu.:2.504\n1st Qu.:-0.65383\n1st Qu.:-0.92193\nCatholic Europe :14\n1st Qu.:0.104813\n\n\n\nMode :character\nMedian : 0.1687\nMedian :22066\nMedian :10076.4\nMedian : 1.6314\nMedian : 5.01\nMedian :43.00\nMedian :0.38584\nMedian :2.797\nMedian :-0.09693\nMedian :-0.05596\nOrthodox Europe :12\nMedian :0.734658\n\n\n\nNA\nMean : 0.1641\nMean :27597\nMean :19974.7\nMean : 3.9043\nMean : 6.66\nMean :48.93\nMean :0.41566\nMean :2.711\nMean : 0.22695\nMean :-0.12081\nLatin America :10\nMean :0.908872\n\n\n\nNA\n3rd Qu.: 0.9790\n3rd Qu.:41522\n3rd Qu.:29554.5\n3rd Qu.: 3.8069\n3rd Qu.: 8.53\n3rd Qu.:64.00\n3rd Qu.:0.53356\n3rd Qu.:3.018\n3rd Qu.: 0.72326\n3rd Qu.: 0.60743\nProtestant Europe: 8\n3rd Qu.:1.513324\n\n\n\nNA\nMax. : 1.6552\nMax. :98283\nMax. :85334.5\nMax. :28.7367\nMax. :19.66\nMax. :88.00\nMax. :0.95441\nMax. :3.448\nMax. : 3.14431\nMax. : 1.64128\nConfucian : 5\nMax. :3.228473\n\n\n\nNA\nNA\nNA\nNA\nNA’s :17\nNA\nNA\nNA\nNA\nNA\nNA\n(Other) :10\nNA\n\n\n\n\n\nWe can see from the overview that the data has 81 observations of 12 variables. Each observation in the data is a country and there are observations for the variables we have discussed earlier. All of our variables of interest are continuous except the Cultural Map variable, which is a factor."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#transparency-international-1",
    "href": "posts/FinalPart2_ToryBarteloni.html#transparency-international-1",
    "title": "DACSS 603: Final Part 2",
    "section": "Transparency International",
    "text": "Transparency International\nThe Corruption Perceptions Index (CPI) is created by Transparency International The CPI Scores are not a direct reflection of the general public, but represent a broad view of corruption for the countries included. The Corruption Perception Score ranges between 1-100 for each of the countries.\n\n\nCode\ndata_final %>% ggplot(aes(x=CPI.Score.2018)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(CPI.Score.2018,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(CPI.Score.2018,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 quantile(CPI.Score.2018, probs=0.75, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               quantile(CPI.Score.2018, probs=0.25, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Corruption Perceptions Index\",\n       subtitle=\"Distribution of CPI 2018\",\n       x=\"CPI Score\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "href": "posts/FinalPart2_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "title": "DACSS 603: Final Part 2",
    "section": "World Bank Development and Governance Indicators",
    "text": "World Bank Development and Governance Indicators\nThe World Bank collects data from many different sources including two distinct projects in World Development Indicators and the World Governance Indicators project.\nWe will be using two primary indicators from the Development Indicators: GDP per capita and Intentional Homicides per 100K people. GDP per capita is derived from the World Bank and OECD National Accounts data while Intentional Homicides are taken from the UN Office on Drugs and Crime’s International Homicide Statistics database.\n\n\nCode\ndata_final %>% ggplot(aes(x=GDP_per_Capita)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(GDP_per_Capita,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(GDP_per_Capita,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 quantile(GDP_per_Capita, probs=0.75, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               quantile(GDP_per_Capita, probs=0.25, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Gross Domestic Product per Capita\",\n       subtitle=\"Distribution of GDP per capita 2019\",\n       x=\"GDP per Capita\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\ndata_final %>% ggplot(aes(x=Homicides_per_100K)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Homicides_per_100K,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Homicides_per_100K,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 quantile(Homicides_per_100K, probs=0.75, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               quantile(Homicides_per_100K, probs=0.25, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Intentional Homicides per 100K Residents\",\n       subtitle=\"Distribution of homicides per 100K 2019\",\n       x=\"Homicides per 100K\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\nThe World Governance Indicators are a combination of enterprise, citizen, and expert survey respondents from around the world. They use more than 30 surveys to create their six indicators with each indicator using different surveys and different data from each survey to aggregate to the final indicator. The one I am most interested in is Voice and Accountability which attempts to measure the level of political freedom (i.e. freedom of speech, press, etc.) and access to participation in governance by the population (i.e. free and fair elections). The indicators are normalized around 0 so we get values ranging approximately from -2.5 to 2.5.\n\n\nCode\ndata_final %>% ggplot(aes(x=WGI_Voice_Accountability)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(WGI_Voice_Accountability,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(WGI_Voice_Accountability,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 quantile(WGI_Voice_Accountability, probs=0.25, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               quantile(WGI_Voice_Accountability, probs=0.75, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"WGI Voice and Accountability\",\n       subtitle=\"Distribution of Voice and Accountability 2019\",\n       x=\"Voice and Accountability\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw() +\n  scale_x_continuous(limits = c(-2.5, 2.5))"
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#world-values-survey-and-european-values-study-1",
    "href": "posts/FinalPart2_ToryBarteloni.html#world-values-survey-and-european-values-study-1",
    "title": "DACSS 603: Final Part 2",
    "section": "World Values Survey and European Values Study",
    "text": "World Values Survey and European Values Study\nOur dependent variable is based on one question where respondents are asked to indicate what level of confidence they have in their government. In the plot below it is displayed as a proportion of respondents that said they had at least some confidence in their government.\n\n\nCode\ndata_final %>% ggplot(aes(x=Gov_Confidence)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Gov_Confidence,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Gov_Confidence,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 quantile(Gov_Confidence, probs=0.25, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               quantile(Gov_Confidence, probs=0.75, na.rm=TRUE),\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Proportion of Population that has Confidence in Government\",\n       subtitle=\"Distribution of Confidence in Government 2017-2020\",\n       x=\"Confidence in Government\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\nLastly we will look at the Inglehart-Welzel Cultural Map that is derived from the World Values Survey. We will look at this in two ways. First we’ll look at a scatter plot of the two indicators used to create the map and the we’ll look at the distribution the map category assignments.\n\n\nCode\ndata_final %>%\n  ggplot(aes(x=Survival, y=Traditional)) +\n  geom_point() +\n  labs(title=\"Inglehart-Welzel Map Indicators\",\n       subtitle=\"Scatter plot of Survival vs. Traditional Values\",\n       x=\"Survival vs. Self-expression Values\",\n       y=\"Traditional vs. Secular Values\",\n       colour=element_blank()) +\n  theme_bw() +\n  scale_x_continuous(limits = c(-3.5, 3.5))\n\n\n\n\n\n\n\nCode\ndata_final %>%\n  ggplot(aes(x=Cultural_Map)) +\n  geom_bar(stat=\"count\") +\n  labs(title=\"Inglehart-Welzel Cultural Map Groups\",\n       subtitle=\"Frequency of Group Assignment in Cultural Map\",\n       x=\"Cultural Map Groups\",\n       y=\"Frequency\",\n       colour=element_blank()) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust=1))"
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#a-model-of-changing-relationship",
    "href": "posts/FinalPart2_ToryBarteloni.html#a-model-of-changing-relationship",
    "title": "DACSS 603: Final Part 2",
    "section": "A Model of Changing Relationship",
    "text": "A Model of Changing Relationship\nThe primary model we assessed showed a significant effect on Trust in Government from Voice and Accountability, but the negative nature of that relationship was unexpected. When examining that relationship visually we see that the relationship changes in an important way over the scale of Voice and Accountability.\n\n\nCode\ndata_final %>%\n  ggplot(aes(x=WGI_Voice_Accountability, y=Gov_Confidence)) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  theme_bw() +\n  labs(title=\"Linear Relationship of Voice & Accountability and Trust in Government\",\n       x= \"WGI Voice and Accountability\",\n       y=\"Trust in Government\")\n\n\n\n\n\nThe relationship does appear to have a net-negative relationship, but we may see a trend that better explains that relationship.\n\n\nCode\ndata_final %>%\n  ggplot(aes(x=WGI_Voice_Accountability, y=Gov_Confidence)) +\n  geom_point() +\n  geom_smooth(method=\"lm\",formula=y~poly(x,2,raw=T)) +\n  theme_bw() +\n  labs(title=\"Quadratic Relationship of Voice & Accountability and Trust in Government\",\n       x= \"WGI Voice and Accountability\",\n       y=\"Trust in Government\")\n\n\n\n\n\nThe quadratic relationship seems to explain the relationship better than a standard linear relationship so we will include this term and re-run the analysis.\n\n\nCode\npoly_data <- as.data.frame(poly(data_final$WGI_Voice_Accountability,2,raw=T))\ndata_final$WGI_Voice_Accountability_Poly2 <- poly_data$`2`\n\npoly_model <- lm(Gov_Confidence ~ WGI_Voice_Accountability +\n                     WGI_Voice_Accountability_Poly2 +\n                     log(GDP_per_Capita) +\n                     Unemployment +\n                     CPI.Score.2018 +\n                     Cultural_Map,\n                   data=data_final)\n\nstargazer(linear_model, poly_model,\n          type=\"text\",\n          title=\"Government Trust Regression Results\",\n          style=\"ajps\"#,\n          #covariate.labels = c(\"Voice Accountability\", \"Voice Accountability Poly2\",\"LN GDP per Capita\",\n                             #  \"Unemployment\",\"CPI Score\",\"Catholic Europe\",\"Confucian\",\"English Speaking\",\n                            #   \"Latin America\", \"Orthodox Europe\",\"Protestant Europe\",\"West and South Asia\")\n          )\n\n\nError in stargazer(linear_model, poly_model, type = \"text\", title = \"Government Trust Regression Results\", : could not find function \"stargazer\"\n\n\nThree factors stand out in the quadratic model. First, the quadratic terms shows a significant effect so we can conclude that the relationship does in fact change as the level of Voice and Accountability changes. Second, our R2 and Adjusted R2 both increase in a substantial way so we can conclude that the model overall explains the relationship better. Lastly, we can derive from the model that the relationship begins negatively and the direction changes when the Voice and Accountability indicator reaches 0.44, or said another way, the relationshop does not become positive until we are discussing countries in about the top third of this type of governance performance."
  },
  {
    "objectID": "posts/FinalPart2_ToryBarteloni.html#a-model-without-culture",
    "href": "posts/FinalPart2_ToryBarteloni.html#a-model-without-culture",
    "title": "DACSS 603: Final Part 2",
    "section": "A Model without Culture",
    "text": "A Model without Culture\nThe other alternative worth discussing is one without culture included. Below we will compare the quadratic model with the cultural map included and excluded.\n\n\nCode\npoly_data <- as.data.frame(poly(data_final$WGI_Voice_Accountability,2,raw=T))\ndata_final$WGI_Voice_Accountability_Poly2 <- poly_data$`2`\n\npoly_model_sans_culture <- lm(Gov_Confidence ~ WGI_Voice_Accountability +\n                     WGI_Voice_Accountability_Poly2 +\n                     log(GDP_per_Capita) +\n                     Unemployment +\n                     CPI.Score.2018,\n                   data=data_final)\n\nstargazer(poly_model, poly_model_sans_culture, \n          type=\"text\",\n          title=\"Government Trust Regression Results\",\n          style=\"ajps\"#,\n          #covariate.labels = c(\"Voice Accountability\", \"Voice Accountability Poly2\",\"LN GDP per Capita\",\n                             #  \"Unemployment\",\"CPI Score\",\"Catholic Europe\",\"Confucian\",\"English Speaking\",\n                            #   \"Latin America\", \"Orthodox Europe\",\"Protestant Europe\",\"West and South Asia\")\n          )\n\n\nError in stargazer(poly_model, poly_model_sans_culture, type = \"text\", : could not find function \"stargazer\"\n\n\nThe model without the cultural map included is worth reviewing because the difference in Adjusted R2 is fairly minimal (0.56 with culture, 0.55 without culture) while the difference in R2 is more substantial (0.63, 0.58). This is caused by the number of cultural variants that are added into the model which does add complexity, but we argue here that culture is an important aspect to include in this and any subsequent analyses for understanding the effects. A larger sample or time-series analyses in particular will suffer less from the inclusion of this variable while it increases the model’s explanatory value substantially."
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html",
    "href": "posts/FinalProject2_EthanCampbell.html",
    "title": "Final Project",
    "section": "",
    "text": "Climate has always been a topic that sparks debate and there is continuous research being done on it every day. I wanted to contribute to this research and analyze the impacts climate factors like temperature and humidity have on bike users. There has been study related to weather conditions and biking and whether or not it results in more accidents which concluded in an increase in accidents. “It suggests that weather conditions should be considered in every analysis where bicycle volume data is needed” (Pazdan, 2020). The paper describes the importance of weather condition and how they should be used in any analysis regarding biking data. Here, I thought about if we know that these factors are important, how important are they? I can find this data online somewhere however, I would like to conduct my own study and determine the results from that and then compare it to results online.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. How has temperature and humidity impacted bike users?\n\n\nMy motivation is driven by my own interest in biking and climate factors and the study of how the climate is impacting human movement. I think it is interesting how significant climate factors can impact certain human activities and I want to learn more about which ones are presenting the largest impact on human activity. The reason this study is different is that it is based on normalized data and is focusing how bikers are impacted by weather condition."
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#reading-in-the-data",
    "href": "posts/FinalProject2_EthanCampbell.html#reading-in-the-data",
    "title": "Final Project",
    "section": "Reading in the data",
    "text": "Reading in the data\n\n\nCode\nbike <- read.csv(\"_data/hour.csv\")\nbike2 <- read.csv(\"_data/day.csv\")\n\ndim(bike)\n\n\n[1] 17379    17\n\n\nCode\ndim(bike2)\n\n\n[1] 731  16\n\n\nCode\nsummary(bike)\n\n\n    instant         dteday              season            yr        \n Min.   :    1   Length:17379       Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 4346   Class :character   1st Qu.:2.000   1st Qu.:0.0000  \n Median : 8690   Mode  :character   Median :3.000   Median :1.0000  \n Mean   : 8690                      Mean   :2.502   Mean   :0.5026  \n 3rd Qu.:13034                      3rd Qu.:3.000   3rd Qu.:1.0000  \n Max.   :17379                      Max.   :4.000   Max.   :1.0000  \n      mnth              hr           holiday           weekday     \n Min.   : 1.000   Min.   : 0.00   Min.   :0.00000   Min.   :0.000  \n 1st Qu.: 4.000   1st Qu.: 6.00   1st Qu.:0.00000   1st Qu.:1.000  \n Median : 7.000   Median :12.00   Median :0.00000   Median :3.000  \n Mean   : 6.538   Mean   :11.55   Mean   :0.02877   Mean   :3.004  \n 3rd Qu.:10.000   3rd Qu.:18.00   3rd Qu.:0.00000   3rd Qu.:5.000  \n Max.   :12.000   Max.   :23.00   Max.   :1.00000   Max.   :6.000  \n   workingday       weathersit         temp           atemp       \n Min.   :0.0000   Min.   :1.000   Min.   :0.020   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.340   1st Qu.:0.3333  \n Median :1.0000   Median :1.000   Median :0.500   Median :0.4848  \n Mean   :0.6827   Mean   :1.425   Mean   :0.497   Mean   :0.4758  \n 3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:0.660   3rd Qu.:0.6212  \n Max.   :1.0000   Max.   :4.000   Max.   :1.000   Max.   :1.0000  \n      hum           windspeed          casual         registered   \n Min.   :0.0000   Min.   :0.0000   Min.   :  0.00   Min.   :  0.0  \n 1st Qu.:0.4800   1st Qu.:0.1045   1st Qu.:  4.00   1st Qu.: 34.0  \n Median :0.6300   Median :0.1940   Median : 17.00   Median :115.0  \n Mean   :0.6272   Mean   :0.1901   Mean   : 35.68   Mean   :153.8  \n 3rd Qu.:0.7800   3rd Qu.:0.2537   3rd Qu.: 48.00   3rd Qu.:220.0  \n Max.   :1.0000   Max.   :0.8507   Max.   :367.00   Max.   :886.0  \n      cnt       \n Min.   :  1.0  \n 1st Qu.: 40.0  \n Median :142.0  \n Mean   :189.5  \n 3rd Qu.:281.0  \n Max.   :977.0"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#data",
    "href": "posts/FinalProject2_EthanCampbell.html#data",
    "title": "Final Project",
    "section": "Data",
    "text": "Data\nThe data was collected from UCI machine learning repository. Where I collected for both daily and hourly information for 17 variables. Each variable is described below. Important variables will be in bold. Their relationship towards the analysis will be described underneath this section.\n\ninstant - This is the record index. This is the count of how many rows there are. This object will not be utilized in this study\ndteday - This is the date. The date is currently in year-day-month format. This will be used to observe change over time\nseason - This is the 4 seasons. This is expressed as: 1-Winter, 2-Spring, 3-Summer, 4- Fall. This will be used as a control variable since this could impact the sales of bikes and the independent variables\nyr - This is the year ranging from 2011-2012. This will be used analyze change over each year.\nmnth - This is the month 1-12. This will be used to analyze the change over months.\nhr - This is hour 0-23. This will be used to analyze the change by the hour.\nholiday - This is the holidays so whether or not it is a holiday. This will be used as an independent variable in conjunction with another\nweekday - Day of the week.\nworkingday - if day is neither weekend nor holiday is 1, otherwise is 0.\nweathersit -\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\natemp - Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\nhum - Normalized humidity. The values are divided to 100 (max)\nwindspeed - Normalized wind speed. The values are divided to 67 (max)\ncasual - count of casual users\nregistered - count of registered users\ncnt - count of total rental bikes including both casual and registered\n\nRegression 1\n\nExplanatory - normalized temperature feeling\nOutcome - cnt\nControl - Season, holiday, weekday, weathersit\n\nRegression 2\n\nExplanatory - Normalized humidity\nOutcome - cnt\nControl - temperature(normalized and normalized feeling), wind speed, weekday, holiday, weathersit\n\nRegression Model\nInteraction Terms\nI do not believe I need to use a quardratic or anything like that however, there is heteroskedascity so using a log may help with the funneling?\n\\[\n\\hat{Y} = b0 + b1X1+b2X2\n\\]\nRegression Model 1\n\\[\nCnt = FeelingTemperature + Humidity + Season + Week Day + Hour + Year + Holiday + Windspeed\n\\] Regression Model 2\n\\[\nCnt = FeelingTemperature + Humidity + Season + Week Day + Weather Type + Year + Holiday + Windspeed\n\\]"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#cleaning-the-data",
    "href": "posts/FinalProject2_EthanCampbell.html#cleaning-the-data",
    "title": "Final Project",
    "section": "Cleaning the data",
    "text": "Cleaning the data\n\n\nCode\nbike <- bike %>%\n  dplyr::rename('Date' = dteday) %>%\n  dplyr::rename('Year' = yr) %>%\n  dplyr::rename('Month' = mnth) %>%\n  dplyr::rename('Hour' = hr) %>%\n  dplyr::rename('Normalized_temperature_C' = temp) %>%\n  dplyr::rename('Normalized_feeling_temperature_C' = atemp) %>%\n  dplyr::rename('Normalized_Humidity' = hum) %>%\n  dplyr::rename(\"Total_bike_users\" = cnt)\n\nbike2 <- bike2 %>%\n  dplyr::rename('Date' = dteday) %>%\n  dplyr::rename('Year' = yr) %>%\n  dplyr::rename('Month' = mnth) %>%\n  dplyr::rename('Normalized_temperature_C' = temp) %>%\n  dplyr::rename('Normalized_feeling_temperature_C' = atemp) %>%\n  dplyr::rename('Normalized_Humidity' = hum) %>%\n  dplyr::rename(\"Total_bike_users\" = cnt)\n\nbike2$Date <- ymd(bike2$Date)\n\n# Checking for multicollinearity (We notice that temp and feeling temp are almost identical) so we removed the normalized temperature form the study since I want to focus on feeling temperature. Also removing instant since it is just the count of rows.\n\nbike <- bike[,-11]\nbike2 <- bike2[,-10]\n\ncor(bike[3:16])\n\n\n                                       season         Year        Month\nseason                            1.000000000 -0.010742486  0.830385892\nYear                             -0.010742486  1.000000000 -0.010472929\nMonth                             0.830385892 -0.010472929  1.000000000\nHour                             -0.006116901 -0.003867005 -0.005771909\nholiday                          -0.009584526  0.006691617  0.018430325\nweekday                          -0.002335350 -0.004484851  0.010400061\nworkingday                        0.013743102 -0.002196005 -0.003476922\nweathersit                       -0.014523552 -0.019156853  0.005399522\nNormalized_feeling_temperature_C  0.319379811  0.039221595  0.208096131\nNormalized_Humidity               0.150624745 -0.083546421  0.164411443\nwindspeed                        -0.149772751 -0.008739533 -0.135386323\ncasual                            0.120206447  0.142778528  0.068457301\nregistered                        0.174225633  0.253684310  0.122272967\nTotal_bike_users                  0.178055731  0.250494899  0.120637760\n                                         Hour      holiday      weekday\nseason                           -0.006116901 -0.009584526 -0.002335350\nYear                             -0.003867005  0.006691617 -0.004484851\nMonth                            -0.005771909  0.018430325  0.010400061\nHour                              1.000000000  0.000479136 -0.003497739\nholiday                           0.000479136  1.000000000 -0.102087791\nweekday                          -0.003497739 -0.102087791  1.000000000\nworkingday                        0.002284998 -0.252471370  0.035955071\nweathersit                       -0.020202528 -0.017036113  0.003310740\nNormalized_feeling_temperature_C  0.133749965 -0.030972737 -0.008820945\nNormalized_Humidity              -0.276497828 -0.010588465 -0.037158268\nwindspeed                         0.137251568  0.003987632  0.011501545\ncasual                            0.301201730  0.031563628  0.032721415\nregistered                        0.374140710 -0.047345424  0.021577888\nTotal_bike_users                  0.394071498 -0.030927303  0.026899860\n                                   workingday   weathersit\nseason                            0.013743102 -0.014523552\nYear                             -0.002196005 -0.019156853\nMonth                            -0.003476922  0.005399522\nHour                              0.002284998 -0.020202528\nholiday                          -0.252471370 -0.017036113\nweekday                           0.035955071  0.003310740\nworkingday                        1.000000000  0.044672224\nweathersit                        0.044672224  1.000000000\nNormalized_feeling_temperature_C  0.054667235 -0.105563108\nNormalized_Humidity               0.015687512  0.418130329\nwindspeed                        -0.011829789  0.026225652\ncasual                           -0.300942486 -0.152627885\nregistered                        0.134325791 -0.120965520\nTotal_bike_users                  0.030284368 -0.142426138\n                                 Normalized_feeling_temperature_C\nseason                                                0.319379811\nYear                                                  0.039221595\nMonth                                                 0.208096131\nHour                                                  0.133749965\nholiday                                              -0.030972737\nweekday                                              -0.008820945\nworkingday                                            0.054667235\nweathersit                                           -0.105563108\nNormalized_feeling_temperature_C                      1.000000000\nNormalized_Humidity                                  -0.051917696\nwindspeed                                            -0.062336043\ncasual                                                0.454080065\nregistered                                            0.332558635\nTotal_bike_users                                      0.400929304\n                                 Normalized_Humidity    windspeed      casual\nseason                                    0.15062475 -0.149772751  0.12020645\nYear                                     -0.08354642 -0.008739533  0.14277853\nMonth                                     0.16441144 -0.135386323  0.06845730\nHour                                     -0.27649783  0.137251568  0.30120173\nholiday                                  -0.01058846  0.003987632  0.03156363\nweekday                                  -0.03715827  0.011501545  0.03272142\nworkingday                                0.01568751 -0.011829789 -0.30094249\nweathersit                                0.41813033  0.026225652 -0.15262788\nNormalized_feeling_temperature_C         -0.05191770 -0.062336043  0.45408007\nNormalized_Humidity                       1.00000000 -0.290104895 -0.34702809\nwindspeed                                -0.29010490  1.000000000  0.09028678\ncasual                                   -0.34702809  0.090286775  1.00000000\nregistered                               -0.27393312  0.082320847  0.50661770\nTotal_bike_users                         -0.32291074  0.093233784  0.69456408\n                                  registered Total_bike_users\nseason                            0.17422563       0.17805573\nYear                              0.25368431       0.25049490\nMonth                             0.12227297       0.12063776\nHour                              0.37414071       0.39407150\nholiday                          -0.04734542      -0.03092730\nweekday                           0.02157789       0.02689986\nworkingday                        0.13432579       0.03028437\nweathersit                       -0.12096552      -0.14242614\nNormalized_feeling_temperature_C  0.33255864       0.40092930\nNormalized_Humidity              -0.27393312      -0.32291074\nwindspeed                         0.08232085       0.09323378\ncasual                            0.50661770       0.69456408\nregistered                        1.00000000       0.97215073\nTotal_bike_users                  0.97215073       1.00000000"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#residuals-vs-fittedwill-need-to-be-adjusted",
    "href": "posts/FinalProject2_EthanCampbell.html#residuals-vs-fittedwill-need-to-be-adjusted",
    "title": "Final Project",
    "section": "Residuals vs Fitted(Will need to be adjusted)",
    "text": "Residuals vs Fitted(Will need to be adjusted)\nThis shows signs of heteroskedasticity and this is when standard deviations of a predicated variable being monitored over different values of an independent variable are non-constant. The problems that arise from this issue is, the standard error is wrong and thus the confidence intervals and hypothesis tests can not be relied on. This issues needs to be resolved before declaring the conclusion.\n\n\nCode\nmod <- lm(Total_bike_users ~ .-Date -casual -registered, data = bike)\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = Total_bike_users ~ . - Date - casual - registered, \n    data = bike)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-392.56  -93.49  -27.65   60.96  641.27 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -3.109e+01  7.337e+00  -4.237 2.27e-05 ***\ninstant                          -4.527e-03  5.126e-03  -0.883  0.37719    \nseason                            1.993e+01  1.819e+00  10.959  < 2e-16 ***\nYear                              1.207e+02  4.487e+01   2.690  0.00714 ** \nMonth                             3.283e+00  3.775e+00   0.870  0.38457    \nHour                              7.671e+00  1.650e-01  46.477  < 2e-16 ***\nholiday                          -2.151e+01  6.692e+00  -3.214  0.00131 ** \nweekday                           1.928e+00  5.403e-01   3.569  0.00036 ***\nworkingday                        4.031e+00  2.396e+00   1.683  0.09245 .  \nweathersit                       -3.334e+00  1.904e+00  -1.751  0.07994 .  \nNormalized_feeling_temperature_C  3.197e+02  6.777e+00  47.176  < 2e-16 ***\nNormalized_Humidity              -1.989e+02  6.880e+00 -28.906  < 2e-16 ***\nwindspeed                         4.612e+01  9.401e+00   4.906 9.39e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 141.9 on 17366 degrees of freedom\nMultiple R-squared:  0.3887,    Adjusted R-squared:  0.3883 \nF-statistic: 920.3 on 12 and 17366 DF,  p-value: < 2.2e-16\n\n\n\nResolving Heteroskedasticity\n\n\n\n\n\n\nH0A\n\n\n\nThere is no Heteroskedasticity\n\n\n\n\n\n\n\n\nH1A\n\n\n\nThere is Heteroskedasticity\n\n\nHere we will conduct the Breusch-Pagan test using the lmtest package and bptest() function. This will let us know if there is heteroskedascity if the P < .05. Here we see that both meet this standard and thus we have evidence to reject the null hypothesis\n\n\nCode\n# Breusch-Pagan test to determine if Heteroskedasticity exist\nbptest(Regression1)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  Regression1\nBP = 1450.9, df = 8, p-value < 2.2e-16\n\n\nCode\nbptest(Regression2)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  Regression2\nBP = 47.248, df = 8, p-value = 1.375e-07\n\n\nCode\n# both of these are p<.05 meaning that we reject the null hypothesis and say yes there is heteroskedacity"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#visualizations",
    "href": "posts/FinalProject2_EthanCampbell.html#visualizations",
    "title": "Final Project",
    "section": "Visualizations",
    "text": "Visualizations\nHere we see as temperature increases we can expect bike users to increase while as humidity increases we expect the opposite.\n\n\nCode\nplot(Regression1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(regression2)\n\n\nError in plot(regression2): object 'regression2' not found\n\n\nCode\nBike_users_plot <- bike2 %>%\n  ggplot(aes(x=Date, y=Total_bike_users)) +\n    geom_area(fill=\"#69b3a2\", alpha=0.5) +\n    geom_line(color=\"#69b3a2\") +\n    ylab(\"total bike user\") +\n    theme_ipsum()\n# Making it interactive\nBike_users_plot <- ggplotly(Bike_users_plot)\nBike_users_plot\n\n\n\n\n\n\nCode\n# Value used to transform the data\ncoeff <- 10000\n\n# A few constants\ntemperatureColor <- \"#69b3a2\"\npriceColor <- rgb(0.2, 0.6, 0.9, 1)\n\nggplot(bike2, aes(x=Date)) +\n  \n  geom_line( aes(y=Normalized_feeling_temperature_C), size=2, color=temperatureColor) + \n  geom_line( aes(y=Total_bike_users / coeff), size=2, color=priceColor) +\n  \n  scale_y_continuous(\n    \n    # Features of the first axis\n    name = \"Temperature (Normalized)\",\n    \n    # Add a second axis and specify its features\n    sec.axis = sec_axis(~.*coeff, name=\"Bike users\")\n  ) + \n  \n  theme_ipsum() +\n\n  theme(\n    axis.title.y = element_text(color = temperatureColor, size=13),\n    axis.title.y.right = element_text(color = priceColor, size=13)\n  ) +\n\n  ggtitle(\"Temperature correlates with bikes users\")\n\n\n\n\n\nCode\n# plotting the data to visualize \nggplot(data = bike2, aes(x=Normalized_feeling_temperature_C, y = Total_bike_users)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_fivethirtyeight(base_size = 10, base_family = 'serif') +\n  theme(axis.title = element_text(family = 'serif', size = 15)) + ylab('Total Bike Users') + xlab('Normalized Feeling Temperature') +\n  labs(title = \"Relationship between Temerpature and Bike users\", caption = \"\")\n\n\n\n\n\nCode\nggplot(data = bike2, aes(x=Normalized_Humidity, y = Total_bike_users)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_fivethirtyeight(base_size = 10, base_family = 'serif') +\n  theme(axis.title = element_text(family = 'serif', size = 15)) + ylab('Total Bike Users') + xlab('Normalized Humidity') +\n  labs(title = \"Relationship between Humidity and Bike users\", caption = \"\")\n\n\n\n\n\nCode\nggpairs(bike2, columns = c(10, 11, 15), ggplot2::aes(colour='red'))"
  },
  {
    "objectID": "posts/FinalProject2_EthanCampbell.html#conclusions",
    "href": "posts/FinalProject2_EthanCampbell.html#conclusions",
    "title": "Final Project",
    "section": "Conclusions",
    "text": "Conclusions\nIn conclusion, for hypothesis one we reject the null hypothesis with evidence of the extremely significant p-value of 2e-16. This give us evidence that we can accept the alternative and say yes temperature has an impact on bike sales. There were two different tests done here, we did the daily data compared to the hourly data. Both were significant and we controlled for 7 variables which are specified at the top. The reason we controlled these variables is that they could impact the outcome variable and thus we controlled them to make sure that they were not impacting the results. So in conclusion, normalized feeling temperature Celsius has an impact on bike users. Looking at our correlation graph we can see that it has a positive correlation with bike users at .631.\nFor the second questions we can also reject the null hypothesis as humidity is significant with a p-value of < 2e-16 in data set one and 0.000323 in data set two. This is further evidence that we can reject the null as in both scales is what significant. This data held the same control variables as temperature and thus we can yes humidity has an impact on bike users and looking at our correlation we see a negative correlation of -.101."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html",
    "href": "posts/FinalProjectPart1_DonnySnyder.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question",
    "text": "Research Question\nAffective polarization describes a heightened state of animosity between partisans that has steadily grown from the 1970s to today (Iyengar et al., 2019). Identifying antecedents of affective polarization is essential to creating intervention strategies into this negative state of politics. Levendusky (2009) proposes a social model where individuals making sense of simplified elite cues enables people to understand the relevant identities of the political landscape, which may lead to downstream affective polarization. I intend to expand on this model, testing a construct of construal level, or the level of abstraction to concreteness (Trope & Liberman, 2010) with which partisans perceive partisan groups and group cues. Prior studies suggest that lower construal may serve as an antecedent to affective polarization when partisans view issues in more concrete, group terms (Snyder, Unpublished). This study will expand these models into extant, large scale, political science datasets. Additionally, this project will employ supervised machine learning models to qualitatively code a large-n sample of free response questions."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "title": "Final Project Part 1",
    "section": "Hypotheses",
    "text": "Hypotheses\nI hypothesize that partisans who are qualitatively coded as having a lower construal level will demonstrate higher levels of group/affective polarization, as measured on a feeling thermometer or measures of feelings about political groups - whichever is available in the datasets.\nI hypothesize that using a sentiment analysis, these tendencies may be moderated by valence of their free response, with stronger valence enhancing the effect of construal level on affective polarization."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "title": "Final Project Part 1",
    "section": "Datasets",
    "text": "Datasets\nI intend to use ANES and/or NAES free response data to provide an initial exploratory analysis. I will qualitatively code these data using a novel construal level paradigm (Snyder, unpublished). i will then use this qualitative coding process to train a supervised machine learning algorithm."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). The origins and consequences of affective polarization in the United States. Annual Review of Political Science, 22(1), 129-146. Levendusky, M. (2009). The partisan sort: How liberals became Democrats and conservatives became Republicans. University of Chicago Press. Snyder, D. (2022). Keep It Simple Stupid: How Individual Differences in Cue Construal Explain Variations in Affective Polarization. Unpublished Manuscript Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. Psychological review, 117(2), 440."
  },
  {
    "objectID": "posts/FinalProjectPart1_ManiShankerKamarapu.html",
    "href": "posts/FinalProjectPart1_ManiShankerKamarapu.html",
    "title": "Final project part 1",
    "section": "",
    "text": "Churning refers to a customer who leaves one company to go to another company. Customer churn introduces not only some loss in income but also other negative effects on the operation of companies. Churn management is the concept of identifying those customers who are intending to move their custom to a competing service provider.\nRisselada et al. (2010) stated that churn management is becoming part of customer relationship management. It is important for companies to consider it as they try to establish long-term relationships with customers and maximize the value of their customer base.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Does churn-rate depend on the geographical factors of the customer?\nB. Do non-active members are probable to churn or not?\n\n\nThis project will be useful to better understand more about the customer difficulties and factors and also give us a pretty good idea on the factors effecting the customers to exit and also about the dormant state of the customers."
  },
  {
    "objectID": "posts/FinalProjectPart1_ManiShankerKamarapu.html#hypothesis",
    "href": "posts/FinalProjectPart1_ManiShankerKamarapu.html#hypothesis",
    "title": "Final project part 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nCustomer churn analysis has become a major concern in almost every industry that offers products and services. The model developed will help banks identify clients who are likely to be churners and develop appropriate marketing actions to retain their valuable clients. And this model also supports information about similar customer group to consider which marketing reactions are to be provided. Thus, due to existing customers are retained, it will provide banks with increased profits and revenues.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nGeographical factors will not be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nGeographical factors will be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nActive members will not churn.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nActive members will churn."
  },
  {
    "objectID": "posts/FinalProjectPart1_ManiShankerKamarapu.html#loading-libraries",
    "href": "posts/FinalProjectPart1_ManiShankerKamarapu.html#loading-libraries",
    "title": "Final project part 1",
    "section": "Loading libraries",
    "text": "Loading libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectPart1_ManiShankerKamarapu.html#reading-the-data-set",
    "href": "posts/FinalProjectPart1_ManiShankerKamarapu.html#reading-the-data-set",
    "title": "Final project part 1",
    "section": "Reading the data set",
    "text": "Reading the data set\n\n\nCode\nChurn <- read_csv(\"_data/Churn_Modelling.csv\")\n\n\nRows: 10000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Surname, Geography, Gender\ndbl (11): RowNumber, CustomerId, CreditScore, Age, Tenure, Balance, NumOfPro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nChurn\n\n\n\n\n  \n\n\n\nThis data set includes 10k bank customer data records with 14 attributes including socio-demographic attributes, account level and behavioural attributes.\nAttribute Description 1. Row Number- Number of customers 2. Customer ID- ID of customer 3. Surname- Customer name 4. Credit Score- Score of credit card usage 5. Geography- Location of customer 6. Gender- Customer gender 7. Age- Age of Customer 8. Tenure- The period of having the account in months 9. Balance- Customer main balance 10. NumOfProducts- No of products used by customer 11. HasCrCard- If the customer has a credit card or not 12. IsActiveMember- Customer account is active or not 13. Estimated Salary- Estimated salary of the customer. 14. Exited- Indicate churned or not\n\n\nCode\nstr(Churn)\n\n\nspc_tbl_ [10,000 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ RowNumber      : num [1:10000] 1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : num [1:10000] 15634602 15647311 15619304 15701354 15737888 ...\n $ Surname        : chr [1:10000] \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : num [1:10000] 619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr [1:10000] \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr [1:10000] \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : num [1:10000] 42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : num [1:10000] 2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num [1:10000] 0 83808 159661 0 125511 ...\n $ NumOfProducts  : num [1:10000] 1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : num [1:10000] 1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : num [1:10000] 1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num [1:10000] 101349 112543 113932 93827 79084 ...\n $ Exited         : num [1:10000] 1 0 1 0 0 1 0 1 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   RowNumber = col_double(),\n  ..   CustomerId = col_double(),\n  ..   Surname = col_character(),\n  ..   CreditScore = col_double(),\n  ..   Geography = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_double(),\n  ..   Tenure = col_double(),\n  ..   Balance = col_double(),\n  ..   NumOfProducts = col_double(),\n  ..   HasCrCard = col_double(),\n  ..   IsActiveMember = col_double(),\n  ..   EstimatedSalary = col_double(),\n  ..   Exited = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "posts/FinalProjectPart1_ManiShankerKamarapu.html#descriptive-statistics",
    "href": "posts/FinalProjectPart1_ManiShankerKamarapu.html#descriptive-statistics",
    "title": "Final project part 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n\nCode\nsummary(Churn)\n\n\n   RowNumber       CustomerId         Surname           CreditScore   \n Min.   :    1   Min.   :15565701   Length:10000       Min.   :350.0  \n 1st Qu.: 2501   1st Qu.:15628528   Class :character   1st Qu.:584.0  \n Median : 5000   Median :15690738   Mode  :character   Median :652.0  \n Mean   : 5000   Mean   :15690941                      Mean   :650.5  \n 3rd Qu.: 7500   3rd Qu.:15753234                      3rd Qu.:718.0  \n Max.   :10000   Max.   :15815690                      Max.   :850.0  \n  Geography            Gender               Age            Tenure      \n Length:10000       Length:10000       Min.   :18.00   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:32.00   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median :37.00   Median : 5.000  \n                                       Mean   :38.92   Mean   : 5.013  \n                                       3rd Qu.:44.00   3rd Qu.: 7.000  \n                                       Max.   :92.00   Max.   :10.000  \n    Balance       NumOfProducts    HasCrCard      IsActiveMember  \n Min.   :     0   Min.   :1.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:     0   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 97199   Median :1.00   Median :1.0000   Median :1.0000  \n Mean   : 76486   Mean   :1.53   Mean   :0.7055   Mean   :0.5151  \n 3rd Qu.:127644   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :250898   Max.   :4.00   Max.   :1.0000   Max.   :1.0000  \n EstimatedSalary         Exited      \n Min.   :    11.58   Min.   :0.0000  \n 1st Qu.: 51002.11   1st Qu.:0.0000  \n Median :100193.91   Median :0.0000  \n Mean   :100090.24   Mean   :0.2037  \n 3rd Qu.:149388.25   3rd Qu.:0.0000  \n Max.   :199992.48   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(Churn)\n\n\nRows: 10,000\nColumns: 14\n$ RowNumber       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ CustomerId      <dbl> 15634602, 15647311, 15619304, 15701354, 15737888, 1557…\n$ Surname         <chr> \"Hargrave\", \"Hill\", \"Onio\", \"Boni\", \"Mitchell\", \"Chu\",…\n$ CreditScore     <dbl> 619, 608, 502, 699, 850, 645, 822, 376, 501, 684, 528,…\n$ Geography       <chr> \"France\", \"Spain\", \"France\", \"France\", \"Spain\", \"Spain…\n$ Gender          <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Female\", \"Mal…\n$ Age             <dbl> 42, 41, 42, 39, 43, 44, 50, 29, 44, 27, 31, 24, 34, 25…\n$ Tenure          <dbl> 2, 1, 8, 1, 2, 8, 7, 4, 4, 2, 6, 3, 10, 5, 7, 3, 1, 9,…\n$ Balance         <dbl> 0.00, 83807.86, 159660.80, 0.00, 125510.82, 113755.78,…\n$ NumOfProducts   <dbl> 1, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, …\n$ HasCrCard       <dbl> 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …\n$ IsActiveMember  <dbl> 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, …\n$ EstimatedSalary <dbl> 101348.88, 112542.58, 113931.57, 93826.63, 79084.10, 1…\n$ Exited          <dbl> 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/FinalProjectPart2.html",
    "href": "posts/FinalProjectPart2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nstudentsurvey <- read.csv(\"student_prediction.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'student_prediction.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nsummary(studentsurvey)\n\n\nError in summary(studentsurvey): object 'studentsurvey' not found\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nQuestion: Does classroom engagement (i.e., taking notes, attending class, listening) result in a higher GPA in university students?\nHypothesis: Classroom engagement factors (i.e., taking notes, attending class, listening) will have a positive correlation with higher GPA.\nExplanatory Variable: Classroom engagement factors (1) taking notes, 2) attending class, 3) listening)\nResponse Variable: University students’ GPA\n\n\nNotes & GPA\n\n\nCode\nnfit <- lm(NOTES ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(nfit)\n\n\nError in summary(nfit): object 'nfit' not found\n\n\nCode\ncor.test(studentsurvey$NOTES, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$NOTES, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nAttendance & GPA\n\n\nCode\nafit <- lm(ATTEND ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(afit)\n\n\nError in summary(afit): object 'afit' not found\n\n\nCode\ncor.test(studentsurvey$ATTEND, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$ATTEND, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nListening & GPA\n\n\nCode\nlfit <- lm(LISTENS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(lfit)\n\n\nError in summary(lfit): object 'lfit' not found\n\n\nCode\ncor.test(studentsurvey$LISTENS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$LISTENS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nsummary(lm(CUML_GPA ~ NOTES + ATTEND + LISTENS, data = studentsurvey))\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\n\n\n\nImpact of Taking Notes on University Student GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = NOTES, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = NOTES, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nImpact of Class Attendance on GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = ATTEND, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = ATTEND, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nImpact of Listening in Class on GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = LISTENS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = LISTENS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nTo analyze the influence of classroom engagement on student GPA, I chose to run a simple linear regression and a correlation test. I did also conduct a multiple regression analysis, but I preferred to separate the three variables within my definition of “classroom engagement” so I could analyze them individually.\nTaking notes did not appear to have a significant impact on cumulative GPA. The p-value (0.08499) was greater than 0.05, indicating the result was not statistically significant. Additionally, the correlation coefficient was positive, but only slightly (0.1435413). The adjusted r squared also indicated a low correlation (0.01376).\nClass attendance was found to be a statistically significant, as the p-value was less than 0.05 (0.0319). Interestingly, classroom attendance actually had a negative correlation with GPA, indicating that students who attended class less frequently obtained higher GPAs. This correlation is also slight, as indicated by the correlation coefficient (-0.1783047) and the adjusted r squared (0.02502).\nStudents’ reported listening during class was not statistically significant on GPA, with a p-value higher than 0.05 (0.5079). The correlation was also extremely slight, with a positive correlation coefficient of 0.05542742 and an adjusted r squared value of -0.003899.\nThus, my hypothesis that classroom engagement would have a positive influence on GPA would be rejected.\n\n\n\n\nQuestion: Does reported studying (i.e., weekly study hours) result in a higher GPA in university students?\nHypothesis: More hours studied will have a positive impact on student cumulative GPA.\nExplanatory Variable: Hours reported studying a week\nResponse Variable: Cumulative GPA\n\n\n\n\nCode\nshfit <- lm(STUDY_HRS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(shfit)\n\n\nError in summary(shfit): object 'shfit' not found\n\n\nCode\ncor.test(studentsurvey$STUDY_HRS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$STUDY_HRS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nggplot(data = studentsurvey, aes(x = STUDY_HRS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = STUDY_HRS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nLike with my previous research question, I chose to run a simple linear regression and a correlation test to analyze the data. The results indicated that hours spent studying had very little impact on cumulative GPA. The p-value was greater than 0.05 (0.9225), and both the correlation coefficient, although positive, and the adjusted r r squared values were extremely small (0.008144991 and -0.006926). Thus, my hypothesis would be refuted.\n\n\n\n\nQuestion: Does collaboration between students (i.e., studying together, positive class discussions) result in a higher GPA in university students?\nHypothesis: Collaboration between students would have a positive impact on cumulative GPA.\nExplanatory Variable: Collaboration (Studying with peers, perceiving classroom discussions as positive)\nResponse Variable: Cumulative GPA\n\n\nStudying with peers & GPA\n\n\nCode\nstudentsurvey$PREP_STUDY <- ifelse(studentsurvey$PREP_STUDY==2, 2, 1)\n\n\nError in ifelse(studentsurvey$PREP_STUDY == 2, 2, 1): object 'studentsurvey' not found\n\n\nCode\nspfit <- lm(PREP_STUDY ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(spfit)\n\n\nError in summary(spfit): object 'spfit' not found\n\n\nCode\ncor.test(studentsurvey$PREP_STUDY, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$PREP_STUDY, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\nPositive discussions & GPA\n\n\nCode\nstudentsurvey$PREP_STUDY <- ifelse(studentsurvey$LIKES_DISCUSS==1, 1, 2)\n\n\nError in ifelse(studentsurvey$LIKES_DISCUSS == 1, 1, 2): object 'studentsurvey' not found\n\n\nCode\nldfit <- lm(LIKES_DISCUSS ~ CUML_GPA, data = studentsurvey)\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\nCode\nsummary(ldfit)\n\n\nError in summary(ldfit): object 'ldfit' not found\n\n\nCode\ncor.test(studentsurvey$LIKES_DISCUSS, studentsurvey$CUML_GPA)\n\n\nError in cor.test(studentsurvey$LIKES_DISCUSS, studentsurvey$CUML_GPA): object 'studentsurvey' not found\n\n\n\n\n\n\n\nCode\nsummary(lm(CUML_GPA ~ PREP_STUDY + LIKES_DISCUSS, data = studentsurvey))\n\n\nError in is.data.frame(data): object 'studentsurvey' not found\n\n\n\n\n\nStudying with peers & GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = PREP_STUDY, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = PREP_STUDY, y = CUML_GPA)): object 'studentsurvey' not found\n\n\nPositive discussions & GPA\n\n\nCode\nggplot(data = studentsurvey, aes(x = LIKES_DISCUSS, y = CUML_GPA)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\nError in ggplot(data = studentsurvey, aes(x = LIKES_DISCUSS, y = CUML_GPA)): object 'studentsurvey' not found\n\n\n\n\n\nStudents who study with their peers are more likely to have higher GPAs, according to the simple linear regression and correlation test. The p-value was less than 0.05 (0.01535). However, the correlation was not extremely high (0.2009882) and neither was the adjusted r-squared value (0.03369). That being said, the results were statistically significant.\nAdditionally, students who found class discussions to be helpful (always or some of the time, compared to those who did not find class discussions to be a positive experience) to their education and learning were significantly more likely to have higher GPAs. The p-value was less than 0.01 (0.007804). Again the correlation was not extreme (0.2201251) as well as the adjusted r-squared (0.0418).\nThe multiple regression analysis also found the combined two variables to be statistically significant (0.01666). Thus, it could be concluded that collaboration has a positive impact on GPA, supporting my hypothesis."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html",
    "href": "posts/FinalProjectPart2_DonnySnyder.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(haven)\nlibrary(pollster)\n\n\nError in library(pollster): there is no package called 'pollster'\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(knitr)\nlibrary(foreign)\nlibrary(scales)\nlibrary(questionr)\n\n\nError in library(questionr): there is no package called 'questionr'\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ tibble  3.1.8     ✔ purrr   0.3.5\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n\n\nCode\nlibrary(tibble)\nlibrary(extrafont)\n\n\nRegistering fonts with R\n\n\nCode\nlibrary(tidyr)\nlibrary(readr)\nlibrary(irr)\n\n\nError in library(irr): there is no package called 'irr'\n#I have added my analyses on to my initial template from Part 1 - I hope this is what you had envisioned for this assignment"
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#research-question",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#research-question",
    "title": "Final Project Part 2",
    "section": "Research Question",
    "text": "Research Question\nAffective polarization describes a heightened state of animosity between partisans that has steadily grown from the 1970s to today (Iyengar et al., 2019). Identifying antecedents of affective polarization is essential to creating intervention strategies into this negative state of politics. Levendusky (2009) proposes a social model where individuals making sense of simplified elite cues enables people to understand the relevant identities of the political landscape, which may lead to downstream affective polarization. I intend to expand on this model, testing a construct of construal level, or the level of abstraction to concreteness (Trope & Liberman, 2010) with which partisans perceive partisan groups and group cues. This levels varies in how individuals describe different constructs as having more concrete or abstract characteristics, such as mentioning specific groups as opposed to vague ideological concepts (view the Appendix at the bottom for more information on how this was qualitatively coded). Prior studies suggest that lower construal may serve as an antecedent to affective polarization when partisans view issues in more concrete, group terms (Snyder, Unpublished). This study will expand these models into extant, large scale, political science datasets. Additionally, this project will employ supervised machine learning models to qualitatively code a large-n sample of free response questions."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#hypotheses",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#hypotheses",
    "title": "Final Project Part 2",
    "section": "Hypotheses",
    "text": "Hypotheses\nI hypothesize that partisans who are qualitatively coded as having a lower construal level will demonstrate higher levels of group/affective polarization, as measured on a feeling thermometer or measures of feelings about political groups - whichever is available in the datasets.\nI hypothesize that using a sentiment analysis, these tendencies may be moderated by valence of their free response, with stronger valence enhancing the effect of construal level on affective polarization - valence being the psychological term for positive vs negative sentiment."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#datasets",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#datasets",
    "title": "Final Project Part 2",
    "section": "Datasets",
    "text": "Datasets\nI intend to use UMass Poll, ANES, and Polarization Research Lab datasets for my studies. UMass Poll data will be used for Study 1, which is shown in the initial analyses here. Studies 2 and 3 will include data (that will be collected later in November) obtained from an accepted application for survey space from Dartmouth’s Polarization Research Lab. ANES data, specifically the free response questions in 1992, will be used to compare the qualitative coding results of the free responses to Mason’s social sorting measures. I hope you will forgive me not having all of these results right now, as I will need to hand code 2000+ cases before these studies are over, and some of the data has not been collected yet, but should be before the Final Project is due.\n#Analyses As mentioned in the previous section, initial analyses have been performed to test the first hypothesis. UMass Poll data has been qualitatively coded and analyzed for this purpose. In order to measure the effectiveness of the qualitative coding, multiple models and portions of variables were compared.\n\n\nCode\n#Clean Data\ndoto <- read_sav(\"12_21_Data.sav\")\ncaitCode <- read_csv(\"CaitlynQualCodingCP.csv\")\n\n\nRows: 815 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Response\ndbl (4): Resp#, Who, How, model1\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndCode <- read_csv(\"qualCodingCRTno99.csv\")\n\n\nNew names:\nRows: 815 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): Who, shows a concrete understanding of group information dbl (4):\nrespondent, Who, How, model1 lgl (1): ...5\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...5`\n\n\nCode\ndoto <- subset(doto, doto$Q33_open != \"__NA__\")\nRdoto <- subset(doto, doto$pid3lean == \"Republicans\")\nDdoto <- subset(doto, doto$pid3lean == \"Democrats\")\nIdoto <- subset(doto, doto$pid3lean == \"Independents\")\n\ndoto$affPol <- doto$Q10_democrats - doto$Q10_republicans\ndoto$CCModel2 <- caitCode$model1\ncaitCode$whoOrHow <- recode(caitCode$model1, \"0\" = \"0\", .default = \"1\")\ndoto$CCModel <- caitCode$whoOrHow\ndoto$absAffPol <- abs(doto$affPol)\ndoto$dummy <- rep(\"Dummy\", 815)\n\ndata <- aggregate(absAffPol ~ CCModel2, doto, mean)\npartyData <- aggregate(absAffPol ~ CCModel2 + pid3lean, doto, mean)\npartyData <- partyData[-(1:3),]\ndata$CCModel2 <- recode(data$CCModel2, \"2\" = \"Low Construal\", \"1\" = \"Medium Construal\", \"0\" = \"High Construal\")\ndata$CCModel2 <- factor(data$CCModel2, levels = c(\"High Construal\", \"Medium Construal\", \"Low Construal\"))\n\npartyData$CCModel2 <- recode(partyData$CCModel2, \"2\" = \"Low Construal\", \"1\" = \"Medium Construal\", \"0\" = \"High Construal\")\npartyData$CCModel2 <- factor(partyData$CCModel2, levels = c(\"High Construal\", \"Medium Construal\", \"Low Construal\"))\n\n#Plot Data\nggplot(data = data, aes(x = CCModel2, y = absAffPol)) + geom_bar(stat = \"identity\")\n\n\n\n\n\nCode\nggplot(data = partyData, aes(x = CCModel2, y = absAffPol, fill = pid3lean)) + geom_bar(stat = \"identity\", position = \"dodge\") + scale_fill_manual(values = c(\"Democrats\" = \"#00405b\", \"Independents\" = \"#4F7942\", \"Republicans\" = \"#7d0000\")) \n\n\n\n\n\nCode\n#comparison of regression models (1 is the best model)\nmodel1 <- lm(formula = absAffPol ~ CCModel2 + educ + newsint, data = doto)\nsummary(model1)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2 + educ + newsint, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.961 -22.197   2.562  21.240  70.032 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  56.3856     3.6274  15.544  < 2e-16 ***\nCCModel2      5.6838     1.4298   3.975 7.67e-05 ***\neduc          0.7301     0.6821   1.070    0.285    \nnewsint      -6.9825     0.9325  -7.488 1.84e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.24 on 803 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.1049,    Adjusted R-squared:  0.1015 \nF-statistic: 31.36 on 3 and 803 DF,  p-value: < 2.2e-16\n\n\nCode\nmodel2 <- lm(formula = absAffPol ~ CCModel2, data = doto)\nsummary(model2)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.184 -24.443   3.299  23.299  54.299 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   45.701      1.473  31.035  < 2e-16 ***\nCCModel2       7.742      1.455   5.322 1.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.3 on 805 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.03399,   Adjusted R-squared:  0.03279 \nF-statistic: 28.33 on 1 and 805 DF,  p-value: 1.331e-07\n\n\nCode\nmodel3 <- lm(formula = absAffPol ~ CCModel, data = doto)\nsummary(model3)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.089 -25.089   3.911  22.911  53.763 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   46.237      1.585   29.16  < 2e-16 ***\nCCModel1       8.852      2.098    4.22 2.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.49 on 805 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.02164,   Adjusted R-squared:  0.02043 \nF-statistic: 17.81 on 1 and 805 DF,  p-value: 2.721e-05\n\n\nCode\nmodel4 <- lm(formula = absAffPol ~ CCModel2 + CCModel, data = doto)\nsummary(model4)\n\n\n\nCall:\nlm(formula = absAffPol ~ CCModel2 + CCModel, data = doto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.705 -23.971   2.763  23.652  53.763 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   46.237      1.575  29.348  < 2e-16 ***\nCCModel2      10.357      3.094   3.347 0.000854 ***\nCCModel1      -4.246      4.433  -0.958 0.338511    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.31 on 804 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.03509,   Adjusted R-squared:  0.03269 \nF-statistic: 14.62 on 2 and 804 DF,  p-value: 5.801e-07\n\n\nCode\n#checking for interrater reliability of qualitative coding\nirrCheck <- cbind(caitCode$Who, caitCode$How, dCode$Who, dCode$How)\nirrCheck <- as.data.frame(irrCheck)\n\nkappa2(irrCheck[,c(1,3)], \"unweighted\")\n\n\nError in kappa2(irrCheck[, c(1, 3)], \"unweighted\"): could not find function \"kappa2\"\n\n\nCode\nkappa2(irrCheck[,c(2,4)], \"unweighted\")\n\n\nError in kappa2(irrCheck[, c(2, 4)], \"unweighted\"): could not find function \"kappa2\"\n\n\nCode\n#Represents very high interrater reliability for both\n\n\n#Choosing the Final Model The Final model (model 1), was chosen because it represents the highest amount of explanatory power. It is the most significant of any of the variables, and this significance does not disappear when put into a multiple regression model with other explanatory variables, both of Affective Polarization and of other qualitative coding models/combinations. Both the “Who” and “How” aspects of the qualitative coding are shown to independently capture portions of what we operationalize as construal level, which is shown to relate to downstream affective polarization. In future studies, I will also test additional qualitative coding models, as this model is most applicable to the vague UMass Poll question of issue-oriented responses to critical race theory. In this first model, valence is partially coded for in the “how” term, which is why a sentiment analysis has not been performed yet for this Study. In subsequent models in Studies 2 and 3, this will be examined in tandem with an alternative potential coding scheme."
  },
  {
    "objectID": "posts/FinalProjectPart2_DonnySnyder.html#references",
    "href": "posts/FinalProjectPart2_DonnySnyder.html#references",
    "title": "Final Project Part 2",
    "section": "References",
    "text": "References\nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). The origins and consequences of affective polarization in the United States. Annual Review of Political Science, 22(1), 129-146. Levendusky, M. (2009). The partisan sort: How liberals became Democrats and conservatives became Republicans. University of Chicago Press. Snyder, D. (2022). Keep It Simple Stupid: How Individual Differences in Cue Construal Explain Variations in Affective Polarization. Unpublished Manuscript Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. Psychological review, 117(2), 440.\n#Appendix: Qualitative Coding Instructions for Raters Construal Level Theory Qualitative Coding Key\nThe purpose of this task is to get an understanding of how clear respondents’ perceptions of the political landscape are, thorough the lens of how they interpret abstract issues. To operationalize this for the current task, you will be qualitatively coding along two dimensions – “Who” and “How”. Each of these dimensions will be coded as either a 1 or a 0. This is taken from the Critical Race Theory data, although the measurement does not apply to CRT in particular, it is somewhat unrelated.\n“Who” Overview: The purpose of coding this “Who” construct is to understand how clearly people perceive the groups involved in political issues. A “1” for this will involve a specific group mentioned. A specific group involves a group name that cannot be interpreted in multiple ways. This includes any concrete demographic.\nWho “1” examples: Mention of… “Democrats”, “Republicans”, “Marxists”, “Kids”, “Blacks”, “Whites”, “Teachers”… any other instances where it is expressly clear that they are referring to a specific group.\nIf there is even one occurrence of any of these in the free response, it should be coded as “1.”\nWho “0” examples: No groups mentioned, or groups are only mentioned in the abstract. Examples of groups mentioned in the abstract: “People of Color”, “Certain groups of people.”\nIf there is no specific group information, or only abstract groups, or they respond “I don’t know”, “Who” should be coded as a “0.”\n“How” Overview: The purpose of coding this “How” construct is to understand whether respondents will try to provide information about their own perspectives to the reader, despite this not being the purpose of the task, which was to “Define critical race theory.”\nHow “1” coding: If you code a response as a “1” for “How”, it should be from a “yes” answer to one or both of these two questions; 1. Does the respondent mention how they personally feel about the merit of Critical Race Theory? 2. Is the respondent trying to suggest to you how you should feel about the merit of Critical Race Theory? This includes both positive and negative perspectives about Critical Race Theory.\nHow “0” Coding: A “How” “0” should be a response that merely attempts to define, and not provide a description of how the respondent feels about critical race theory."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html",
    "title": "Final project part 2",
    "section": "",
    "text": "Churning refers to a customer who leaves one company to go to another company. Customer churn introduces not only some loss in income but also other negative effects on the operation of companies. Churn management is the concept of identifying those customers who are intending to move their custom to a competing service provider.\nRisselada et al. (2010) stated that churn management is becoming part of customer relationship management. It is important for companies to consider it as they try to establish long-term relationships with customers and maximize the value of their customer base.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Does churn-rate depend on the geographical factors(Customer’s location) of the customer?\nB. Do non-active members are probable to churn or not?\n\n\nThis project will be useful to better understand more about the customer difficulties and factors and also give us a pretty good idea on the factors effecting the customers to exit and also about the dormant state of the customers."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#hypothesis",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#hypothesis",
    "title": "Final project part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nCustomer churn analysis has become a major concern in almost every industry that offers products and services. The model developed will help banks identify clients who are likely to be churners and develop appropriate marketing actions to retain their valuable clients. And this model also supports information about similar customer group to consider which marketing reactions are to be provided. Thus, due to existing customers are retained, it will provide banks with increased profits and revenues. By the end of this article, let’s attempt to solve some of the key business challenges pertaining to customer attrition like say, (1) what is the likelihood of an active customer leaving an organization? (2) what are key indicators of a customer churn? (3) what retention strategies can be implemented based on the results to diminish prospective customer churn?\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nCustomer’s location will not be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nCustomer’s location will be statistically predict the churn-rate.\n\n\nI believe that the customer’s location have an effect on customer’s churn rate as based on location there is statistical difference in customer’s salary and balance.\n\n\n\n\n\n\nH0B\n\n\n\nActive members will not churn.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nActive members will churn.\n\n\nI think that inactive members are more likely to exit rather than active members as there is a high chance of them churning out as they are are inactive for a longtime."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#loading-libraries",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#loading-libraries",
    "title": "Final project part 2",
    "section": "Loading libraries",
    "text": "Loading libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(reshape2)\nlibrary(skimr)\nlibrary(randomForest)\nlibrary(caret)\nlibrary(interactions)\nlibrary(lmtest)\nlibrary(sandwich)\nlibrary(plotly)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#reading-the-data-set",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#reading-the-data-set",
    "title": "Final project part 2",
    "section": "Reading the data set",
    "text": "Reading the data set\n\n\nCode\nChurn <- read_csv(\"_data/Churn_Modelling.csv\")\n\n\nRows: 10000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Surname, Geography, Gender\ndbl (11): RowNumber, CustomerId, CreditScore, Age, Tenure, Balance, NumOfPro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nChurn\n\n\n\n\n  \n\n\n\nThis data set is originated from a U.S. bank and is downloaded from kaggle. This data set includes 10k bank customer data records with 14 attributes including socio-demographic attributes, account level and behavioral attributes."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#attribute-description",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#attribute-description",
    "title": "Final project part 2",
    "section": "Attribute Description",
    "text": "Attribute Description\n\nRow Number- Number of customers\nCustomer ID- ID of customer 3.Surname- Customer name\nCredit Score- Score of credit card usage\nGeography- Location of customer\nGender- Customer gender\nAge- Age of Customer\nTenure- The period of having the account in months\nBalance- Customer main balance\nNumOfProducts- No of products used by customer(No of accounts the customer have)\nHasCrCard- If the customer has a credit card or not\nIsActiveMember- Customer account is active or not(if he haven’t used his savings or current account for any transactions for over 1 year, then he is treated as inactive.)\nEstimated Salary- Estimated salary of the customer.\nExited- Indicate churned or not, i.e, if the customer left the bank or not.\n\nThe response variable is Exited variable and the main explanatory variables are Geography and IsActiveMember. And the other explanatory variables are Credit Score, Gender, Age and Balance.\n\n\nCode\nstr(Churn)\n\n\nspc_tbl_ [10,000 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ RowNumber      : num [1:10000] 1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : num [1:10000] 15634602 15647311 15619304 15701354 15737888 ...\n $ Surname        : chr [1:10000] \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : num [1:10000] 619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr [1:10000] \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr [1:10000] \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : num [1:10000] 42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : num [1:10000] 2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num [1:10000] 0 83808 159661 0 125511 ...\n $ NumOfProducts  : num [1:10000] 1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : num [1:10000] 1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : num [1:10000] 1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num [1:10000] 101349 112543 113932 93827 79084 ...\n $ Exited         : num [1:10000] 1 0 1 0 0 1 0 1 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   RowNumber = col_double(),\n  ..   CustomerId = col_double(),\n  ..   Surname = col_character(),\n  ..   CreditScore = col_double(),\n  ..   Geography = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_double(),\n  ..   Tenure = col_double(),\n  ..   Balance = col_double(),\n  ..   NumOfProducts = col_double(),\n  ..   HasCrCard = col_double(),\n  ..   IsActiveMember = col_double(),\n  ..   EstimatedSalary = col_double(),\n  ..   Exited = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#descriptive-statistics",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#descriptive-statistics",
    "title": "Final project part 2",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n\nCode\nsummary(Churn)\n\n\n   RowNumber       CustomerId         Surname           CreditScore   \n Min.   :    1   Min.   :15565701   Length:10000       Min.   :350.0  \n 1st Qu.: 2501   1st Qu.:15628528   Class :character   1st Qu.:584.0  \n Median : 5000   Median :15690738   Mode  :character   Median :652.0  \n Mean   : 5000   Mean   :15690941                      Mean   :650.5  \n 3rd Qu.: 7500   3rd Qu.:15753234                      3rd Qu.:718.0  \n Max.   :10000   Max.   :15815690                      Max.   :850.0  \n  Geography            Gender               Age            Tenure      \n Length:10000       Length:10000       Min.   :18.00   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:32.00   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median :37.00   Median : 5.000  \n                                       Mean   :38.92   Mean   : 5.013  \n                                       3rd Qu.:44.00   3rd Qu.: 7.000  \n                                       Max.   :92.00   Max.   :10.000  \n    Balance       NumOfProducts    HasCrCard      IsActiveMember  \n Min.   :     0   Min.   :1.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:     0   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 97199   Median :1.00   Median :1.0000   Median :1.0000  \n Mean   : 76486   Mean   :1.53   Mean   :0.7055   Mean   :0.5151  \n 3rd Qu.:127644   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :250898   Max.   :4.00   Max.   :1.0000   Max.   :1.0000  \n EstimatedSalary         Exited      \n Min.   :    11.58   Min.   :0.0000  \n 1st Qu.: 51002.11   1st Qu.:0.0000  \n Median :100193.91   Median :0.0000  \n Mean   :100090.24   Mean   :0.2037  \n 3rd Qu.:149388.25   3rd Qu.:0.0000  \n Max.   :199992.48   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(Churn)\n\n\nRows: 10,000\nColumns: 14\n$ RowNumber       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ CustomerId      <dbl> 15634602, 15647311, 15619304, 15701354, 15737888, 1557…\n$ Surname         <chr> \"Hargrave\", \"Hill\", \"Onio\", \"Boni\", \"Mitchell\", \"Chu\",…\n$ CreditScore     <dbl> 619, 608, 502, 699, 850, 645, 822, 376, 501, 684, 528,…\n$ Geography       <chr> \"France\", \"Spain\", \"France\", \"France\", \"Spain\", \"Spain…\n$ Gender          <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Female\", \"Mal…\n$ Age             <dbl> 42, 41, 42, 39, 43, 44, 50, 29, 44, 27, 31, 24, 34, 25…\n$ Tenure          <dbl> 2, 1, 8, 1, 2, 8, 7, 4, 4, 2, 6, 3, 10, 5, 7, 3, 1, 9,…\n$ Balance         <dbl> 0.00, 83807.86, 159660.80, 0.00, 125510.82, 113755.78,…\n$ NumOfProducts   <dbl> 1, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, …\n$ HasCrCard       <dbl> 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …\n$ IsActiveMember  <dbl> 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, …\n$ EstimatedSalary <dbl> 101348.88, 112542.58, 113931.57, 93826.63, 79084.10, 1…\n$ Exited          <dbl> 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#tidying-the-data",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#tidying-the-data",
    "title": "Final project part 2",
    "section": "Tidying the data",
    "text": "Tidying the data\n\n\nCode\nChurn <- Churn %>%\n  select(-c(RowNumber, CustomerId, Surname))\nChurn\n\n\n\n\n  \n\n\n\nDimensions of the data set\n\n\nCode\ndim(Churn)\n\n\n[1] 10000    11\n\n\nThe data set has 10000 rows and 11 columns now after removing the first 3 columns which are not necessary for analysis and will not effect the model.\nChecking for Null values\n\n\nCode\napply(is.na(Churn), MARGIN = 2, FUN = sum)\n\n\n    CreditScore       Geography          Gender             Age          Tenure \n              0               0               0               0               0 \n        Balance   NumOfProducts       HasCrCard  IsActiveMember EstimatedSalary \n              0               0               0               0               0 \n         Exited \n              0"
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#detecting-the-outliers",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#detecting-the-outliers",
    "title": "Final project part 2",
    "section": "Detecting the outliers",
    "text": "Detecting the outliers\n\n\nCode\nChurn %>%\n  ggplot(aes(CreditScore)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, Credit score variable has few outliers, but they cannot potentially affect the data set.\n\n\nCode\nChurn %>%\n  ggplot(aes(Age)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, age variable has outliers (age group above 60 constitutes outliers), however there are few outliers. But they cannot potentially affect the data set.\n\n\nCode\nChurn %>%\n  ggplot(aes(Tenure)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, Tenure variable has no outliers.\n\n\nCode\nChurn %>%\n  ggplot(aes(Balance)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, Balance variable has no outliers.\n\n\nCode\nChurn %>%\n  ggplot(aes(NumOfProducts)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, NumofProducts variable has no outliers.\n\n\nCode\nChurn %>%\n  ggplot(aes(EstimatedSalary)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\nFrom above box plot, EstimatedSalary variable has no outliers."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#visualing-and-interpreting-the-variables",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#visualing-and-interpreting-the-variables",
    "title": "Final project part 2",
    "section": "Visualing and interpreting the variables",
    "text": "Visualing and interpreting the variables\n\n\nCode\nChurn %>%\n  ggplot(aes(CreditScore)) +\n  geom_density(color=\"Green\", alpha=0.8) +\n  ggtitle(\"Credit score of customers\") + \n  theme_classic()\n\n\n\n\n\nThe credit score is looking to be normal with median in range of 650-700.\n\n\nCode\nChurn %>% \n  group_by(Geography) %>% # Variable to be transformed\n  count() %>% \n  ungroup() %>% \n  mutate(perc = `n` / sum(`n`)) %>% \n  arrange(perc) %>%\n  mutate(labels = scales::percent(perc)) %>%\n  ggplot(aes(x = \"\", y = perc, fill = Geography)) +\n  ggtitle(\"Location of customers\") +\n  geom_col(color = \"black\") +\n  geom_label(aes(label = labels), color = c(1, \"white\", \"white\"),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"Geography\")) +\n  scale_fill_viridis_d() +\n  coord_polar(theta = \"y\") + \n  theme_void()\n\n\n\n\n\nCode\n  scale_fill_brewer(palette=\"Set1\")\n\n\n<ggproto object: Class ScaleDiscrete, Scale, gg>\n    aesthetics: fill\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: <ggproto object: Class RangeDiscrete, Range, gg>\n        range: NULL\n        reset: function\n        train: function\n        super:  <ggproto object: Class RangeDiscrete, Range, gg>\n    rescale: function\n    reset: function\n    scale_name: brewer\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  <ggproto object: Class ScaleDiscrete, Scale, gg>\n\n\nThe Geography variable consists of 3 values, i.e, France(50%), Germany(25%) and Spain(25%).\n\n\nCode\np <- Churn %>%\n  ggplot() +\n  geom_bar(aes(Gender)) +\n  ggtitle(\"Gender of customers\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nThe Gender variable consists of Male and Female values and male count(5457) is more than female count(4543).\n\n\nCode\np <- Churn %>%\n  ggplot() +\n  geom_bar(aes(Tenure)) +\n  theme_classic() +\n  ggtitle(\"No of customers over their tenure\")\nggplotly(p)\n\n\n\n\n\n\nThe tenure of all customers is between 0-10 years and is almost equal no of customers in each year.\n\n\nCode\np <- Churn %>%\n  filter(Balance != 0) %>%\n  ggplot(aes(Balance)) +\n  geom_histogram(col = \"white\") +\n  theme_classic() +\n  ggtitle(\"Balance of customers\")\nggplotly(p)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nWe have a lot of people with balance as zero but if we ignore that the other values form a normal at 120000 in median.\n\n\nCode\nChurn %>% \n  group_by(NumOfProducts) %>% # Variable to be transformed\n  count() %>% \n  ungroup() %>% \n  mutate(perc = `n` / sum(`n`)) %>% \n  arrange(perc) %>%\n  mutate(labels = scales::percent(perc)) %>%\n  ggplot(aes(x = \"\", y = perc, fill = as.factor(NumOfProducts))) +\n  ggtitle(\"No of products owned by customers\") +\n  geom_col(color = \"black\") +\n  geom_label(aes(label = labels), color = c(1, \"white\", \"white\", \"white\"),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"NumofProducts\")) +\n  scale_fill_viridis_d() +\n  coord_polar(theta = \"y\") + \n  theme_void()\n\n\n\n\n\nCode\n  scale_fill_brewer(palette=\"Set1\")\n\n\n<ggproto object: Class ScaleDiscrete, Scale, gg>\n    aesthetics: fill\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: <ggproto object: Class RangeDiscrete, Range, gg>\n        range: NULL\n        reset: function\n        train: function\n        super:  <ggproto object: Class RangeDiscrete, Range, gg>\n    rescale: function\n    reset: function\n    scale_name: brewer\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  <ggproto object: Class ScaleDiscrete, Scale, gg>\n\n\nAccording to above plot, the maximum no of the products owned by customers is 4 and minimum is 1. Majority of customers own either 1 or 2 products.\n\n\nCode\np <- Churn %>%\n  ggplot() +\n  geom_bar(aes(HasCrCard)) +\n  ggtitle(\"No of customers having credit card\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nAccording to above plot, 7055 customers have credit card and 2945 customers does not have credit card.\n\n\nCode\np <- Churn %>%\n  ggplot() +\n  geom_bar(aes(IsActiveMember)) +\n  ggtitle(\"Active customers\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nForm the above plot, it looks like there are as many inactive members(4849) as active members(5151).\n\n\nCode\nChurn %>%\n  ggplot(aes(EstimatedSalary)) +\n  geom_density(color=\"Blue\", alpha=0.8) +\n  ggtitle(\"Estimated salary of customers\") + \n  theme_classic()\n\n\n\n\n\nFrom above graph, the data set contains the customers of all types of income from 0-200000.\n\n\nCode\nChurn %>% \n  group_by(Exited) %>% # Variable to be transformed\n  count() %>% \n  ungroup() %>% \n  mutate(perc = `n` / sum(`n`)) %>% \n  arrange(perc) %>%\n  mutate(labels = scales::percent(perc)) %>%\n  ggplot(aes(x = \"\", y = perc, fill = as.factor(Exited))) +\n  ggtitle(\"Churn-rate of customers\") +\n  geom_col(color = \"black\") +\n  geom_label(aes(label = labels), color = c(1, \"white\"),\n            position = position_stack(vjust = 0.5),\n            show.legend = FALSE) +\n  guides(fill = guide_legend(title = \"Churn-rate\")) +\n  scale_fill_viridis_d() +\n  coord_polar(theta = \"y\") + \n  theme_void()\n\n\n\n\n\nCode\n  scale_fill_brewer(palette=\"Set1\")\n\n\n<ggproto object: Class ScaleDiscrete, Scale, gg>\n    aesthetics: fill\n    axis_order: function\n    break_info: function\n    break_positions: function\n    breaks: waiver\n    call: call\n    clone: function\n    dimension: function\n    drop: TRUE\n    expand: waiver\n    get_breaks: function\n    get_breaks_minor: function\n    get_labels: function\n    get_limits: function\n    guide: legend\n    is_discrete: function\n    is_empty: function\n    labels: waiver\n    limits: NULL\n    make_sec_title: function\n    make_title: function\n    map: function\n    map_df: function\n    n.breaks.cache: NULL\n    na.translate: TRUE\n    na.value: NA\n    name: waiver\n    palette: function\n    palette.cache: NULL\n    position: left\n    range: <ggproto object: Class RangeDiscrete, Range, gg>\n        range: NULL\n        reset: function\n        train: function\n        super:  <ggproto object: Class RangeDiscrete, Range, gg>\n    rescale: function\n    reset: function\n    scale_name: brewer\n    train: function\n    train_df: function\n    transform: function\n    transform_df: function\n    super:  <ggproto object: Class ScaleDiscrete, Scale, gg>\n\n\nFrom the pie chart, 80% of customers are not churned and 20% have already exited."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-the-variables",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-the-variables",
    "title": "Final project part 2",
    "section": "Relationship between the variables",
    "text": "Relationship between the variables\n\n\nCode\ntemp <- Churn %>%\n  select(-c(Geography, Gender))\nround(cor(temp),3) %>%\n  melt() %>% \n  ggplot(aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) + \n  labs(x = NULL, y = NULL) + \n  ggtitle(\"Correlation plot\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\nChurn has a positive correlation with age, balance and estimated salary. Generally the correlation coefficients are not so high."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-churn-rate-and-categorical-variables",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-churn-rate-and-categorical-variables",
    "title": "Final project part 2",
    "section": "Relationship between churn-rate and categorical variables",
    "text": "Relationship between churn-rate and categorical variables\nThere are 4 categorical variables in the data set as follows:\n\n\nCode\np <- Churn %>%\n  group_by(Geography, Exited) %>%\n  count() %>%\n  ggplot(aes(fill = Exited, y = n, x = Geography)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  ggtitle(\"Churn-rate on basis of location\") +\n  xlab(\"Geography\") +\n  ylab(\"Frequency\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nMajority of the data is from persons from France. However, the proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem (maybe not enough customer service resources allocated) in the areas where it has fewer clients.\n\n\nCode\np <- Churn %>%\n  group_by(Gender, Exited) %>%\n  count() %>%\n  ggplot(aes(fill = Exited, y = n, x = Gender)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  ggtitle(\"Churn-rate per Gender\") +\n  xlab(\"Gender\") +\n  ylab(\"Frequency\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nThe proportion of female customers churning is also greater than that of male customers.\n\n\nCode\np <- Churn %>%\n  group_by(HasCrCard, Exited) %>%\n  count() %>%\n  ggplot(aes(fill = Exited, y = n, x = HasCrCard)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  ggtitle(\"Churn-rate per customer's credit card status\") +\n  xlab(\"Credit Card status\") +\n  ylab(\"Frequency\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nMajority of the customers that churned are those with credit cards. Given that majority of the customers have credit cards could prove this to be just a coincidence.\n\n\nCode\np <- Churn %>%\n  group_by(IsActiveMember, Exited) %>%\n  count() %>%\n  ggplot(aes(fill = Exited, y = n, x = IsActiveMember)) + \n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  ggtitle(\"Churn-rate on basis of customer's activity\") +\n  xlab(\"Active Member\") +\n  ylab(\"Frequency\") +\n  theme_classic()\nggplotly(p)\n\n\n\n\n\n\nUnsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-churn-rate-and-continuous-variables",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#relationship-between-churn-rate-and-continuous-variables",
    "title": "Final project part 2",
    "section": "Relationship between churn-rate and continuous variables",
    "text": "Relationship between churn-rate and continuous variables\nThere are 6 continuous variables in the data set as follows:\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = CreditScore, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of customer's credit score\")\nggplotly(p)\n\n\n\n\n\n\nThere is no significant difference in the credit score distribution between retained and churned customers.\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = Age, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of customer's age\")\nggplotly(p)\n\n\n\n\n\n\nThe older customers are churning at more rate than the younger ones alluding to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups.\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = Tenure, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of customer's tenure\")\nggplotly(p)\n\n\n\n\n\n\nWith regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = Balance, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of customer's balance\")\nggplotly(p)\n\n\n\n\n\n\nThe bank is losing customers with significant bank balances which is likely to hit their available capital for lending.\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = NumOfProducts, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of no of accounts customer's own\")\nggplotly(p)\n\n\n\n\n\n\nThe no of products not has a significant effect on the likelihood to churn.\n\n\nCode\np <- Churn %>% \n  ggplot(aes(x = Exited, y = EstimatedSalary, fill = as.factor(Exited))) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Churn-rate on basis of customer's salary\")\nggplotly(p)\n\n\n\n\n\n\nThe Estimated salary not has a significant effect on the likelihood to churn."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#regression-models",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#regression-models",
    "title": "Final project part 2",
    "section": "Regression models",
    "text": "Regression models\n\n\nCode\nmodel1 <- lm(Exited ~ as.factor(IsActiveMember), data = Churn)\nsummary(model1)\n\n\n\nCall:\nlm(formula = Exited ~ as.factor(IsActiveMember), data = Churn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2685 -0.2685 -0.1427 -0.1427  0.8573 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 0.268509   0.005713    47.0   <2e-16 ***\nas.factor(IsActiveMember)1 -0.125818   0.007961   -15.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3978 on 9998 degrees of freedom\nMultiple R-squared:  0.02438,   Adjusted R-squared:  0.02428 \nF-statistic: 249.8 on 1 and 9998 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nmodel2 <- lm(Exited ~ ., data = Churn)\nsummary(model2)\n\n\n\nCall:\nlm(formula = Exited ~ ., data = Churn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.84083 -0.23374 -0.12020  0.03515  1.20544 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      -8.034e-02  3.372e-02  -2.382   0.0172 *  \nCreditScore      -9.386e-05  3.844e-05  -2.441   0.0146 *  \nGeographyGermany  1.262e-01  9.915e-03  12.728  < 2e-16 ***\nGeographySpain    4.043e-03  9.123e-03   0.443   0.6577    \nGenderMale       -7.455e-02  7.470e-03  -9.980  < 2e-16 ***\nAge               1.110e-02  3.562e-04  31.165  < 2e-16 ***\nTenure           -1.906e-03  1.285e-03  -1.483   0.1381    \nBalance           3.139e-07  6.881e-08   4.562 5.12e-06 ***\nNumOfProducts    -1.577e-02  6.764e-03  -2.331   0.0198 *  \nHasCrCard        -4.944e-03  8.154e-03  -0.606   0.5443    \nIsActiveMember   -1.411e-01  7.470e-03 -18.891  < 2e-16 ***\nEstimatedSalary   6.850e-08  6.461e-08   1.060   0.2890    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3714 on 9988 degrees of freedom\nMultiple R-squared:  0.1508,    Adjusted R-squared:  0.1499 \nF-statistic: 161.2 on 11 and 9988 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nmodel3 <- lm(Exited ~ Geography, data = Churn)\nsummary(model3)\n\n\n\nCall:\nlm(formula = Exited ~ Geography, data = Churn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3244 -0.1667 -0.1615 -0.1615  0.8385 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.161548   0.005602  28.836   <2e-16 ***\nGeographyGermany 0.162884   0.009701  16.791   <2e-16 ***\nGeographySpain   0.005186   0.009743   0.532    0.595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3967 on 9997 degrees of freedom\nMultiple R-squared:  0.03013,   Adjusted R-squared:  0.02993 \nF-statistic: 155.3 on 2 and 9997 DF,  p-value: < 2.2e-16\n\n\nSummarizing the above three models, the model1 describes the regression between Exited and Active member and secone model predicts the churn rate based on all the variables and third model is analysis between exited and geography. And model1 and model3 seems to be significant proving our hypothesis."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#backward-elimination",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#backward-elimination",
    "title": "Final project part 2",
    "section": "Backward Elimination",
    "text": "Backward Elimination\n\n\nCode\nmodel4 <- lm(Exited ~ . -Geography -HasCrCard -NumOfProducts -EstimatedSalary -Tenure, data = Churn)\nsummary(model4)\n\n\n\nCall:\nlm(formula = Exited ~ . - Geography - HasCrCard - NumOfProducts - \n    EstimatedSalary - Tenure, data = Churn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7805 -0.2361 -0.1251  0.0271  1.1939 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -1.144e-01  2.977e-02  -3.843 0.000122 ***\nCreditScore    -9.307e-05  3.877e-05  -2.401 0.016388 *  \nGenderMale     -7.748e-02  7.529e-03 -10.291  < 2e-16 ***\nAge             1.132e-02  3.588e-04  31.539  < 2e-16 ***\nBalance         7.081e-07  6.007e-08  11.789  < 2e-16 ***\nIsActiveMember -1.430e-01  7.528e-03 -18.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3746 on 9994 degrees of freedom\nMultiple R-squared:  0.1356,    Adjusted R-squared:  0.1352 \nF-statistic: 313.5 on 5 and 9994 DF,  p-value: < 2.2e-16\n\n\nIn the model4, I have used backward elimination process by removing the highest p values to get a significant model and it is significant after removing above 5 variables removed in call."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#model-evaluation",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#model-evaluation",
    "title": "Final project part 2",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\nCode\npar(mfrow = c(2,3)); plot(model1, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(model2, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(model3, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(model4, which = 1:6)\n\n\n\n\n\nAccording to the diagnostic plots, none of the models seem to fit super well. There are violations of assumptions in models. In some the residuals seem to have a trend (higher fitted values have lower residuals). Same with the Q-Q, plot, lower theoretical quantiles gave significantly lower standardized residuals. The scale location graph has a negative trend, suggesting variance may not be constant. Cooks dist to leverage has a high cooks distance and leverage and likely has a large influence on the model. The other models display similar issues."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#further-study",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#further-study",
    "title": "Final project part 2",
    "section": "Further Study",
    "text": "Further Study\nMoving into part three of the project, I may look into other control variables that may improve the model or other transformations to improve R squared. I would try the logistic regression and randomforest models and check them in part3."
  },
  {
    "objectID": "posts/FinalProjectPart2_ManiShankerKamarapu.html#bibliography",
    "href": "posts/FinalProjectPart2_ManiShankerKamarapu.html#bibliography",
    "title": "Final project part 2",
    "section": "Bibliography",
    "text": "Bibliography\nChicco, D. & Jurman, G., 2020. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC genomics, 21(1), pp. 6-13.\nColgate, M., Stewart, K. & Kinsella, R., 1996. Customer Defection: A study of the student market in Ireland. International Journal of Bank Marketing, 14(3), pp. 23-29.\nDe Caigny, A., Coussement, K. & De Bock, K. W., 2018. A new hybrid classification algorithm for customer churn prediction based on logistic regression and decision trees. European Journal of Operational Research, 269(2), pp. 760-772.\nDelgado, R. & Tibau, X. 2019. Why Cohen’s Kappa should be avoided as performance measure in classification, PLOS ONE, 14(9), pp. e0222916.\nGanesh, J., Arnold, M. J. & Reynolds, K. E., 2000. Understanding the Customer Base of Service Providers: An Examination of the Differences between Switchers and Stayers. Journal of Marketing, 64(3), pp. 65-87.\nGorodkin, J., 2004. Comparing two K-category assignments by a K-category correlation coefficient. Computational Biology and Chemistry, 28(5), pp. 367-374.\nHair, J. F., Black, J. W. C., Babin, B. J. & Anderson, R. E., 2014. Multivariate Data Analysis. 7th ed. Harlow: Pearson international edn.\nHastie, T., Tibshirani, R. & Friedman, J., 2009. The Elements of Statistical Learning: data mining, inference, and prediction. 2nd ed. New York, NY: Springer New York.\nHosmer, D. W., Lemeshow, S. & Sturdivant, R. X., 2013. Applied logistic regression, 3rd ed. New Jersey, NJ: Wiley.\nJames, G., Witten, D., Hastie, T. & Tibshirani, R., 2013. An Introduction to Statistical Learning: with Applications in R. New York, NY: Springer New York.\nMcHugh, M. L., 2012. Interrater reliability: the Kappa Statistic. Biochemia Medica, 22(3), pp. 276-282.\nThe Economist, 2019. A Whole New World: How technology is driving the evolution of intelligent banking, London: The Economist Intelligence Unit (EIU).\nVerbeke, W. et al., 2012. New insights into churn prediction in the Telecommunication Sector: A profit driven data mining approach. European Journal of Operational Research, 218(1), pp. 211- 229."
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html",
    "href": "posts/FinalProjectProposal_Saaradhaa.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\n# load libraries.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nPrior research literature in the social sciences has continually stressed the need for more research on the Global South. However, few papers actually focus on it. Hence, I am interested to learn more about this region. A data source that lends itself useful for this is the World Values Survey, a global survey with an easily accessible database.\nI am specifically interested in understanding what drives subjective well-being, which can be interpreted via happiness and life satisfaction (Addai et al., 2013).\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. What predicts happiness and life satisfaction in the Global South?\nB. Do predictors of happiness and life satisfaction differ between the Global North and South?\n\n\nThis project will be useful to better understand motivations and desires in the Global South, reduce inter-cultural tensions and enhance cross-cultural cohesion. Governments can also benefit from this research in terms of policy prioritization to maximize citizens’ well-being."
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nPast researchers have studied happiness and life satisfaction in the Global South via the World Values Survey (Addai et al., 2013; Ngamaba, 2016). The studies focused on Ghana and Rwanda respectively. The common predictors of happiness and life satisfaction across both countries were satisfaction with health and income.\nTo the best of my knowledge, few studies comparing well-being in the Global North and South exist. Alba (2019) found that happiness was generally greater in the Global North than the Global South, and indicated that future research should attempt to cover the factors behind this. I think happiness and well-being in the Global North may depend on more subjective measures, given that health and income-related issues should be relatively more accounted for.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nHealth and financial satisfaction will not be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nHealth and financial satisfaction will be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nPredictors of happiness and life satisfaction will not differ between the Global North and South.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nPredictors of happiness and life satisfaction will differ between the Global North and South."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html",
    "title": "Final Project Update",
    "section": "",
    "text": "Code\n# load libraries.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\nlibrary(interactions)\nlibrary(lmtest)\nlibrary(sandwich)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-intro",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-intro",
    "title": "Final Project Update",
    "section": "Part 1: Intro",
    "text": "Part 1: Intro\nPrior research literature in the social sciences has continually stressed the need for more research on the Global South. However, few papers actually focus on it. Hence, I am interested to learn more about this region. A data source that lends itself useful for this is the World Values Survey, a global survey with an easily accessible database.\nI am specifically interested in understanding what drives subjective well-being, which can be interpreted via happiness and life satisfaction (Addai et al., 2013).\n\n\n\n\n\n\nPart 1: Research Questions\n\n\n\nA. What predicts happiness and life satisfaction in the Global South?\nB. Do predictors of happiness and life satisfaction differ between the Global North and South?\n\n\nThis project will be useful to better understand motivations and desires in the Global South, reduce inter-cultural tensions and enhance cross-cultural cohesion. Governments can also benefit from this research in terms of policy prioritization to maximize citizens’ well-being."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-hypotheses-edited-based-on-feedback",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-hypotheses-edited-based-on-feedback",
    "title": "Final Project Update",
    "section": "Part 1: Hypotheses (Edited Based on Feedback)",
    "text": "Part 1: Hypotheses (Edited Based on Feedback)\nPast researchers have studied happiness and life satisfaction in the Global South via the World Values Survey (Addai et al., 2013; Ngamaba, 2016). The studies focused on Ghana and Rwanda respectively. The common predictors of happiness and life satisfaction across both countries were satisfaction with health and income.\nAlba (2019) found that happiness was generally greater in the Global North than the Global South, and indicated that future research should attempt to cover the factors behind this, which gave me the impetus for this project.\nEdited based on feedback: I refer to Maslow’s hierarchy of needs, where physical and safety needs come first. My thinking is that happiness and well-being in the Global North may depend on more subjective measures, given that health and income-related problems should be relatively more accounted for.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nHealth and financial satisfaction will not positively predict happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nHealth and financial satisfaction will positively predict happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nHealth and financial satisfaction will not have a greater impact on happiness and life satisfaction on the Global South than the Global North.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nHealth and financial satisfaction will have a greater impact on happiness and life satisfaction on the Global South than the Global North."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-read-in-data",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-read-in-data",
    "title": "Final Project Update",
    "section": "Part 1: Read In Data",
    "text": "Part 1: Read In Data\nI will be working with the most recent wave of the World Values Survey, Wave 7, which was conducted from 2017 to 2022. The data is freely available for non-profit purposes. It must be cited properly and not re-distributed (Haerpfer et al., 2022).\nRepresentative samples of the population aged 18 and above were collected from 59 countries. Data was mostly collected by interviewing respondents at their homes (“WVS Database”, 2022).\nI am using the version of Wave 7 released in May 2022.\nI will indicate my comments in each code chunk to keep track of my progress.\n\n\nCode\n# read in dataset.\nwvs <- read_csv(\"~/Desktop/2022_Fall/DACSS 603/General/Final Project/WVS/4. Data/WVS_Cross-National_Wave_7_csv_v4_0.csv\", show_col_types = FALSE) %>% select(\"B_COUNTRY_ALPHA\", \"G_TOWNSIZE\", \"H_SETTLEMENT\", \"H_URBRURAL\", \"O1_LONGITUDE\", \"O2_LATITUDE\", \"Q1\", \"Q2\", \"Q3\", \"Q6\", \"Q46\", \"Q47\", \"Q48\", \"Q49\", \"Q50\", \"Q57\", \"Q171\", \"Q260\", \"Q262\", \"Q263\", \"Q269\", \"Q270\", \"Q271\", \"Q273\", \"Q274\", \"Q275\", \"Q279\", \"Q287\", \"Q288\", \"Q288R\", \"Q289\", \"I_WOMJOB\", \"I_WOMPOL\", \"I_WOMEDU\", \"Q182\", \"Q184\")\n\n\nThe dataset originally had 552 columns. I have selected a subset of columns based on variables used in past papers, as well as some variables I am interested to examine. These include place/area of residence, literacy, demographics, importance of various social aspects, happiness and wellbeing indicators, trust, religiosity, equality of gender/sexual orientation and abortion attitudes.\nI will first create a dummy variable for Global North/South. The Global South comprises low- and lower-middle income countries, as defined by the World Bank (“World Bank Country and Lending Groups”, 2022). Global South countries surveyed include Ethiopia, Philippines, Indonesia, Bangladesh, Iran, Kenya, Bolivia, Kyrgyzstan, Lebanon, Tajikistan, Tunisia, Ukraine, Mongolia, Morocco, Egypt, Myanmar, Vietnam, Nicaragua, Zimbabwe, Nigeria and Pakistan.\n\n\nCode\n# create dummy.\nwvs <- mutate(wvs, NS = case_when(B_COUNTRY_ALPHA == \"ETH\" | B_COUNTRY_ALPHA == \"PHL\" | B_COUNTRY_ALPHA == \"IDN\" | B_COUNTRY_ALPHA == \"BGD\" | B_COUNTRY_ALPHA == \"IRN\" | B_COUNTRY_ALPHA == \"KEN\" | B_COUNTRY_ALPHA == \"BOL\" | B_COUNTRY_ALPHA == \"KGZ\" | B_COUNTRY_ALPHA == \"LBN\" | B_COUNTRY_ALPHA == \"TJK\" | B_COUNTRY_ALPHA == \"TUN\" | B_COUNTRY_ALPHA == \"MOR\" | B_COUNTRY_ALPHA == \"UKR\" | B_COUNTRY_ALPHA == \"MNG\" | B_COUNTRY_ALPHA == \"EGY\" | B_COUNTRY_ALPHA == \"MMR\" | B_COUNTRY_ALPHA == \"VNM\" | B_COUNTRY_ALPHA == \"NIC\" | B_COUNTRY_ALPHA == \"ZWE\" | B_COUNTRY_ALPHA == \"NGA\" | B_COUNTRY_ALPHA == \"PAK\" ~ \"1\"))\n\n# replace \"NA\" with \"O\" (for Global North).\nwvs$NS <- replace_na(wvs$NS, \"0\")\n\n# change to factor.\nwvs$NS <- as.factor(wvs$NS)\n\n# check counts of levels.\nwvs %>% select(NS) %>% summary()\n\n\n NS       \n 0:59178  \n 1:28644  \n\n\nCode\n# sanity check.\nwvs %>% filter(B_COUNTRY_ALPHA == \"ETH\" | B_COUNTRY_ALPHA == \"PHL\" | B_COUNTRY_ALPHA == \"IDN\" | B_COUNTRY_ALPHA == \"BGD\" | B_COUNTRY_ALPHA == \"IRN\" | B_COUNTRY_ALPHA == \"KEN\" | B_COUNTRY_ALPHA == \"BOL\" | B_COUNTRY_ALPHA == \"KGZ\" | B_COUNTRY_ALPHA == \"LBN\" | B_COUNTRY_ALPHA == \"TJK\" | B_COUNTRY_ALPHA == \"TUN\" | B_COUNTRY_ALPHA == \"MOR\" | B_COUNTRY_ALPHA == \"UKR\" | B_COUNTRY_ALPHA == \"MNG\" | B_COUNTRY_ALPHA == \"EGY\" | B_COUNTRY_ALPHA == \"MMR\" | B_COUNTRY_ALPHA == \"VNM\" | B_COUNTRY_ALPHA == \"NIC\" | B_COUNTRY_ALPHA == \"ZWE\" | B_COUNTRY_ALPHA == \"NGA\" | B_COUNTRY_ALPHA == \"PAK\") %>% nrow()\n\n\n[1] 28644\n\n\nCode\n# rename columns.\nnames(wvs) <- c(\"B_COUNTRY_ALPHA\", \"G_TOWNSIZE\", \"H_SETTLEMENT\", \"H_URBRURAL\", \"Long\", \"Lat\", \"FamImpt\", \"FriendsImpt\", \"LeisureImpt\", \"ReligionImpt\", \"Happiness\", \"PerceivedHealth\", \"FOC\", \"LS\", \"FS\", \"Trust\", \"AttendReligious\", \"Sex\", \"Age\", \"Immigrant\", \"Citizen\", \"HHSize\", \"Parents\", \"Married\", \"Kids\", \"Edu\", \"Job\", \"SocialClass\", \"Income\", \"IncomeR\", \"Religion\", \"I_WOMJOB\", \"I_WOMPOL\", \"I_WOMEDU\", \"homolib\", \"abortlib\", \"NS\")\n\n\nThe sanity check shows that the creation of the dummy was successful, with 28,644 data points from the Global South."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-exploratory-analysis",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-1-exploratory-analysis",
    "title": "Final Project Update",
    "section": "Part 1: Exploratory Analysis",
    "text": "Part 1: Exploratory Analysis\n\n\nCode\n# check rows, columns and variable types.\nstr(wvs)\n\n\ntibble [87,822 × 37] (S3: tbl_df/tbl/data.frame)\n $ B_COUNTRY_ALPHA: chr [1:87822] \"CYP\" \"CYP\" \"CYP\" \"CYP\" ...\n $ G_TOWNSIZE     : num [1:87822] 6 6 6 6 6 6 6 6 6 6 ...\n $ H_SETTLEMENT   : num [1:87822] 4 4 4 4 4 4 4 4 4 4 ...\n $ H_URBRURAL     : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ Long           : num [1:87822] 34.8 34.8 34.8 34.8 34.8 ...\n $ Lat            : num [1:87822] 32.4 32.4 32.4 32.4 32.5 ...\n $ FamImpt        : num [1:87822] 1 1 1 1 1 1 2 1 1 1 ...\n $ FriendsImpt    : num [1:87822] 1 3 2 2 NA 1 2 2 1 2 ...\n $ LeisureImpt    : num [1:87822] 1 1 1 1 2 1 2 1 1 2 ...\n $ ReligionImpt   : num [1:87822] 1 1 1 1 1 3 2 1 3 1 ...\n $ Happiness      : num [1:87822] 2 1 2 2 3 2 2 1 2 3 ...\n $ PerceivedHealth: num [1:87822] 4 2 1 3 3 1 1 1 1 4 ...\n $ FOC            : num [1:87822] 10 5 5 5 3 7 5 5 5 NA ...\n $ LS             : num [1:87822] 8 7 9 5 5 8 4 7 8 9 ...\n $ FS             : num [1:87822] 8 5 5 5 5 7 3 5 8 4 ...\n $ Trust          : num [1:87822] 2 2 2 2 2 2 2 2 2 2 ...\n $ AttendReligious: num [1:87822] 7 2 3 2 4 4 2 4 4 2 ...\n $ Sex            : num [1:87822] 1 2 2 2 2 1 1 2 1 2 ...\n $ Age            : num [1:87822] 61 61 42 64 52 39 61 25 36 77 ...\n $ Immigrant      : num [1:87822] 1 1 2 1 2 2 1 1 1 1 ...\n $ Citizen        : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ HHSize         : num [1:87822] 2 4 6 2 8 1 2 3 2 3 ...\n $ Parents        : num [1:87822] 1 1 1 1 1 1 1 1 1 1 ...\n $ Married        : num [1:87822] 1 1 1 1 5 3 1 1 2 1 ...\n $ Kids           : num [1:87822] 2 2 4 2 3 2 2 1 2 3 ...\n $ Edu            : num [1:87822] 1 1 4 3 3 3 2 3 1 0 ...\n $ Job            : num [1:87822] 1 1 5 1 1 1 1 7 1 5 ...\n $ SocialClass    : num [1:87822] 3 4 3 3 5 5 5 5 NA 4 ...\n $ Income         : num [1:87822] 5 5 3 5 3 5 3 5 7 3 ...\n $ IncomeR        : num [1:87822] 2 2 1 2 1 2 1 2 2 1 ...\n $ Religion       : num [1:87822] 3 3 3 3 3 3 3 3 3 3 ...\n $ I_WOMJOB       : num [1:87822] 0.75 0.5 1 0.75 0.5 0.5 0.5 0.75 0.5 0.5 ...\n $ I_WOMPOL       : num [1:87822] 0.66 NA 1 0.66 NA 0.33 0.66 0.66 NA 0.33 ...\n $ I_WOMEDU       : num [1:87822] 0.66 1 1 0.66 0.33 1 0.66 0.66 0.66 0.66 ...\n $ homolib        : num [1:87822] 1 1 5 1 1 3 1 4 1 1 ...\n $ abortlib       : num [1:87822] 1 1 1 1 1 3 1 3 1 1 ...\n $ NS             : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\n# check basic descriptive statistics.\nsummary(wvs)\n\n\n B_COUNTRY_ALPHA      G_TOWNSIZE     H_SETTLEMENT     H_URBRURAL   \n Length:87822       Min.   :1.000   Min.   :1.000   Min.   :1.000  \n Class :character   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:1.000  \n Mode  :character   Median :6.000   Median :3.000   Median :1.000  \n                    Mean   :5.312   Mean   :3.066   Mean   :1.318  \n                    3rd Qu.:8.000   3rd Qu.:5.000   3rd Qu.:2.000  \n                    Max.   :8.000   Max.   :5.000   Max.   :2.000  \n                    NA's   :1274    NA's   :207     NA's   :32     \n      Long              Lat            FamImpt       FriendsImpt   \n Min.   :-156.34   Min.   :-43.26   Min.   :1.000   Min.   :1.000  \n 1st Qu.:   7.66   1st Qu.:  6.99   1st Qu.:1.000   1st Qu.:1.000  \n Median :  39.94   Median : 24.75   Median :1.000   Median :2.000  \n Mean   :  36.16   Mean   : 21.35   Mean   :1.112   Mean   :1.721  \n 3rd Qu.: 100.27   3rd Qu.: 35.70   3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   : 156.89   Max.   :100.35   Max.   :4.000   Max.   :4.000  \n NA's   :27098     NA's   :27094    NA's   :146     NA's   :289    \n  LeisureImpt     ReligionImpt     Happiness     PerceivedHealth\n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :1.788   Mean   :1.938   Mean   :1.857   Mean   :2.194  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:3.000  \n Max.   :4.000   Max.   :4.000   Max.   :4.000   Max.   :5.000  \n NA's   :473     NA's   :831     NA's   :574     NA's   :254    \n      FOC               LS               FS             Trust      \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   :1.000  \n 1st Qu.: 6.000   1st Qu.: 6.000   1st Qu.: 5.000   1st Qu.:2.000  \n Median : 7.000   Median : 7.000   Median : 6.000   Median :2.000  \n Mean   : 7.203   Mean   : 7.043   Mean   : 6.172   Mean   :1.765  \n 3rd Qu.: 9.000   3rd Qu.: 9.000   3rd Qu.: 8.000   3rd Qu.:2.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :2.000  \n NA's   :800      NA's   :393      NA's   :545      NA's   :1198   \n AttendReligious      Sex             Age           Immigrant    \n Min.   :1.000   Min.   :1.000   Min.   : 16.00   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.: 29.00   1st Qu.:1.000  \n Median :4.000   Median :2.000   Median : 41.00   Median :1.000  \n Mean   :4.139   Mean   :1.525   Mean   : 42.85   Mean   :1.059  \n 3rd Qu.:6.000   3rd Qu.:2.000   3rd Qu.: 55.00   3rd Qu.:1.000  \n Max.   :7.000   Max.   :2.000   Max.   :103.00   Max.   :2.000  \n NA's   :1034    NA's   :62      NA's   :339      NA's   :344    \n    Citizen          HHSize          Parents         Married    \n Min.   :1.000   Min.   : 1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.: 2.000   1st Qu.:1.000   1st Qu.:1.00  \n Median :1.000   Median : 4.000   Median :1.000   Median :1.00  \n Mean   :1.022   Mean   : 3.945   Mean   :1.353   Mean   :2.65  \n 3rd Qu.:1.000   3rd Qu.: 5.000   3rd Qu.:2.000   3rd Qu.:5.00  \n Max.   :2.000   Max.   :63.000   Max.   :4.000   Max.   :6.00  \n NA's   :5164    NA's   :852      NA's   :1438    NA's   :504   \n      Kids             Edu             Job        SocialClass   \n Min.   : 0.000   Min.   :0.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.: 0.000   1st Qu.:2.000   1st Qu.:1.00   1st Qu.:3.000  \n Median : 2.000   Median :3.000   Median :3.00   Median :3.000  \n Mean   : 1.766   Mean   :3.546   Mean   :3.13   Mean   :3.255  \n 3rd Qu.: 3.000   3rd Qu.:5.000   3rd Qu.:5.00   3rd Qu.:4.000  \n Max.   :24.000   Max.   :8.000   Max.   :8.00   Max.   :5.000  \n NA's   :1201     NA's   :818     NA's   :1143   NA's   :2302   \n     Income          IncomeR         Religion        I_WOMJOB     \n Min.   : 1.000   Min.   :1.000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:0.2500  \n Median : 5.000   Median :2.000   Median :3.000   Median :0.5000  \n Mean   : 4.859   Mean   :1.841   Mean   :3.005   Mean   :0.5075  \n 3rd Qu.: 6.000   3rd Qu.:2.000   3rd Qu.:5.000   3rd Qu.:0.7500  \n Max.   :10.000   Max.   :3.000   Max.   :9.000   Max.   :1.0000  \n NA's   :2330     NA's   :2330    NA's   :2485    NA's   :648     \n    I_WOMPOL         I_WOMEDU         homolib          abortlib      NS       \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.000   Min.   : 1.000   0:59178  \n 1st Qu.:0.3300   1st Qu.:0.6600   1st Qu.: 1.000   1st Qu.: 1.000   1:28644  \n Median :0.6600   Median :0.6600   Median : 2.000   Median : 2.000            \n Mean   :0.5427   Mean   :0.6649   Mean   : 3.864   Mean   : 3.407            \n 3rd Qu.:0.6600   3rd Qu.:1.0000   3rd Qu.: 6.000   3rd Qu.: 5.000            \n Max.   :1.0000   Max.   :1.0000   Max.   :10.000   Max.   :10.000            \n NA's   :2222     NA's   :1250     NA's   :5691     NA's   :1979              \n\n\nCode\nprint(dfSummary(wvs, varnumbers = FALSE, plain.ascii = FALSE, graph.magnif = 0.30, style = \"grid\", valid.col = FALSE), \n      method = 'render', table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nwvs\nDimensions: 87822 x 37\n  Duplicates: 45\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      B_COUNTRY_ALPHA\n[character]\n      1. CAN2. IDN3. CHN4. USA5. TUR6. NLD7. HKG8. BOL9. SGP10. PAK[ 49 others ]\n      4018(4.6%)3200(3.6%)3036(3.5%)2596(3.0%)2415(2.7%)2145(2.4%)2075(2.4%)2067(2.4%)2012(2.3%)1995(2.3%)62263(70.9%)\n      \n      0\n(0.0%)\n    \n    \n      G_TOWNSIZE\n[numeric]\n      Mean (sd) : 5.3 (2.4)min ≤ med ≤ max:1 ≤ 6 ≤ 8IQR (CV) : 5 (0.5)\n      1:8337(9.6%)2:8407(9.7%)3:7237(8.4%)4:6240(7.2%)5:10100(11.7%)6:8447(9.8%)7:15461(17.9%)8:22319(25.8%)\n      \n      1274\n(1.5%)\n    \n    \n      H_SETTLEMENT\n[numeric]\n      Mean (sd) : 3.1 (1.5)min ≤ med ≤ max:1 ≤ 3 ≤ 5IQR (CV) : 3 (0.5)\n      1:18302(20.9%)2:17840(20.4%)3:14116(16.1%)4:14500(16.5%)5:22857(26.1%)\n      \n      207\n(0.2%)\n    \n    \n      H_URBRURAL\n[numeric]\n      Min  : 1Mean : 1.3Max  : 2\n      1:59862(68.2%)2:27928(31.8%)\n      \n      32\n(0.0%)\n    \n    \n      Long\n[numeric]\n      Mean (sd) : 36.2 (68.1)min ≤ med ≤ max:-156.3 ≤ 39.9 ≤ 156.9IQR (CV) : 92.6 (1.9)\n      5482 distinct values\n      \n      27098\n(30.9%)\n    \n    \n      Lat\n[numeric]\n      Mean (sd) : 21.4 (20)min ≤ med ≤ max:-43.3 ≤ 24.8 ≤ 100.3IQR (CV) : 28.7 (0.9)\n      3911 distinct values\n      \n      27094\n(30.9%)\n    \n    \n      FamImpt\n[numeric]\n      Mean (sd) : 1.1 (0.4)min ≤ med ≤ max:1 ≤ 1 ≤ 4IQR (CV) : 0 (0.3)\n      1:78979(90.1%)2:7722(8.8%)3:793(0.9%)4:182(0.2%)\n      \n      146\n(0.2%)\n    \n    \n      FriendsImpt\n[numeric]\n      Mean (sd) : 1.7 (0.7)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:38026(43.4%)2:37657(43.0%)3:10105(11.5%)4:1745(2.0%)\n      \n      289\n(0.3%)\n    \n    \n      LeisureImpt\n[numeric]\n      Mean (sd) : 1.8 (0.8)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:35509(40.7%)2:37328(42.7%)3:12046(13.8%)4:2466(2.8%)\n      \n      473\n(0.5%)\n    \n    \n      ReligionImpt\n[numeric]\n      Mean (sd) : 1.9 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.6)\n      1:42455(48.8%)2:18300(21.0%)3:15396(17.7%)4:10840(12.5%)\n      \n      831\n(0.9%)\n    \n    \n      Happiness\n[numeric]\n      Mean (sd) : 1.9 (0.7)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 1 (0.4)\n      1:27071(31.0%)2:47564(54.5%)3:10659(12.2%)4:1954(2.2%)\n      \n      574\n(0.7%)\n    \n    \n      PerceivedHealth\n[numeric]\n      Mean (sd) : 2.2 (0.9)min ≤ med ≤ max:1 ≤ 2 ≤ 5IQR (CV) : 1 (0.4)\n      1:19021(21.7%)2:38932(44.5%)3:24210(27.6%)4:4434(5.1%)5:971(1.1%)\n      \n      254\n(0.3%)\n    \n    \n      FOC\n[numeric]\n      Mean (sd) : 7.2 (2.3)min ≤ med ≤ max:1 ≤ 7 ≤ 10IQR (CV) : 3 (0.3)\n      1:2162(2.5%)2:1243(1.4%)3:2445(2.8%)4:3500(4.0%)5:10992(12.6%)6:9521(10.9%)7:13833(15.9%)8:16714(19.2%)9:7937(9.1%)10:18675(21.5%)\n      \n      800\n(0.9%)\n    \n    \n      LS\n[numeric]\n      Mean (sd) : 7 (2.3)min ≤ med ≤ max:1 ≤ 7 ≤ 10IQR (CV) : 3 (0.3)\n      1:2484(2.8%)2:1284(1.5%)3:2875(3.3%)4:4064(4.6%)5:10620(12.1%)6:10040(11.5%)7:14730(16.8%)8:17517(20.0%)9:8939(10.2%)10:14876(17.0%)\n      \n      393\n(0.4%)\n    \n    \n      FS\n[numeric]\n      Mean (sd) : 6.2 (2.4)min ≤ med ≤ max:1 ≤ 6 ≤ 10IQR (CV) : 3 (0.4)\n      1:4989(5.7%)2:2834(3.2%)3:5059(5.8%)4:6455(7.4%)5:14206(16.3%)6:11865(13.6%)7:13932(16.0%)8:13170(15.1%)9:5783(6.6%)10:8984(10.3%)\n      \n      545\n(0.6%)\n    \n    \n      Trust\n[numeric]\n      Min  : 1Mean : 1.8Max  : 2\n      1:20326(23.5%)2:66298(76.5%)\n      \n      1198\n(1.4%)\n    \n    \n      AttendReligious\n[numeric]\n      Mean (sd) : 4.1 (2.2)min ≤ med ≤ max:1 ≤ 4 ≤ 7IQR (CV) : 4 (0.5)\n      1:11941(13.8%)2:16094(18.5%)3:8890(10.2%)4:13579(15.6%)5:4512(5.2%)6:10835(12.5%)7:20937(24.1%)\n      \n      1034\n(1.2%)\n    \n    \n      Sex\n[numeric]\n      Min  : 1Mean : 1.5Max  : 2\n      1:41654(47.5%)2:46106(52.5%)\n      \n      62\n(0.1%)\n    \n    \n      Age\n[numeric]\n      Mean (sd) : 42.9 (16.4)min ≤ med ≤ max:16 ≤ 41 ≤ 103IQR (CV) : 26 (0.4)\n      85 distinct values\n      \n      339\n(0.4%)\n    \n    \n      Immigrant\n[numeric]\n      Min  : 1Mean : 1.1Max  : 2\n      1:82299(94.1%)2:5179(5.9%)\n      \n      344\n(0.4%)\n    \n    \n      Citizen\n[numeric]\n      Min  : 1Mean : 1Max  : 2\n      1:80826(97.8%)2:1832(2.2%)\n      \n      5164\n(5.9%)\n    \n    \n      HHSize\n[numeric]\n      Mean (sd) : 3.9 (2.2)min ≤ med ≤ max:1 ≤ 4 ≤ 63IQR (CV) : 3 (0.6)\n      33 distinct values\n      \n      852\n(1.0%)\n    \n    \n      Parents\n[numeric]\n      Mean (sd) : 1.4 (0.6)min ≤ med ≤ max:1 ≤ 1 ≤ 4IQR (CV) : 1 (0.4)\n      1:61004(70.6%)2:20796(24.1%)3:4048(4.7%)4:536(0.6%)\n      \n      1438\n(1.6%)\n    \n    \n      Married\n[numeric]\n      Mean (sd) : 2.6 (2.1)min ≤ med ≤ max:1 ≤ 1 ≤ 6IQR (CV) : 4 (0.8)\n      1:49193(56.3%)2:6782(7.8%)3:3614(4.1%)4:1909(2.2%)5:4770(5.5%)6:21050(24.1%)\n      \n      504\n(0.6%)\n    \n    \n      Kids\n[numeric]\n      Mean (sd) : 1.8 (1.7)min ≤ med ≤ max:0 ≤ 2 ≤ 24IQR (CV) : 3 (1)\n      23 distinct values\n      \n      1201\n(1.4%)\n    \n    \n      Edu\n[numeric]\n      Mean (sd) : 3.5 (2)min ≤ med ≤ max:0 ≤ 3 ≤ 8IQR (CV) : 3 (0.6)\n      0:4690(5.4%)1:10721(12.3%)2:12179(14.0%)3:22178(25.5%)4:8235(9.5%)5:7446(8.6%)6:15158(17.4%)7:5402(6.2%)8:995(1.1%)\n      \n      818\n(0.9%)\n    \n    \n      Job\n[numeric]\n      Mean (sd) : 3.1 (2.1)min ≤ med ≤ max:1 ≤ 3 ≤ 8IQR (CV) : 4 (0.7)\n      1:31351(36.2%)2:7467(8.6%)3:12868(14.8%)4:10126(11.7%)5:12073(13.9%)6:5034(5.8%)7:6783(7.8%)8:977(1.1%)\n      \n      1143\n(1.3%)\n    \n    \n      SocialClass\n[numeric]\n      Mean (sd) : 3.3 (1)min ≤ med ≤ max:1 ≤ 3 ≤ 5IQR (CV) : 1 (0.3)\n      1:1453(1.7%)2:18048(21.1%)3:33302(38.9%)4:22649(26.5%)5:10068(11.8%)\n      \n      2302\n(2.6%)\n    \n    \n      Income\n[numeric]\n      Mean (sd) : 4.9 (2.1)min ≤ med ≤ max:1 ≤ 5 ≤ 10IQR (CV) : 3 (0.4)\n      1:6904(8.1%)2:5163(6.0%)3:9612(11.2%)4:11839(13.8%)5:20733(24.3%)6:13216(15.5%)7:9950(11.6%)8:4966(5.8%)9:1417(1.7%)10:1692(2.0%)\n      \n      2330\n(2.7%)\n    \n    \n      IncomeR\n[numeric]\n      Mean (sd) : 1.8 (0.6)min ≤ med ≤ max:1 ≤ 2 ≤ 3IQR (CV) : 1 (0.3)\n      1:21679(25.4%)2:55738(65.2%)3:8075(9.4%)\n      \n      2330\n(2.7%)\n    \n    \n      Religion\n[numeric]\n      Mean (sd) : 3 (2.6)min ≤ med ≤ max:0 ≤ 3 ≤ 9IQR (CV) : 4 (0.9)\n      0:19919(23.3%)1:16027(18.8%)2:6508(7.6%)3:7762(9.1%)4:235(0.3%)5:23807(27.9%)6:569(0.7%)7:5556(6.5%)8:2777(3.3%)9:2177(2.6%)\n      \n      2485\n(2.8%)\n    \n    \n      I_WOMJOB\n[numeric]\n      Mean (sd) : 0.5 (0.3)min ≤ med ≤ max:0 ≤ 0.5 ≤ 1IQR (CV) : 0.5 (0.7)\n      5 distinct values\n      \n      648\n(0.7%)\n    \n    \n      I_WOMPOL\n[numeric]\n      Mean (sd) : 0.5 (0.3)min ≤ med ≤ max:0 ≤ 0.7 ≤ 1IQR (CV) : 0.3 (0.6)\n      4 distinct values\n      \n      2222\n(2.5%)\n    \n    \n      I_WOMEDU\n[numeric]\n      Mean (sd) : 0.7 (0.3)min ≤ med ≤ max:0 ≤ 0.7 ≤ 1IQR (CV) : 0.3 (0.4)\n      4 distinct values\n      \n      1250\n(1.4%)\n    \n    \n      homolib\n[numeric]\n      Mean (sd) : 3.9 (3.3)min ≤ med ≤ max:1 ≤ 2 ≤ 10IQR (CV) : 5 (0.9)\n      1:37570(45.7%)2:4967(6.0%)3:4037(4.9%)4:3211(3.9%)5:9115(11.1%)6:3866(4.7%)7:2893(3.5%)8:3366(4.1%)9:2090(2.5%)10:11016(13.4%)\n      \n      5691\n(6.5%)\n    \n    \n      abortlib\n[numeric]\n      Mean (sd) : 3.4 (2.9)min ≤ med ≤ max:1 ≤ 2 ≤ 10IQR (CV) : 4 (0.9)\n      1:39931(46.5%)2:6502(7.6%)3:5802(6.8%)4:4207(4.9%)5:10290(12.0%)6:4357(5.1%)7:3450(4.0%)8:3748(4.4%)9:2019(2.4%)10:5537(6.5%)\n      \n      1979\n(2.3%)\n    \n    \n      NS\n[factor]\n      1. 02. 1\n      59178(67.4%)28644(32.6%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-11-11\n\n\n\nThe dataset has 87,822 rows, each representing one participant, and 37 columns. Referring to the codebook, these are some noteworthy descriptive statistics:\n\nRespondents tended to come from more urban settings (H_URBRURAL).\nOn average, family was more likely to be perceived as important than friends, leisure time and religion (FamImpt, FriendsImpt, LeisureImpt, ReligionImpt).\nOn average, people were “quite happy” (the second-highest option for Happiness).\nLife satisfaction tended to be 7/10 (LS).\nPeople tended to err on the side of caution when it came to trusting others (Trust).\nHouseholds had 4 people on average, with maximum household size being 63 (HHSize)!\nThe interquartile range for education was lower secondary to short-cycle tertiary education (Edu).\nFor the survey variables (FamImpt to abortlib), missing data ranged from 0.2% to 6.5%, which is acceptable.\n67.4% of respondents were from the Global North (NS).\n\nLet’s check if life satisfaction and happiness differ between the Global North and South.\n\n\nCode\nt.test(Happiness ~ NS, wvs)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Happiness by NS\nt = 4.1272, df = 49878, p-value = 3.677e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.01164249 0.03270103\nsample estimates:\nmean in group 0 mean in group 1 \n       1.863945        1.841774 \n\n\nCode\nt.test(LS ~ NS, wvs)\n\n\n\n    Welch Two Sample t-test\n\ndata:  LS by NS\nt = 13.283, df = 47990, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.1962795 0.2642322\nsample estimates:\nmean in group 0 mean in group 1 \n       7.117885        6.887629 \n\n\nThe Welch’s two-sample t-tests show that there is a significant difference in happiness and life satisfaction between the Global North and South, where the former has higher mean values for both, p < .001. This echoes Alba (2019)’s finding on happiness, and adds new knowledge to the literature regarding life satisfaction.\nWe can also create graphs to visualize the latitude and longitude of countries in the Global North and Global South.\n\n\nCode\nggplot(wvs) + geom_bin2d(mapping = aes(x = Long, y = Lat)) + facet_wrap(vars(NS))\n\n\nWarning: Removed 27098 rows containing non-finite values (stat_bin2d).\n\n\n\n\n\nThe graph above shows that the Global North (“0”) and South (“1”) are not neatly divided by physical location, due to the existence of developed countries physically located in the South (e.g., South Korea) and developing countries physically located in the North (e.g., Ukraine)."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-intro",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-intro",
    "title": "Final Project Update",
    "section": "Part 2: Intro",
    "text": "Part 2: Intro\nIn the previous post, I discussed the prior literature on the topic (predictors of happiness and life satisfaction in Global South vs. North), my RQs and hypotheses, and explored the dataset (WVS Wave 7). I was given feedback to explain my hypotheses better and have edited that section accordingly.\nI have added one variable, SocialClass, which was a significant predictor in past papers that I missed out previously. I have also removed several variables I feel are not useful for my RQs (e.g., number of women in parliament).\nThe response variables are Happiness and LS (life satisfaction). They will be measured separately, as done in prior papers.\nThe main explanatory variables are PerceivedHealth, FS (financial satisfaction) and NS (country type: North vs. South). A potential interaction between the explanatory variables will be included.\nSome notes before commencing analysis:\n\nIt is important to note that NS cannot be transformed, since it is categorical. However, if required, log/quadratic transformations can be done for PerceivedHealth or FS.\nSince each row represents one participant, the unit of analysis is at the participant level. The NS dummy refers to where the participant comes from, either the Global North or the Global South.\nI will be treating FamImpt, FriendsImpt, LeisureImpt, ReligionImpt and Happiness as continuous. I will attempt to verify this for RQ B by running two regressions with FamImpt as categorical vs. continuous.\nI went through the variables again and realized I have to change many of them to factors, and reverse code some of them. I will do this in the code chunk below before generating the models.\n\n\n\nCode\n# change the following variables to factor type.\nwvs <- wvs %>% mutate(across(c(B_COUNTRY_ALPHA,H_SETTLEMENT,H_URBRURAL,Trust,Sex,Immigrant,Citizen,Parents,Married,Job,Religion), as.factor))\n\n# reverse code the following variables, such that the largest number reflects agreement.\nwvs$FamImpt <- 5-wvs$FamImpt\nwvs$FriendsImpt <- 5-wvs$FriendsImpt\nwvs$LeisureImpt <- 5-wvs$LeisureImpt\nwvs$ReligionImpt <- 5-wvs$ReligionImpt\nwvs$Happiness <- 5-wvs$Happiness\nwvs$PerceivedHealth <- 6-wvs$PerceivedHealth\nwvs$AttendReligious <-8-wvs$AttendReligious\nwvs$SocialClass <- 6-wvs$SocialClass"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-a",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-a",
    "title": "Final Project Update",
    "section": "Part 2: Regression Models (RQ A)",
    "text": "Part 2: Regression Models (RQ A)\n\n\n\n\n\n\nRecap: H1A\n\n\n\nHealth and financial satisfaction will positively predict happiness and life satisfaction in the Global South.\n\n\n\nTo test this hypothesis, I will:\n\nFilter the dataset to only include observations from the Global South.\nCreate plots of health and financial satisfaction against happiness and life satisfaction.\nRun a regression model with these variables. Previous papers did not test an interaction between the variables, so I will not do so. There also does not seem to be a meaningful reason to do so.\n\n\n\n\nCode\n# create subset of dataset.\nsubset <- wvs %>% filter(NS == \"1\")\n\n# i first made bar plots with facet wrapping, but this was not ideal. i also tried a jitter plot (code below), but it also didn't work. i'll use boxplots in the end.\n# ggplot(subset, aes(x = FS, y = LS)) + geom_jitter(stat = \"identity\", width = 0.2, height = 0.1, na.rm = T)\n\n# generate boxplots.\nboxplot(LS ~ PerceivedHealth, subset)\n\n\n\n\n\nCode\nboxplot(LS ~ FS, subset)\n\n\n\n\n\nCode\nboxplot(Happiness ~ PerceivedHealth, subset)\n\n\n\n\n\nCode\nboxplot(Happiness ~ FS, subset)\n\n\n\n\n\nLooking at the boxplots, there seems to be a roughly linear positive relationship between all 4 variables. No transformations should be required to run the regression models.\n\n\nCode\n# run regression model for Happiness.\nModelA_H <- lm(Happiness ~ PerceivedHealth + FS, subset)\nsummary(ModelA_H)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth + FS, data = subset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.78824 -0.37097  0.01499  0.47464  1.95803 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     1.701889   0.018399   92.50   <2e-16 ***\nPerceivedHealth 0.262885   0.004639   56.67   <2e-16 ***\nFS              0.077193   0.001582   48.78   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6944 on 28499 degrees of freedom\n  (142 observations deleted due to missingness)\nMultiple R-squared:  0.2025,    Adjusted R-squared:  0.2024 \nF-statistic:  3618 on 2 and 28499 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for Happiness with demographic controls.\nModelA_H_controls <- lm(Happiness ~ PerceivedHealth + FS + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion, subset)\nsummary(ModelA_H_controls)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth + FS + Sex + Age + Immigrant + \n    Citizen + HHSize + Parents + Married + Kids + Edu + Job + \n    SocialClass + Income + Religion, data = subset)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.85651 -0.38630  0.02411  0.50703  2.13673 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.629e+00  3.488e-02  46.707  < 2e-16 ***\nPerceivedHealth  2.608e-01  4.956e-03  52.615  < 2e-16 ***\nFS               7.508e-02  1.700e-03  44.172  < 2e-16 ***\nSex2             8.090e-02  9.718e-03   8.324  < 2e-16 ***\nAge             -4.815e-05  4.261e-04  -0.113 0.910044    \nImmigrant2       5.271e-02  5.848e-02   0.901 0.367467    \nCitizen2        -8.387e-02  8.628e-02  -0.972 0.331025    \nHHSize           6.807e-03  2.053e-03   3.316 0.000914 ***\nParents2         6.084e-03  1.274e-02   0.478 0.632937    \nParents3         2.492e-02  1.878e-02   1.327 0.184554    \nParents4        -7.208e-02  5.673e-02  -1.270 0.203939    \nMarried2        -2.291e-02  1.926e-02  -1.190 0.234094    \nMarried3        -1.878e-01  2.980e-02  -6.302 2.99e-10 ***\nMarried4        -1.990e-01  3.518e-02  -5.658 1.55e-08 ***\nMarried5        -1.065e-01  2.120e-02  -5.024 5.09e-07 ***\nMarried6        -7.052e-02  1.496e-02  -4.715 2.43e-06 ***\nKids             4.017e-03  3.149e-03   1.276 0.202050    \nEdu             -1.390e-02  2.447e-03  -5.679 1.37e-08 ***\nJob2            -3.250e-03  1.648e-02  -0.197 0.843711    \nJob3            -1.926e-02  1.238e-02  -1.555 0.119967    \nJob4             2.946e-02  2.234e-02   1.318 0.187377    \nJob5            -8.471e-02  1.505e-02  -5.630 1.82e-08 ***\nJob6            -2.189e-02  2.068e-02  -1.058 0.289894    \nJob7            -5.892e-02  1.607e-02  -3.667 0.000246 ***\nJob8            -3.689e-02  4.336e-02  -0.851 0.394827    \nSocialClass      3.316e-02  4.753e-03   6.977 3.10e-12 ***\nIncome          -1.856e-04  2.287e-03  -0.081 0.935317    \nReligion1        6.950e-02  1.742e-02   3.991 6.61e-05 ***\nReligion2        7.609e-02  2.102e-02   3.620 0.000295 ***\nReligion3       -1.818e-02  2.265e-02  -0.803 0.422249    \nReligion4       -1.593e-01  8.440e-02  -1.887 0.059135 .  \nReligion5        1.240e-02  1.533e-02   0.809 0.418618    \nReligion6       -3.304e-02  4.661e-02  -0.709 0.478414    \nReligion7       -1.247e-02  2.091e-02  -0.597 0.550772    \nReligion8        1.105e-01  2.569e-02   4.302 1.70e-05 ***\nReligion9        3.676e-02  4.022e-02   0.914 0.360654    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.69 on 26907 degrees of freedom\n  (1701 observations deleted due to missingness)\nMultiple R-squared:  0.2118,    Adjusted R-squared:  0.2107 \nF-statistic: 206.5 on 35 and 26907 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for LS. for whatever reason, logging PerceivedHealth produced a slightly higher adjusted R^2, but i did not include that because (1) it did not improve the diagnostic plots and (2) PerceivedHealth is not count data (e.g., population/income) that traditionally improves with logging.\nModelA_LS <- lm(LS ~ PerceivedHealth + FS, subset)\nsummary(ModelA_LS)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth + FS, data = subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3728 -1.1136  0.0639  1.0965  6.6294 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.515388   0.054654   46.02   <2e-16 ***\nPerceivedHealth 0.338881   0.013790   24.57   <2e-16 ***\nFS              0.516304   0.004706  109.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.064 on 28509 degrees of freedom\n  (132 observations deleted due to missingness)\nMultiple R-squared:  0.3396,    Adjusted R-squared:  0.3395 \nF-statistic:  7329 on 2 and 28509 DF,  p-value: < 2.2e-16\n\n\nCode\n# run regression model for LS with demographic controls.\nModelA_LS_controls <- lm(LS ~ PerceivedHealth + FS + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion, subset)\nsummary(ModelA_LS_controls)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth + FS + Sex + Age + Immigrant + \n    Citizen + HHSize + Parents + Married + Kids + Edu + Job + \n    SocialClass + Income + Religion, data = subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0351 -1.1901  0.0939  1.1606  7.0551 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      2.252840   0.103753  21.714  < 2e-16 ***\nPerceivedHealth  0.374906   0.014743  25.429  < 2e-16 ***\nFS               0.501032   0.005058  99.058  < 2e-16 ***\nSex2             0.111760   0.028898   3.867  0.00011 ***\nAge              0.005052   0.001267   3.987 6.71e-05 ***\nImmigrant2       0.024794   0.172774   0.144  0.88589    \nCitizen2         0.012178   0.256561   0.047  0.96214    \nHHSize          -0.003993   0.006105  -0.654  0.51306    \nParents2         0.137374   0.037897   3.625  0.00029 ***\nParents3         0.127139   0.055866   2.276  0.02287 *  \nParents4        -0.175944   0.169331  -1.039  0.29879    \nMarried2         0.038214   0.057252   0.667  0.50448    \nMarried3        -0.281920   0.088449  -3.187  0.00144 ** \nMarried4        -0.363767   0.104647  -3.476  0.00051 ***\nMarried5        -0.136939   0.062998  -2.174  0.02974 *  \nMarried6        -0.234748   0.044475  -5.278 1.31e-07 ***\nKids             0.003537   0.009349   0.378  0.70518    \nEdu             -0.010311   0.007279  -1.417  0.15663    \nJob2             0.125980   0.049015   2.570  0.01017 *  \nJob3            -0.061988   0.036841  -1.683  0.09247 .  \nJob4            -0.043148   0.066305  -0.651  0.51521    \nJob5            -0.108559   0.044760  -2.425  0.01530 *  \nJob6            -0.098398   0.061520  -1.599  0.10973    \nJob7            -0.220107   0.047815  -4.603 4.18e-06 ***\nJob8             0.376925   0.128739   2.928  0.00342 ** \nSocialClass      0.003458   0.014134   0.245  0.80673    \nIncome           0.019709   0.006803   2.897  0.00377 ** \nReligion1        0.153145   0.051783   2.957  0.00310 ** \nReligion2       -0.323565   0.062492  -5.178 2.26e-07 ***\nReligion3       -0.405155   0.067274  -6.022 1.74e-09 ***\nReligion4       -0.218467   0.251068  -0.870  0.38423    \nReligion5       -0.039050   0.045606  -0.856  0.39187    \nReligion6        0.236925   0.138646   1.709  0.08749 .  \nReligion7        0.021554   0.062184   0.347  0.72888    \nReligion8        0.397010   0.076415   5.195 2.06e-07 ***\nReligion9        0.440675   0.119625   3.684  0.00023 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.053 on 26916 degrees of freedom\n  (1692 observations deleted due to missingness)\nMultiple R-squared:  0.3474,    Adjusted R-squared:  0.3466 \nF-statistic: 409.4 on 35 and 26916 DF,  p-value: < 2.2e-16\n\n\nTo summarise, I ran 4 regression models above - 2 each for Happiness and LS (1 with just the main predictors, and 1 with demographic controls). What’s important to note is that in all the models, even with the addition of demographic control variables, PerceivedHealth and FS positively predict Happiness and LS in the Global South, p < .001. The addition of demographic controls also did not improve adjusted R2 by much - just ~0.01. For RQ A, we can reject the null hypothesis.\n\n\nCode\n# diagnostic plots for the models above.\npar(mfrow = c(2,3)); plot(ModelA_H_controls, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelA_H_controls)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelA_H_controls\nBP = 1111.1, df = 35, p-value < 2.2e-16\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelA_LS_controls, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelA_LS_controls)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelA_LS_controls\nBP = 1753, df = 35, p-value < 2.2e-16\n\n\nThe diagnostic plots generally seem fine except the scale-location plot, which indicates heteroskedasticity. The Breusch-Pagan test helps to confirm this. One way to correct this would be to use robust standard errors.\n\n\nCode\n# obtain robust standard errors for models.\ncoeftest(ModelA_H_controls, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                   Estimate  Std. Error t value  Pr(>|t|)    \n(Intercept)      1.6290e+00  3.6811e-02 44.2526 < 2.2e-16 ***\nPerceivedHealth  2.6076e-01  5.5823e-03 46.7122 < 2.2e-16 ***\nFS               7.5078e-02  1.8435e-03 40.7247 < 2.2e-16 ***\nSex2             8.0897e-02  9.7539e-03  8.2938 < 2.2e-16 ***\nAge             -4.8146e-05  4.3017e-04 -0.1119 0.9108859    \nImmigrant2       5.2707e-02  5.7875e-02  0.9107 0.3624530    \nCitizen2        -8.3868e-02  8.0561e-02 -1.0411 0.2978586    \nHHSize           6.8075e-03  2.3891e-03  2.8494 0.0043833 ** \nParents2         6.0841e-03  1.3093e-02  0.4647 0.6421611    \nParents3         2.4917e-02  1.8159e-02  1.3722 0.1700264    \nParents4        -7.2076e-02  6.2383e-02 -1.1554 0.2479456    \nMarried2        -2.2913e-02  2.0262e-02 -1.1308 0.2581384    \nMarried3        -1.8780e-01  3.2263e-02 -5.8210 5.918e-09 ***\nMarried4        -1.9903e-01  4.1600e-02 -4.7843 1.725e-06 ***\nMarried5        -1.0652e-01  2.2111e-02 -4.8175 1.462e-06 ***\nMarried6        -7.0524e-02  1.5482e-02 -4.5552 5.256e-06 ***\nKids             4.0168e-03  3.3643e-03  1.1939 0.2325112    \nEdu             -1.3898e-02  2.4639e-03 -5.6406 1.711e-08 ***\nJob2            -3.2496e-03  1.5661e-02 -0.2075 0.8356224    \nJob3            -1.9258e-02  1.2231e-02 -1.5744 0.1153957    \nJob4             2.9459e-02  2.2008e-02  1.3386 0.1807214    \nJob5            -8.4713e-02  1.4778e-02 -5.7324 1.001e-08 ***\nJob6            -2.1886e-02  2.0707e-02 -1.0569 0.2905639    \nJob7            -5.8923e-02  1.7249e-02 -3.4160 0.0006365 ***\nJob8            -3.6892e-02  5.0343e-02 -0.7328 0.4636812    \nSocialClass      3.3162e-02  4.9316e-03  6.7244 1.799e-11 ***\nIncome          -1.8561e-04  2.3702e-03 -0.0783 0.9375819    \nReligion1        6.9505e-02  1.8209e-02  3.8170 0.0001354 ***\nReligion2        7.6088e-02  2.2839e-02  3.3315 0.0008650 ***\nReligion3       -1.8177e-02  2.1665e-02 -0.8390 0.4014735    \nReligion4       -1.5928e-01  8.7627e-02 -1.8177 0.0691182 .  \nReligion5        1.2403e-02  1.5230e-02  0.8144 0.4154368    \nReligion6       -3.3038e-02  3.8184e-02 -0.8652 0.3869116    \nReligion7       -1.2473e-02  2.0610e-02 -0.6052 0.5450570    \nReligion8        1.1052e-01  2.5547e-02  4.3261 1.523e-05 ***\nReligion9        3.6764e-02  4.3284e-02  0.8494 0.3956905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(ModelA_LS_controls, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                  Estimate Std. Error t value  Pr(>|t|)    \n(Intercept)      2.2528402  0.1064129 21.1707 < 2.2e-16 ***\nPerceivedHealth  0.3749064  0.0162830 23.0244 < 2.2e-16 ***\nFS               0.5010321  0.0062128 80.6452 < 2.2e-16 ***\nSex2             0.1117604  0.0289195  3.8645 0.0001116 ***\nAge              0.0050525  0.0013206  3.8258 0.0001306 ***\nImmigrant2       0.0247937  0.1779629  0.1393 0.8891987    \nCitizen2         0.0121777  0.2740275  0.0444 0.9645543    \nHHSize          -0.0039932  0.0065037 -0.6140 0.5392264    \nParents2         0.1373743  0.0386397  3.5553 0.0003782 ***\nParents3         0.1271390  0.0562443  2.2605 0.0237995 *  \nParents4        -0.1759444  0.1602356 -1.0980 0.2721988    \nMarried2         0.0382137  0.0594300  0.6430 0.5202275    \nMarried3        -0.2819201  0.0976985 -2.8856 0.0039096 ** \nMarried4        -0.3637666  0.1143297 -3.1817 0.0014656 ** \nMarried5        -0.1369391  0.0649089 -2.1097 0.0348923 *  \nMarried6        -0.2347479  0.0454759 -5.1620 2.460e-07 ***\nKids             0.0035369  0.0101206  0.3495 0.7267341    \nEdu             -0.0103113  0.0071848 -1.4352 0.1512529    \nJob2             0.1259797  0.0478372  2.6335 0.0084555 ** \nJob3            -0.0619882  0.0358968 -1.7268 0.0842075 .  \nJob4            -0.0431482  0.0646541 -0.6674 0.5045419    \nJob5            -0.1085591  0.0442199 -2.4550 0.0140953 *  \nJob6            -0.0983980  0.0588659 -1.6716 0.0946227 .  \nJob7            -0.2201066  0.0511284 -4.3050 1.676e-05 ***\nJob8             0.3769250  0.1471414  2.5617 0.0104230 *  \nSocialClass      0.0034580  0.0152135  0.2273 0.8201950    \nIncome           0.0197092  0.0074973  2.6289 0.0085720 ** \nReligion1        0.1531451  0.0503513  3.0415 0.0023560 ** \nReligion2       -0.3235648  0.0648191 -4.9918 6.019e-07 ***\nReligion3       -0.4051554  0.0654854 -6.1870 6.222e-10 ***\nReligion4       -0.2184666  0.2857574 -0.7645 0.4445656    \nReligion5       -0.0390497  0.0411714 -0.9485 0.3429004    \nReligion6        0.2369245  0.1113743  2.1273 0.0334058 *  \nReligion7        0.0215541  0.0577008  0.3735 0.7087426    \nReligion8        0.3970097  0.0738658  5.3747 7.733e-08 ***\nReligion9        0.4406754  0.1037644  4.2469 2.175e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI generated robust standard errors for the models above. Although the t-values have reduced, the predictors are still positive and significant. Hence, we can still reject the null hypothesis."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-b",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#part-2-regression-models-rq-b",
    "title": "Final Project Update",
    "section": "Part 2: Regression Models (RQ B)",
    "text": "Part 2: Regression Models (RQ B)\n\n\n\n\n\n\nRecap: H1B\n\n\n\nHealth and financial satisfaction will have a greater impact on happiness and life satisfaction on the Global South than the Global North.\n\n\nI will first run a correlation matrix with all potential numeric variables that might be relevant.\n\n\nCode\n# run correlations for numeric variables (except DVs, which are Happiness and LS).\nmatrix <- wvs %>% select(PerceivedHealth, FS, G_TOWNSIZE, FamImpt, FriendsImpt, LeisureImpt, ReligionImpt, FOC, AttendReligious, Age, HHSize, Kids, Edu, SocialClass, IncomeR, I_WOMJOB, I_WOMPOL, I_WOMEDU, homolib, abortlib)\ncor <- cor(matrix, use=\"complete.obs\")\n\n\nIn the correlation matrix, I am concerned about correlations where r ≥ 0.5 (more conservative than a cut-off of r ≥ 0.7). This is observed between AttendReligious and ReligionImpt; and homolib and abortlib. I will try a few different models to figure out which combination of variables might work best.\n\n\nCode\n# try model with \"AttendReligious\" and \"abortlib\".\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + FOC + Trust + AttendReligious + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + FOC + \n    Trust + AttendReligious + Sex + Age + Immigrant + Citizen + \n    HHSize + Parents + Married + Kids + Edu + Job + SocialClass + \n    Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, \n    data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84562 -0.35131 -0.02747  0.46085  2.53907 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1.0023646  0.0485735  20.636  < 2e-16 ***\nPerceivedHealth         0.2417281  0.0091906  26.302  < 2e-16 ***\nFS                      0.0752496  0.0055272  13.614  < 2e-16 ***\nNS1                    -0.3760471  0.0494237  -7.609 2.81e-14 ***\nG_TOWNSIZE             -0.0109366  0.0013678  -7.996 1.31e-15 ***\nH_URBRURAL2             0.0398046  0.0071450   5.571 2.54e-08 ***\nFamImpt                 0.1207622  0.0070036  17.243  < 2e-16 ***\nFriendsImpt             0.0249529  0.0034003   7.339 2.18e-13 ***\nLeisureImpt             0.0239952  0.0032088   7.478 7.64e-14 ***\nFOC                     0.0324218  0.0011304  28.682  < 2e-16 ***\nTrust2                 -0.0242134  0.0061949  -3.909 9.29e-05 ***\nAttendReligious         0.0077179  0.0013120   5.882 4.06e-09 ***\nSex2                    0.0448632  0.0053685   8.357  < 2e-16 ***\nAge                    -0.0003398  0.0002341  -1.452 0.146603    \nImmigrant2              0.0048528  0.0121882   0.398 0.690517    \nCitizen2                0.0346332  0.0198507   1.745 0.081045 .  \nHHSize                  0.0049448  0.0013326   3.711 0.000207 ***\nParents2               -0.0080024  0.0073853  -1.084 0.278566    \nParents3                0.0269953  0.0116900   2.309 0.020932 *  \nParents4               -0.0226720  0.0315412  -0.719 0.472263    \nMarried2                0.0392866  0.0093657   4.195 2.74e-05 ***\nMarried3               -0.1071227  0.0130624  -8.201 2.43e-16 ***\nMarried4               -0.0757703  0.0165779  -4.571 4.87e-06 ***\nMarried5               -0.0653700  0.0119429  -5.474 4.43e-08 ***\nMarried6               -0.0475473  0.0081482  -5.835 5.39e-09 ***\nKids                    0.0104236  0.0019030   5.477 4.33e-08 ***\nEdu                    -0.0049675  0.0014208  -3.496 0.000472 ***\nJob2                    0.0321825  0.0093302   3.449 0.000562 ***\nJob3                    0.0065385  0.0076058   0.860 0.389974    \nJob4                    0.0400777  0.0098986   4.049 5.15e-05 ***\nJob5                    0.0308835  0.0085066   3.631 0.000283 ***\nJob6                    0.0239344  0.0116517   2.054 0.039965 *  \nJob7                   -0.0086893  0.0095815  -0.907 0.364470    \nJob8                    0.0333899  0.0243857   1.369 0.170928    \nSocialClass             0.0325389  0.0029396  11.069  < 2e-16 ***\nIncome                  0.0007032  0.0014035   0.501 0.616341    \nReligion1               0.0519833  0.0083318   6.239 4.43e-10 ***\nReligion2               0.0046457  0.0107478   0.432 0.665564    \nReligion3              -0.0727053  0.0104960  -6.927 4.34e-12 ***\nReligion4              -0.1164567  0.0458200  -2.542 0.011036 *  \nReligion5              -0.0820015  0.0084759  -9.675  < 2e-16 ***\nReligion6              -0.0568803  0.0292758  -1.943 0.052031 .  \nReligion7              -0.0326901  0.0113023  -2.892 0.003825 ** \nReligion8               0.0617715  0.0152881   4.040 5.34e-05 ***\nReligion9               0.0269946  0.0157294   1.716 0.086132 .  \nI_WOMJOB                0.0183741  0.0090170   2.038 0.041582 *  \nI_WOMPOL               -0.0080608  0.0094226  -0.855 0.392295    \nI_WOMEDU               -0.0738107  0.0093836  -7.866 3.72e-15 ***\nabortlib               -0.0091560  0.0009459  -9.679  < 2e-16 ***\nPerceivedHealth:FS     -0.0042135  0.0014208  -2.966 0.003022 ** \nPerceivedHealth:NS1     0.0915541  0.0132100   6.931 4.23e-12 ***\nFS:NS1                  0.0522839  0.0080080   6.529 6.67e-11 ***\nPerceivedHealth:FS:NS1 -0.0115482  0.0020588  -5.609 2.04e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68392 degrees of freedom\n  (19377 observations deleted due to missingness)\nMultiple R-squared:  0.2346,    Adjusted R-squared:  0.2341 \nF-statistic: 403.2 on 52 and 68392 DF,  p-value: < 2.2e-16\n\n\nCode\n# try model with \"ReligionImpt\" and \"abortlib\".\nModelB_H <- lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs)\nsummary(ModelB_H)\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + \n    FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + \n    Parents + Married + Kids + Edu + Job + SocialClass + Income + \n    Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.83838 -0.35003 -0.02486  0.45985  2.58146 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             0.9653591  0.0487346  19.808  < 2e-16 ***\nPerceivedHealth         0.2427013  0.0091832  26.429  < 2e-16 ***\nFS                      0.0753615  0.0055248  13.641  < 2e-16 ***\nNS1                    -0.3875722  0.0493653  -7.851 4.18e-15 ***\nG_TOWNSIZE             -0.0104199  0.0013679  -7.617 2.62e-14 ***\nH_URBRURAL2             0.0409348  0.0071384   5.734 9.82e-09 ***\nFamImpt                 0.1139240  0.0070337  16.197  < 2e-16 ***\nFriendsImpt             0.0239321  0.0033986   7.042 1.92e-12 ***\nLeisureImpt             0.0218539  0.0032127   6.802 1.04e-11 ***\nReligionImpt            0.0359167  0.0031719  11.323  < 2e-16 ***\nFOC                     0.0322169  0.0011295  28.522  < 2e-16 ***\nTrust2                 -0.0291083  0.0062043  -4.692 2.72e-06 ***\nSex2                    0.0415730  0.0053662   7.747 9.52e-15 ***\nAge                    -0.0002997  0.0002339  -1.281 0.200055    \nImmigrant2              0.0069104  0.0121815   0.567 0.570520    \nCitizen2                0.0388779  0.0198208   1.961 0.049829 *  \nHHSize                  0.0045529  0.0013320   3.418 0.000631 ***\nParents2               -0.0086985  0.0073777  -1.179 0.238390    \nParents3                0.0216617  0.0116667   1.857 0.063356 .  \nParents4               -0.0282459  0.0314608  -0.898 0.369288    \nMarried2                0.0394323  0.0093613   4.212 2.53e-05 ***\nMarried3               -0.1125812  0.0130569  -8.622  < 2e-16 ***\nMarried4               -0.0795349  0.0165963  -4.792 1.65e-06 ***\nMarried5               -0.0668593  0.0119429  -5.598 2.17e-08 ***\nMarried6               -0.0500088  0.0081394  -6.144 8.09e-10 ***\nKids                    0.0101190  0.0019017   5.321 1.04e-07 ***\nEdu                    -0.0039301  0.0014214  -2.765 0.005694 ** \nJob2                    0.0299734  0.0093275   3.213 0.001312 ** \nJob3                    0.0051187  0.0076043   0.673 0.500861    \nJob4                    0.0367885  0.0099071   3.713 0.000205 ***\nJob5                    0.0263369  0.0084946   3.100 0.001933 ** \nJob6                    0.0255340  0.0116272   2.196 0.028091 *  \nJob7                   -0.0123139  0.0095726  -1.286 0.198321    \nJob8                    0.0260058  0.0243860   1.066 0.286237    \nSocialClass             0.0331230  0.0029360  11.282  < 2e-16 ***\nIncome                  0.0007745  0.0014028   0.552 0.580883    \nReligion1               0.0356580  0.0084815   4.204 2.62e-05 ***\nReligion2              -0.0148305  0.0108456  -1.367 0.171498    \nReligion3              -0.0949489  0.0107237  -8.854  < 2e-16 ***\nReligion4              -0.1352025  0.0458731  -2.947 0.003207 ** \nReligion5              -0.1116511  0.0090220 -12.375  < 2e-16 ***\nReligion6              -0.0804296  0.0294147  -2.734 0.006252 ** \nReligion7              -0.0484974  0.0114371  -4.240 2.23e-05 ***\nReligion8               0.0410340  0.0154060   2.664 0.007735 ** \nReligion9               0.0127811  0.0157734   0.810 0.417775    \nI_WOMJOB                0.0209233  0.0090190   2.320 0.020349 *  \nI_WOMPOL               -0.0063906  0.0094151  -0.679 0.497292    \nI_WOMEDU               -0.0730143  0.0093696  -7.793 6.65e-15 ***\nabortlib               -0.0075929  0.0009610  -7.901 2.82e-15 ***\nPerceivedHealth:FS     -0.0042494  0.0014202  -2.992 0.002771 ** \nPerceivedHealth:NS1     0.0932809  0.0131979   7.068 1.59e-12 ***\nFS:NS1                  0.0535637  0.0080021   6.694 2.19e-11 ***\nPerceivedHealth:FS:NS1 -0.0117965  0.0020574  -5.734 9.86e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68517 degrees of freedom\n  (19252 observations deleted due to missingness)\nMultiple R-squared:  0.2363,    Adjusted R-squared:  0.2357 \nF-statistic: 407.7 on 52 and 68517 DF,  p-value: < 2.2e-16\n\n\nCode\n# run model with \"FamImpt\" as categorical.\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + as.factor(FamImpt) + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + as.factor(FamImpt) + FriendsImpt + LeisureImpt + \n    ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + \n    HHSize + Parents + Married + Kids + Edu + Job + SocialClass + \n    Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, \n    data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.83836 -0.34998 -0.02473  0.45999  2.60795 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1.1072339  0.0681742  16.241  < 2e-16 ***\nPerceivedHealth         0.2425875  0.0091842  26.414  < 2e-16 ***\nFS                      0.0752720  0.0055256  13.622  < 2e-16 ***\nNS1                    -0.3882081  0.0493691  -7.863 3.79e-15 ***\nG_TOWNSIZE             -0.0104176  0.0013681  -7.615 2.68e-14 ***\nH_URBRURAL2             0.0409434  0.0071384   5.736 9.75e-09 ***\nas.factor(FamImpt)2     0.0592525  0.0602280   0.984 0.325216    \nas.factor(FamImpt)3     0.2033328  0.0549154   3.703 0.000214 ***\nas.factor(FamImpt)4     0.3138016  0.0544706   5.761 8.40e-09 ***\nFriendsImpt             0.0239401  0.0033987   7.044 1.89e-12 ***\nLeisureImpt             0.0218856  0.0032129   6.812 9.72e-12 ***\nReligionImpt            0.0359340  0.0031724  11.327  < 2e-16 ***\nFOC                     0.0322200  0.0011296  28.524  < 2e-16 ***\nTrust2                 -0.0291466  0.0062056  -4.697 2.65e-06 ***\nSex2                    0.0415226  0.0053664   7.738 1.03e-14 ***\nAge                    -0.0002972  0.0002339  -1.270 0.203936    \nImmigrant2              0.0069407  0.0121817   0.570 0.568840    \nCitizen2                0.0387761  0.0198213   1.956 0.050435 .  \nHHSize                  0.0045466  0.0013320   3.413 0.000642 ***\nParents2               -0.0088173  0.0073794  -1.195 0.232147    \nParents3                0.0217277  0.0116670   1.862 0.062561 .  \nParents4               -0.0280973  0.0314612  -0.893 0.371818    \nMarried2                0.0393610  0.0093636   4.204 2.63e-05 ***\nMarried3               -0.1123870  0.0130579  -8.607  < 2e-16 ***\nMarried4               -0.0794961  0.0165964  -4.790 1.67e-06 ***\nMarried5               -0.0668742  0.0119429  -5.599 2.16e-08 ***\nMarried6               -0.0499130  0.0081398  -6.132 8.73e-10 ***\nKids                    0.0101127  0.0019017   5.318 1.05e-07 ***\nEdu                    -0.0039281  0.0014214  -2.764 0.005719 ** \nJob2                    0.0300083  0.0093277   3.217 0.001295 ** \nJob3                    0.0051541  0.0076045   0.678 0.497914    \nJob4                    0.0368074  0.0099073   3.715 0.000203 ***\nJob5                    0.0263615  0.0084947   3.103 0.001915 ** \nJob6                    0.0255713  0.0116272   2.199 0.027863 *  \nJob7                   -0.0122479  0.0095730  -1.279 0.200754    \nJob8                    0.0259292  0.0243864   1.063 0.287665    \nSocialClass             0.0331054  0.0029361  11.275  < 2e-16 ***\nIncome                  0.0007985  0.0014030   0.569 0.569271    \nReligion1               0.0355768  0.0084819   4.194 2.74e-05 ***\nReligion2              -0.0147994  0.0108457  -1.365 0.172402    \nReligion3              -0.0949489  0.0107240  -8.854  < 2e-16 ***\nReligion4              -0.1347230  0.0458750  -2.937 0.003318 ** \nReligion5              -0.1115545  0.0090242 -12.362  < 2e-16 ***\nReligion6              -0.0803182  0.0294174  -2.730 0.006329 ** \nReligion7              -0.0486226  0.0114391  -4.251 2.14e-05 ***\nReligion8               0.0408256  0.0154072   2.650 0.008057 ** \nReligion9               0.0128689  0.0157737   0.816 0.414593    \nI_WOMJOB                0.0209234  0.0090190   2.320 0.020348 *  \nI_WOMPOL               -0.0064471  0.0094153  -0.685 0.493502    \nI_WOMEDU               -0.0729682  0.0093718  -7.786 7.02e-15 ***\nabortlib               -0.0075775  0.0009611  -7.884 3.22e-15 ***\nPerceivedHealth:FS     -0.0042300  0.0014204  -2.978 0.002902 ** \nPerceivedHealth:NS1     0.0934470  0.0131990   7.080 1.46e-12 ***\nFS:NS1                  0.0537264  0.0080034   6.713 1.92e-11 ***\nPerceivedHealth:FS:NS1 -0.0118354  0.0020577  -5.752 8.87e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6226 on 68515 degrees of freedom\n  (19252 observations deleted due to missingness)\nMultiple R-squared:  0.2363,    Adjusted R-squared:  0.2357 \nF-statistic: 392.6 on 54 and 68515 DF,  p-value: < 2.2e-16\n\n\nCode\n# try model with \"ReligionImpt\" and \"homolib\": adjusted R^2 was lower, so i'm sticking to ModelB_H. significance of main predictors doesn't change either.\nsummary(lm(Happiness ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + IncomeR + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + homolib, wvs))\n\n\n\nCall:\nlm(formula = Happiness ~ PerceivedHealth * FS * NS + G_TOWNSIZE + \n    H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + \n    FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + \n    Parents + Married + Kids + Edu + Job + SocialClass + IncomeR + \n    Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + homolib, data = wvs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82527 -0.34776 -0.02813  0.46170  2.57719 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             9.352e-01  4.939e-02  18.934  < 2e-16 ***\nPerceivedHealth         2.391e-01  9.295e-03  25.728  < 2e-16 ***\nFS                      7.412e-02  5.575e-03  13.296  < 2e-16 ***\nNS1                    -3.655e-01  5.080e-02  -7.194 6.34e-13 ***\nG_TOWNSIZE             -7.377e-03  1.393e-03  -5.295 1.20e-07 ***\nH_URBRURAL2             4.442e-02  7.274e-03   6.107 1.02e-09 ***\nFamImpt                 1.179e-01  7.079e-03  16.661  < 2e-16 ***\nFriendsImpt             2.194e-02  3.486e-03   6.293 3.14e-10 ***\nLeisureImpt             1.999e-02  3.324e-03   6.013 1.83e-09 ***\nReligionImpt            4.532e-02  3.186e-03  14.224  < 2e-16 ***\nFOC                     3.203e-02  1.156e-03  27.711  < 2e-16 ***\nTrust2                 -1.941e-02  6.334e-03  -3.064 0.002184 ** \nSex2                    3.663e-02  5.473e-03   6.693 2.21e-11 ***\nAge                    -6.294e-04  2.394e-04  -2.629 0.008555 ** \nImmigrant2              8.568e-03  1.221e-02   0.702 0.482983    \nCitizen2                3.916e-02  1.985e-02   1.973 0.048524 *  \nHHSize                  2.562e-03  1.377e-03   1.861 0.062696 .  \nParents2               -6.188e-03  7.519e-03  -0.823 0.410536    \nParents3                3.009e-02  1.212e-02   2.483 0.013020 *  \nParents4               -2.584e-02  3.217e-02  -0.803 0.421753    \nMarried2                3.385e-02  9.428e-03   3.590 0.000331 ***\nMarried3               -1.082e-01  1.326e-02  -8.162 3.34e-16 ***\nMarried4               -7.639e-02  1.688e-02  -4.525 6.06e-06 ***\nMarried5               -6.989e-02  1.231e-02  -5.676 1.39e-08 ***\nMarried6               -5.380e-02  8.288e-03  -6.491 8.58e-11 ***\nKids                    9.797e-03  1.952e-03   5.019 5.21e-07 ***\nEdu                    -6.213e-03  1.460e-03  -4.256 2.09e-05 ***\nJob2                    3.939e-02  9.561e-03   4.120 3.80e-05 ***\nJob3                    1.194e-02  7.739e-03   1.543 0.122926    \nJob4                    3.650e-02  1.012e-02   3.607 0.000310 ***\nJob5                    3.441e-02  8.731e-03   3.941 8.11e-05 ***\nJob6                    2.356e-02  1.187e-02   1.985 0.047189 *  \nJob7                   -2.361e-02  9.918e-03  -2.381 0.017287 *  \nJob8                    2.882e-02  2.483e-02   1.161 0.245692    \nSocialClass             2.947e-02  2.934e-03  10.046  < 2e-16 ***\nIncomeR                 6.624e-03  4.955e-03   1.337 0.181299    \nReligion1               3.819e-02  8.529e-03   4.478 7.56e-06 ***\nReligion2              -1.602e-02  1.094e-02  -1.464 0.143063    \nReligion3              -9.595e-02  1.093e-02  -8.777  < 2e-16 ***\nReligion4              -1.403e-01  4.582e-02  -3.061 0.002207 ** \nReligion5              -1.144e-01  9.290e-03 -12.318  < 2e-16 ***\nReligion6              -8.127e-02  2.935e-02  -2.769 0.005632 ** \nReligion7              -4.454e-02  1.148e-02  -3.881 0.000104 ***\nReligion8               4.505e-02  1.560e-02   2.888 0.003875 ** \nReligion9               1.857e-02  1.585e-02   1.172 0.241385    \nI_WOMJOB                8.150e-03  9.281e-03   0.878 0.379913    \nI_WOMPOL               -1.148e-02  9.708e-03  -1.182 0.237049    \nI_WOMEDU               -6.899e-02  9.770e-03  -7.062 1.66e-12 ***\nhomolib                 2.897e-05  9.231e-04   0.031 0.974960    \nPerceivedHealth:FS     -3.838e-03  1.433e-03  -2.678 0.007418 ** \nPerceivedHealth:NS1     9.339e-02  1.355e-02   6.891 5.60e-12 ***\nFS:NS1                  4.705e-02  8.226e-03   5.719 1.08e-08 ***\nPerceivedHealth:FS:NS1 -1.121e-02  2.111e-03  -5.309 1.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.622 on 65328 degrees of freedom\n  (22441 observations deleted due to missingness)\nMultiple R-squared:  0.231, Adjusted R-squared:  0.2304 \nF-statistic: 377.4 on 52 and 65328 DF,  p-value: < 2.2e-16\n\n\nLooking at adjusted R2, ReligionImpt is preferable to AttendReligious, and abortlib is preferable to homolib.\nI also ran another model to see how things change when FamImpt is treated as categorical: adjusted R2 remained the same, but the standard errors rose and t-values went down. Hence, I will stick to treating it as numeric (as well as other ordered variables with 4 levels).\nAdditionally, in past papers, adjusted R2 ranged between 0.15 to 0.3 for happiness. Ours is in the higher part of that range (adjusted R2 = 0.24).\nFor the final model I settled on (labelled as ModelB_H above), a three-way interaction between PerceivedHealth, FS and NS was observed. I will plot this graphically.\n\n\nCode\ninteract_plot(ModelB_H, pred = PerceivedHealth, modx = FS, modx.values = c(1,10), mod2 = NS)\n\n\n\n\n\nFor people in the Global North (NS = 0), it seems like the magnitude of the relationship between PerceivedHealth and Happiness does not change for different values of FS. However, for those in the Global South, the relationship seems to become less steep for greater values of FS. As FS increases, the impact of PerceivedHealth on Happiness reduces. I think at least partially, this shows that Happiness depends more on PerceivedHealth and FS in the Global South (but I am not 100% sure that my interpretation is correct).\nTo fully answer the RQ, I now need to re-run the same model with LS as the DV.\n\n\nCode\n# run model.\nModelB_LS <- lm(LS ~ PerceivedHealth*FS*NS + G_TOWNSIZE + H_URBRURAL + FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + Married + Kids + Edu + Job + SocialClass + Income + Religion + I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, wvs)\nsummary(ModelB_LS)\n\n\n\nCall:\nlm(formula = LS ~ PerceivedHealth * FS * NS + G_TOWNSIZE + H_URBRURAL + \n    FamImpt + FriendsImpt + LeisureImpt + ReligionImpt + FOC + \n    Trust + Sex + Age + Immigrant + Citizen + HHSize + Parents + \n    Married + Kids + Edu + Job + SocialClass + Income + Religion + \n    I_WOMJOB + I_WOMPOL + I_WOMEDU + abortlib, data = wvs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0761 -0.9137  0.0669  0.9278  8.1280 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             0.118394   0.135781   0.872 0.383240    \nPerceivedHealth         0.517614   0.025575  20.239  < 2e-16 ***\nFS                      0.489514   0.015392  31.803  < 2e-16 ***\nNS1                    -0.329556   0.137541  -2.396 0.016575 *  \nG_TOWNSIZE             -0.024345   0.003814  -6.384 1.74e-10 ***\nH_URBRURAL2            -0.028291   0.019904  -1.421 0.155220    \nFamImpt                 0.167848   0.019595   8.566  < 2e-16 ***\nFriendsImpt            -0.018183   0.009471  -1.920 0.054868 .  \nLeisureImpt             0.018509   0.008955   2.067 0.038750 *  \nReligionImpt            0.058195   0.008839   6.584 4.62e-11 ***\nFOC                     0.241036   0.003148  76.576  < 2e-16 ***\nTrust2                  0.030117   0.017293   1.742 0.081591 .  \nSex2                    0.055853   0.014955   3.735 0.000188 ***\nAge                     0.004196   0.000652   6.436 1.23e-10 ***\nImmigrant2              0.048708   0.033967   1.434 0.151591    \nCitizen2               -0.005785   0.055315  -0.105 0.916707    \nHHSize                 -0.012284   0.003713  -3.308 0.000940 ***\nParents2                0.005514   0.020563   0.268 0.788601    \nParents3                0.026205   0.032533   0.805 0.420553    \nParents4               -0.232785   0.087756  -2.653 0.007988 ** \nMarried2                0.209746   0.026089   8.040 9.15e-16 ***\nMarried3               -0.128045   0.036375  -3.520 0.000432 ***\nMarried4               -0.107591   0.046250  -2.326 0.020005 *  \nMarried5               -0.093412   0.033257  -2.809 0.004974 ** \nMarried6               -0.101216   0.022679  -4.463 8.10e-06 ***\nKids                    0.019528   0.005297   3.687 0.000227 ***\nEdu                    -0.011649   0.003962  -2.940 0.003285 ** \nJob2                    0.095599   0.025989   3.678 0.000235 ***\nJob3                   -0.028597   0.021202  -1.349 0.177418    \nJob4                    0.069394   0.027600   2.514 0.011932 *  \nJob5                    0.052595   0.023680   2.221 0.026348 *  \nJob6                    0.054279   0.032410   1.675 0.093984 .  \nJob7                   -0.109330   0.026686  -4.097 4.19e-05 ***\nJob8                    0.125121   0.068019   1.840 0.065846 .  \nSocialClass             0.019626   0.008183   2.398 0.016474 *  \nIncome                  0.005628   0.003911   1.439 0.150099    \nReligion1               0.111241   0.023636   4.706 2.53e-06 ***\nReligion2              -0.155857   0.030234  -5.155 2.54e-07 ***\nReligion3              -0.269519   0.029872  -9.023  < 2e-16 ***\nReligion4              -0.290707   0.127955  -2.272 0.023093 *  \nReligion5              -0.314493   0.025148 -12.506  < 2e-16 ***\nReligion6              -0.042414   0.081963  -0.517 0.604825    \nReligion7              -0.152603   0.031879  -4.787 1.70e-06 ***\nReligion8               0.233037   0.042930   5.428 5.71e-08 ***\nReligion9              -0.001271   0.043950  -0.029 0.976926    \nI_WOMJOB                0.034039   0.025140   1.354 0.175740    \nI_WOMPOL                0.081730   0.026240   3.115 0.001842 ** \nI_WOMEDU                0.036452   0.026119   1.396 0.162834    \nabortlib               -0.025280   0.002678  -9.439  < 2e-16 ***\nPerceivedHealth:FS     -0.027077   0.003957  -6.843 7.81e-12 ***\nPerceivedHealth:NS1    -0.027232   0.036778  -0.740 0.459037    \nFS:NS1                  0.085490   0.022307   3.832 0.000127 ***\nPerceivedHealth:FS:NS1 -0.004343   0.005735  -0.757 0.448892    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.737 on 68623 degrees of freedom\n  (19146 observations deleted due to missingness)\nMultiple R-squared:  0.411, Adjusted R-squared:  0.4106 \nF-statistic: 920.9 on 52 and 68623 DF,  p-value: < 2.2e-16\n\n\nFor the model with LS as the DV, the adjusted R2 of 0.41 for life satisfaction is in line with past papers (range: 0.16 to 0.5).\nThere is no three-way interaction. Only 2 of the two-way interactions are significant - PerceivedHealth*FS and FS*NS. I will plot these graphs to interpret them.\n\n\nCode\n# plot interactions.\ninteract_plot(ModelB_LS, pred = PerceivedHealth, modx = FS, modx.values = c(1,10))\n\n\n\n\n\nCode\ninteract_plot(ModelB_LS, pred = FS, modx = NS)\n\n\n\n\n\n\nPerceivedHealth*FS: FS seems to limit the effect of PerceivedHealth on LS, regardless of whether the person is from the Global North/South.\nFS*NS: There seems to be a stronger effect of FS on LS for the Global South (NS = 1).\n\nNow I will generate the diagnostic plots for ModelB_H and ModelB_LS.\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelB_H, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelB_H)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelB_H\nBP = 3520.9, df = 52, p-value < 2.2e-16\n\n\nCode\npar(mfrow = c(2,3)); plot(ModelB_LS, which = 1:6)\n\n\n\n\n\nCode\nbptest(ModelB_LS)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  ModelB_LS\nBP = 6066.6, df = 52, p-value < 2.2e-16\n\n\nSimilar to what was observed for RQ A, the scale-location plot indicates heteroskedasticity (confirmed by the Breusch-Pagan test). I will generate robust standard errors for both models.\n\n\nCode\n# obtain robust standard errors for models.\ncoeftest(ModelB_H, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                          Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)             0.96535914  0.05634466  17.1331 < 2.2e-16 ***\nPerceivedHealth         0.24270127  0.01144805  21.2002 < 2.2e-16 ***\nFS                      0.07536150  0.00666454  11.3078 < 2.2e-16 ***\nNS1                    -0.38757224  0.06503656  -5.9593 2.546e-09 ***\nG_TOWNSIZE             -0.01041991  0.00139078  -7.4922 6.856e-14 ***\nH_URBRURAL2             0.04093482  0.00730926   5.6004 2.147e-08 ***\nFamImpt                 0.11392400  0.00748668  15.2169 < 2.2e-16 ***\nFriendsImpt             0.02393210  0.00367876   6.5055 7.798e-11 ***\nLeisureImpt             0.02185385  0.00344548   6.3428 2.271e-10 ***\nReligionImpt            0.03591670  0.00323280  11.1101 < 2.2e-16 ***\nFOC                     0.03221690  0.00130500  24.6873 < 2.2e-16 ***\nTrust2                 -0.02910827  0.00572731  -5.0824 3.737e-07 ***\nSex2                    0.04157302  0.00534434   7.7789 7.418e-15 ***\nAge                    -0.00029973  0.00023855  -1.2565  0.208953    \nImmigrant2              0.00691044  0.01077333   0.6414  0.521240    \nCitizen2                0.03887792  0.01796379   2.1642  0.030449 *  \nHHSize                  0.00455286  0.00154128   2.9539  0.003138 ** \nParents2               -0.00869855  0.00767204  -1.1338  0.256883    \nParents3                0.02166174  0.01212325   1.7868  0.073975 .  \nParents4               -0.02824595  0.03476356  -0.8125  0.416498    \nMarried2                0.03943227  0.00968852   4.0700 4.707e-05 ***\nMarried3               -0.11258117  0.01339883  -8.4023 < 2.2e-16 ***\nMarried4               -0.07953490  0.01853927  -4.2901 1.789e-05 ***\nMarried5               -0.06685929  0.01270208  -5.2636 1.416e-07 ***\nMarried6               -0.05000883  0.00828032  -6.0395 1.554e-09 ***\nKids                    0.01011896  0.00206435   4.9018 9.520e-07 ***\nEdu                    -0.00393011  0.00142772  -2.7527  0.005912 ** \nJob2                    0.02997341  0.00914822   3.2764  0.001052 ** \nJob3                    0.00511872  0.00782594   0.6541  0.513068    \nJob4                    0.03678854  0.00968219   3.7996  0.000145 ***\nJob5                    0.02633692  0.00851043   3.0947  0.001971 ** \nJob6                    0.02553402  0.01142623   2.2347  0.025441 *  \nJob7                   -0.01231387  0.01055162  -1.1670  0.243209    \nJob8                    0.02600579  0.02677949   0.9711  0.331498    \nSocialClass             0.03312301  0.00311454  10.6349 < 2.2e-16 ***\nIncome                  0.00077450  0.00147361   0.5256  0.599183    \nReligion1               0.03565801  0.00846033   4.2147 2.504e-05 ***\nReligion2              -0.01483054  0.01087377  -1.3639  0.172609    \nReligion3              -0.09494892  0.01080759  -8.7854 < 2.2e-16 ***\nReligion4              -0.13520247  0.04550734  -2.9710  0.002969 ** \nReligion5              -0.11165106  0.00919236 -12.1461 < 2.2e-16 ***\nReligion6              -0.08042960  0.02698805  -2.9802  0.002882 ** \nReligion7              -0.04849738  0.01087110  -4.4611 8.166e-06 ***\nReligion8               0.04103402  0.01560114   2.6302  0.008536 ** \nReligion9               0.01278109  0.01514373   0.8440  0.398680    \nI_WOMJOB                0.02092325  0.00951658   2.1986  0.027909 *  \nI_WOMPOL               -0.00639059  0.01005827  -0.6354  0.525198    \nI_WOMEDU               -0.07301433  0.01000503  -7.2978 2.958e-13 ***\nabortlib               -0.00759285  0.00097437  -7.7926 6.657e-15 ***\nPerceivedHealth:FS     -0.00424937  0.00167731  -2.5334  0.011297 *  \nPerceivedHealth:NS1     0.09328088  0.01707000   5.4646 4.655e-08 ***\nFS:NS1                  0.05356371  0.00996608   5.3746 7.700e-08 ***\nPerceivedHealth:FS:NS1 -0.01179646  0.00251070  -4.6985 2.626e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(ModelB_LS, vcov = vcovHC, type = 'HC1')\n\n\n\nt test of coefficients:\n\n                          Estimate  Std. Error  t value  Pr(>|t|)    \n(Intercept)             0.11839390  0.15647523   0.7566 0.4492740    \nPerceivedHealth         0.51761407  0.03262995  15.8632 < 2.2e-16 ***\nFS                      0.48951395  0.01878447  26.0595 < 2.2e-16 ***\nNS1                    -0.32955582  0.19624677  -1.6793 0.0930995 .  \nG_TOWNSIZE             -0.02434490  0.00386763  -6.2945 3.102e-10 ***\nH_URBRURAL2            -0.02829065  0.02086088  -1.3562 0.1750534    \nFamImpt                 0.16784807  0.02052719   8.1769 2.962e-16 ***\nFriendsImpt            -0.01818347  0.01028087  -1.7687 0.0769533 .  \nLeisureImpt             0.01850893  0.00977097   1.8943 0.0581924 .  \nReligionImpt            0.05819543  0.00862565   6.7468 1.523e-11 ***\nFOC                     0.24103567  0.00427540  56.3773 < 2.2e-16 ***\nTrust2                  0.03011720  0.01557963   1.9331 0.0532263 .  \nSex2                    0.05585260  0.01478280   3.7782 0.0001581 ***\nAge                     0.00419594  0.00067759   6.1925 5.957e-10 ***\nImmigrant2              0.04870756  0.02822134   1.7259 0.0843677 .  \nCitizen2               -0.00578498  0.04589833  -0.1260 0.8997014    \nHHSize                 -0.01228380  0.00417933  -2.9392 0.0032919 ** \nParents2                0.00551351  0.02111411   0.2611 0.7939937    \nParents3                0.02620458  0.03506813   0.7472 0.4549165    \nParents4               -0.23278509  0.08357550  -2.7853 0.0053488 ** \nMarried2                0.20974578  0.02636583   7.9552 1.815e-15 ***\nMarried3               -0.12804540  0.03611182  -3.5458 0.0003917 ***\nMarried4               -0.10759093  0.04846076  -2.2202 0.0264107 *  \nMarried5               -0.09341168  0.03561537  -2.6228 0.0087232 ** \nMarried6               -0.10121559  0.02296190  -4.4080 1.045e-05 ***\nKids                    0.01952766  0.00589338   3.3135 0.0009219 ***\nEdu                    -0.01164860  0.00398015  -2.9267 0.0034272 ** \nJob2                    0.09559875  0.02580836   3.7042 0.0002122 ***\nJob3                   -0.02859702  0.02178343  -1.3128 0.1892589    \nJob4                    0.06939360  0.02637211   2.6313 0.0085072 ** \nJob5                    0.05259493  0.02402791   2.1889 0.0286067 *  \nJob6                    0.05427933  0.03125783   1.7365 0.0824793 .  \nJob7                   -0.10932964  0.03054728  -3.5790 0.0003451 ***\nJob8                    0.12512155  0.07189406   1.7404 0.0818003 .  \nSocialClass             0.01962597  0.00901434   2.1772 0.0294694 *  \nIncome                  0.00562853  0.00438793   1.2827 0.1995906    \nReligion1               0.11124113  0.02298752   4.8392 1.306e-06 ***\nReligion2              -0.15585713  0.02994334  -5.2051 1.945e-07 ***\nReligion3              -0.26951940  0.02976146  -9.0560 < 2.2e-16 ***\nReligion4              -0.29070744  0.12026620  -2.4172 0.0156430 *  \nReligion5              -0.31449248  0.02519223 -12.4837 < 2.2e-16 ***\nReligion6              -0.04241378  0.07059987  -0.6008 0.5480000    \nReligion7              -0.15260268  0.02876183  -5.3057 1.126e-07 ***\nReligion8               0.23303695  0.04394884   5.3025 1.146e-07 ***\nReligion9              -0.00127116  0.03863632  -0.0329 0.9737539    \nI_WOMJOB                0.03403916  0.02709696   1.2562 0.2090483    \nI_WOMPOL                0.08173040  0.02906210   2.8123 0.0049208 ** \nI_WOMEDU                0.03645214  0.02900196   1.2569 0.2087994    \nabortlib               -0.02528014  0.00273160  -9.2547 < 2.2e-16 ***\nPerceivedHealth:FS     -0.02707689  0.00475419  -5.6954 1.236e-08 ***\nPerceivedHealth:NS1    -0.02723172  0.05270169  -0.5167 0.6053572    \nFS:NS1                  0.08549005  0.02984951   2.8640 0.0041841 ** \nPerceivedHealth:FS:NS1 -0.00434316  0.00765981  -0.5670 0.5707115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor ModelB_H, zooming in on the three-way interaction between PerceivedHealth*FS*NS: the magnitude of the t-value reduces, but remains significant, p < .001.\nFor ModelB_LS, the results are similar: t-values go up, but both PerceivedHealth*FS and FS*NS remain significant.\nSummarizing the results for both happiness and life satisfaction:\n\nPerceived health and financial satisfaction seem to have a greater impact on happiness in the Global South.\nFinancial satisfaction has a greater impact on life satisfaction in the Global South.\nHowever, perceived health did not have a different effect on life satisfaction in the Global North vs. South.\n\nGiven the third point, we cannot reject the null hypothesis for RQ B. However, I want to caution that I am not sure if I am interpreting the results correctly.\nI am also wondering…is there a function/package in R that can generate the model equations easily for me, especially since I included so many controls?"
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#moving-forward",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#moving-forward",
    "title": "Final Project Update",
    "section": "Moving Forward",
    "text": "Moving Forward\nThis is definitely a work-in-progress and will be edited further upon receiving feedback. Other things I want to try before submitting the final product in December:\n\nI had included variables measuring equality of gender/sexual orientation and abortion attitudes in the regression, just out of curiosity. I would like to interpret whether they had significant effects on happiness and life satisfaction.\nThere are some outliers indicated in the diagnostic plots. Removing them could potentially improve my final 4 models (ModelA_H_controls, ModelA_LS_controls, ModelB_H and ModelB_LS)."
  },
  {
    "objectID": "posts/FinalProjectUpdate_Saaradhaa.html#bibliography",
    "href": "posts/FinalProjectUpdate_Saaradhaa.html#bibliography",
    "title": "Final Project Update",
    "section": "Bibliography",
    "text": "Bibliography\nAddai, I., Opoku-Agyeman, C., & Amanfu, S. (2013). Exploring Predictors of Subjective Well-Being in Ghana: A Micro-Level Study. Journal Of Happiness Studies, 15(4), 869-890.\nAlba, C. (2019). A Data Analysis of the World Happiness Index and its Relation to the North-South Divide. Undergraduate Economic Review, 16(1).\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen (eds.). 2022. World Values Survey: Round Seven - Country-Pooled Datafile Version 4.0. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat.\nNgamaba, K. (2016). Happiness and life satisfaction in Rwanda. Journal Of Psychology In Africa, 26(5), 407-414.\nWorld Bank Country and Lending Groups. World Bank Data Help Desk. (2022). Retrieved from https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups.\nWVS Database. World Values Survey. (2022). Retrieved from https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp."
  },
  {
    "objectID": "posts/FinalProject_EthanCampbell.html",
    "href": "posts/FinalProject_EthanCampbell.html",
    "title": "Final Project",
    "section": "",
    "text": "Extensive research has been done on climate change and economic changes respectively but there is not a significant amount of research about their relation towards one another. There are research papers that touch on this but in different aspects and focus more on other factors like political aspects. I would like to look a little broader and look at the difference between each climate zone and their economic differences. This can be taken with a grain of salt as there are many factors that could effect the economic situation being left out. The data was pulled from NASA’s POWER data access viewer; here I pulled the data by region since pulling the whole country in one go was unavailable. Thus, I will conduct research on each region respectively and then compare the results. This study will be conducted on the Köppen climate classification scale to determine climate types for study. I will keep the this one the first level of the scale as further scaling would take significantly more time to process.\n\n\nThere are three levels to this climate classification the first scale is the 5 main climate groups A(tropical), B(Arid), C(Temperate), D(Continental), and E(Polar), the second layer is the seasonal precipitation type, and the final layer indicates the heat levels. Through this three layer system that was created by Wladimir Köppen in 1884 we are able to accurately dial in on a specific climate type. getwd() \n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Is there a relation between climate zone and economic growth?\nB. Do Southern climates have the largest economic growth?"
  },
  {
    "objectID": "posts/FinalProject_EthanCampbell.html#reading-in-the-data",
    "href": "posts/FinalProject_EthanCampbell.html#reading-in-the-data",
    "title": "Final Project",
    "section": "Reading in the data",
    "text": "Reading in the data\nData was collected\n\n\nCode\n# Reading in all the weather data \n\nAmherst <- read.csv(\"_data/amherst.csv\", skip = 14)\nFlorida <- read.csv(\"_data/flordia.csv\", skip = 14)\nIllinois <- read.csv(\"_data/illinois.csv\", skip = 14)\nMiddle <- read.csv(\"_data/middle.csv\", skip = 14)\nNewmexico <- read.csv(\"_data/Newmexico.csv\", skip = 14)\nNorth <- read.csv(\"_data/North.csv\", skip = 14)\nSouth <- read.csv(\"_data/South.csv\", skip = 14)\nSouthCali <- read.csv(\"_data/SouthCali.csv\", skip = 14)\nTexas <- read.csv(\"_data/Texas.csv\", skip = 14)\nWashington <- read.csv(\"_data/washington.csv\", skip = 14)\nWestV <- read.csv(\"_data/WestV.csv\", skip = 14)\n\n\n\nAmherst\nHad trouble with pivot_wider since it would split the values up by each name but then would fill in the values with NA for the other sections. This added a ton of NA values that one looked bad and were hard to deal with. I had to go a more manual way and do it for each part of PARAMETER to get the exact number of rows I needed. This stopped the NA values and got them all lined up so it reduced the size of the document from 500k+ rows to 89290 rows. This is huge in terms of running the data and working with it. Finally, I just merged the data together and then I was able to rename all the columns and start regression analysis.\n\n\nCode\n# Bringing all the month columns into one column\nregion <- Amherst %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n# trying to create a function that would apply to all regions\ntidy_function <- function(region, Tidy_region, t2m, rh2m, wh10m, wh50m, PRECTOTCORR){\n  Tidy_region <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH) %>%\n  filter(PARAMETER == 'PS')\n  Tidy_region <- Tidy_region %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  Tidy_region <- Tidy_region %>%\n  select(PS, YEAR, MONTH, LAT, LON)\n  t2m <- region %>%\n  select(PARAMETER, Month_Average, YEAR) %>%\n  filter(PARAMETER == 'T2M')\n  t2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  t2m <- t2m %>%\n  select(T2M, YEAR)\n  Tidy_region$T2M <- t2m$T2M\n  rh2m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n  rh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  rh2m <- rh2m %>%\n  select(RH2M, YEAR)\n  Tidy_region$RH2M <- rh2m$RH2M\n  wh10m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n  wh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  wh10m <- wh10m %>%\n  select(WS10M, YEAR)\n  Tidy_region$WS10M <- wh10m$WS10M\n  wh50m <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n  wh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  wh50m <- wh50m %>%\n  select(WS50M, YEAR)\n  Tidy_region$WS50M <- wh50m$WS50M\n  PRECTOTCORR <- region %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n  PRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  dplyr::mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n  PRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n  Tidy_region$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n  # renaming all the variables to easier to digest names\n  Tidy_region <- Tidy_region %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n}\n\nAmherst <- tidy_function(region)\n\n## Getting them in clean looking order\nAmherst <- Amherst %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\n\nAmherst <- Amherst %>%\n  mutate(Temperature = Temperature* 9/5 + 32)\n\nview_amherst <- Amherst %>%\n  slice(1:10)\n\nkable(view_amherst, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nAmherst Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    35.25 \n    -71.75 \n    66.88 \n    72.58 \n    5.12 \n    101.90 \n    7.68 \n    8.44 \n  \n  \n    1990 \n    JAN \n    35.25 \n    -71.75 \n    63.14 \n    76.22 \n    5.03 \n    102.01 \n    8.38 \n    9.44 \n  \n  \n    1990 \n    FEB \n    35.25 \n    -71.75 \n    62.62 \n    77.36 \n    4.96 \n    102.22 \n    9.68 \n    10.94 \n  \n  \n    1990 \n    MAR \n    35.25 \n    -71.75 \n    63.61 \n    78.92 \n    5.29 \n    102.36 \n    7.51 \n    8.26 \n  \n  \n    1990 \n    APR \n    35.25 \n    -71.75 \n    65.52 \n    75.35 \n    3.96 \n    101.87 \n    7.51 \n    8.26 \n  \n  \n    1990 \n    MAY \n    35.25 \n    -71.75 \n    69.93 \n    76.08 \n    3.31 \n    101.63 \n    7.21 \n    8.15 \n  \n  \n    1990 \n    JUN \n    35.25 \n    -71.75 \n    74.68 \n    82.84 \n    4.56 \n    101.67 \n    6.56 \n    7.42 \n  \n  \n    1990 \n    JUL \n    35.25 \n    -71.75 \n    78.75 \n    83.94 \n    4.89 \n    101.81 \n    6.32 \n    7.19 \n  \n  \n    1990 \n    AUG \n    35.25 \n    -71.75 \n    79.18 \n    79.47 \n    3.31 \n    101.76 \n    4.12 \n    4.57 \n  \n  \n    1990 \n    SEP \n    35.25 \n    -71.75 \n    76.84 \n    74.81 \n    2.72 \n    101.67 \n    5.11 \n    5.57 \n  \n\n\n\n\n\nCode\n# aggregate the months into 1 year\n# Converting abbreviation to normal word\nWeather_region_Amherst$Month <- mapvalues(Weather_region_Amherst$Month, from = c(\"NOV\", \"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEP\", \"OCT\", \"DEC\"), to = c(\"November\", \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"December\"))\n\n\nError in mapvalues(Weather_region_Amherst$Month, from = c(\"NOV\", \"JAN\", : object 'Weather_region_Amherst' not found\n\n\nCode\n# Change word to numeric value\nWeather_region_Amherst <- Weather_region_Amherst %>%\n  mutate(Month = recode(Month,\n                        January = 1,\n                        February = 2,\n                        March = 3,\n                        April = 4,\n                        May = 5,\n                        June = 6,\n                        July = 7,\n                        August = 8,\n                        September = 9,\n                        October = 10,\n                        November = 11,\n                        December = 12))\n\n\nError in is.data.frame(.data): object 'Weather_region_Amherst' not found\n\n\nCode\n# Changing from year month to date column\n\nWeather_region_Amherst$Date <- with(Weather_region_Amherst, ym(sprintf('%04d%02d', Year, Month)))\n\n\nError in with(Weather_region_Amherst, ym(sprintf(\"%04d%02d\", Year, Month))): object 'Weather_region_Amherst' not found\n\n\nCode\nYear <- format(as.Date(Weather_region_Amherst$Date), format = \"%Y\")\n\n\nError in as.Date(Weather_region_Amherst$Date): object 'Weather_region_Amherst' not found\n\n\nCode\nWeather_region_Amherst %>%\n  group_by(Latitude, Longitude) %>%\n  mutate(Annual_Temperature = mean(Temperature),\n      Annual_Humidity = mean(Humidity),\n      Annual_Precipitation = sum(Precipitation))\n\n\nError in group_by(., Latitude, Longitude): object 'Weather_region_Amherst' not found\n\n\nCode\n# creating the mean value for each long and lat for each variable\nMeans_variables <- ddply(Weather_region_Amherst, .(Year, Latitude, Longitude), summarise,\n      Annual_Temperature = mean(Temperature),\n      Annual_Humidity = mean(Humidity),\n      Annual_Precipitation = sum(Precipitation),\n      Average_Pressure = mean(Surface_Pressure),\n      Average_Wind_10Meter = mean(Wind_10_meter),\n      Average_Wind_50Meter = mean(Wind_50_meter))\n\n\nError in empty(.data): object 'Weather_region_Amherst' not found\n\n\nCode\n# Creating the regions based on longitude and latitude \nWeather_region_Amherst <- Means_variables %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in is.data.frame(.data): object 'Means_variables' not found\n\n\nCode\ndata <- merge(Means_variables, Economy)\n\n\nError in merge(Means_variables, Economy): object 'Means_variables' not found\n\n\nCode\ndata <- data %>%\n  distinct(Annual_Temperature, Annual_Humidity, Annual_Precipitation, Year_Money_Millions)\n\n\nError in UseMethod(\"distinct\"): no applicable method for 'distinct' applied to an object of class \"function\"\n\n\nCode\ncor(data$Year_Money_Millions ~ data$Annual_Temperature)\n\n\nError in cor(data$Year_Money_Millions ~ data$Annual_Temperature): supply both 'x' and 'y' or a matrix-like 'x'\n\n\nCode\ntest <- lm(Year_Money_Millions ~ log(Annual_Temperature), data = data)\n\n\nError in model.frame.default(formula = Year_Money_Millions ~ log(Annual_Temperature), : 'data' must be a data.frame, environment, or list\n\n\nCode\nsummary(test)\n\n\nError in summary(test): object 'test' not found\n\n\nCode\nplot(test)\n\n\nError in plot(test): object 'test' not found\n\n\nCode\ncorrplot(data)\n\n\nError in corrplot(data): could not find function \"corrplot\"\n\n\nCode\nplot(Year_Money_Millions ~ Annual_Temperature + Annual_Humidity + Annual_Precipitation, \n     data = data, \n     col = \"steelblue\", \n     pch = 20, \n     xlim = c(0, 100),\n     cex.main = 0.9,\n     main = \"Percentage of English language learners\")\n\n\nError in FUN(X[[i]], ...): invalid 'envir' argument of type 'closure'\n\n\nCode\n# Creating weather types\n\n# Creating the 4 regions \n\n\n\n\nFlorida\n\n\nCode\nregion <- Florida %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nFlorida <- tidy_function(region)\nFlorida <- Florida %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(Florida)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:88536       Min.   :25.25   Min.   :-85.75  \n 1st Qu.:1997   Class :character   1st Qu.:26.75   1st Qu.:-83.75  \n Median :2005   Mode  :character   Median :28.50   Median :-81.75  \n Mean   :2005                      Mean   :28.50   Mean   :-81.75  \n 3rd Qu.:2013                      3rd Qu.:30.25   3rd Qu.:-79.75  \n Max.   :2020                      Max.   :31.75   Max.   :-77.75  \n  Temperature       Humidity     Precipitation    Surface_Pressure\n Min.   : 3.76   Min.   :51.16   Min.   : 0.030   Min.   : 99.85  \n 1st Qu.:20.46   1st Qu.:74.69   1st Qu.: 1.760   1st Qu.:101.45  \n Median :23.99   Median :77.32   Median : 3.050   Median :101.64  \n Mean   :23.20   Mean   :77.26   Mean   : 3.529   Mean   :101.61  \n 3rd Qu.:27.24   3rd Qu.:80.12   3rd Qu.: 4.760   3rd Qu.:101.82  \n Max.   :30.83   Max.   :91.78   Max.   :22.510   Max.   :102.48  \n Wind_10_meter  Wind_50_meter   \n Min.   :1.24   Min.   : 2.520  \n 1st Qu.:3.51   1st Qu.: 4.660  \n Median :4.69   Median : 5.660  \n Mean   :4.74   Mean   : 5.719  \n 3rd Qu.:5.95   3rd Qu.: 6.690  \n Max.   :9.74   Max.   :10.840  \n\n\nCode\nview_Florida <- Florida %>%\n  slice(1:10)\n\nkable(view_Florida, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Florida Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nFlorida Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    25.25 \n    -77.75 \n    24.69 \n    74.40 \n    1.29 \n    101.74 \n    6.76 \n    7.59 \n  \n  \n    1990 \n    JAN \n    25.25 \n    -77.75 \n    24.01 \n    76.50 \n    0.49 \n    101.99 \n    5.74 \n    6.45 \n  \n  \n    1990 \n    FEB \n    25.25 \n    -77.75 \n    23.83 \n    73.11 \n    0.72 \n    102.09 \n    7.30 \n    8.20 \n  \n  \n    1990 \n    MAR \n    25.25 \n    -77.75 \n    23.44 \n    75.67 \n    1.04 \n    102.02 \n    7.12 \n    7.99 \n  \n  \n    1990 \n    APR \n    25.25 \n    -77.75 \n    24.18 \n    72.43 \n    0.82 \n    101.72 \n    5.53 \n    6.10 \n  \n  \n    1990 \n    MAY \n    25.25 \n    -77.75 \n    25.89 \n    78.25 \n    2.60 \n    101.65 \n    5.60 \n    6.24 \n  \n  \n    1990 \n    JUN \n    25.25 \n    -77.75 \n    27.27 \n    77.06 \n    2.91 \n    101.74 \n    4.18 \n    4.62 \n  \n  \n    1990 \n    JUL \n    25.25 \n    -77.75 \n    28.25 \n    76.18 \n    2.41 \n    101.74 \n    4.47 \n    4.93 \n  \n  \n    1990 \n    AUG \n    25.25 \n    -77.75 \n    28.77 \n    74.23 \n    5.13 \n    101.71 \n    3.16 \n    3.44 \n  \n  \n    1990 \n    SEP \n    25.25 \n    -77.75 \n    28.55 \n    74.47 \n    2.09 \n    101.51 \n    3.85 \n    4.16 \n  \n\n\n\n\n\nCode\nWeather_region_Florida <- Florida %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Florida\n\n\n\n\n  \n\n\n\n\n\nIllinois\n\n\nCode\nregion <- Illinois %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\n\nIllinois <- tidy_function(region)\n\nIllinois <- Illinois %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(Illinois)\n\n\n      Year         Month              Latitude       Longitude      \n Min.   :1990   Length:66960       Min.   :43.25   Min.   :-102.75  \n 1st Qu.:1997   Class :character   1st Qu.:44.25   1st Qu.:-100.75  \n Median :2005   Mode  :character   Median :45.50   Median : -98.50  \n Mean   :2005                      Mean   :45.50   Mean   : -98.50  \n 3rd Qu.:2013                      3rd Qu.:46.75   3rd Qu.: -96.25  \n Max.   :2020                      Max.   :47.75   Max.   : -94.25  \n  Temperature         Humidity     Precipitation    Surface_Pressure\n Min.   :-22.850   Min.   :31.92   Min.   : 0.010   Min.   :89.54   \n 1st Qu.: -2.780   1st Qu.:61.57   1st Qu.: 0.570   1st Qu.:94.45   \n Median :  7.450   Median :68.34   Median : 1.220   Median :95.83   \n Mean   :  7.186   Mean   :69.63   Mean   : 1.615   Mean   :95.46   \n 3rd Qu.: 18.120   3rd Qu.:77.34   3rd Qu.: 2.340   3rd Qu.:96.79   \n Max.   : 30.490   Max.   :97.99   Max.   :10.000   Max.   :99.02   \n Wind_10_meter   Wind_50_meter   \n Min.   :3.130   Min.   : 4.740  \n 1st Qu.:4.690   1st Qu.: 6.660  \n Median :5.130   Median : 7.180  \n Mean   :5.104   Mean   : 7.155  \n 3rd Qu.:5.520   3rd Qu.: 7.650  \n Max.   :7.600   Max.   :10.240  \n\n\nCode\nview_Illinois <- Illinois %>%\n  slice(1:10)\n\nkable(view_Illinois, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Illinois Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nIllinois Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    43.25 \n    -100.25 \n    4.22 \n    58.51 \n    0.38 \n    93.25 \n    5.64 \n    8.36 \n  \n  \n    1990 \n    JAN \n    43.25 \n    -100.25 \n    -0.49 \n    64.46 \n    0.05 \n    93.03 \n    6.12 \n    8.84 \n  \n  \n    1990 \n    FEB \n    43.25 \n    -100.25 \n    -1.83 \n    59.56 \n    0.35 \n    93.50 \n    5.79 \n    7.96 \n  \n  \n    1990 \n    MAR \n    43.25 \n    -100.25 \n    2.89 \n    63.46 \n    1.46 \n    93.49 \n    5.29 \n    7.35 \n  \n  \n    1990 \n    APR \n    43.25 \n    -100.25 \n    8.98 \n    52.07 \n    1.52 \n    93.29 \n    5.24 \n    7.18 \n  \n  \n    1990 \n    MAY \n    43.25 \n    -100.25 \n    13.64 \n    62.47 \n    4.21 \n    93.15 \n    5.01 \n    6.97 \n  \n  \n    1990 \n    JUN \n    43.25 \n    -100.25 \n    22.23 \n    54.08 \n    2.60 \n    93.06 \n    5.26 \n    7.45 \n  \n  \n    1990 \n    JUL \n    43.25 \n    -100.25 \n    23.62 \n    53.65 \n    2.88 \n    93.46 \n    4.07 \n    5.88 \n  \n  \n    1990 \n    AUG \n    43.25 \n    -100.25 \n    24.19 \n    55.66 \n    1.93 \n    93.42 \n    3.73 \n    5.50 \n  \n  \n    1990 \n    SEP \n    43.25 \n    -100.25 \n    20.72 \n    43.86 \n    0.50 \n    93.55 \n    4.70 \n    6.89 \n  \n\n\n\n\n\nCode\nWeather_region_Illinois <- Illinois %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Illinois\n\n\n\n\n  \n\n\n\n\n\nMiddle\n\n\nCode\nregion <- Middle %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nMiddle <- tidy_function(region)\n\nMiddle <- Middle %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(Middle)\n\n\n      Year         Month              Latitude       Longitude      \n Min.   :1990   Length:77748       Min.   :38.25   Min.   :-102.75  \n 1st Qu.:1997   Class :character   1st Qu.:39.25   1st Qu.:-100.75  \n Median :2005   Mode  :character   Median :40.75   Median : -98.25  \n Mean   :2005                      Mean   :40.75   Mean   : -98.25  \n 3rd Qu.:2013                      3rd Qu.:42.25   3rd Qu.: -95.75  \n Max.   :2020                      Max.   :43.25   Max.   : -93.75  \n  Temperature        Humidity     Precipitation    Surface_Pressure\n Min.   :-14.92   Min.   :32.96   Min.   : 0.000   Min.   :85.57   \n 1st Qu.:  1.38   1st Qu.:58.52   1st Qu.: 0.690   1st Qu.:91.58   \n Median : 10.92   Median :66.34   Median : 1.540   Median :95.39   \n Mean   : 10.92   Mean   :66.23   Mean   : 1.949   Mean   :94.30   \n 3rd Qu.: 20.83   3rd Qu.:74.14   3rd Qu.: 2.840   3rd Qu.:97.13   \n Max.   : 32.89   Max.   :96.56   Max.   :15.870   Max.   :99.52   \n Wind_10_meter   Wind_50_meter   \n Min.   :2.440   Min.   : 3.890  \n 1st Qu.:4.530   1st Qu.: 6.550  \n Median :5.030   Median : 7.160  \n Mean   :4.994   Mean   : 7.111  \n 3rd Qu.:5.480   3rd Qu.: 7.710  \n Max.   :7.450   Max.   :10.050  \n\n\nCode\nview_Middle <- Middle %>%\n  slice(1:10)\n\nkable(view_Middle, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Middle Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nMiddle Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    38.25 \n    -100.25 \n    8.05 \n    51.61 \n    0.62 \n    92.47 \n    5.22 \n    7.63 \n  \n  \n    1990 \n    JAN \n    38.25 \n    -100.25 \n    1.32 \n    60.67 \n    0.99 \n    92.31 \n    5.20 \n    7.99 \n  \n  \n    1990 \n    FEB \n    38.25 \n    -100.25 \n    2.00 \n    56.02 \n    1.05 \n    92.53 \n    5.04 \n    7.06 \n  \n  \n    1990 \n    MAR \n    38.25 \n    -100.25 \n    7.51 \n    59.24 \n    1.22 \n    92.45 \n    5.05 \n    7.07 \n  \n  \n    1990 \n    APR \n    38.25 \n    -100.25 \n    12.46 \n    55.89 \n    2.66 \n    92.34 \n    5.83 \n    7.93 \n  \n  \n    1990 \n    MAY \n    38.25 \n    -100.25 \n    16.73 \n    59.86 \n    3.97 \n    92.05 \n    5.70 \n    7.80 \n  \n  \n    1990 \n    JUN \n    38.25 \n    -100.25 \n    26.79 \n    47.33 \n    1.43 \n    92.11 \n    6.06 \n    8.42 \n  \n  \n    1990 \n    JUL \n    38.25 \n    -100.25 \n    26.56 \n    50.47 \n    3.16 \n    92.47 \n    5.35 \n    7.31 \n  \n  \n    1990 \n    AUG \n    38.25 \n    -100.25 \n    26.06 \n    52.42 \n    1.77 \n    92.52 \n    4.82 \n    6.97 \n  \n  \n    1990 \n    SEP \n    38.25 \n    -100.25 \n    22.98 \n    47.95 \n    1.43 \n    92.63 \n    4.49 \n    6.54 \n  \n\n\n\n\n\nCode\nWeather_region_Middle <- Middle %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Middle\n\n\n\n\n  \n\n\n\n\n\nNew Mexico\n\n\nCode\nregion <- Newmexico %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nNew_Mexico <- tidy_function(region)\n\nNew_Mexico <- New_Mexico %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(New_Mexico)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:81840       Min.   :32.75   Min.   :-112.8  \n 1st Qu.:1997   Class :character   1st Qu.:33.75   1st Qu.:-110.4  \n Median :2005   Mode  :character   Median :35.25   Median :-108.0  \n Mean   :2005                      Mean   :35.25   Mean   :-108.0  \n 3rd Qu.:2013                      3rd Qu.:36.75   3rd Qu.:-105.6  \n Max.   :2020                      Max.   :37.75   Max.   :-103.2  \n  Temperature        Humidity     Precipitation    Surface_Pressure\n Min.   :-14.10   Min.   :11.31   Min.   :0.0000   Min.   :69.19   \n 1st Qu.:  4.54   1st Qu.:38.12   1st Qu.:0.3000   1st Qu.:79.63   \n Median : 12.08   Median :46.83   Median :0.7200   Median :82.13   \n Mean   : 12.43   Mean   :46.47   Mean   :0.9725   Mean   :82.40   \n 3rd Qu.: 20.55   3rd Qu.:54.76   3rd Qu.:1.3900   3rd Qu.:84.65   \n Max.   : 36.19   Max.   :95.05   Max.   :8.0500   Max.   :97.50   \n Wind_10_meter   Wind_50_meter   \n Min.   :1.630   Min.   : 2.530  \n 1st Qu.:3.240   1st Qu.: 4.750  \n Median :3.770   Median : 5.510  \n Mean   :3.879   Mean   : 5.638  \n 3rd Qu.:4.460   3rd Qu.: 6.460  \n Max.   :7.730   Max.   :10.510  \n\n\nCode\nview_Newmexico <- New_Mexico %>%\n  slice(1:10)\n\nkable(view_Newmexico, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nAmherst Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    32.75 \n    -103.25 \n    10.38 \n    53.91 \n    0.81 \n    89.29 \n    4.55 \n    6.89 \n  \n  \n    1990 \n    JAN \n    32.75 \n    -103.25 \n    5.43 \n    44.72 \n    0.42 \n    89.19 \n    5.05 \n    7.59 \n  \n  \n    1990 \n    FEB \n    32.75 \n    -103.25 \n    8.46 \n    41.76 \n    0.68 \n    89.11 \n    5.54 \n    7.98 \n  \n  \n    1990 \n    MAR \n    32.75 \n    -103.25 \n    11.39 \n    47.90 \n    0.83 \n    89.07 \n    4.88 \n    6.90 \n  \n  \n    1990 \n    APR \n    32.75 \n    -103.25 \n    17.02 \n    45.33 \n    0.92 \n    88.92 \n    5.27 \n    7.37 \n  \n  \n    1990 \n    MAY \n    32.75 \n    -103.25 \n    21.71 \n    32.06 \n    0.35 \n    88.77 \n    5.65 \n    7.89 \n  \n  \n    1990 \n    JUN \n    32.75 \n    -103.25 \n    30.18 \n    28.31 \n    0.07 \n    88.93 \n    5.54 \n    7.52 \n  \n  \n    1990 \n    JUL \n    32.75 \n    -103.25 \n    25.56 \n    52.40 \n    2.68 \n    89.26 \n    4.55 \n    6.00 \n  \n  \n    1990 \n    AUG \n    32.75 \n    -103.25 \n    25.00 \n    53.48 \n    1.68 \n    89.34 \n    3.80 \n    5.52 \n  \n  \n    1990 \n    SEP \n    32.75 \n    -103.25 \n    22.76 \n    57.93 \n    2.12 \n    89.33 \n    3.37 \n    4.93 \n  \n\n\n\n\n\nCode\nWeather_region_Newmexico <- New_Mexico %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Newmexico\n\n\n\n\n  \n\n\n\n\n\nNorth\n\n\nCode\nregion <- North %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nNorth <- tidy_function(region)\n\nNorth <- North %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(North)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:74400       Min.   :43.25   Min.   :-113.2  \n 1st Qu.:1997   Class :character   1st Qu.:44.25   1st Qu.:-110.9  \n Median :2005   Mode  :character   Median :45.50   Median :-108.5  \n Mean   :2005                      Mean   :45.50   Mean   :-108.5  \n 3rd Qu.:2013                      3rd Qu.:46.75   3rd Qu.:-106.1  \n Max.   :2020                      Max.   :47.75   Max.   :-103.8  \n  Temperature         Humidity     Precipitation   Surface_Pressure\n Min.   :-18.250   Min.   :21.78   Min.   :0.010   Min.   :72.47   \n 1st Qu.: -2.540   1st Qu.:53.77   1st Qu.:0.620   1st Qu.:81.50   \n Median :  5.350   Median :63.19   Median :1.090   Median :85.36   \n Mean   :  6.032   Mean   :64.19   Mean   :1.318   Mean   :85.14   \n 3rd Qu.: 14.840   3rd Qu.:73.32   3rd Qu.:1.780   3rd Qu.:89.66   \n Max.   : 28.130   Max.   :99.59   Max.   :8.280   Max.   :94.15   \n Wind_10_meter   Wind_50_meter   \n Min.   :1.520   Min.   : 2.780  \n 1st Qu.:3.180   1st Qu.: 4.880  \n Median :4.080   Median : 5.950  \n Mean   :4.042   Mean   : 5.959  \n 3rd Qu.:4.860   3rd Qu.: 6.960  \n Max.   :8.080   Max.   :10.790  \n\n\nCode\nview_North <- North %>%\n  slice(1:10)\n\nkable(view_North, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Northern Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nNorthern Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    43.25 \n    -103.75 \n    3.48 \n    54.59 \n    0.69 \n    87.57 \n    5.30 \n    7.95 \n  \n  \n    1990 \n    JAN \n    43.25 \n    -103.75 \n    -1.23 \n    64.91 \n    0.13 \n    87.29 \n    5.90 \n    8.62 \n  \n  \n    1990 \n    FEB \n    43.25 \n    -103.75 \n    -2.22 \n    63.34 \n    0.49 \n    87.60 \n    4.96 \n    7.16 \n  \n  \n    1990 \n    MAR \n    43.25 \n    -103.75 \n    1.79 \n    62.66 \n    1.24 \n    87.66 \n    4.54 \n    6.50 \n  \n  \n    1990 \n    APR \n    43.25 \n    -103.75 \n    7.32 \n    57.72 \n    1.78 \n    87.56 \n    4.28 \n    6.03 \n  \n  \n    1990 \n    MAY \n    43.25 \n    -103.75 \n    11.66 \n    59.12 \n    2.78 \n    87.45 \n    4.49 \n    6.32 \n  \n  \n    1990 \n    JUN \n    43.25 \n    -103.75 \n    20.12 \n    47.00 \n    1.33 \n    87.55 \n    4.55 \n    6.61 \n  \n  \n    1990 \n    JUL \n    43.25 \n    -103.75 \n    22.43 \n    47.81 \n    2.41 \n    87.90 \n    3.38 \n    5.03 \n  \n  \n    1990 \n    AUG \n    43.25 \n    -103.75 \n    22.72 \n    47.12 \n    1.01 \n    87.90 \n    3.29 \n    4.93 \n  \n  \n    1990 \n    SEP \n    43.25 \n    -103.75 \n    18.85 \n    45.72 \n    1.11 \n    88.01 \n    3.91 \n    5.89 \n  \n\n\n\n\n\nCode\nWeather_region_North <- North %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_North\n\n\n\n\n  \n\n\n\n\n\nSouth\n\n\nCode\nregion <- South %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nSouth <- tidy_function(region)\n\nSouth <- South %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(South)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:84816       Min.   :32.25   Min.   :-91.25  \n 1st Qu.:1997   Class :character   1st Qu.:33.62   1st Qu.:-89.25  \n Median :2005   Mode  :character   Median :35.00   Median :-86.75  \n Mean   :2005                      Mean   :35.00   Mean   :-86.75  \n 3rd Qu.:2013                      3rd Qu.:36.38   3rd Qu.:-84.25  \n Max.   :2020                      Max.   :37.75   Max.   :-82.25  \n  Temperature       Humidity     Precipitation    Surface_Pressure\n Min.   :-5.29   Min.   :36.84   Min.   : 0.010   Min.   : 92.41  \n 1st Qu.: 8.10   1st Qu.:72.24   1st Qu.: 2.380   1st Qu.: 98.34  \n Median :15.73   Median :78.50   Median : 3.340   Median : 99.68  \n Mean   :15.46   Mean   :76.75   Mean   : 3.612   Mean   : 99.15  \n 3rd Qu.:23.33   3rd Qu.:82.34   3rd Qu.: 4.570   3rd Qu.:100.43  \n Max.   :32.63   Max.   :92.29   Max.   :16.850   Max.   :102.22  \n Wind_10_meter   Wind_50_meter  \n Min.   :1.140   Min.   :2.360  \n 1st Qu.:1.980   1st Qu.:3.880  \n Median :2.430   Median :4.520  \n Mean   :2.605   Mean   :4.626  \n 3rd Qu.:3.100   3rd Qu.:5.260  \n Max.   :6.430   Max.   :9.270  \n\n\nCode\nview_South <- South %>%\n  slice(1:10)\n\nkable(view_South, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Southern Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nSouthern Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    32.25 \n    -82.25 \n    13.54 \n    79.79 \n    1.68 \n    101.36 \n    3.13 \n    5.15 \n  \n  \n    1990 \n    JAN \n    32.25 \n    -82.25 \n    10.43 \n    86.30 \n    4.15 \n    101.39 \n    3.21 \n    5.16 \n  \n  \n    1990 \n    FEB \n    32.25 \n    -82.25 \n    12.68 \n    85.29 \n    3.16 \n    101.49 \n    4.03 \n    6.34 \n  \n  \n    1990 \n    MAR \n    32.25 \n    -82.25 \n    15.02 \n    82.99 \n    2.07 \n    101.50 \n    3.30 \n    5.19 \n  \n  \n    1990 \n    APR \n    32.25 \n    -82.25 \n    17.33 \n    73.48 \n    1.24 \n    101.13 \n    3.53 \n    5.58 \n  \n  \n    1990 \n    MAY \n    32.25 \n    -82.25 \n    23.49 \n    64.33 \n    2.01 \n    100.85 \n    3.49 \n    5.38 \n  \n  \n    1990 \n    JUN \n    32.25 \n    -82.25 \n    28.00 \n    58.58 \n    1.52 \n    100.91 \n    3.20 \n    4.89 \n  \n  \n    1990 \n    JUL \n    32.25 \n    -82.25 \n    29.27 \n    61.62 \n    3.32 \n    100.95 \n    3.30 \n    5.03 \n  \n  \n    1990 \n    AUG \n    32.25 \n    -82.25 \n    28.28 \n    68.83 \n    4.06 \n    100.90 \n    2.13 \n    3.45 \n  \n  \n    1990 \n    SEP \n    32.25 \n    -82.25 \n    26.26 \n    58.85 \n    0.97 \n    100.96 \n    2.71 \n    4.34 \n  \n\n\n\n\n\nCode\nWeather_region_South <- South %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_South\n\n\n\n\n  \n\n\n\n\n\nSouth California\n\n\nCode\nregion <- SouthCali %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nSouth_California <- tidy_function(region)\n\nSouth_California <- South_California %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(South_California)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:78120       Min.   :33.75   Min.   :-121.8  \n 1st Qu.:1997   Class :character   1st Qu.:35.25   1st Qu.:-120.2  \n Median :2005   Mode  :character   Median :37.25   Median :-118.5  \n Mean   :2005                      Mean   :37.25   Mean   :-118.5  \n 3rd Qu.:2013                      3rd Qu.:39.25   3rd Qu.:-116.8  \n Max.   :2020                      Max.   :40.75   Max.   :-115.2  \n  Temperature       Humidity     Precipitation     Surface_Pressure\n Min.   :-8.91   Min.   :11.89   Min.   : 0.0000   Min.   : 73.88  \n 1st Qu.: 7.29   1st Qu.:33.86   1st Qu.: 0.1100   1st Qu.: 82.98  \n Median :13.43   Median :49.53   Median : 0.4100   Median : 89.04  \n Mean   :13.38   Mean   :50.99   Mean   : 0.9714   Mean   : 89.33  \n 3rd Qu.:19.72   3rd Qu.:66.72   3rd Qu.: 1.1300   3rd Qu.: 96.48  \n Max.   :35.71   Max.   :97.56   Max.   :19.2300   Max.   :102.32  \n Wind_10_meter  Wind_50_meter   \n Min.   :1.65   Min.   : 2.360  \n 1st Qu.:2.80   1st Qu.: 3.900  \n Median :3.26   Median : 4.480  \n Mean   :3.44   Mean   : 4.632  \n 3rd Qu.:3.79   3rd Qu.: 5.150  \n Max.   :9.93   Max.   :11.320  \n\n\nCode\nview_SouthCali <- South_California %>%\n  slice(1:10)\n\nkable(view_SouthCali, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"South California Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nSouth California Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    33.75 \n    -115.25 \n    15.08 \n    28.55 \n    0.05 \n    97.24 \n    4.21 \n    6.12 \n  \n  \n    1990 \n    JAN \n    33.75 \n    -115.25 \n    9.52 \n    44.28 \n    0.36 \n    97.38 \n    3.46 \n    5.05 \n  \n  \n    1990 \n    FEB \n    33.75 \n    -115.25 \n    10.93 \n    38.61 \n    0.16 \n    97.32 \n    3.84 \n    5.48 \n  \n  \n    1990 \n    MAR \n    33.75 \n    -115.25 \n    16.89 \n    31.98 \n    0.09 \n    96.99 \n    3.46 \n    4.86 \n  \n  \n    1990 \n    APR \n    33.75 \n    -115.25 \n    21.34 \n    34.41 \n    0.10 \n    96.76 \n    3.72 \n    5.14 \n  \n  \n    1990 \n    MAY \n    33.75 \n    -115.25 \n    23.52 \n    27.32 \n    0.09 \n    96.58 \n    4.18 \n    5.66 \n  \n  \n    1990 \n    JUN \n    33.75 \n    -115.25 \n    30.54 \n    21.35 \n    0.15 \n    96.54 \n    3.69 \n    5.03 \n  \n  \n    1990 \n    JUL \n    33.75 \n    -115.25 \n    33.59 \n    28.24 \n    0.32 \n    96.62 \n    3.58 \n    4.67 \n  \n  \n    1990 \n    AUG \n    33.75 \n    -115.25 \n    30.76 \n    31.84 \n    0.59 \n    96.78 \n    3.12 \n    4.21 \n  \n  \n    1990 \n    SEP \n    33.75 \n    -115.25 \n    28.74 \n    31.53 \n    0.38 \n    96.73 \n    2.85 \n    3.96 \n  \n\n\n\n\n\nCode\nWeather_region_SouthCali <- South_California %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_SouthCali\n\n\n\n\n  \n\n\n\n\n\nTexas\n\n\nCode\nregion <- Texas %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nTexas <- tidy_function(region)\n\nTexas <- Texas %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(Texas)\n\n\n      Year         Month              Latitude       Longitude      \n Min.   :1990   Length:104160      Min.   :29.25   Min.   :-103.75  \n 1st Qu.:1997   Class :character   1st Qu.:30.75   1st Qu.:-101.38  \n Median :2005   Mode  :character   Median :32.50   Median : -99.00  \n Mean   :2005                      Mean   :32.50   Mean   : -99.00  \n 3rd Qu.:2013                      3rd Qu.:34.25   3rd Qu.: -96.62  \n Max.   :2020                      Max.   :35.75   Max.   : -94.25  \n  Temperature       Humidity     Precipitation    Surface_Pressure\n Min.   :-2.62   Min.   :14.00   Min.   : 0.000   Min.   : 84.89  \n 1st Qu.:10.66   1st Qu.:52.22   1st Qu.: 0.750   1st Qu.: 92.21  \n Median :18.42   Median :62.76   Median : 1.710   Median : 96.59  \n Mean   :17.99   Mean   :62.28   Mean   : 2.156   Mean   : 95.64  \n 3rd Qu.:25.80   3rd Qu.:73.30   3rd Qu.: 3.050   3rd Qu.: 99.23  \n Max.   :35.35   Max.   :94.14   Max.   :33.700   Max.   :102.51  \n Wind_10_meter   Wind_50_meter  \n Min.   :1.220   Min.   :2.690  \n 1st Qu.:3.840   1st Qu.:5.670  \n Median :4.470   Median :6.490  \n Mean   :4.348   Mean   :6.387  \n 3rd Qu.:5.010   3rd Qu.:7.200  \n Max.   :7.200   Max.   :9.780  \n\n\nCode\nview_Texas <- Texas %>%\n  slice(1:10)\n\nkable(view_Texas, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Texas Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nTexas Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    29.25 \n    -100.25 \n    16.35 \n    68.49 \n    1.11 \n    97.75 \n    3.91 \n    6.00 \n  \n  \n    1990 \n    JAN \n    29.25 \n    -100.25 \n    11.95 \n    59.48 \n    0.54 \n    97.73 \n    3.83 \n    5.90 \n  \n  \n    1990 \n    FEB \n    29.25 \n    -100.25 \n    14.51 \n    57.53 \n    2.57 \n    97.67 \n    4.33 \n    6.40 \n  \n  \n    1990 \n    MAR \n    29.25 \n    -100.25 \n    17.19 \n    68.09 \n    1.73 \n    97.56 \n    4.81 \n    6.83 \n  \n  \n    1990 \n    APR \n    29.25 \n    -100.25 \n    20.98 \n    69.78 \n    3.48 \n    97.30 \n    4.92 \n    6.99 \n  \n  \n    1990 \n    MAY \n    29.25 \n    -100.25 \n    25.52 \n    63.69 \n    2.56 \n    96.99 \n    4.54 \n    6.46 \n  \n  \n    1990 \n    JUN \n    29.25 \n    -100.25 \n    31.81 \n    45.02 \n    0.03 \n    97.17 \n    5.94 \n    7.84 \n  \n  \n    1990 \n    JUL \n    29.25 \n    -100.25 \n    27.49 \n    65.36 \n    6.48 \n    97.45 \n    5.04 \n    6.84 \n  \n  \n    1990 \n    AUG \n    29.25 \n    -100.25 \n    28.14 \n    61.81 \n    0.96 \n    97.49 \n    4.19 \n    6.15 \n  \n  \n    1990 \n    SEP \n    29.25 \n    -100.25 \n    25.36 \n    70.53 \n    3.77 \n    97.48 \n    3.30 \n    4.91 \n  \n\n\n\n\n\nCode\nWeather_region_Texas <- Texas %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Texas\n\n\n\n\n  \n\n\n\n\n\nWashington\n\n\nCode\nregion <- Washington %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nWashington <- tidy_function(region)\n\nWashington <- Washington %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(Washington)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:82212       Min.   :41.75   Min.   :-122.8  \n 1st Qu.:1997   Class :character   1st Qu.:43.25   1st Qu.:-120.8  \n Median :2005   Mode  :character   Median :44.75   Median :-118.8  \n Mean   :2005                      Mean   :44.75   Mean   :-118.8  \n 3rd Qu.:2013                      3rd Qu.:46.25   3rd Qu.:-116.8  \n Max.   :2020                      Max.   :47.75   Max.   :-114.8  \n  Temperature         Humidity     Precipitation   Surface_Pressure\n Min.   :-14.040   Min.   :20.81   Min.   : 0.00   Min.   :76.13   \n 1st Qu.:  0.670   1st Qu.:55.59   1st Qu.: 0.63   1st Qu.:84.95   \n Median :  6.970   Median :70.02   Median : 1.35   Median :87.59   \n Mean   :  7.653   Mean   :68.57   Mean   : 1.90   Mean   :88.43   \n 3rd Qu.: 14.840   3rd Qu.:82.60   3rd Qu.: 2.46   3rd Qu.:92.25   \n Max.   : 28.950   Max.   :99.83   Max.   :19.67   Max.   :99.34   \n Wind_10_meter   Wind_50_meter  \n Min.   :1.180   Min.   :1.960  \n 1st Qu.:2.160   1st Qu.:3.700  \n Median :2.860   Median :4.420  \n Mean   :2.923   Mean   :4.509  \n 3rd Qu.:3.550   3rd Qu.:5.190  \n Max.   :6.830   Max.   :9.480  \n\n\nCode\nview_Washington <- Washington %>%\n  slice(1:10)\n\nkable(view_Washington, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"Washington Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nWashington Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    41.75 \n    -114.75 \n    0.34 \n    59.24 \n    0.73 \n    81.59 \n    4.81 \n    6.77 \n  \n  \n    1990 \n    JAN \n    41.75 \n    -114.75 \n    -4.07 \n    78.51 \n    1.28 \n    81.30 \n    5.15 \n    7.25 \n  \n  \n    1990 \n    FEB \n    41.75 \n    -114.75 \n    -4.71 \n    70.26 \n    0.56 \n    81.31 \n    4.92 \n    6.56 \n  \n  \n    1990 \n    MAR \n    41.75 \n    -114.75 \n    1.91 \n    61.00 \n    0.90 \n    81.29 \n    4.47 \n    6.01 \n  \n  \n    1990 \n    APR \n    41.75 \n    -114.75 \n    8.08 \n    52.46 \n    1.38 \n    81.25 \n    4.31 \n    5.77 \n  \n  \n    1990 \n    MAY \n    41.75 \n    -114.75 \n    8.88 \n    49.42 \n    1.40 \n    81.13 \n    4.24 \n    5.68 \n  \n  \n    1990 \n    JUN \n    41.75 \n    -114.75 \n    16.01 \n    40.19 \n    0.74 \n    81.46 \n    3.93 \n    5.37 \n  \n  \n    1990 \n    JUL \n    41.75 \n    -114.75 \n    21.67 \n    32.96 \n    0.18 \n    81.65 \n    3.37 \n    4.63 \n  \n  \n    1990 \n    AUG \n    41.75 \n    -114.75 \n    19.79 \n    34.71 \n    0.86 \n    81.70 \n    3.22 \n    4.41 \n  \n  \n    1990 \n    SEP \n    41.75 \n    -114.75 \n    17.79 \n    35.25 \n    0.22 \n    81.74 \n    3.07 \n    4.32 \n  \n\n\n\n\n\nCode\nWeather_region_Washington <- Washington %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_Washington\n\n\n\n\n  \n\n\n\n\n\nWest Virgina\n\n\nCode\nregion <- WestV %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\nWest_Virginia <- tidy_function(region)\n\nWest_Virginia <- West_Virginia %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter)\n\nsummary(West_Virginia)\n\n\n      Year         Month              Latitude       Longitude     \n Min.   :1990   Length:74400       Min.   :38.75   Min.   :-92.25  \n 1st Qu.:1997   Class :character   1st Qu.:39.75   1st Qu.:-89.88  \n Median :2005   Mode  :character   Median :41.00   Median :-87.50  \n Mean   :2005                      Mean   :41.00   Mean   :-87.50  \n 3rd Qu.:2013                      3rd Qu.:42.25   3rd Qu.:-85.12  \n Max.   :2020                      Max.   :43.25   Max.   :-82.75  \n  Temperature        Humidity     Precipitation    Surface_Pressure\n Min.   :-15.01   Min.   :42.91   Min.   : 0.070   Min.   : 96.84  \n 1st Qu.:  1.16   1st Qu.:74.30   1st Qu.: 1.620   1st Qu.: 98.52  \n Median : 10.68   Median :78.66   Median : 2.470   Median : 98.91  \n Mean   : 10.17   Mean   :78.26   Mean   : 2.712   Mean   : 98.89  \n 3rd Qu.: 19.68   3rd Qu.:82.78   3rd Qu.: 3.500   3rd Qu.: 99.27  \n Max.   : 31.10   Max.   :96.86   Max.   :11.770   Max.   :100.75  \n Wind_10_meter   Wind_50_meter  \n Min.   :1.230   Min.   : 2.73  \n 1st Qu.:3.720   1st Qu.: 5.64  \n Median :4.600   Median : 6.69  \n Mean   :4.527   Mean   : 6.55  \n 3rd Qu.:5.280   3rd Qu.: 7.43  \n Max.   :9.630   Max.   :10.79  \n\n\nCode\nview_WestV <- West_Virginia %>%\n  slice(1:10)\n\nkable(view_WestV, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\"), caption = \"West Virginia Data\") %>%\n  kable_styling(font_size = 16)\n\n\n\n\nWest Virginia Data\n \n  \n    Year \n    Month \n    Latitude \n    Longitude \n    Temperature \n    Humidity \n    Precipitation \n    Surface Pressure \n    Wind 10 Meters \n    Wind 50 Meters \n  \n \n\n  \n    1990 \n    NOV \n    38.75 \n    -82.75 \n    7.51 \n    78.61 \n    1.90 \n    99.06 \n    2.12 \n    4.52 \n  \n  \n    1990 \n    JAN \n    38.75 \n    -82.75 \n    2.18 \n    85.99 \n    2.72 \n    98.83 \n    2.79 \n    5.31 \n  \n  \n    1990 \n    FEB \n    38.75 \n    -82.75 \n    3.81 \n    84.92 \n    3.60 \n    99.03 \n    2.87 \n    5.39 \n  \n  \n    1990 \n    MAR \n    38.75 \n    -82.75 \n    7.16 \n    81.56 \n    1.94 \n    99.27 \n    2.28 \n    4.45 \n  \n  \n    1990 \n    APR \n    38.75 \n    -82.75 \n    10.65 \n    76.54 \n    2.76 \n    98.84 \n    2.13 \n    4.24 \n  \n  \n    1990 \n    MAY \n    38.75 \n    -82.75 \n    15.35 \n    80.81 \n    6.58 \n    98.51 \n    2.16 \n    4.49 \n  \n  \n    1990 \n    JUN \n    38.75 \n    -82.75 \n    20.81 \n    80.24 \n    2.98 \n    98.66 \n    1.84 \n    3.98 \n  \n  \n    1990 \n    JUL \n    38.75 \n    -82.75 \n    23.19 \n    77.11 \n    3.71 \n    98.81 \n    1.52 \n    3.40 \n  \n  \n    1990 \n    AUG \n    38.75 \n    -82.75 \n    23.23 \n    68.86 \n    3.53 \n    98.85 \n    1.28 \n    2.81 \n  \n  \n    1990 \n    SEP \n    38.75 \n    -82.75 \n    19.94 \n    70.16 \n    2.86 \n    98.81 \n    1.60 \n    3.48 \n  \n\n\n\n\n\nCode\nWeather_region_WestV <- West_Virginia %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\nWeather_region_WestV\n\n\n\n\n  \n\n\n\n\n\nState Economy data\nThe economic data is pulled from the Bureau of Economic Analysis (Analysis, n.d.). This data is the ins, outs, and the difference between the former two in income by state. The data ranges from 1990 to 2020 and covers every state in the US.\n\n\nCode\n# Reading in economic data\n\nEconomy <- read.csv(\"_data/Economy.csv\")\n\n# Renaming the columns to remove the X\nEconomy <- Economy %>%\n  dplyr::rename('1990' = X1990) %>%\n  dplyr::rename('1991' = X1991) %>%\n  dplyr::rename('1992' = X1992) %>%\n  dplyr::rename('1993' = X1993) %>%\n  dplyr::rename('1994' = X1994) %>%\n  dplyr::rename('1995' = X1995) %>%\n  dplyr::rename('1996' = X1996) %>%\n  dplyr::rename('1997' = X1997) %>%\n  dplyr::rename('1998' = X1998) %>%\n  dplyr::rename('1999' = X1999) %>%\n  dplyr::rename('2000' = X2000) %>%\n  dplyr::rename('2001' = X2001) %>%\n  dplyr::rename('2002' = X2002) %>%\n  dplyr::rename('2003' = X2003) %>%\n  dplyr::rename('2004' = X2004) %>%\n  dplyr::rename('2005' = X2005) %>%\n  dplyr::rename('2006' = X2006) %>%\n  dplyr::rename('2007' = X2007) %>%\n  dplyr::rename('2008' = X2008) %>%\n  dplyr::rename('2009' = X2009) %>%\n  dplyr::rename('2010' = X2010) %>%\n  dplyr::rename('2011' = X2011) %>%\n  dplyr::rename('2012' = X2012) %>%\n  dplyr::rename('2013' = X2013) %>%\n  dplyr::rename('2014' = X2014) %>%\n  dplyr::rename('2015' = X2015) %>%\n  dplyr::rename('2016' = X2016) %>%\n  dplyr::rename('2017' = X2017) %>%\n  dplyr::rename('2018' = X2018) %>%\n  dplyr::rename('2019' = X2019) %>%\n  dplyr::rename('2020' = X2020) \n\n# Pivoting to combine all the years into one column\nEconomy <- Economy %>%\npivot_longer(\n  cols = c('1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'),\n  names_to = \"Year\",\n  values_to = \"Yearly_Fianace\",\n)\n\n## Change from char to numeric\nEconomy$Year <- as.numeric(Economy$Year)\n\n# Changing the finance column to be in millions\nEconomy <- Economy %>%\n  mutate(Year_Money_Millions = Yearly_Fianace/1000)\n\nEconomy <- Economy %>%\n  select(State, Year, Year_Money_Millions, Description) %>%\n  filter(Description == \"Adjustment for residence\")\n\nEconomy <- Economy %>%\n  mutate(Regions = case_when(\n    Latitude >= 23.25 & Latitude <= 40.75 & Longitude >= -95.5 & Longitude <= -70.5 ~ 'Southeast',\n    Latitude >= 25 & Latitude <= 37 & Longitude >= -114.75 & Longitude <= -95.6 ~ 'Southwest',\n    Latitude >= 32.3 & Latitude <= 49 & Longitude >= -124.36 & Longitude <= -102 ~ 'West',\n    Latitude >= 39.7 & Latitude <= 47.4 & Longitude >= -81 & Longitude <= -66 ~ 'Northeast',\n    Latitude >= 35 & Latitude <= 49 & Longitude >= -104 & Longitude <= -80.5 ~ 'Midwest'\n  ))\n\n\nError in eval_tidy(pair$lhs, env = default_env): object 'Latitude' not found"
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html",
    "href": "posts/FinalPt1_KarenKimble.html",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"_data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\nNYC_2021 <- read_csv(\"_data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nWarning: NAs introduced by coercion"
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#research-question",
    "href": "posts/FinalPt1_KarenKimble.html#research-question",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "href": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "href": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nRows: 18,142\nColumns: 36\n$ DBN                            <chr> \"01M015\", \"01M015\", \"01M015\", \"01M015\",…\n$ `School Name`                  <chr> \"P.S. 015 Roberto Clemente\", \"P.S. 015 …\n$ Year                           <chr> \"2014-15\", \"2015-16\", \"2016-17\", \"2017-…\n$ `Total Enrollment`             <dbl> 183, 176, 178, 190, 174, 270, 270, 271,…\n$ `Grade K`                      <dbl> 27, 32, 28, 28, 20, 44, 47, 37, 34, 30,…\n$ `Grade 1`                      <dbl> 47, 33, 33, 32, 33, 40, 43, 46, 38, 39,…\n$ `Grade 2`                      <dbl> 31, 39, 27, 33, 30, 39, 41, 47, 42, 43,…\n$ `Grade 3`                      <dbl> 19, 23, 31, 23, 30, 35, 43, 40, 46, 41,…\n$ `Grade 4`                      <dbl> 17, 17, 24, 31, 20, 40, 35, 43, 42, 44,…\n$ `Grade 5`                      <dbl> 24, 18, 18, 26, 28, 42, 40, 34, 42, 42,…\n$ `Grade 6`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 7`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 8`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 9`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 10`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 11`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 12`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `# Female`                     <dbl> 84, 83, 83, 99, 85, 132, 125, 127, 114,…\n$ `% Female`                     <dbl> 0.459, 0.472, 0.466, 0.521, 0.489, 0.48…\n$ `# Male`                       <dbl> 99, 93, 95, 91, 89, 138, 145, 144, 143,…\n$ `% Male`                       <dbl> 0.541, 0.528, 0.534, 0.479, 0.511, 0.51…\n$ `# Asian`                      <dbl> 8, 9, 14, 20, 24, 30, 27, 24, 23, 14, 2…\n$ `% Asian`                      <dbl> 0.044, 0.051, 0.079, 0.105, 0.138, 0.11…\n$ `# Black`                      <dbl> 65, 57, 51, 52, 48, 47, 55, 51, 49, 52,…\n$ `% Black`                      <dbl> 0.355, 0.324, 0.287, 0.274, 0.276, 0.17…\n$ `# Hispanic`                   <dbl> 107, 105, 105, 110, 95, 158, 169, 180, …\n$ `% Hispanic`                   <dbl> 0.585, 0.597, 0.590, 0.579, 0.546, 0.58…\n$ `# White`                      <dbl> 2, 2, 4, 6, 6, 27, 16, 15, 16, 18, 25, …\n$ `% White`                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ `# Students with Disabilities` <dbl> 64, 60, 51, 49, 38, 82, 82, 88, 90, 92,…\n$ `% Students with Disabilities` <dbl> 0.350, 0.341, 0.287, 0.258, 0.218, 0.30…\n$ `# English Language Learners`  <dbl> 17, 16, 12, 8, 8, 18, 13, 9, 8, 8, 120,…\n$ `% English Language Learners`  <dbl> 0.093, 0.091, 0.067, 0.042, 0.046, 0.06…\n$ `# Poverty`                    <chr> \"169\", \"149\", \"152\", \"161\", \"145\", \"200…\n$ `% Poverty`                    <dbl> 92.3, 84.7, 85.4, 84.7, 83.3, 74.1, 80.…\n$ `Economic Need Index`          <dbl> 0.930, 0.889, 0.882, 0.890, 0.880, 0.60…\n\n\n\n\nCode\nsummary(school_data)\n\n\n     DBN            School Name            Year           Total Enrollment\n Length:18142       Length:18142       Length:18142       Min.   :   7.0  \n Class :character   Class :character   Class :character   1st Qu.: 323.0  \n Mode  :character   Mode  :character   Mode  :character   Median : 477.0  \n                                                          Mean   : 592.3  \n                                                          3rd Qu.: 695.0  \n                                                          Max.   :6040.0  \n                                                                          \n    Grade K          Grade 1          Grade 2          Grade 3      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 32.00   Median : 33.00   Median : 32.00   Median : 28.00  \n Mean   : 44.25   Mean   : 45.79   Mean   : 45.73   Mean   : 45.33  \n 3rd Qu.: 78.00   3rd Qu.: 81.00   3rd Qu.: 82.00   3rd Qu.: 81.00  \n Max.   :393.00   Max.   :383.00   Max.   :349.00   Max.   :369.00  \n                                                                    \n    Grade 4         Grade 5          Grade 6          Grade 7      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.0   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 22.0   Median : 19.00   Median :  0.00   Median :  0.00  \n Mean   : 44.8   Mean   : 44.18   Mean   : 43.15   Mean   : 42.37  \n 3rd Qu.: 80.0   3rd Qu.: 80.00   3rd Qu.: 64.00   3rd Qu.: 62.00  \n Max.   :376.0   Max.   :351.00   Max.   :771.00   Max.   :796.00  \n                                                                   \n    Grade 8          Grade 9           Grade 10         Grade 11      \n Min.   :  0.00   Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n 1st Qu.:  0.00   1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00  \n Median :  0.00   Median :   0.00   Median :   0.0   Median :   0.00  \n Mean   : 41.88   Mean   :  49.34   Mean   :  48.7   Mean   :  39.85  \n 3rd Qu.: 60.00   3rd Qu.:  68.00   3rd Qu.:  69.0   3rd Qu.:  54.00  \n Max.   :784.00   Max.   :1555.00   Max.   :3832.0   Max.   :1529.00  \n                                                                      \n    Grade 12          # Female         % Female          # Male      \n Min.   :   0.00   Min.   :   0.0   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:   0.00   1st Qu.: 146.0   1st Qu.:0.4620   1st Qu.: 163.0  \n Median :   0.00   Median : 232.0   Median :0.4880   Median : 248.0  \n Mean   :  39.58   Mean   : 287.4   Mean   :0.4827   Mean   : 304.9  \n 3rd Qu.:  53.00   3rd Qu.: 347.0   3rd Qu.:0.5130   3rd Qu.: 364.0  \n Max.   :1566.00   Max.   :2405.0   Max.   :1.0000   Max.   :3635.0  \n                                                                     \n     % Male          # Asian           % Asian          # Black      \n Min.   :0.0000   Min.   :   0.00   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:0.4870   1st Qu.:   5.00   1st Qu.:0.0130   1st Qu.:  42.0  \n Median :0.5120   Median :  17.00   Median :0.0400   Median : 105.0  \n Mean   :0.5173   Mean   :  95.38   Mean   :0.1136   Mean   : 154.1  \n 3rd Qu.:0.5380   3rd Qu.:  79.00   3rd Qu.:0.1400   3rd Qu.: 198.0  \n Max.   :1.0000   Max.   :3671.00   Max.   :0.9470   Max.   :1493.0  \n                                                                     \n    % Black        # Hispanic     % Hispanic        # White       \n Min.   :0.000   Min.   :   1   Min.   :0.0060   Min.   :   0.00  \n 1st Qu.:0.083   1st Qu.:  89   1st Qu.:0.1980   1st Qu.:   6.00  \n Median :0.251   Median : 180   Median :0.3990   Median :  15.00  \n Mean   :0.316   Mean   : 241   Mean   :0.4251   Mean   :  87.24  \n 3rd Qu.:0.502   3rd Qu.: 313   3rd Qu.:0.6323   3rd Qu.:  78.00  \n Max.   :0.987   Max.   :2056   Max.   :1.0000   Max.   :3190.00  \n                                                                  \n    % White       # Students with Disabilities % Students with Disabilities\n Min.   :0.0000   Min.   :  0.0                Min.   :0.0000              \n 1st Qu.:0.0140   1st Qu.: 66.0                1st Qu.:0.1570              \n Median :0.0330   Median : 98.0                Median :0.2030              \n Mean   :0.1205   Mean   :121.6                Mean   :0.2295              \n 3rd Qu.:0.1440   3rd Qu.:146.0                3rd Qu.:0.2540              \n Max.   :0.9450   Max.   :925.0                Max.   :1.0000              \n                                                                           \n # English Language Learners % English Language Learners  # Poverty        \n Min.   :   0.0              Min.   :0.0000              Length:18142      \n 1st Qu.:  18.0              1st Qu.:0.0430              Class :character  \n Median :  43.0              Median :0.0950              Mode  :character  \n Mean   :  81.1              Mean   :0.1363                                \n 3rd Qu.: 100.0              3rd Qu.:0.1800                                \n Max.   :1219.0              Max.   :1.0000                                \n                                                                           \n   % Poverty      Economic Need Index\n Min.   :  2.90   Min.   :0.030      \n 1st Qu.: 69.30   1st Qu.:0.579      \n Median : 81.40   Median :0.743      \n Mean   : 75.89   Mean   :0.691      \n 3rd Qu.: 89.90   3rd Qu.:0.846      \n Max.   :100.00   Max.   :0.998      \n                  NA's   :9169       \n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#references",
    "href": "posts/FinalPt1_KarenKimble.html#references",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html",
    "href": "posts/FinalPt2_KarenKimble.html",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\nlibrary(ggplot2)\n\n# Importing datasets\n\nNYC_2019 <- read_csv(\"_data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\nNYC_2021 <- read_csv(\"_data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n# Renaming some columns\n\nschool_data$pct_white <- school_data$'% White'\n\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\n# Creating a new variable of post-Covid (1) and pre-Covid (0)\n\nschool_data$Post_Covid <- ifelse(school_data$Year == \"2020-21\", c(\"1\"), c(\"0\"))"
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#research-question",
    "href": "posts/FinalPt2_KarenKimble.html#research-question",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#hypothesis",
    "href": "posts/FinalPt2_KarenKimble.html#hypothesis",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between pre- and post-Covid than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#descriptive-statistics",
    "href": "posts/FinalPt2_KarenKimble.html#descriptive-statistics",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year, which I transformed into pre-Covid and post-Covid variables\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nRows: 18,142\nColumns: 38\n$ DBN                            <chr> \"01M015\", \"01M015\", \"01M015\", \"01M015\",…\n$ `School Name`                  <chr> \"P.S. 015 Roberto Clemente\", \"P.S. 015 …\n$ Year                           <chr> \"2014-15\", \"2015-16\", \"2016-17\", \"2017-…\n$ `Total Enrollment`             <dbl> 183, 176, 178, 190, 174, 270, 270, 271,…\n$ `Grade K`                      <dbl> 27, 32, 28, 28, 20, 44, 47, 37, 34, 30,…\n$ `Grade 1`                      <dbl> 47, 33, 33, 32, 33, 40, 43, 46, 38, 39,…\n$ `Grade 2`                      <dbl> 31, 39, 27, 33, 30, 39, 41, 47, 42, 43,…\n$ `Grade 3`                      <dbl> 19, 23, 31, 23, 30, 35, 43, 40, 46, 41,…\n$ `Grade 4`                      <dbl> 17, 17, 24, 31, 20, 40, 35, 43, 42, 44,…\n$ `Grade 5`                      <dbl> 24, 18, 18, 26, 28, 42, 40, 34, 42, 42,…\n$ `Grade 6`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 7`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 8`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 9`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 10`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 11`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 12`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `# Female`                     <dbl> 84, 83, 83, 99, 85, 132, 125, 127, 114,…\n$ `% Female`                     <dbl> 0.459, 0.472, 0.466, 0.521, 0.489, 0.48…\n$ `# Male`                       <dbl> 99, 93, 95, 91, 89, 138, 145, 144, 143,…\n$ `% Male`                       <dbl> 0.541, 0.528, 0.534, 0.479, 0.511, 0.51…\n$ `# Asian`                      <dbl> 8, 9, 14, 20, 24, 30, 27, 24, 23, 14, 2…\n$ `% Asian`                      <dbl> 0.044, 0.051, 0.079, 0.105, 0.138, 0.11…\n$ `# Black`                      <dbl> 65, 57, 51, 52, 48, 47, 55, 51, 49, 52,…\n$ `% Black`                      <dbl> 0.355, 0.324, 0.287, 0.274, 0.276, 0.17…\n$ `# Hispanic`                   <dbl> 107, 105, 105, 110, 95, 158, 169, 180, …\n$ `% Hispanic`                   <dbl> 0.585, 0.597, 0.590, 0.579, 0.546, 0.58…\n$ `# White`                      <dbl> 2, 2, 4, 6, 6, 27, 16, 15, 16, 18, 25, …\n$ `% White`                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ `# Students with Disabilities` <dbl> 64, 60, 51, 49, 38, 82, 82, 88, 90, 92,…\n$ `% Students with Disabilities` <dbl> 0.350, 0.341, 0.287, 0.258, 0.218, 0.30…\n$ `# English Language Learners`  <dbl> 17, 16, 12, 8, 8, 18, 13, 9, 8, 8, 120,…\n$ `% English Language Learners`  <dbl> 0.093, 0.091, 0.067, 0.042, 0.046, 0.06…\n$ `# Poverty`                    <chr> \"169\", \"149\", \"152\", \"161\", \"145\", \"200…\n$ `% Poverty`                    <dbl> 92.3, 84.7, 85.4, 84.7, 83.3, 74.1, 80.…\n$ `Economic Need Index`          <dbl> 0.930, 0.889, 0.882, 0.890, 0.880, 0.60…\n$ pct_white                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ Post_Covid                     <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\",…\n\n\n\n\nCode\nsummary(school_data)\n\n\n     DBN            School Name            Year           Total Enrollment\n Length:18142       Length:18142       Length:18142       Min.   :   7.0  \n Class :character   Class :character   Class :character   1st Qu.: 323.0  \n Mode  :character   Mode  :character   Mode  :character   Median : 477.0  \n                                                          Mean   : 592.3  \n                                                          3rd Qu.: 695.0  \n                                                          Max.   :6040.0  \n                                                                          \n    Grade K          Grade 1          Grade 2          Grade 3      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 32.00   Median : 33.00   Median : 32.00   Median : 28.00  \n Mean   : 44.25   Mean   : 45.79   Mean   : 45.73   Mean   : 45.33  \n 3rd Qu.: 78.00   3rd Qu.: 81.00   3rd Qu.: 82.00   3rd Qu.: 81.00  \n Max.   :393.00   Max.   :383.00   Max.   :349.00   Max.   :369.00  \n                                                                    \n    Grade 4         Grade 5          Grade 6          Grade 7      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.0   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 22.0   Median : 19.00   Median :  0.00   Median :  0.00  \n Mean   : 44.8   Mean   : 44.18   Mean   : 43.15   Mean   : 42.37  \n 3rd Qu.: 80.0   3rd Qu.: 80.00   3rd Qu.: 64.00   3rd Qu.: 62.00  \n Max.   :376.0   Max.   :351.00   Max.   :771.00   Max.   :796.00  \n                                                                   \n    Grade 8          Grade 9           Grade 10         Grade 11      \n Min.   :  0.00   Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n 1st Qu.:  0.00   1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00  \n Median :  0.00   Median :   0.00   Median :   0.0   Median :   0.00  \n Mean   : 41.88   Mean   :  49.34   Mean   :  48.7   Mean   :  39.85  \n 3rd Qu.: 60.00   3rd Qu.:  68.00   3rd Qu.:  69.0   3rd Qu.:  54.00  \n Max.   :784.00   Max.   :1555.00   Max.   :3832.0   Max.   :1529.00  \n                                                                      \n    Grade 12          # Female         % Female          # Male      \n Min.   :   0.00   Min.   :   0.0   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:   0.00   1st Qu.: 146.0   1st Qu.:0.4620   1st Qu.: 163.0  \n Median :   0.00   Median : 232.0   Median :0.4880   Median : 248.0  \n Mean   :  39.58   Mean   : 287.4   Mean   :0.4827   Mean   : 304.9  \n 3rd Qu.:  53.00   3rd Qu.: 347.0   3rd Qu.:0.5130   3rd Qu.: 364.0  \n Max.   :1566.00   Max.   :2405.0   Max.   :1.0000   Max.   :3635.0  \n                                                                     \n     % Male          # Asian           % Asian          # Black      \n Min.   :0.0000   Min.   :   0.00   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:0.4870   1st Qu.:   5.00   1st Qu.:0.0130   1st Qu.:  42.0  \n Median :0.5120   Median :  17.00   Median :0.0400   Median : 105.0  \n Mean   :0.5173   Mean   :  95.38   Mean   :0.1136   Mean   : 154.1  \n 3rd Qu.:0.5380   3rd Qu.:  79.00   3rd Qu.:0.1400   3rd Qu.: 198.0  \n Max.   :1.0000   Max.   :3671.00   Max.   :0.9470   Max.   :1493.0  \n                                                                     \n    % Black        # Hispanic     % Hispanic        # White       \n Min.   :0.000   Min.   :   1   Min.   :0.0060   Min.   :   0.00  \n 1st Qu.:0.083   1st Qu.:  89   1st Qu.:0.1980   1st Qu.:   6.00  \n Median :0.251   Median : 180   Median :0.3990   Median :  15.00  \n Mean   :0.316   Mean   : 241   Mean   :0.4251   Mean   :  87.24  \n 3rd Qu.:0.502   3rd Qu.: 313   3rd Qu.:0.6323   3rd Qu.:  78.00  \n Max.   :0.987   Max.   :2056   Max.   :1.0000   Max.   :3190.00  \n                                                                  \n    % White       # Students with Disabilities % Students with Disabilities\n Min.   :0.0000   Min.   :  0.0                Min.   :0.0000              \n 1st Qu.:0.0140   1st Qu.: 66.0                1st Qu.:0.1570              \n Median :0.0330   Median : 98.0                Median :0.2030              \n Mean   :0.1205   Mean   :121.6                Mean   :0.2295              \n 3rd Qu.:0.1440   3rd Qu.:146.0                3rd Qu.:0.2540              \n Max.   :0.9450   Max.   :925.0                Max.   :1.0000              \n                                                                           \n # English Language Learners % English Language Learners  # Poverty        \n Min.   :   0.0              Min.   :0.0000              Length:18142      \n 1st Qu.:  18.0              1st Qu.:0.0430              Class :character  \n Median :  43.0              Median :0.0950              Mode  :character  \n Mean   :  81.1              Mean   :0.1363                                \n 3rd Qu.: 100.0              3rd Qu.:0.1800                                \n Max.   :1219.0              Max.   :1.0000                                \n                                                                           \n   % Poverty      Economic Need Index   pct_white       Post_Covid       \n Min.   :  2.90   Min.   :0.030       Min.   :0.0000   Length:18142      \n 1st Qu.: 69.30   1st Qu.:0.579       1st Qu.:0.0140   Class :character  \n Median : 81.40   Median :0.743       Median :0.0330   Mode  :character  \n Mean   : 75.89   Mean   :0.691       Mean   :0.1205                     \n 3rd Qu.: 89.90   3rd Qu.:0.846       3rd Qu.:0.1440                     \n Max.   :100.00   Max.   :0.998       Max.   :0.9450                     \n                  NA's   :9169                                           \n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#visualizations",
    "href": "posts/FinalPt2_KarenKimble.html#visualizations",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Visualizations",
    "text": "Visualizations\n\n\nCode\npost_white <- filter(school_data, pct_white > 0.5 & Post_Covid == \"1\")\npost_nonwhite <- filter(school_data, pct_white <= 0.5 & Post_Covid == \"1\")\npre_white <- filter(school_data, pct_white > 0.5 & Post_Covid == \"0\")\npre_nonwhite <- filter(school_data, pct_white <= 0.5 & Post_Covid == \"0\")\npost <- filter(school_data, Post_Covid == \"1\")\npre <- filter(school_data, Post_Covid == \"0\")\nwhite <- filter(school_data, pct_white > 0.5)\nnonwhite <- filter(school_data, pct_white <= 0.5)\n\n\n\n\nCode\nboxplot(school_data$`% Poverty` ~ school_data$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Poverty\")\n\n\n\n\n\nFrom the box plot above, we can see that poverty levels between pre- and post-Covid did not change very dramatically for overall students. However, the maximum percentage of poverty did decrease from close to 100% to about 95%. Yet the median percentage level of poverty increased from pre- to post-Covid, though still remained under 90%. There are a large number of outliers below the minimum level of poverty, which is interesting–yet the minimum level of poverty increased between pre- and post-Covid.\n\n\nCode\nboxplot(white$`% Poverty` ~ white$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Majority White Districts in Poverty\",\n        cex.lab = 0.75)\n\n\n\n\n\nThe box plot showing majority white districts’ poverty levels pre- and post-Covid indicate that, while the median poverty level did not change dramatically, the maximum percentage and the third quartile percentage of poverty decreased between pre- and post-Covid. There was a high outlier pre-Covid, but this disappeared in the post-Covid box plot. The majority of observations for predominantly white schools fall in between about 15%-40% of students in poverty, for both pre- and post-Covid levels.\n\n\nCode\nboxplot(nonwhite$`% Poverty` ~ nonwhite$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Majority Non-White Districts in Poverty\",\n        cex.lab = 0.75)\n\n\n\n\n\nThe box plot for majority non-white districts’ poverty levels has a large number of outliers under the poverty level of 50%, similar to the plot for all school districts. The median level of poverty actually increased slightly between pre- and post-Covid years, along with the minimum level of poverty (both in the plot and in outliers). The maximum level of poverty, however, decreased more between these years. A majority of obersvations for both pre- and post-Covid are between 70%-90% of students in poverty for majority non-white districts.\n\n\nCode\nggplot(school_data, aes(x=`% Poverty`)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(`% Poverty`)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nFrom the histogram above, we can see that the mean percentage of poverty for all NYC school districts is just above 75%. Most observations are above the mean, with very few school districts below 50% poverty levels. This is most likely why there were so many outliers in the box plots for all school districts and majority non-white school districts.\n\n\nCode\nggplot(pre, aes(x=`% Poverty`)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"gray\") +\n  labs(title=\"School Districts Pre-Covid\") +\n  geom_vline(aes(xintercept=mean(`% Poverty`)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nCode\nggplot(post, aes(x=`% Poverty`)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"light gray\") +\n  labs(title=\"School Districts Post-Covid\") +\n  geom_vline(aes(xintercept=mean(`% Poverty`)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nThe two different plots above show that a alightly larger share of districts post-Covid had poverty levels above the mean. The observations are very skewed to the left in all three plots, however."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#hypothesis-testing",
    "href": "posts/FinalPt2_KarenKimble.html#hypothesis-testing",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nResponse variables: Percentage of students in poverty\nExplanatory variables: Post- or pre-Covid, whether the school is majority or minority white\nI’m including the interaction term of school demographics because I think that this does play a major role in how Covid effects were mitigated or compounded. Schools with more white students may have received more support during Covid, whether directly to the school or to the students themselves.\nNull Hypothesis: The percentage of students in poverty is the same both pre- and post-Covid for both majority and minority white school districts.\nAlternative Hypothesis: The percentage of students in poverty is higher post-Covid than pre-Covid, and this effect will be more drastic in minority white school districts than in majority white school districts.\n\n\nCode\nmean(post_white$'% Poverty') - mean(pre_white$'% Poverty')\n\n\n[1] -0.9168508\n\n\nCode\nmean(post_nonwhite$'% Poverty') - mean(pre_nonwhite$'% Poverty')\n\n\n[1] 0.4610488\n\n\nCode\nmean(post$'% Poverty') - mean(pre$'% Poverty')\n\n\n[1] 0.8529465\n\n\nBefore conducting the hypothesis test, we already see that the differences between pre- and post-Covid poverty levels are very different for majority white and majorty nonwhite schools. Majority white schools actually had a decrease in poverty of 0.91% between pre-Covid and post-Covid school years. Yet majority nonwhite schools saw poverty increase between pre- and post-Covid by 0.46%. All school districts saw an overall 0.85% poverty increase in this time period. However, these numbers could be skewed since there is only one post-Covid year while there are a lot of years included in the data-set pre-Covid.\n\n\nCode\n# Hypothesis Test\n\nmodel <- lm(school_data$'% Poverty' ~ school_data$Post_Covid + school_data$pct_white + school_data$Post_Covid * school_data$pct_white)\nsummary(model)\n\n\n\nCall:\nlm(formula = school_data$\"% Poverty\" ~ school_data$Post_Covid + \n    school_data$pct_white + school_data$Post_Covid * school_data$pct_white)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.760  -7.076   1.633   8.534  46.497 \n\nCoefficients:\n                                              Estimate Std. Error  t value\n(Intercept)                                    86.0034     0.1154  745.344\nschool_data$Post_Covid1                         1.2192     0.3620    3.368\nschool_data$pct_white                         -84.3586     0.5311 -158.838\nschool_data$Post_Covid1:school_data$pct_white  -6.1271     1.7540   -3.493\n                                              Pr(>|t|)    \n(Intercept)                                    < 2e-16 ***\nschool_data$Post_Covid1                       0.000757 ***\nschool_data$pct_white                          < 2e-16 ***\nschool_data$Post_Covid1:school_data$pct_white 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.22 on 18138 degrees of freedom\nMultiple R-squared:  0.6083,    Adjusted R-squared:  0.6082 \nF-statistic:  9389 on 3 and 18138 DF,  p-value: < 2.2e-16\n\n\nAs seen from the results above, the p-values for all three coefficients are smaller than the significance level of a = 0.05. The results for the coefficient for the varaible Post-Covid indicate that there is statistically significant evidence to reject the null hypothesis, that the percentage of students in poverty is the same both pre- and post-Covid."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#model-comparisons",
    "href": "posts/FinalPt2_KarenKimble.html#model-comparisons",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\n\nCode\nmodel2 <- lm(school_data$'% Poverty' ~ school_data$Post_Covid + school_data$pct_white)\nsummary(model2)\n\n\n\nCall:\nlm(formula = school_data$\"% Poverty\" ~ school_data$Post_Covid + \n    school_data$pct_white)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.822  -7.128   1.638   8.595  46.731 \n\nCoefficients:\n                        Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)              86.0713     0.1138  756.533   <2e-16 ***\nschool_data$Post_Covid1   0.5011     0.2980    1.681   0.0927 .  \nschool_data$pct_white   -84.9203     0.5063 -167.720   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 18139 degrees of freedom\nMultiple R-squared:  0.608, Adjusted R-squared:  0.608 \nF-statistic: 1.407e+04 on 2 and 18139 DF,  p-value: < 2.2e-16\n\n\nIn this second model without the interaction term, the Covid variable has a much higher p-value of 0.9, larger than the 0.05 significance level. This means that in this model, there is not significant evidence to reject the hypothesis that the percentage of students in poverty is the same both pre- and post-Covid. However, the p-value for the variable of the percentage of white students remains much smaller than the 0.05 significance level. There is still significant evidence in this model to reject the hypothesis that the percentage of students in poverty is the same for all percentages of white students in the school.\nThough this model could still work, I think the first model with the interaction term would still be more accurate. As stated before, the effects of Covid on poverty levels would be mitigated by how much support students in these school districts had, and there is ample evidence that majority white areas receive much more support or are generally more affluent than majority nonwhite areas."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#diagnostics",
    "href": "posts/FinalPt2_KarenKimble.html#diagnostics",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nCode\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Residuals vs Fitted plot, the Residuals seem to be random around the linear line, indicating that the assumption of a linear relationship is reasonable.\nIn the Normal Q-Q plot, the points generally fall along the line, supporting the assumption of normal residuals.\nThe Scale-Location plot does show a slight downward trend, but is approximately horizontal for most of the graph. This means that standardized residuals could be changing because of the fitted values and it’s possible that there is not constant variance.\nThere are no points outside of the proper distance in the Residuals vs. Leverage plot, showing that there is no single influential observation."
  },
  {
    "objectID": "posts/FinalPt2_KarenKimble.html#references",
    "href": "posts/FinalPt2_KarenKimble.html#references",
    "title": "DACSS 603 Final Project Pt 2",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/final_check_Project_Yakub Rabiutheen.html",
    "href": "posts/final_check_Project_Yakub Rabiutheen.html",
    "title": "Project Rough Draft Proposal",
    "section": "",
    "text": "The Research Question in this Project explores the Democratic Outcomes of Ex-French and British Colonies. How do French and British Colonies differ in terms of their Democratic ranking?\nIn terms of Colonial rule, Patrick Ziltener and Daniel K�nzler have come up with a scale to categorize the different levels of colonial rule(Ziltener & Kunzler,2013).\n0 = no colonial domination / not applicable 1 = semi-colonialism 2 = indirect rule with little interference in internal affairs v 3 = indirect rule with strong interference in internal affairs 4 = direct rule\nThis Project looks at countries with ranking 2-4, as Semi-Colonial Countries are not official colonies.\nFor both former French and Former British Colonies, there are 3 categories of Colonial Rule for both of them.\n\nWeakly Influenced Colonies\nStrongly Influenced Colonies\nDirectly Ruled Colonies."
  },
  {
    "objectID": "posts/final_check_Project_Yakub Rabiutheen.html#all-summary-tables",
    "href": "posts/final_check_Project_Yakub Rabiutheen.html#all-summary-tables",
    "title": "Project Rough Draft Proposal",
    "section": "All Summary Tables",
    "text": "All Summary Tables\nAs shown below, Directly Ruled Colonies had average of Positive Democratic Rankings in contrast to Weak and Strong Colonies that had negative Democratic Rankings on Average.\n\n\nCode\ndirect_rule_french.descriptive\n\n\nError in eval(expr, envir, enclos): object 'direct_rule_french.descriptive' not found\n\n\nCode\nweak_rule_french.descriptive\n\n\nError in eval(expr, envir, enclos): object 'weak_rule_french.descriptive' not found\n\n\nCode\nstrong_rule_french.descriptive\n\n\nError in eval(expr, envir, enclos): object 'strong_rule_french.descriptive' not found\n\n\n##Regression Analysis Democracy vs Autocracy for Durability\n\n\nCode\nunique((strong_rule_french$`Country Name`))\n\n\nError in unique((strong_rule_french$`Country Name`)): object 'strong_rule_french' not found\n\n\n\n\nCode\nunique(direct_rule_french$`Country Name`)\n\n\nError in unique(direct_rule_french$`Country Name`): object 'direct_rule_french' not found\n\n\n\n\nCode\nunique(weak_rule_french$`Country Name`)\n\n\nError in unique(weak_rule_french$`Country Name`): object 'weak_rule_french' not found\n\n\n\n\nCode\ngeneral_french.fit = lm(durable~democ+autoc, data=polity_french)\n\n\nError in is.data.frame(data): object 'polity_french' not found\n\n\nCode\ndirect_rule_french.fit = lm(durable~democ+autoc, data=direct_rule_french)\n\n\nError in is.data.frame(data): object 'direct_rule_french' not found\n\n\nCode\nstrong_rule_french.fit= lm(durable~democ+autoc, data=strong_rule_french)\n\n\nError in is.data.frame(data): object 'strong_rule_french' not found\n\n\nCode\nweak_rule_french.fit = lm(durable~democ+autoc, data=weak_rule_french)\n\n\nError in is.data.frame(data): object 'weak_rule_french' not found\n\n\n\n\nCode\ngeneral_french.fit\n\n\nError in eval(expr, envir, enclos): object 'general_french.fit' not found\n\n\nAs shown below, the former UK colonies had much greater stability under Democracy than Autocracy, Post-Independence. In contrast to former French Colonies. This is relevant as if Democracy is shown to bring less stablity in a post-colonial country that might create a preference for autocracy.\n\n\nCode\ngeneral_british.fit\n\n\nError in eval(expr, envir, enclos): object 'general_british.fit' not found\n\n\nDirect French Ruled Ex Colonies had much more stability Post-Independence under an Autocracy than a Democratic Government.\n\n\nCode\ndirect_rule_french.fit\n\n\nError in eval(expr, envir, enclos): object 'direct_rule_french.fit' not found\n\n\nFrench Colonies that were Strongly Ruled had more Stability in a Autocracy than a Democracy.\n\n\nCode\nstrong_rule_french.fit\n\n\nError in eval(expr, envir, enclos): object 'strong_rule_french.fit' not found\n\n\nFrench Colonies that were Weakly Ruled had more Stability in a Autocracy than a Democracy.\n\n\nCode\nweak_rule_french.fit\n\n\nError in eval(expr, envir, enclos): object 'weak_rule_french.fit' not found\n\n\n##Conclusion\nMy findings have found that on average British Colonies had better Democratic Outcomes than French Colonies, when divided into groups, Direct Rule Colonies that were British had positive Democratic outcomes on average compared to Weak and Strong Ruled Colonies. My analysis of French Colonies had similar findings that directly ruled colonies had better Democratic outcomes compared to Weak and Strong ruled colonies. For both French and British Colonies, Strong Ruled Colonies had the worst average Democratic Rankings."
  },
  {
    "objectID": "posts/final_check_Project_Yakub Rabiutheen.html#all-summary-tables-1",
    "href": "posts/final_check_Project_Yakub Rabiutheen.html#all-summary-tables-1",
    "title": "Project Rough Draft Proposal",
    "section": "All Summary Tables",
    "text": "All Summary Tables\nAs shown below, Directly Ruled Colonies had average of Positive Democratic Rankings in contrast to Weak and Strong Colonies that had negative Democratic Rankings on Average. quarto-executable-code-5450563D\ndirect_rule_french.descriptive\nweak_rule_french.descriptive\nstrong_rule_french.descriptive\n##Regression Analysis Democracy vs Autocracy for Durability\nquarto-executable-code-5450563D\nunique((strong_rule_french$`Country Name`))\nquarto-executable-code-5450563D\nunique(direct_rule_french$`Country Name`)\nquarto-executable-code-5450563D\nunique(weak_rule_french$`Country Name`)\nquarto-executable-code-5450563D\ngeneral_french.fit = lm(durable~democ+autoc, data=polity_french)\ndirect_rule_french.fit = lm(durable~democ+autoc, data=direct_rule_french)\nstrong_rule_french.fit= lm(durable~democ+autoc, data=strong_rule_french)\nweak_rule_french.fit = lm(durable~democ+autoc, data=weak_rule_french)\nquarto-executable-code-5450563D\ngeneral_french.fit\nAs shown below, the former UK colonies had much greater stability under Democracy than Autocracy, Post-Independence. In contrast to former French Colonies. This is relevant as if Democracy is shown to bring less stablity in a post-colonial country that might create a preference for autocracy. quarto-executable-code-5450563D\ngeneral_british.fit\nDirect French Ruled Ex Colonies had much more stability Post-Independence under an Autocracy than a Democratic Government.\nquarto-executable-code-5450563D\ndirect_rule_french.fit\nFrench Colonies that were Strongly Ruled had more Stability in a Autocracy than a Democracy.\nquarto-executable-code-5450563D\nstrong_rule_french.fit\nFrench Colonies that were Weakly Ruled had more Stability in a Autocracy than a Democracy. quarto-executable-code-5450563D\nweak_rule_french.fit\n##Conclusion\nMy findings have found that on average British Colonies had better Democratic Outcomes than French Colonies, when divided into groups, Direct Rule Colonies that were British had positive Democratic outcomes on average compared to Weak and Strong Ruled Colonies. My analysis of French Colonies had similar findings that directly ruled colonies had better Democratic outcomes compared to Weak and Strong ruled colonies. For both French and British Colonies, Strong Ruled Colonies had the worst average Democratic Rankings."
  },
  {
    "objectID": "posts/Final_KarenKimble.html",
    "href": "posts/Final_KarenKimble.html",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\nlibrary(ggplot2)\n\n# Importing datasets\n\nNYC_2019 <- read_csv(\"_data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\nNYC_2021 <- read_csv(\"_data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n# Renaming some columns\n\nschool_data$pct_white <- school_data$'% White'\n\nschool_data$pct_poverty <- school_data$'% Poverty'\n\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$pct_poverty <- recode(school_data$pct_poverty, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n# Re-coding variables as numeric\n\nschool_data$pct_poverty <- sapply(school_data$pct_poverty, function(x) gsub(\"%\", \"\", x))\n\nschool_data$pct_poverty <- as.numeric(school_data$pct_poverty)\n\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nWarning: NAs introduced by coercion\n\n\nCode\n# Creating a new variable of post-Covid (1) and pre-Covid (0)\n\nschool_data$Post_Covid <- ifelse(school_data$Year == \"2020-21\", c(\"1\"), c(\"0\"))"
  },
  {
    "objectID": "posts/Final_KarenKimble.html#research-question",
    "href": "posts/Final_KarenKimble.html#research-question",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#hypothesis",
    "href": "posts/Final_KarenKimble.html#hypothesis",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between pre- and post-Covid than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#descriptive-statistics",
    "href": "posts/Final_KarenKimble.html#descriptive-statistics",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year, which I transformed into pre-Covid and post-Covid variables\nNumber and percentage of Asian, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nRows: 18,142\nColumns: 39\n$ DBN                            <chr> \"01M015\", \"01M015\", \"01M015\", \"01M015\",…\n$ `School Name`                  <chr> \"P.S. 015 Roberto Clemente\", \"P.S. 015 …\n$ Year                           <chr> \"2014-15\", \"2015-16\", \"2016-17\", \"2017-…\n$ `Total Enrollment`             <dbl> 183, 176, 178, 190, 174, 270, 270, 271,…\n$ `Grade K`                      <dbl> 27, 32, 28, 28, 20, 44, 47, 37, 34, 30,…\n$ `Grade 1`                      <dbl> 47, 33, 33, 32, 33, 40, 43, 46, 38, 39,…\n$ `Grade 2`                      <dbl> 31, 39, 27, 33, 30, 39, 41, 47, 42, 43,…\n$ `Grade 3`                      <dbl> 19, 23, 31, 23, 30, 35, 43, 40, 46, 41,…\n$ `Grade 4`                      <dbl> 17, 17, 24, 31, 20, 40, 35, 43, 42, 44,…\n$ `Grade 5`                      <dbl> 24, 18, 18, 26, 28, 42, 40, 34, 42, 42,…\n$ `Grade 6`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 7`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 8`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 9`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 10`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 11`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 12`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `# Female`                     <dbl> 84, 83, 83, 99, 85, 132, 125, 127, 114,…\n$ `% Female`                     <dbl> 0.459, 0.472, 0.466, 0.521, 0.489, 0.48…\n$ `# Male`                       <dbl> 99, 93, 95, 91, 89, 138, 145, 144, 143,…\n$ `% Male`                       <dbl> 0.541, 0.528, 0.534, 0.479, 0.511, 0.51…\n$ `# Asian`                      <dbl> 8, 9, 14, 20, 24, 30, 27, 24, 23, 14, 2…\n$ `% Asian`                      <dbl> 0.044, 0.051, 0.079, 0.105, 0.138, 0.11…\n$ `# Black`                      <dbl> 65, 57, 51, 52, 48, 47, 55, 51, 49, 52,…\n$ `% Black`                      <dbl> 0.355, 0.324, 0.287, 0.274, 0.276, 0.17…\n$ `# Hispanic`                   <dbl> 107, 105, 105, 110, 95, 158, 169, 180, …\n$ `% Hispanic`                   <dbl> 0.585, 0.597, 0.590, 0.579, 0.546, 0.58…\n$ `# White`                      <dbl> 2, 2, 4, 6, 6, 27, 16, 15, 16, 18, 25, …\n$ `% White`                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ `# Students with Disabilities` <dbl> 64, 60, 51, 49, 38, 82, 82, 88, 90, 92,…\n$ `% Students with Disabilities` <dbl> 0.350, 0.341, 0.287, 0.258, 0.218, 0.30…\n$ `# English Language Learners`  <dbl> 17, 16, 12, 8, 8, 18, 13, 9, 8, 8, 120,…\n$ `% English Language Learners`  <dbl> 0.093, 0.091, 0.067, 0.042, 0.046, 0.06…\n$ `# Poverty`                    <chr> \"169\", \"149\", \"152\", \"161\", \"145\", \"200…\n$ `% Poverty`                    <chr> \"92.3%\", \"84.7%\", \"85.4%\", \"84.7%\", \"83…\n$ `Economic Need Index`          <dbl> 0.930, 0.889, 0.882, 0.890, 0.880, 0.60…\n$ pct_white                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ pct_poverty                    <dbl> 92.3, 84.7, 85.4, 84.7, 83.3, 74.1, 80.…\n$ Post_Covid                     <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\",…\n\n\n\n\nCode\nsummary(school_data)\n\n\n     DBN            School Name            Year           Total Enrollment\n Length:18142       Length:18142       Length:18142       Min.   :   7.0  \n Class :character   Class :character   Class :character   1st Qu.: 323.0  \n Mode  :character   Mode  :character   Mode  :character   Median : 477.0  \n                                                          Mean   : 592.3  \n                                                          3rd Qu.: 695.0  \n                                                          Max.   :6040.0  \n                                                                          \n    Grade K          Grade 1          Grade 2          Grade 3      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 32.00   Median : 33.00   Median : 32.00   Median : 28.00  \n Mean   : 44.25   Mean   : 45.79   Mean   : 45.73   Mean   : 45.33  \n 3rd Qu.: 78.00   3rd Qu.: 81.00   3rd Qu.: 82.00   3rd Qu.: 81.00  \n Max.   :393.00   Max.   :383.00   Max.   :349.00   Max.   :369.00  \n                                                                    \n    Grade 4         Grade 5          Grade 6          Grade 7      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.0   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 22.0   Median : 19.00   Median :  0.00   Median :  0.00  \n Mean   : 44.8   Mean   : 44.18   Mean   : 43.15   Mean   : 42.37  \n 3rd Qu.: 80.0   3rd Qu.: 80.00   3rd Qu.: 64.00   3rd Qu.: 62.00  \n Max.   :376.0   Max.   :351.00   Max.   :771.00   Max.   :796.00  \n                                                                   \n    Grade 8          Grade 9           Grade 10         Grade 11      \n Min.   :  0.00   Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n 1st Qu.:  0.00   1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00  \n Median :  0.00   Median :   0.00   Median :   0.0   Median :   0.00  \n Mean   : 41.88   Mean   :  49.34   Mean   :  48.7   Mean   :  39.85  \n 3rd Qu.: 60.00   3rd Qu.:  68.00   3rd Qu.:  69.0   3rd Qu.:  54.00  \n Max.   :784.00   Max.   :1555.00   Max.   :3832.0   Max.   :1529.00  \n                                                                      \n    Grade 12          # Female         % Female          # Male      \n Min.   :   0.00   Min.   :   0.0   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:   0.00   1st Qu.: 146.0   1st Qu.:0.4620   1st Qu.: 163.0  \n Median :   0.00   Median : 232.0   Median :0.4880   Median : 248.0  \n Mean   :  39.58   Mean   : 287.4   Mean   :0.4827   Mean   : 304.9  \n 3rd Qu.:  53.00   3rd Qu.: 347.0   3rd Qu.:0.5130   3rd Qu.: 364.0  \n Max.   :1566.00   Max.   :2405.0   Max.   :1.0000   Max.   :3635.0  \n                                                                     \n     % Male          # Asian           % Asian          # Black      \n Min.   :0.0000   Min.   :   0.00   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:0.4870   1st Qu.:   5.00   1st Qu.:0.0130   1st Qu.:  42.0  \n Median :0.5120   Median :  17.00   Median :0.0400   Median : 105.0  \n Mean   :0.5173   Mean   :  95.38   Mean   :0.1136   Mean   : 154.1  \n 3rd Qu.:0.5380   3rd Qu.:  79.00   3rd Qu.:0.1400   3rd Qu.: 198.0  \n Max.   :1.0000   Max.   :3671.00   Max.   :0.9470   Max.   :1493.0  \n                                                                     \n    % Black        # Hispanic     % Hispanic        # White       \n Min.   :0.000   Min.   :   1   Min.   :0.0060   Min.   :   0.00  \n 1st Qu.:0.083   1st Qu.:  89   1st Qu.:0.1980   1st Qu.:   6.00  \n Median :0.251   Median : 180   Median :0.3990   Median :  15.00  \n Mean   :0.316   Mean   : 241   Mean   :0.4251   Mean   :  87.24  \n 3rd Qu.:0.502   3rd Qu.: 313   3rd Qu.:0.6323   3rd Qu.:  78.00  \n Max.   :0.987   Max.   :2056   Max.   :1.0000   Max.   :3190.00  \n                                                                  \n    % White       # Students with Disabilities % Students with Disabilities\n Min.   :0.0000   Min.   :  0.0                Min.   :0.0000              \n 1st Qu.:0.0140   1st Qu.: 66.0                1st Qu.:0.1570              \n Median :0.0330   Median : 98.0                Median :0.2030              \n Mean   :0.1205   Mean   :121.6                Mean   :0.2295              \n 3rd Qu.:0.1440   3rd Qu.:146.0                3rd Qu.:0.2540              \n Max.   :0.9450   Max.   :925.0                Max.   :1.0000              \n                                                                           \n # English Language Learners % English Language Learners  # Poverty        \n Min.   :   0.0              Min.   :0.0000              Length:18142      \n 1st Qu.:  18.0              1st Qu.:0.0430              Class :character  \n Median :  43.0              Median :0.0950              Mode  :character  \n Mean   :  81.1              Mean   :0.1363                                \n 3rd Qu.: 100.0              3rd Qu.:0.1800                                \n Max.   :1219.0              Max.   :1.0000                                \n                                                                           \n  % Poverty         Economic Need Index   pct_white       pct_poverty    \n Length:18142       Min.   :0.030       Min.   :0.0000   Min.   :  2.90  \n Class :character   1st Qu.:0.579       1st Qu.:0.0140   1st Qu.: 69.30  \n Mode  :character   Median :0.743       Median :0.0330   Median : 81.40  \n                    Mean   :0.691       Mean   :0.1205   Mean   : 75.89  \n                    3rd Qu.:0.846       3rd Qu.:0.1440   3rd Qu.: 89.90  \n                    Max.   :0.998       Max.   :0.9450   Max.   :100.00  \n                    NA's   :9169                                         \n  Post_Covid       \n Length:18142      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#visualizations",
    "href": "posts/Final_KarenKimble.html#visualizations",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Visualizations",
    "text": "Visualizations\n\n\nCode\npost_white <- filter(school_data, pct_white > 0.5 & Post_Covid == \"1\")\npost_nonwhite <- filter(school_data, pct_white <= 0.5 & Post_Covid == \"1\")\npre_white <- filter(school_data, pct_white > 0.5 & Post_Covid == \"0\")\npre_nonwhite <- filter(school_data, pct_white <= 0.5 & Post_Covid == \"0\")\npost <- filter(school_data, Post_Covid == \"1\")\npre <- filter(school_data, Post_Covid == \"0\")\nwhite <- filter(school_data, pct_white > 0.5)\nnonwhite <- filter(school_data, pct_white <= 0.5)\n\n\n\n\nCode\nboxplot(school_data$pct_poverty ~ school_data$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Poverty\")\n\n\n\n\n\nFrom the box plot above, we can see that poverty levels between pre- and post-Covid did not change very dramatically for overall students. However, the maximum percentage of poverty did decrease from close to 100% to about 95%. Yet the median percentage level of poverty increased from pre- to post-Covid, though still remained under 90%. There are a large number of outliers below the minimum level of poverty, which is interesting–yet the minimum level of poverty increased between pre- and post-Covid.\n\n\nCode\nboxplot(white$pct_poverty ~ white$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Majority White Districts in Poverty\",\n        cex.lab = 0.75)\n\n\n\n\n\nThe box plot showing majority white districts’ poverty levels pre- and post-Covid indicate that, while the median poverty level did not change dramatically, the maximum percentage and the third quartile percentage of poverty decreased between pre- and post-Covid. There was a high outlier pre-Covid, but this disappeared in the post-Covid box plot. The majority of observations for predominantly white schools fall in between about 15%-40% of students in poverty, for both pre- and post-Covid levels.\n\n\nCode\nboxplot(nonwhite$pct_poverty ~ nonwhite$Post_Covid,\n        xlab = \"Pre-Covid (0) or Post-Covid (1)\",\n        ylab = \"Percent of Students in Majority Non-White Districts in Poverty\",\n        cex.lab = 0.75)\n\n\n\n\n\nThe box plot for majority non-white districts’ poverty levels has a large number of outliers under the poverty level of 50%, similar to the plot for all school districts. The median level of poverty actually increased slightly between pre- and post-Covid years, along with the minimum level of poverty (both in the plot and in outliers). The maximum level of poverty, however, decreased more between these years. A majority of obersvations for both pre- and post-Covid are between 70%-90% of students in poverty for majority non-white districts.\n\n\nCode\nggplot(school_data, aes(x=pct_poverty)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(pct_poverty)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nFrom the histogram above, we can see that the mean percentage of poverty for all NYC school districts is just above 75%. Most observations are above the mean, with very few school districts below 50% poverty levels. This is most likely why there were so many outliers in the box plots for all school districts and majority non-white school districts.\n\n\nCode\nggplot(pre, aes(x=pct_poverty)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"gray\") +\n  labs(title=\"School Districts Pre-Covid\") +\n  geom_vline(aes(xintercept=mean(pct_poverty)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nCode\nggplot(post, aes(x=pct_poverty)) +\n  geom_histogram(binwidth = 5, color=\"black\", fill=\"light gray\") +\n  labs(title=\"School Districts Post-Covid\") +\n  geom_vline(aes(xintercept=mean(pct_poverty)), color=\"red\", linetype=\"dashed\")\n\n\n\n\n\nThe two different plots above show that a alightly larger share of districts post-Covid had poverty levels above the mean. The observations are very skewed to the left in all three plots, however."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#hypothesis-testing",
    "href": "posts/Final_KarenKimble.html#hypothesis-testing",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nResponse variables: Percentage of students in poverty\nExplanatory variables: Post- or pre-Covid, whether the school is majority or minority white\nI’m including the interaction term of school demographics because I think that this does play a major role in how Covid effects were mitigated or compounded. Schools with more white students may have received more support during Covid, whether directly to the school or to the students themselves.\nNull Hypothesis: The percentage of students in poverty is the same both pre- and post-Covid for both majority and minority white school districts.\nAlternative Hypothesis: The percentage of students in poverty is higher post-Covid than pre-Covid, and this effect will be more drastic in minority white school districts than in majority white school districts.\n\n\nCode\nmean(post_white$pct_poverty) - mean(pre_white$pct_poverty)\n\n\n[1] -0.9168508\n\n\nCode\nmean(post_nonwhite$pct_poverty) - mean(pre_nonwhite$pct_poverty)\n\n\n[1] 0.4610488\n\n\nCode\nmean(post$pct_poverty) - mean(pre$pct_poverty)\n\n\n[1] 0.8529465\n\n\nBefore conducting the hypothesis test, we already see that the differences between pre- and post-Covid poverty levels are very different for majority white and majorty nonwhite schools. Majority white schools actually had a decrease in poverty of 0.91% between pre-Covid and post-Covid school years. Yet majority nonwhite schools saw poverty increase between pre- and post-Covid by 0.46%. All school districts saw an overall 0.85% poverty increase in this time period. However, these numbers could be skewed since there is only one post-Covid year while there are a lot of years included in the data-set pre-Covid.\n\n\nCode\n# Hypothesis Test\n\nmodel <- lm(pct_poverty ~ Post_Covid + pct_white + Post_Covid * pct_white, data = school_data)\nsummary(model)\n\n\n\nCall:\nlm(formula = pct_poverty ~ Post_Covid + pct_white + Post_Covid * \n    pct_white, data = school_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.760  -7.076   1.633   8.534  46.497 \n\nCoefficients:\n                      Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)            86.0034     0.1154  745.344  < 2e-16 ***\nPost_Covid1             1.2192     0.3620    3.368 0.000757 ***\npct_white             -84.3586     0.5311 -158.838  < 2e-16 ***\nPost_Covid1:pct_white  -6.1271     1.7540   -3.493 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.22 on 18138 degrees of freedom\nMultiple R-squared:  0.6083,    Adjusted R-squared:  0.6082 \nF-statistic:  9389 on 3 and 18138 DF,  p-value: < 2.2e-16\n\n\nThe p-value for the interaction term is much smaller than 0.05, indicating that there is statistically significant evidence to reject the null hypothesis, that the percentage of students in poverty is the same both pre- and post-Covid for both majority non-white and white school districts.\nAdditionally, when looking at how the coefficient of the Post-Covid variable changes for different values of percent white, we can see disproportionate changes in majority nonwhite and majority white districts.\n\n\nCode\nquantile(school_data$pct_white)\n\n\n   0%   25%   50%   75%  100% \n0.000 0.014 0.033 0.144 0.945 \n\n\nCode\n# First quartile\n1.2192 * 0.014\n\n\n[1] 0.0170688\n\n\nCode\n# Third Quartile\n1.2192 * 0.144\n\n\n[1] 0.1755648\n\n\nUsing the first and third quartile values for the variable percent white and combining those with the coefficient for Post-Covid, we can see that the predicted poverty rate between the first and third quartile levels increases by over 9 times. This could indicate that schools with a higher percentage of white students have had poverty increase more rapidly than in schools with less white students. Since the data from earlier showed that majority white districts already had lower levels of poverty, there was more room for the percentage of students in poverty to increase compared to majority nonwhite districts."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#model-comparisons",
    "href": "posts/Final_KarenKimble.html#model-comparisons",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\n\nCode\nmodel2 <- lm(pct_poverty ~ Post_Covid + pct_white, data = school_data)\nsummary(model2)\n\n\n\nCall:\nlm(formula = pct_poverty ~ Post_Covid + pct_white, data = school_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.822  -7.128   1.638   8.595  46.731 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)  86.0713     0.1138  756.533   <2e-16 ***\nPost_Covid1   0.5011     0.2980    1.681   0.0927 .  \npct_white   -84.9203     0.5063 -167.720   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 18139 degrees of freedom\nMultiple R-squared:  0.608, Adjusted R-squared:  0.608 \nF-statistic: 1.407e+04 on 2 and 18139 DF,  p-value: < 2.2e-16\n\n\nIn this second model without the interaction term, the Covid variable has a much higher p-value of 0.9, larger than the 0.05 significance level. This means that in this model, there is not significant evidence to reject the hypothesis that the percentage of students in poverty is the same both pre- and post-Covid. However, the p-value for the variable of the percentage of white students remains much smaller than the 0.05 significance level. There is still significant evidence in this model to reject the hypothesis that the percentage of students in poverty is the same for all percentages of white students in the school.\n\n\nCode\nPRESS <- function(linear.model) {\n\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n\n  PRESS <- sum(pr^2)\n  \n  return(PRESS)\n}\n\nPRESS(model)\n\n\n[1] 2711598\n\n\nCode\nPRESS(model2)\n\n\n[1] 2712870\n\n\nThough this model could still work, I think the first model with the interaction term would still be more accurate. The adjusted R-squared of the first model is higher, indicating that there is better prediction power. Additionally, the PRESS (Predicted Residual Sum of Squares) for each model shows that the first model’s PRESS is lower, meaning that it is a better fit for the existing data."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#diagnostics",
    "href": "posts/Final_KarenKimble.html#diagnostics",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nCode\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the Residuals vs Fitted plot, the Residuals seem to be random around the linear line, indicating that the assumption of a linear relationship is reasonable.\nIn the Normal Q-Q plot, the points generally fall along the line, supporting the assumption of normal residuals.\nThe Scale-Location plot does show a slight downward trend, but is approximately horizontal for most of the graph. This means that standardized residuals could be changing because of the fitted values and it’s possible that there is not constant variance.\nThere are no points outside of the proper distance in the Residuals vs. Leverage plot, showing that there is no single influential observation."
  },
  {
    "objectID": "posts/Final_KarenKimble.html#references",
    "href": "posts/Final_KarenKimble.html#references",
    "title": "DACSS 603 Final Project Write-Up",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/Final_Project_1.html",
    "href": "posts/Final_Project_1.html",
    "title": "Final_Project_1",
    "section": "",
    "text": "Research Question : examining the relationship between the maximum heart rate one can achieve during exercise and the likelihood of developing heart disease. Using multiple logistic regression, examining handle the confounding effects of age and gender.\nHypothesis Testing : Is there any statistical difference between the gender and age in terms of heart attack prediction.\n#Loading Dataset\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v dplyr   1.0.8\nv tibble  3.1.6     v stringr 1.4.0\nv tidyr   1.2.0     v forcats 0.5.1\nv purrr   0.3.4     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nheart_cleveland_upload <- read_csv(\"heart_cleveland_upload.csv\")\n\n\nRows: 297 Columns: 14\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(heart_cleveland_upload)\n\n\n\n\n  \n\n\n\n\n\nCode\ndim(heart_cleveland_upload)\n\n\n[1] 297  14\n\n\nData set contains 297 Columns and 14 columns\n\n\nCode\ncolnames(heart_cleveland_upload)\n\n\n [1] \"age\"       \"sex\"       \"cp\"        \"trestbps\"  \"chol\"      \"fbs\"      \n [7] \"restecg\"   \"thalach\"   \"exang\"     \"oldpeak\"   \"slope\"     \"ca\"       \n[13] \"thal\"      \"condition\"\n\n\nhere are 13 attributes\nage: age in years sex: sex (1 = male; 0 = female) cp: chest pain type – Value 0: typical angina – Value 1: atypical angina – Value 2: non-anginal pain – Value 3: asymptomatic trestbps: resting blood pressure (in mm Hg on admission to the hospital) chol: serum cholestoral in mg/dl fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) restecg: resting electrocardiographic results – Value 0: normal – Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) – Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria thalach: maximum heart rate achieved exang: exercise induced angina (1 = yes; 0 = no) oldpeak = ST depression induced by exercise relative to rest slope: the slope of the peak exercise ST segment – Value 0: upsloping – Value 1: flat – Value 2: downsloping ca: number of major vessels (0-3) colored by flourosopy thal: 0 = normal; 1 = fixed defect; 2 = reversable defect and the label condition: 0 = no disease, 1 = disease\n\nDescriptive statistics\n\n\nCode\nsummary(heart_cleveland_upload)\n\n\n      age             sex               cp           trestbps    \n Min.   :29.00   Min.   :0.0000   Min.   :0.000   Min.   : 94.0  \n 1st Qu.:48.00   1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:120.0  \n Median :56.00   Median :1.0000   Median :2.000   Median :130.0  \n Mean   :54.54   Mean   :0.6768   Mean   :2.158   Mean   :131.7  \n 3rd Qu.:61.00   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:140.0  \n Max.   :77.00   Max.   :1.0000   Max.   :3.000   Max.   :200.0  \n      chol            fbs            restecg          thalach     \n Min.   :126.0   Min.   :0.0000   Min.   :0.0000   Min.   : 71.0  \n 1st Qu.:211.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:133.0  \n Median :243.0   Median :0.0000   Median :1.0000   Median :153.0  \n Mean   :247.4   Mean   :0.1448   Mean   :0.9966   Mean   :149.6  \n 3rd Qu.:276.0   3rd Qu.:0.0000   3rd Qu.:2.0000   3rd Qu.:166.0  \n Max.   :564.0   Max.   :1.0000   Max.   :2.0000   Max.   :202.0  \n     exang           oldpeak          slope              ca        \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.800   Median :1.0000   Median :0.0000  \n Mean   :0.3266   Mean   :1.056   Mean   :0.6027   Mean   :0.6768  \n 3rd Qu.:1.0000   3rd Qu.:1.600   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :6.200   Max.   :2.0000   Max.   :3.0000  \n      thal         condition     \n Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000  \n Mean   :0.835   Mean   :0.4613  \n 3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :2.000   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(heart_cleveland_upload)\n\n\nRows: 297\nColumns: 14\n$ age       <dbl> 69, 69, 66, 65, 64, 64, 63, 61, 60, 59, 59, 59, 59, 58, 56, ~\n$ sex       <dbl> 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ~\n$ cp        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ trestbps  <dbl> 160, 140, 150, 138, 110, 170, 145, 134, 150, 178, 170, 160, ~\n$ chol      <dbl> 234, 239, 226, 282, 211, 227, 233, 234, 240, 270, 288, 273, ~\n$ fbs       <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ~\n$ restecg   <dbl> 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, ~\n$ thalach   <dbl> 131, 151, 114, 174, 144, 155, 150, 145, 171, 145, 159, 125, ~\n$ exang     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n$ oldpeak   <dbl> 0.1, 1.8, 2.6, 1.4, 1.8, 0.6, 2.3, 2.6, 0.9, 4.2, 0.2, 0.0, ~\n$ slope     <dbl> 1, 0, 2, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, ~\n$ ca        <dbl> 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, ~\n$ thal      <dbl> 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, ~\n$ condition <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, ~"
  },
  {
    "objectID": "posts/Final_Project_2.html",
    "href": "posts/Final_Project_2.html",
    "title": "",
    "section": "",
    "text": "title: “Final_Project_2” author: “Mani Kanta Gogula” description: “Project Analysis” date: “11/11/2022” format: html: df-print: paged css: styles.css toc: true code-fold: true code-copy: true code-tools: true categories: - Project Analysis - —\nIntroduction :\nHeart Disease in the United States Heart disease is the leading cause of death for men, women, and people of most racial and ethnic groups in the United States.1 One person dies every 34 seconds in the United States from cardiovascular disease.1 About 697,000 people in the United States died from heart disease in 2020—that’s 1 in every 5 deaths.1,2 Heart disease cost the United States about $229 billion each year from 2017 to 2018.3 This includes the cost of health care services, medicines, and lost productivity due to death.\nResearch :\n\nExamining the relationship between maximum heart rate one can achieve during excercise and likelihood of developing heart disease .\nUsing Multiple logistic regression confounding effects of age and gender.\n\nLoading Packages and Dataset :\n\nlibrary(readr)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.5     v dplyr   1.0.8\nv tibble  3.1.6     v stringr 1.4.0\nv tidyr   1.2.0     v forcats 0.5.1\nv purrr   0.3.4     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nheart_cleveland_upload <- read_csv(\"posts/heart_cleveland_upload.csv\")\n\nError: 'posts/heart_cleveland_upload.csv' does not exist in current working directory ('C:/Users/manik/Desktop/DACSS 603/603_Fall_2022/posts').\n\nhead(heart_cleveland_upload)\n\nError in head(heart_cleveland_upload): object 'heart_cleveland_upload' not found\n\n\n\ncolnames(heart_cleveland_upload)\n\nError in is.data.frame(x): object 'heart_cleveland_upload' not found\n\n\nIn the respective Data set , We have 14 variables :\nAGE : Age in years SEX : Sex(1=MALE;O=FEMALE) CP : chest Pain type 0:typical angina 1:atypical angina 2:non-anginal pain 3:asymptomatic TRESTBPS :Resting blood pressure (in mm Hg on admission to the hospital)\nCHOL :Serum cholestrol in mg/dl FBS :Fasting blood sugar> 120mg/dl( 1=true ; 0=false) RESTECG :Resting electrocardiographic results (0=Normal ;1=having ST-T Wave abnormality ; 2=Showing probable or definite left ventricular hypertrophy THALACH : Maximum heart acheived EXANG :Excercise induced angina (1=Yes ; 0=No) OLDPEAK :ST depression induced by excercise relative to rest SLOPE :The slope of the peak excercise relative to rest Value 0 :upsloping Value 1 :Flat Value 2 :Downsloping CA :Number of major vessels (0-3) colored by fluroscopy CONDITION :0=no disease , 1= disease\n\ndim(heart_cleveland_upload)\n\nError in eval(expr, envir, enclos): object 'heart_cleveland_upload' not found\n\n\n\nsummary(heart_cleveland_upload)\n\nError in summary(heart_cleveland_upload): object 'heart_cleveland_upload' not found\n\n\nRecoding the sex variable into categorical variable where 0 as Female and 1 as Male\n\nheart_cleveland_upload$sex[heart_cleveland_upload$sex == 0] <- \"female\"\n\nError in heart_cleveland_upload$sex[heart_cleveland_upload$sex == 0] <- \"female\": object 'heart_cleveland_upload' not found\n\nheart_cleveland_upload$sex[heart_cleveland_upload$sex == 1] <- \"male\"\n\nError in heart_cleveland_upload$sex[heart_cleveland_upload$sex == 1] <- \"male\": object 'heart_cleveland_upload' not found\n\nhead(heart_cleveland_upload)\n\nError in head(heart_cleveland_upload): object 'heart_cleveland_upload' not found\n\n\nCHI-SQUARED TEST: Chi-Square test in R is a statistical method which used to determine if a categorical variable have a significant correlation between them. The two variables are selected from the same population.\nParticularly in this test, we have to check the p-values. Moreover, like all statistical tests, we assume this test as a null hypothesis and an alternate hypothesis.\nThe main thing is, we reject the null hypothesis if the p-value that comes out in the result is less than a predetermined significance level, which is 0.05 usually, then we reject the null hypothesis.\nH0: The two variables are independent. H1: The two variables relate to each other.\nIn the case of a null hypothesis, a chi-square test is to test the two variables that are independent.\n\nheart_sex<- chisq.test(heart_cleveland_upload$sex,heart_cleveland_upload$condition)\n\nError in is.data.frame(x): object 'heart_cleveland_upload' not found\n\nprint(heart_sex)\n\nError in print(heart_sex): object 'heart_sex' not found\n\n\nWe have a high chi-squared value and a p-value of less than 0.05 significance level. So we reject the null hypothesis and conclude that sex and condition have a significant relationship.\n\n# Does age have an effect? Age is continuous, so we use t-test here\nheart_age<-t.test(heart_cleveland_upload$age ~ heart_cleveland_upload$condition)\n\nError in eval(predvars, data, env): object 'heart_cleveland_upload' not found\n\nprint(heart_age)\n\nError in print(heart_age): object 'heart_age' not found\n\n\nHere’s how to interpret the results of the t-test:\ndata: This tells us the data that was used in the two sample t-test. In this case, we used the variables called age and condition.\nt: This is the t test-statistic. In this case, it is -4.0636.\ndf: This is the degrees of freedom associated with the t test-statistic. In this case, it’s 294.66\np-value: This is the p-value that corresponds to a t test-statistic of -4.0636 and df = 294.66. The p-value turns out to be 0.00006204.\nalternative hypothesis: This tells us the alternative hypothesis used for this particular t-test. In this case, the alternative hypothesis is that the true difference in means between the two groups is not equal to zero.\n95 percent confidence interval: This tells us the 95% confidence interval for the true difference in means between the two groups. It turns out to be [-6.108, -2.12].\nBecause the p-value of our test (0.00006204) is less than alpha = 0.05, we reject the null hypothesis of the test. This means we have sufficient evidence to say that the mean of group 0 and group 1 is different.\n\nheart_thalach<-t.test(heart_cleveland_upload$thalach ~ heart_cleveland_upload$condition)\n\nError in eval(predvars, data, env): object 'heart_cleveland_upload' not found\n\nprint(heart_thalach)\n\nError in print(heart_thalach): object 'heart_thalach' not found\n\n\ndata: This tells us the data that was used in the two sample t-test. In this case, we used the variables called thalach and condition.\nt: This is the t test-statistic. In this case, it is 7.9286.\ndf: This is the degrees of freedom associated with the t test-statistic. In this case, it’s 266.44\np-value: This is the p-value that corresponds to a t test-statistic of 7.9286 and df = 266.44. The p-value turns out to be 0.00000000000006108.\nalternative hypothesis: This tells us the alternative hypothesis used for this particular t-test. In this case, the alternative hypothesis is that the true difference in means between the two groups is not equal to zero.\n95 percent confidence interval: This tells us the 95% confidence interval for the true difference in means between the two groups. It turns out to be [14.636, 24.30715].\nBecause the p-value of our test (0.0000000000000610) is less than alpha = 0.05, we reject the null hypothesis of the test. This means we have sufficient evidence to say that the mean of group 0 and group 1 is different.\nExploring the association graphically\n\n# Recode condition to be labelled\nheart_cleveland_upload %>% mutate(condition_labelled = ifelse(condition == 0, \"No disease\", \"Disease\")) ->heart_cleveland_upload\n\nError in mutate(., condition_labelled = ifelse(condition == 0, \"No disease\", : object 'heart_cleveland_upload' not found\n\n# age vs condition\nggplot(data = heart_cleveland_upload, aes(x = condition_labelled, y = age)) + geom_boxplot()\n\nError in ggplot(data = heart_cleveland_upload, aes(x = condition_labelled, : object 'heart_cleveland_upload' not found\n\n\n\n# sex vs condition\nggplot(data = heart_cleveland_upload, aes(x = condition_labelled, fill = sex)) + geom_bar(position = \"fill\") + ylab(\"Sex %\")\n\nError in ggplot(data = heart_cleveland_upload, aes(x = condition_labelled, : object 'heart_cleveland_upload' not found\n\n\nLogistics Regression :\nLogistic regression is a predictive modelling algorithm that is used when the Y variable is binary categorical. That is, it can take only two values like 1 or 0. The goal is to determine a mathematical equation that can be used to predict the probability of event 1. Once the equation is established, it can be used to predict the Y when only the Xs are known.\nLogistic regression is a classic predictive modelling technique and still remains a popular choice for modelling binary categorical variables.\nAnother advantage of logistic regression is that it computes a prediction probability score of an event. More on that when you actually start building the models.\nBuilding the model and classifying the Y is only half work done. Actually, not even half. Because, the scope of evaluation metrics to judge the efficacy of the model is vast and requires careful judgement to choose the right model. In the next part, I will discuss various evaluation metrics that will help to understand how well the classification model performs from different perspectives.\nIn this Project , I have used two Models to compare various Metrics Like AUC, Accuracy and propse best model based on these metrics.\nModel 1 : Our goal in Model is to predict condition of the patient based on different input parameters.\nIn Model 1 , I have used age, sex and thalach as my input variables and predicted the condition of the patient.\n\n# using glm function from base R and specify the family argument as binomial\nmodel <- glm(data = heart_cleveland_upload, condition ~ age + sex + thalach, family = \"binomial\" )\n\nError in is.data.frame(data): object 'heart_cleveland_upload' not found\n\n# extract the model summary\nsummary(model)\n\nError in summary(model): object 'model' not found\n\n\nIn the output above, the first thing we see is the call, this is R reminding us what the model we ran was, what options we specified, etc.\nNext we see the deviance residuals, which are a measure of model fit. This part of output shows the distribution of the deviance residuals for individual cases used in the model.\nThe next part of the output shows the coefficients, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values.. The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable.\nFor every one unit change in age, the log odds of likelihood of developing heart disease increases by 0.03. For a one unit increase in sexmale, the log odds likelihood of developing heart disease increases by 1.46\nBelow the table of coefficients are fit indices, including the null and deviance residuals and the AIC.\nPrediction :\nLet’s say a patient have a profile with age 45, sex =male and thalach 150. And we now predict the chances of that patient likelihood of developing heart disease based on our model 1\n\n# get the predicted probability in our dataset using the predict() function\n# We include the argument type=”response” in order to get our prediction.\npred_prob <- predict(model, heart_cleveland_upload, type=\"response\")\n\nError in predict(model, heart_cleveland_upload, type = \"response\"): object 'model' not found\n\n# create a decision rule using probability 0.5 as cutoff and save the predicted decision into the main data frame\nheart_cleveland_upload$pred_condition <- ifelse(pred_prob >= 0.5, 1, 0)\n\nError in ifelse(pred_prob >= 0.5, 1, 0): object 'pred_prob' not found\n\n# create a newdata data frame to save a new case information\nnewdata <- data.frame(age=45, sex='female', thalach=150)\n\n# predict probability for this new case and print out the predicted value\np_new <- predict(model, newdata, type=\"response\")\n\nError in predict(model, newdata, type = \"response\"): object 'model' not found\n\np_new\n\nError in eval(expr, envir, enclos): object 'p_new' not found\n\n\nWe see that this patient have 18% of chance developing heart disease.\nMETRICS :\nAre the predictions accurate? How well does the model fit our data? We are going to use some common metrics to evaluate the model performance. The most straightforward one is- Accuracy, which is the proportion of the total number of predictions that were correct. On the other hand, we can calculate the classification error rate using 1- accuracy. However, accuracy can be misleading when the response is rare (i.e., imbalanced response). Another popular metric is Area Under the ROC curve (AUC), has the advantage that it’s independent of the change in the proportion of responders. AUC ranges from 0 to 1. The closer it gets to 1 the better the model performance. Lastly, a confusion matrix is an N X N matrix, where N is the level of outcome. For the problem at hand, we have N=2, and hence we get a 2 X 2 matrix. It cross-tabulates the predicted outcome levels against the true outcome levels.\n\n# load Metrics package\nlibrary(Metrics)\n\nWarning: package 'Metrics' was built under R version 4.1.3\n\n# calculate auc, accuracy, clasification error\nauc <- auc(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition) \n\nError in auc(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition): object 'heart_cleveland_upload' not found\n\naccuracy <- accuracy(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition)\n\nError in mean(actual != predicted): object 'heart_cleveland_upload' not found\n\nclassification_error <- ce(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition) \n\nError in mean(actual != predicted): object 'heart_cleveland_upload' not found\n\n# print out the metrics on to screen\nprint(paste(\"AUC=\", auc))\n\nError in paste(\"AUC=\", auc): cannot coerce type 'closure' to vector of type 'character'\n\nprint(paste(\"Accuracy=\", accuracy))\n\nError in paste(\"Accuracy=\", accuracy): cannot coerce type 'closure' to vector of type 'character'\n\nprint(paste(\"Classification Error=\", classification_error))\n\nError in paste(\"Classification Error=\", classification_error): object 'classification_error' not found\n\n# confusion matrix\ntable(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition, dnn=c(\"True Status\", \"Predicted Status\")) # confusion matrix\n\nError in table(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition, : object 'heart_cleveland_upload' not found\n\n\nAfter these metrics are calculated, For a 45 years old female who has a max heart rate of 150, our model generated a heart disease probability of 0.18 indicating low risk of heart disease. Although our model has an overall accuracy of 0.71, there are cases that were misclassified as shown in the confusion matrix. One way to improve our current model is to include other relevant predictors from the dataset into our model.\nMODEL 2 : In Model 1 , I have used age, sex and chol as my input variables and predicted the condition of the patient.\n\nmodel_2 <- glm(data = heart_cleveland_upload, condition ~ age + sex + chol, family = \"binomial\" )\n\nError in is.data.frame(data): object 'heart_cleveland_upload' not found\n\n# extract the model summary\nsummary(model_2)\n\nError in summary(model_2): object 'model_2' not found\n\n\n\n# get the predicted probability in our dataset using the predict() function\n# We include the argument type=”response” in order to get our prediction.\npred_prob_2 <- predict(model_2, heart_cleveland_upload, type=\"response\")\n\nError in predict(model_2, heart_cleveland_upload, type = \"response\"): object 'model_2' not found\n\n# create a decision rule using probability 0.5 as cutoff and save the predicted decision into the main data frame\nheart_cleveland_upload$pred_condition_2 <- ifelse(pred_prob_2 >= 0.5, 1, 0)\n\nError in ifelse(pred_prob_2 >= 0.5, 1, 0): object 'pred_prob_2' not found\n\n# create a newdata data frame to save a new case information\nnewdata_2 <- data.frame(age=45, sex='female', chol=150)\n\n# predict probability for this new case and print out the predicted value\np_new_2 <- predict(model_2, newdata_2, type=\"response\")\n\nError in predict(model_2, newdata_2, type = \"response\"): object 'model_2' not found\n\np_new_2\n\nError in eval(expr, envir, enclos): object 'p_new_2' not found\n\n\nWe see that this patient have 8.6% of chance developing heart disease. i.e indicating low risk of heart disease.\n\n# load Metrics package\nlibrary(Metrics)\n\n# calculate auc, accuracy, clasification error\nauc <- auc(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2) \n\nError in auc(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2): object 'heart_cleveland_upload' not found\n\naccuracy <- accuracy(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2)\n\nError in mean(actual != predicted): object 'heart_cleveland_upload' not found\n\nclassification_error <- ce(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2) \n\nError in mean(actual != predicted): object 'heart_cleveland_upload' not found\n\n# print out the metrics on to screen\nprint(paste(\"AUC=\", auc))\n\nError in paste(\"AUC=\", auc): cannot coerce type 'closure' to vector of type 'character'\n\nprint(paste(\"Accuracy=\", accuracy))\n\nError in paste(\"Accuracy=\", accuracy): cannot coerce type 'closure' to vector of type 'character'\n\nprint(paste(\"Classification Error=\", classification_error))\n\nError in paste(\"Classification Error=\", classification_error): object 'classification_error' not found\n\n# confusion matrix\ntable(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2, dnn=c(\"True Status\", \"Predicted Status\")) # confusion matrix\n\nError in table(heart_cleveland_upload$condition, heart_cleveland_upload$pred_condition_2, : object 'heart_cleveland_upload' not found\n\n\nWhen we compare our Model 1 and Model 2 based on metrics - AUC, Accuray .\nModel 1 has a accuracy of 71 % and Model 2 has a acuuracy of 67 %. Based on these two models , We can conculde that Model 1 is the best model with predictor variables age , sex and thalach to predict likelihood of developing heart disease."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#introduction",
    "href": "posts/Final_SteveONeill.html#introduction",
    "title": "Final Part 1",
    "section": "Introduction",
    "text": "Introduction\nBy analyzing public Paycheck Protection Program data, I have the ability to examine correlations between PPP loan size, forgiveness status, business status, geographical location, and - most optimistically - political affiliation. In the United States, the PPP was introduced as an emergency economic measure in 2020 during the Covid-19 pandemic. As a massive direct-loan program, some have criticized it for lax oversight.\nPPP fraud is an important issue, but finding lawbreakers is probably better left to the Feds. Instead, I think there is room in the academic body of work to study political affiliation for loan recipients.\n\nExisting Work\nThere have already been many studies about the PPP focusing on overall economic impact and the emergence of non-traditional, non-bank lenders.\nSeparately, two interesting studies have been done about racial disparities re: access to PPP loans, with one finding that Black-owned businesses in Florida were 25% less likely to receive PPP funds (Chernenko & Scharfstein, 2022), and another finding even higher of a disparity (50%) but taking note of mitigating factors (e.g. access to “fintech” lending instead of traditional banks) (Atkins, Cook & Seamans)\nI am not so much interested in the efficacy of the PPP or the racial make-up of PPP borrowers. But I am interested in their political affiliation, i.e. if they registered are Democrat or Republican. So, similarly to Chernenko & Scharfstein, I intend to use public-access data to cross-reference federal PPP data with state-level voter registration and corporation search data.\nA relevant paper I found after I started looking at this data - “Buying the Vote? The Economics of Electoral Politics and Small-Business Loans” - does look at data from SBA and PPP loans and pit them against the hypothesis ====that “electoral considerations may have tilted the allocation of PPP funds toward firms in areas or industries that could have a significant impact on the results of the 2020 election”.(Duchin & Hackney, 2021)\nThat particular study measured political ad spending and FEC filings as indicators of electoral considerations and used the Partisan Voting Index (PVI) from the Cook Political National Report to find which states were “battleground” or solid-R states. PPP lending outcomes were compared with electoral considerations to find if the SBA (under Trump) preferred to give PPP loans to swing voters or members of the party “base” in anticipation of an upcoming general election.\nRather than aggregate-level state data, my analysis will compare individuals’ personal voter affiliation - if they are registered, and to which party - with their loan amount, forgiveness status, and other metrics."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#research-question",
    "href": "posts/Final_SteveONeill.html#research-question",
    "title": "Final Part 1",
    "section": "Research Question",
    "text": "Research Question\nI am still developing my research question, but a simple one could be:\nHow does political affiliation affect PPP loan forgiveness status?\nThe data for PPP loans is current available on ProPublica, but not in a queryable way.\nFortunately, the underlying data has been made public at https://data.sba.gov/dataset/ppp-foia.\nMy hypotheses could be that:\nHypothesis 1 (H1): PPP loan forgiveness was given to registered Republicans more often than registered Democrats\nNull Hypothesis (H0): There is no difference in loan forgiveness given to registered Democrats vs registered Republicans.\nIn some ways this is similar to Duchin & Hackney, but this benefits from a narrower look at who actually received PPP stimulus rather than eventual outcomes on the state level.\nClearly a few effects that would need to be controlled for. For example, Republicans could be more likely to be in any kind of business in the first place, confounding my results.\nIdeally, I would have focused on my home state of Massachusetts. However, although voter registration data is personally available in MA, you can’t download it all in one .csv, and scraping the Commonwealth’s website is not allowed. The National Conference of State Legislatures keeps track of which states have full voter registration data for download. For now, I will use Ohio as an example (although later on I will explain why I could have chosen better):\n\nPPP Data Dictionary\nTo help understand which fields mean what, the PPP data comes with a “Data Dictionary” with an explanation of columns [scroll right to see explanation]:\n\n\nCode\nppp_data_dictionary <- read_excel(\"_data/ppp-data-dictionary.xlsx\")\nppp_data_dictionary\n\n\n\n\n  \n\n\n\n\n\nPPP Data (Ohio)\nImporting the data of the actual PPP loans is pretty easy. This .csv contains just the PPP loans below $150k - others are contained in a smaller spreadsheet that covers all 50 states, and will be part of my final project.\n\n\nCode\n#ppp_all <- read_csv(\"_data/public_up_to_150k_9_220930.csv\")\n#ppp_all\n\n\nIt still seems to capture all of Ohio, because Ohio doesn’t come last alphabetically. But to manage it better I need to sample it to 200k rows. The code below is commented out because this is a one-time process. Henceforth, I will be working with the .csv I generated in this step.\n\n\nCode\n#ppp_ohio <- ppp_all %>% filter(BorrowerState == \"OH\")\n#ppp_ohio_sampled_10k <- ppp_ohio %>% sample_n(10000)\n#write_csv(ppp_ohio_sampled_10k, \"_data/ppp_ohio_sampled_10k.csv\")\n\nppp_ohio <- read_csv(\"_data/ppp_ohio_sampled_10k.csv\")\n\n\nRows: 10000 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (35): DateApproved, SBAOfficeCode, ProcessingMethod, BorrowerName, Borro...\ndbl (18): LoanNumber, Term, SBAGuarantyPercentage, InitialApprovalAmount, Cu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nLLCs, Corporations\nVisually, you may notice that businesses and LLCs make up a majority of the loan recipients.\nWhen I was preparing to do this project for Massachusetts - before I knew about the limitation of voter registration data in that state - I contacted the Secretary of the Commonwealth and they sent me a document with the entire “Corporation Search” data in tabular format, with the name of the founder, every board member, business type, year established, etc. I anticipated I would be able to match those individuals with the voter registration data, but that data was unfortunately available.\nThe Corporation Search data would be essential to the successful completion of the project.\nI have not inquired yet, but assume Ohio will provide the same Corporation Search data. And if they do not, I will just pick another state that 1.) has public voter registration information, and 2.) can supply the Corporation Search data in .csv or .xlsx (as Massachusetts did).\n\n\nDeeper Looks\n\n\nCode\nglimpse(ppp_ohio)\n\n\nRows: 10,000\nColumns: 53\n$ LoanNumber                  <dbl> 8316258510, 2043838906, 2695968905, 786696…\n$ DateApproved                <chr> \"03/09/2021\", \"04/26/2021\", \"04/27/2021\", …\n$ SBAOfficeCode               <chr> \"0593\", \"0549\", \"0593\", \"0593\", \"0593\", \"0…\n$ ProcessingMethod            <chr> \"PPS\", \"PPP\", \"PPS\", \"PPP\", \"PPP\", \"PPS\", …\n$ BorrowerName                <chr> \"JASON MILLER\", \"JOHN HOLLY\", \"SHELLESSA D…\n$ BorrowerAddress             <chr> \"1008 Gateway Dr\", \"3977 E 186th St\", \"42 …\n$ BorrowerCity                <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ BorrowerState               <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ BorrowerZip                 <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ LoanStatusDate              <chr> \"11/19/2021\", \"01/31/2022\", \"09/25/2021\", …\n$ LoanStatus                  <chr> \"Paid in Full\", \"Paid in Full\", \"Paid in F…\n$ Term                        <dbl> 60, 60, 60, 24, 60, 60, 24, 60, 24, 60, 60…\n$ SBAGuarantyPercentage       <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 10…\n$ InitialApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ CurrentApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ UndisbursedAmount           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FranchiseName               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ServicingLenderLocationID   <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ ServicingLenderName         <chr> \"Civista Bank\", \"American Lending Center\",…\n$ ServicingLenderAddress      <chr> \"100 E Water St\", \"1 World Trade Center, S…\n$ ServicingLenderCity         <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ ServicingLenderState        <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ ServicingLenderZip          <chr> \"44870-2524\", \"90831\", \"92653\", \"37030-120…\n$ RuralUrbanIndicator         <chr> \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U…\n$ HubzoneIndicator            <chr> \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ LMIIndicator                <chr> \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ BusinessAgeDescription      <chr> \"Existing or more than 2 years old\", \"Exis…\n$ ProjectCity                 <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ ProjectCountyName           <chr> \"MONTGOMERY\", \"CUYAHOGA\", \"MONTGOMERY\", \"H…\n$ ProjectState                <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ ProjectZip                  <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ CD                          <chr> \"OH-10\", \"OH-11\", \"OH-10\", \"OH-01\", \"OH-01…\n$ JobsReported                <dbl> 12, 1, 1, 14, 1, 1, 17, 1, 2, 1, 2, 1, 1, …\n$ NAICSCode                   <dbl> 722410, 722320, 624110, 721110, 531210, 56…\n$ Race                        <chr> \"White\", \"Unanswered\", \"Unanswered\", \"Unan…\n$ Ethnicity                   <chr> \"Not Hispanic or Latino\", \"Unknown/NotStat…\n$ UTILITIES_PROCEED           <dbl> 1, NA, NA, NA, NA, 1, NA, NA, NA, 1, 1, NA…\n$ PAYROLL_PROCEED             <dbl> 61853.00, 20833.33, 11414.00, 57540.00, 20…\n$ MORTGAGE_INTEREST_PROCEED   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RENT_PROCEED                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ REFINANCE_EIDL_PROCEED      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ HEALTH_CARE_PROCEED         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ DEBT_INTEREST_PROCEED       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ BusinessType                <chr> \"Limited  Liability Company(LLC)\", \"Sole P…\n$ OriginatingLenderLocationID <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ OriginatingLender           <chr> \"Civista Bank\", \"American Lending Center\",…\n$ OriginatingLenderCity       <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ OriginatingLenderState      <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ Gender                      <chr> \"Male Owned\", \"Unanswered\", \"Unanswered\", …\n$ Veteran                     <chr> \"Non-Veteran\", \"Unanswered\", \"Unanswered\",…\n$ NonProfit                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ForgivenessAmount           <dbl> 62252.49, 20924.77, 11447.46, 57990.86, 20…\n$ ForgivenessDate             <chr> \"10/26/2021\", \"10/06/2021\", \"08/20/2021\", …\n\n\nForgivenessAmount, ForgivenessDate, and BorrowerName promise to be the most useful variables.\n\n\nCode\nprint(summarytools::dfSummary(ppp_ohio,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nppp_ohio\nDimensions: 10000 x 53\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      LoanNumber\n[numeric]\n      Mean (sd) : 5492418880 (2600653757)min ≤ med ≤ max:1000358703 ≤ 5564758604 ≤ 9997128810IQR (CV) : 4509922449 (0.5)\n      10000 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DateApproved\n[character]\n      1. 05/01/20202. 04/28/20203. 04/15/20204. 04/27/20205. 04/14/20206. 05/12/20217. 04/30/20208. 04/29/20209. 04/22/202110. 03/23/2021[ 212 others ]\n      724(7.2%)312(3.1%)265(2.6%)240(2.4%)179(1.8%)164(1.6%)155(1.6%)154(1.5%)150(1.5%)149(1.5%)7508(75.1%)\n      \n      0\n(0.0%)\n    \n    \n      SBAOfficeCode\n[character]\n      1. 01012. 05493. 0593\n      1(0.0%)4830(48.3%)5169(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      ProcessingMethod\n[character]\n      1. PPP2. PPS\n      7835(78.3%)2165(21.6%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerName\n[character]\n      1. MICHAEL JOHNSON2. 16ELEVEN LLC3. ALL SERVICE AERATION4. ANDREW BRUSH5. ANTONIO JONES6. ASHLEY JACKSON7. BLUE FITNESS INC8. BRITTANY JONES9. BRITTNEY HAYWARD10. CAMERON JOHNSON[ 9924 others ]\n      3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9979(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerAddress\n[character]\n      1. 24 E North St2. 24455 Lake Shore Blvd3. 4503 Marburg Ave4. 4910 Tiedeman Dr5. 1 E Campus View Blvd6. 100 E Campus View Blvd St7. 100 E Wilson Bridge Rd8. 1005 W 3rd Ave9. 109 Elm St10. 11271 Reading Rd[ 9911 others ]\n      3(0.0%)3(0.0%)3(0.0%)3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9976(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1299 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerZip\n[character]\n      1. 441452. 441243. 432154. 450115. 430176. 430657. 452388. 440959. 4413910. 45242[ 8358 others ]\n      21(0.2%)20(0.2%)17(0.2%)17(0.2%)16(0.2%)16(0.2%)15(0.1%)14(0.1%)14(0.1%)14(0.1%)9836(98.4%)\n      \n      0\n(0.0%)\n    \n    \n      LoanStatusDate\n[character]\n      1. 03/22/20222. 11/17/20213. 01/06/20224. 08/17/20215. 09/28/20216. 09/25/20217. 10/21/20218. 11/20/20219. 10/20/202110. 10/15/2021[ 380 others ]\n      392(4.5%)258(2.9%)243(2.8%)240(2.7%)166(1.9%)159(1.8%)131(1.5%)130(1.5%)121(1.4%)102(1.2%)6844(77.9%)\n      \n      1214\n(12.1%)\n    \n    \n      LoanStatus\n[character]\n      1. Exemption 42. Paid in Full\n      1214(12.1%)8786(87.9%)\n      \n      0\n(0.0%)\n    \n    \n      Term\n[numeric]\n      Mean (sd) : 47.7 (17.2)min ≤ med ≤ max:0 ≤ 60 ≤ 75IQR (CV) : 36 (0.4)\n      28 distinct values\n      \n      0\n(0.0%)\n    \n    \n      SBAGuarantyPercentage\n[numeric]\n      1 distinct value\n      100:10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      InitialApprovalAmount\n[numeric]\n      Mean (sd) : 26700 (28548)min ≤ med ≤ max:105 ≤ 20207 ≤ 252947IQR (CV) : 16700 (1.1)\n      5789 distinct values\n      \n      0\n(0.0%)\n    \n    \n      CurrentApprovalAmount\n[numeric]\n      Mean (sd) : 26570.5 (28170.1)min ≤ med ≤ max:105 ≤ 20207 ≤ 149700IQR (CV) : 16654.9 (1.1)\n      5803 distinct values\n      \n      0\n(0.0%)\n    \n    \n      UndisbursedAmount\n[numeric]\n      1 distinct value\n      0:9998(100.0%)\n      \n      2\n(0.0%)\n    \n    \n      FranchiseName\n[character]\n      1. Subway2. Comfort Inn by Choice Hot3. H&R Block - Franchise Lic4. Hot Head Burritos5. Vision Source6. Dunkin' Donuts7. Holiday Inn Express (Lice8. Orange Leaf Frozen Yogurt9. Wild Birds Unlimited10. Zoup![ 79 others ]\n      9(8.2%)3(2.7%)3(2.7%)3(2.7%)3(2.7%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)79(71.8%)\n      \n      9890\n(98.9%)\n    \n    \n      ServicingLenderLocationID\n[numeric]\n      Mean (sd) : 214404.3 (203525.1)min ≤ med ≤ max:4282 ≤ 79184 ≤ 538160IQR (CV) : 441813 (0.9)\n      327 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderName\n[character]\n      1. The Huntington National B2. Harvest Small Business Fi3. Prestamos CDFI, LLC4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. Cross River Bank[ 314 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderAddress\n[character]\n      1. 17 S High St.2. 24422 Avenida de la Carlo3. 1024 East Buckeye Road Su4. 2247 Central Drive5. 38 Fountain Sq Plz6. 7000 SW 97th Avenue Suite7. 222 Delaware Ave8. 1111 Polaris Pkwy9. 425 Walnut St10. 885 Teaneck Rd[ 315 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Laguna Hills4. Phoenix5. Bedford6. Miami7. WILMINGTON8. TEANECK9. CLEVELAND10. LAKE MARY[ 255 others ]\n      1275(12.8%)975(9.8%)718(7.2%)624(6.2%)484(4.8%)427(4.3%)382(3.8%)306(3.1%)302(3.0%)248(2.5%)4259(42.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. NJ7. DE8. PA9. NY10. RI[ 29 others ]\n      5178(51.8%)907(9.1%)681(6.8%)625(6.2%)515(5.1%)379(3.8%)378(3.8%)362(3.6%)243(2.4%)216(2.2%)516(5.2%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderZip\n[character]\n      1. 43215-34132. 926533. 850344. 760215. 452636. 331737. 19801-16218. 43240-20319. 45202-395610. 07666-4546[ 316 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      RuralUrbanIndicator\n[character]\n      1. R2. U\n      2381(23.8%)7619(76.2%)\n      \n      0\n(0.0%)\n    \n    \n      HubzoneIndicator\n[character]\n      1. N2. Y\n      6935(69.3%)3065(30.6%)\n      \n      0\n(0.0%)\n    \n    \n      LMIIndicator\n[character]\n      1. N2. Y\n      6823(68.2%)3177(31.8%)\n      \n      0\n(0.0%)\n    \n    \n      BusinessAgeDescription\n[character]\n      1. Existing or more than 2 y2. New Business or 2 years o3. Startup, Loan Funds will 4. Unanswered\n      9005(90.0%)433(4.3%)1(0.0%)561(5.6%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1298 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCountyName\n[character]\n      1. CUYAHOGA2. FRANKLIN3. HAMILTON4. SUMMIT5. LUCAS6. MONTGOMERY7. STARK8. BUTLER9. MAHONING10. LORAIN[ 78 others ]\n      1810(18.1%)1277(12.8%)800(8.0%)491(4.9%)473(4.7%)473(4.7%)273(2.7%)237(2.4%)224(2.2%)197(2.0%)3745(37.5%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectZip\n[character]\n      1. 44145-00012. 43215-00013. 44124-00014. 45011-00015. 43065-00016. 45242-00017. 43017-00018. 44122-00019. 44139-000110. 43015-0001[ 8476 others ]\n      21(0.2%)19(0.2%)19(0.2%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)9847(98.5%)\n      \n      0\n(0.0%)\n    \n    \n      CD\n[character]\n      1. OH-112. OH-033. OH-054. OH-015. OH-096. OH-147. OH-128. OH-089. OH-1010. OH-13[ 7 others ]\n      1352(13.5%)832(8.3%)699(7.0%)679(6.8%)646(6.5%)613(6.1%)610(6.1%)555(5.6%)555(5.6%)541(5.4%)2918(29.2%)\n      \n      0\n(0.0%)\n    \n    \n      JobsReported\n[numeric]\n      Mean (sd) : 4.1 (11.9)min ≤ med ≤ max:1 ≤ 1 ≤ 500IQR (CV) : 3 (2.9)\n      69 distinct values\n      \n      0\n(0.0%)\n    \n    \n      NAICSCode\n[numeric]\n      Mean (sd) : 540071.1 (202394.1)min ≤ med ≤ max:111110 ≤ 541511 ≤ 999990IQR (CV) : 267779.5 (0.4)\n      686 distinct values\n      \n      113\n(1.1%)\n    \n    \n      Race\n[character]\n      1. American Indian or Alaska2. Asian3. Black or African American4. Native Hawaiian or Other 5. Unanswered6. White\n      23(0.2%)129(1.3%)1103(11.0%)7(0.1%)6905(69.0%)1833(18.3%)\n      \n      0\n(0.0%)\n    \n    \n      Ethnicity\n[character]\n      1. Hispanic or Latino2. Not Hispanic or Latino3. Unknown/NotStated\n      119(1.2%)3279(32.8%)6602(66.0%)\n      \n      0\n(0.0%)\n    \n    \n      UTILITIES_PROCEED\n[numeric]\n      Mean (sd) : 513.4 (3183.8)min ≤ med ≤ max:0 ≤ 1 ≤ 134700IQR (CV) : 0 (6.2)\n      345 distinct values\n      \n      6802\n(68.0%)\n    \n    \n      PAYROLL_PROCEED\n[numeric]\n      Mean (sd) : 26102.4 (27604.3)min ≤ med ≤ max:0 ≤ 20052 ≤ 149700IQR (CV) : 16222 (1.1)\n      6362 distinct values\n      \n      10\n(0.1%)\n    \n    \n      MORTGAGE_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 2901 (7033.4)min ≤ med ≤ max:0 ≤ 1210.7 ≤ 86712IQR (CV) : 2737.8 (2.4)\n      140 distinct values\n      \n      9805\n(98.0%)\n    \n    \n      RENT_PROCEED\n[numeric]\n      Mean (sd) : 5396.7 (5788.6)min ≤ med ≤ max:0 ≤ 3700 ≤ 36502IQR (CV) : 5746.7 (1.1)\n      263 distinct values\n      \n      9632\n(96.3%)\n    \n    \n      REFINANCE_EIDL_PROCEED\n[numeric]\n      Mean (sd) : 1550.9 (3835.3)min ≤ med ≤ max:0 ≤ 0 ≤ 25200IQR (CV) : 1000 (2.5)\n      13 distinct values\n      \n      9931\n(99.3%)\n    \n    \n      HEALTH_CARE_PROCEED\n[numeric]\n      Mean (sd) : 4122 (5865.4)min ≤ med ≤ max:0 ≤ 2070 ≤ 32900IQR (CV) : 4600 (1.4)\n      86 distinct values\n      \n      9871\n(98.7%)\n    \n    \n      DEBT_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 1349.6 (4310.4)min ≤ med ≤ max:0 ≤ 150 ≤ 36100IQR (CV) : 1186.5 (3.2)\n      34 distinct values\n      \n      9924\n(99.2%)\n    \n    \n      BusinessType\n[character]\n      1. Sole Proprietorship2. Limited  Liability Compan3. Corporation4. Independent Contractors5. Subchapter S Corporation6. Self-Employed Individuals7. Non-Profit Organization8. Single Member LLC9. Partnership10. Limited Liability Partner[ 7 others ]\n      3250(32.5%)2545(25.5%)1285(12.9%)865(8.7%)790(7.9%)760(7.6%)227(2.3%)89(0.9%)70(0.7%)60(0.6%)52(0.5%)\n      \n      7\n(0.1%)\n    \n    \n      OriginatingLenderLocationID\n[numeric]\n      Mean (sd) : 213535.4 (203054.8)min ≤ med ≤ max:4282 ≤ 78723 ≤ 531105IQR (CV) : 430853.2 (1)\n      331 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLender\n[character]\n      1. The Huntington National B2. Prestamos CDFI, LLC3. Harvest Small Business Fi4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. KeyBank National Associat[ 317 others ]\n      846(8.5%)624(6.2%)497(5.0%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)292(2.9%)5295(52.9%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Phoenix4. Laguna Hills5. Bedford6. Miami7. WILMINGTON8. CLEVELAND9. TEANECK10. LAKE MARY[ 259 others ]\n      1275(12.8%)975(9.8%)624(6.2%)497(5.0%)484(4.8%)427(4.3%)382(3.8%)302(3.0%)268(2.7%)247(2.5%)4519(45.2%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. DE7. NJ8. PA9. RI10. UT[ 29 others ]\n      5183(51.8%)849(8.5%)681(6.8%)625(6.2%)519(5.2%)378(3.8%)349(3.5%)311(3.1%)216(2.2%)209(2.1%)680(6.8%)\n      \n      0\n(0.0%)\n    \n    \n      Gender\n[character]\n      1. Female Owned2. Male Owned3. Unanswered\n      1604(16.0%)2753(27.5%)5643(56.4%)\n      \n      0\n(0.0%)\n    \n    \n      Veteran\n[character]\n      1. Non-Veteran2. Unanswered3. Veteran\n      3701(37.0%)6096(61.0%)203(2.0%)\n      \n      0\n(0.0%)\n    \n    \n      NonProfit\n[character]\n      1. Y\n      251(100.0%)\n      \n      9749\n(97.5%)\n    \n    \n      ForgivenessAmount\n[numeric]\n      Mean (sd) : 27785.1 (29446.5)min ≤ med ≤ max:12.6 ≤ 20307.2 ≤ 150842.8IQR (CV) : 20463.8 (1.1)\n      8422 distinct values\n      \n      1145\n(11.5%)\n    \n    \n      ForgivenessDate\n[character]\n      1. 06/15/20212. 10/06/20213. 01/07/20214. 09/07/20215. 09/29/20216. 09/01/20217. 06/23/20218. 09/22/20219. 11/10/202110. 10/14/2021[ 432 others ]\n      164(1.9%)147(1.7%)132(1.5%)98(1.1%)96(1.1%)92(1.0%)91(1.0%)91(1.0%)82(0.9%)79(0.9%)7783(87.9%)\n      \n      1145\n(11.5%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-10-24\n\n\n\n(Data below from unsampled, larger dataset)\nFrom a first look, most applicants reported only one employee needing coverage under the PPP. In fact, “Sole Proprietorship” was the highest category of BusinessType, above even LLCs, with 33.3%.\n19% of applicants were white, leading the race category, and 68% were unreported.\nMedian ForgivenessAmount was $20,438.7, and most loans were forgiven in 2021."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#voter-registration",
    "href": "posts/Final_SteveONeill.html#voter-registration",
    "title": "Final Part 1",
    "section": "Voter Registration",
    "text": "Voter Registration\nThe Ohio voter registration data is easily read-in:\n\n\nCode\n#ohio_voters <- read_csv(\"_data/SWVF_1_22.txt\")\n#ohio_voters_sampled <- ohio_voters %>% sample_n(10000)\n#write_csv(ohio_voters_sampled, \"_data/ohio_voters_sampled.csv\")\n\nohio_voters <- read_csv(\"_data/ohio_voters_sampled.csv\")\n\n\nRows: 10000 Columns: 116\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (91): SOS_VOTERID, COUNTY_NUMBER, LAST_NAME, FIRST_NAME, MIDDLE_NAME, S...\ndbl   (4): COUNTY_ID, RESIDENTIAL_ZIP, MAILING_ZIP, STATE_REPRESENTATIVE_DIS...\nlgl  (19): RESIDENTIAL_COUNTRY, RESIDENTIAL_POSTALCODE, MAILING_SECONDARY_AD...\ndate  (2): DATE_OF_BIRTH, REGISTRATION_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis is a 1gb+ file, which needs to be fully complete in order for the join of sampled PPP data. For the actual project, I will do that in a one-time process on my local computer.\nZip codes will let us know with high confidence that we are dealing with the same business owner even if names are duplicated.\nThe historical information tells us if they have changed their party affiliation from year-to-year, which can be useful in determining if political rent-seeking was successful - we can even see ‘D’ or ‘R’ going back 22 years:\n\n\nCode\nglimpse(ohio_voters)\n\n\nRows: 10,000\nColumns: 116\n$ SOS_VOTERID                   <chr> \"OH0010417183\", \"OH0024878858\", \"OH00159…\n$ COUNTY_NUMBER                 <chr> \"19\", \"18\", \"18\", \"12\", \"18\", \"03\", \"10\"…\n$ COUNTY_ID                     <dbl> 9500876, 2859748, 73683, 122609458, 2158…\n$ LAST_NAME                     <chr> \"COLLINS\", \"SAFO\", \"WHELAN\", \"OVERHOLT\",…\n$ FIRST_NAME                    <chr> \"SHANNON\", \"SHAREEF\", \"PATRICK\", \"ADDISO…\n$ MIDDLE_NAME                   <chr> \"AMY\", \"RESHAUN\", \"HOWARD\", \"ELIZABETH\",…\n$ SUFFIX                        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ DATE_OF_BIRTH                 <date> 1977-02-22, 1998-12-18, 1969-07-21, 200…\n$ REGISTRATION_DATE             <date> 1995-05-19, 2021-09-01, 2022-08-02, 202…\n$ VOTER_STATUS                  <chr> \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", …\n$ PARTY_AFFILIATION             <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ RESIDENTIAL_ADDRESS1          <chr> \"372 S MAIN ST\", \"1629 CARLYON RD\", \"232…\n$ RESIDENTIAL_SECONDARY_ADDR    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_CITY              <chr> \"NEW MADISON\", \"EAST CLEVELAND\", \"NORTH …\n$ RESIDENTIAL_STATE             <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\"…\n$ RESIDENTIAL_ZIP               <dbl> 45346, 44112, 44070, 45502, 44109, 44805…\n$ RESIDENTIAL_ZIP_PLUS4         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_COUNTRY           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_POSTALCODE        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ADDRESS1              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_SECONDARY_ADDRESS     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_CITY                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_STATE                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP_PLUS4             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_COUNTRY               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_POSTAL_CODE           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CAREER_CENTER                 <chr> \"MIAMI VALLEY CAREER TECH\", NA, \"POLARIS…\n$ CITY                          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CITY_SCHOOL_DISTRICT          <chr> NA, \"EAST CLEVELAND CITY SD\", \"NORTH OLM…\n$ COUNTY_COURT_DISTRICT         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CONGRESSIONAL_DISTRICT        <chr> \"08\", \"11\", \"07\", \"15\", \"11\", \"04\", \"06\"…\n$ COURT_OF_APPEALS              <chr> \"02\", \"08\", \"08\", \"02\", \"08\", \"05\", \"07\"…\n$ EDU_SERVICE_CENTER_DISTRICT   <chr> \"DARKE COUNTY ESC\", NA, NA, \"CLARK COUNT…\n$ EXEMPTED_VILL_SCHOOL_DISTRICT <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LIBRARY                       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LOCAL_SCHOOL_DISTRICT         <chr> \"TRI-VILLAGE LOCAL SD (DARKE)\", NA, NA, …\n$ MUNICIPAL_COURT_DISTRICT      <chr> \"DARKE-CO\", \"EAST-CLEVELAND\", \"ROCKY-RIV…\n$ PRECINCT_NAME                 <chr> \"PRECINCT HARRISON EAST & NEW MADISON\", …\n$ PRECINCT_CODE                 <chr> \"19AAZ\", \"18-P-BMR\", \"18-P-CDT\", \"12ADJ\"…\n$ STATE_BOARD_OF_EDUCATION      <chr> \"03\", \"10\", \"11\", \"10\", \"11\", \"05\", \"08\"…\n$ STATE_REPRESENTATIVE_DISTRICT <dbl> 80, 22, 16, 74, 15, 67, 79, 71, 47, 14, …\n$ STATE_SENATE_DISTRICT         <chr> \"05\", \"21\", \"24\", \"10\", \"24\", \"22\", \"33\"…\n$ TOWNSHIP                      <chr> \"HARRISON TWP\", NA, NA, \"BETHEL TOWNSHIP…\n$ VILLAGE                       <chr> \"NEW MADISON VILLAGE\", NA, NA, NA, NA, N…\n$ WARD                          <chr> NA, \"EAST CLEVELAND WARD 2\", \"NORTH OLMS…\n$ `PRIMARY-03/07/2000`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/07/2000`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, NA…\n$ `SPECIAL-05/08/2001`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/06/2001`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-05/07/2002`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/05/2002`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, \"X\", NA…\n$ `SPECIAL-05/06/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/04/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-03/02/2004`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/02/2004`          <chr> \"X\", NA, \"X\", NA, NA, NA, \"X\", NA, \"X\", …\n$ `SPECIAL-02/08/2005`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/03/2005`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/13/2005`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2005`          <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, NA, NA…\n$ `SPECIAL-02/07/2006`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/02/2006`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `GENERAL-11/07/2006`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/11/2007`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-12/11/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-03/04/2008`          <chr> NA, NA, NA, NA, \"D\", NA, \"D\", NA, \"R\", N…\n$ `PRIMARY-10/14/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/04/2008`          <chr> \"X\", NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\",…\n$ `GENERAL-11/18/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/05/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/08/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/15/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/29/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2009`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/04/2010`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `PRIMARY-07/13/2010`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/07/2010`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2010`          <chr> \"X\", NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/03/2011`          <chr> \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `PRIMARY-09/13/2011`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2011`          <chr> \"X\", NA, \"X\", NA, NA, NA, NA, NA, \"X\", N…\n$ `PRIMARY-03/06/2012`          <chr> NA, NA, NA, NA, \"R\", NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/06/2012`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/07/2013`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/10/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-10/01/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2013`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/06/2014`          <chr> \"L\", NA, NA, NA, NA, NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/04/2014`          <chr> NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", N…\n$ `PRIMARY-05/05/2015`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/15/2015`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2015`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-03/15/2016`          <chr> \"D\", NA, NA, NA, NA, \"R\", \"D\", NA, \"R\", …\n$ `GENERAL-06/07/2016`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/13/2016`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2016`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"X…\n$ `PRIMARY-05/02/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/12/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/07/2017`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2018`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, NA, \"R…\n$ `GENERAL-08/07/2018`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/06/2018`          <chr> \"X\", \"X\", NA, NA, NA, \"X\", \"X\", \"X\", \"X\"…\n$ `PRIMARY-05/07/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/10/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2019`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X…\n$ `PRIMARY-03/17/2020`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, \"R\"…\n$ `GENERAL-11/03/2020`          <chr> \"X\", \"X\", \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"…\n$ `PRIMARY-05/04/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"X\",…\n$ `PRIMARY-08/03/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/14/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2021`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", \"X\", NA, …\n$ `PRIMARY-05/03/2022`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ `PRIMARY-08/02/2022`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n\n\n\n\nCode\nprint(summarytools::dfSummary(ohio_voters,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nohio_voters\nDimensions: 10000 x 116\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      SOS_VOTERID\n[character]\n      1. OH00100003302. OH00100003443. OH00100008674. OH00100012975. OH00100022616. OH00100028027. OH00100028178. OH00100028989. OH001000297910. OH0010003129[ 9990 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)9990(99.9%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_NUMBER\n[character]\n      1. 182. 093. 214. 135. 126. 157. 028. 049. 2210. 07[ 12 others ]\n      4086(40.9%)1187(11.9%)685(6.9%)683(6.8%)392(3.9%)346(3.5%)326(3.3%)286(2.9%)249(2.5%)208(2.1%)1552(15.5%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_ID\n[numeric]\n      Mean (sd) : 4804609 (14312683)min ≤ med ≤ max:30 ≤ 1939790 ≤ 122614854IQR (CV) : 2624441 (3)\n      9992 distinct values\n      \n      0\n(0.0%)\n    \n    \n      LAST_NAME\n[character]\n      1. SMITH2. JOHNSON3. WILLIAMS4. MILLER5. JONES6. BROWN7. THOMPSON8. DAVIS9. JACKSON10. MOORE[ 6190 others ]\n      91(0.9%)81(0.8%)72(0.7%)63(0.6%)59(0.6%)58(0.6%)38(0.4%)35(0.4%)33(0.3%)33(0.3%)9437(94.4%)\n      \n      0\n(0.0%)\n    \n    \n      FIRST_NAME\n[character]\n      1. MICHAEL2. ROBERT3. JOHN4. DAVID5. WILLIAM6. JAMES7. MARY8. THOMAS9. CHRISTOPHER10. RICHARD[ 2495 others ]\n      160(1.6%)143(1.4%)130(1.3%)126(1.3%)125(1.2%)123(1.2%)94(0.9%)88(0.9%)78(0.8%)74(0.7%)8859(88.6%)\n      \n      0\n(0.0%)\n    \n    \n      MIDDLE_NAME\n[character]\n      1. A2. M3. L4. J5. E6. D7. R8. C9. S10. ANN[ 1186 others ]\n      713(7.8%)696(7.6%)677(7.4%)496(5.4%)322(3.5%)314(3.4%)302(3.3%)227(2.5%)220(2.4%)216(2.4%)4975(54.3%)\n      \n      842\n(8.4%)\n    \n    \n      SUFFIX\n[character]\n      1. I2. II3. III4. IV5. JR6. SR7. V\n      2(0.5%)35(8.4%)57(13.6%)8(1.9%)259(61.8%)56(13.4%)2(0.5%)\n      \n      9581\n(95.8%)\n    \n    \n      DATE_OF_BIRTH\n[Date]\n      min : 1922-07-17med : 1972-01-18max : 2004-10-25range : 82y 3m 8d\n      8207 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REGISTRATION_DATE\n[Date]\n      min : 1900-01-01med : 2015-12-31max : 2022-10-07range : 122y 9m 6d\n      4484 distinct values\n      \n      0\n(0.0%)\n    \n    \n      VOTER_STATUS\n[character]\n      1. ACTIVE2. CONFIRMATION\n      8534(85.3%)1466(14.7%)\n      \n      0\n(0.0%)\n    \n    \n      PARTY_AFFILIATION\n[character]\n      1. D2. L3. R\n      1431(45.7%)2(0.1%)1697(54.2%)\n      \n      6870\n(68.7%)\n    \n    \n      RESIDENTIAL_ADDRESS1\n[character]\n      1. 2201 ACACIA PARK DR2. 1055 OLD RIVER RD3. 11050 FANCHER RD4. 112 S CLINTON ST5. 12900 LAKE AVE6. 16700 LAKE SHORE BLVD7. 18221 EUCLID AVE8. 19101 VAN AKEN BLVD9. 19201 EUCLID AVE10. 2020 TAYLOR RD[ 9834 others ]\n      4(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)9969(99.7%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_SECONDARY_ADDR\n[character]\n      1. APT 12. UPPR3. APT 24. APT 35. LOWR6. APT 47. APT B8. APT 79. APT 20110. APT A[ 565 others ]\n      54(4.4%)43(3.5%)39(3.2%)34(2.8%)32(2.6%)27(2.2%)22(1.8%)17(1.4%)16(1.3%)16(1.3%)923(75.5%)\n      \n      8777\n(87.8%)\n    \n    \n      RESIDENTIAL_CITY\n[character]\n      1. CLEVELAND2. HAMILTON3. SPRINGFIELD4. PARMA5. MIDDLETOWN6. LIMA7. WEST CHESTER8. LAKEWOOD9. DELAWARE10. CLEVELAND HTS[ 324 others ]\n      1166(11.7%)429(4.3%)286(2.9%)244(2.4%)235(2.4%)225(2.2%)191(1.9%)181(1.8%)180(1.8%)177(1.8%)6686(66.9%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_STATE\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP\n[numeric]\n      Mean (sd) : 44459.3 (713.5)min ≤ med ≤ max:43003 ≤ 44138 ≤ 45896IQR (CV) : 949 (0)\n      338 distinct values\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP_PLUS4\n[character]\n      1. 11012. 16383. 96084. 10305. 10526. 11127. 12018. 12419. 132010. 1325[ 531 others ]\n      3(0.5%)3(0.5%)3(0.5%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)568(96.1%)\n      \n      9409\n(94.1%)\n    \n    \n      RESIDENTIAL_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      RESIDENTIAL_POSTALCODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_ADDRESS1\n[character]\n      1. PO BOX 32. PO BOX 410523. 10 CHRISTOPHER WAY APT 44. 100 ELM AV5. 10072A HORIZON ST6. 10115 VANDERBILT CIRCLE7. 1012 PROSPECT AVE APT 1028. 10919 GLENVIEW AVENUE9. 11 N FISHER DR ROOM 22110. 1121 W COLUMBIA AVE APT  [ 201 others ]\n      2(0.9%)2(0.9%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)201(94.4%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_SECONDARY_ADDRESS\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_CITY\n[character]\n      1. OXFORD2. CLEVELAND3. ANDOVER4. LIMA5. BETHESDA6. CLARKSVILLE7. DELAWARE8. FELICITY9. GENEVA10. GETTYSBURG[ 133 others ]\n      15(7.0%)8(3.8%)6(2.8%)4(1.9%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)162(76.1%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_STATE\n[character]\n      1. OH2. CA3. IL4. VA5. AE6. IN7. MA8. WA9. AL10. BC[ 4 others ]\n      189(89.2%)3(1.4%)3(1.4%)3(1.4%)2(0.9%)2(0.9%)2(0.9%)2(0.9%)1(0.5%)1(0.5%)4(1.9%)\n      \n      9788\n(97.9%)\n    \n    \n      MAILING_ZIP\n[numeric]\n      Mean (sd) : 44768.7 (10335.5)min ≤ med ≤ max:1821 ≤ 44731 ≤ 98433IQR (CV) : 1355.2 (0.2)\n      150 distinct values\n      \n      9790\n(97.9%)\n    \n    \n      MAILING_ZIP_PLUS4\n[character]\n      1. 00622. 01093. 01334. 01655. 01736. 01817. 02088. 02709. 028210. 0367[ 15 others ]\n      1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)15(60.0%)\n      \n      9975\n(99.8%)\n    \n    \n      MAILING_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_POSTAL_CODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CAREER_CENTER\n[character]\n      1. GREAT OAKS CAREER CAMPUSE2. POLARIS CAREER CENTER3. DELAWARE AREA CAREER CENT4. SPRINGFIELD-CLARK COUNTY 5. CUYAHOGA VALLEY CAREER CE6. APOLLO CAREER CENTER7. COLUMBIANA COUNTY CAREER 8. BELMONT-HARRISON CAREER C9. EHOVE CAREER CENTER10. ASHLAND COUNTY-WEST HOLME[ 21 others ]\n      691(15.9%)571(13.2%)553(12.7%)392(9.0%)356(8.2%)281(6.5%)197(4.5%)196(4.5%)162(3.7%)147(3.4%)794(18.3%)\n      \n      5660\n(56.6%)\n    \n    \n      CITY\n[character]\n      1. HAMILTON CITY2. SPRINGFIELD CITY3. MIDDLETOWN CITY4. FAIRFIELD CITY5. DELAWARE CITY6. LIMA CITY7. SANDUSKY CITY8. ASHLAND CITY9. ATHENS CITY10. OXFORD CITY[ 32 others ]\n      172(8.7%)171(8.7%)137(6.9%)124(6.3%)120(6.1%)95(4.8%)79(4.0%)67(3.4%)62(3.1%)56(2.8%)892(45.2%)\n      \n      8025\n(80.2%)\n    \n    \n      CITY_SCHOOL_DISTRICT\n[character]\n      1. CLEVELAND MUNICIPAL CITY 2. PARMA CITY SD3. FAIRFIELD CITY SD4. CLEVELAND HTS-UNIV HTS CI5. LAKEWOOD CITY SD6. HAMILTON CITY SD7. SPRINGFIELD CITY SD8. EUCLID CITY SD9. BEREA CITY SD10. STRONGSVILLE CITY SD[ 55 others ]\n      1167(18.4%)343(5.4%)218(3.4%)214(3.4%)181(2.8%)173(2.7%)166(2.6%)153(2.4%)148(2.3%)148(2.3%)3442(54.2%)\n      \n      3647\n(36.5%)\n    \n    \n      COUNTY_COURT_DISTRICT\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CONGRESSIONAL_DISTRICT\n[character]\n      1. 112. 073. 084. 045. 026. 067. 128. 099. 1010. 14[ 2 others ]\n      2522(25.2%)1564(15.6%)1358(13.6%)1274(12.7%)1006(10.1%)641(6.4%)445(4.4%)374(3.7%)294(2.9%)286(2.9%)236(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      COURT_OF_APPEALS\n[character]\n      1. 022. 033. 044. 055. 066. 077. 088. 119. 12\n      683(6.8%)725(7.2%)244(2.4%)961(9.6%)249(2.5%)641(6.4%)4086(40.9%)286(2.9%)2125(21.2%)\n      \n      0\n(0.0%)\n    \n    \n      EDU_SERVICE_CENTER_DISTRICT\n[character]\n      1. ESC OF CENTRAL OHIO2. BUTLER COUNTY ESC3. CLARK COUNTY ESC4. ALLEN COUNTY ESC5. ASHTABULA COUNTY ESC6. BROWN COUNTY ESC7. NORTH POINT ESC8. COLUMBIANA COUNTY ESC9. SOUTHERN OHIO ESC10. DARKE COUNTY ESC[ 23 others ]\n      503(18.8%)496(18.6%)226(8.5%)202(7.6%)124(4.6%)120(4.5%)120(4.5%)116(4.3%)116(4.3%)80(3.0%)569(21.3%)\n      \n      7328\n(73.3%)\n    \n    \n      EXEMPTED_VILL_SCHOOL_DISTRICT\n[character]\n      1. MILFORD EX VILL SD (CLERM2. CARROLLTON EX VILL SD (CA3. NEW RICHMOND EX VILL SD (4. COLUMBIANA EX VILL SD (CO5. BARNESVILLE EX VILL SD (B6. CHAGRIN FALLS EX VILL SD 7. VERSAILLES EX VILL SD (DA8. MECHANICSBURG EX VILL SD 9. BLUFFTON EX VILL SD (ALLE10. HICKSVILLE EX VILL SD (DE[ 7 others ]\n      176(32.8%)57(10.6%)43(8.0%)32(6.0%)28(5.2%)25(4.7%)24(4.5%)21(3.9%)20(3.7%)19(3.5%)91(17.0%)\n      \n      9464\n(94.6%)\n    \n    \n      LIBRARY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      LOCAL_SCHOOL_DISTRICT\n[character]\n      1. LAKOTA LOCAL SD (BUTLER)2. OLENTANGY LOCAL SD (DELAW3. WEST CLERMONT LOCAL SD (C4. ELIDA LOCAL SD (ALLEN)5. ROSS LOCAL SD (BUTLER)6. SHAWNEE LOCAL SD (ALLEN)7. NORTHEASTERN LOCAL SD (CL8. WESTERN BROWN LOCAL SD (B9. BUCKEYE VALLEY LOCAL SD (10. BIG WALNUT LOCAL SD (DELA[ 103 others ]\n      347(11.2%)324(10.4%)220(7.1%)68(2.2%)67(2.2%)66(2.1%)63(2.0%)60(1.9%)58(1.9%)56(1.8%)1782(57.3%)\n      \n      6889\n(68.9%)\n    \n    \n      MUNICIPAL_COURT_DISTRICT\n[character]\n      1. CLEVELAND2. DELAWARE3. PARMA4. ROCKY-RIVER5. BEREA6. LIMA7. BEDFORD8. GARFIELD-HEIGHTS9. COLUMBIANA-CO10. SHAKER-HEIGHTS[ 16 others ]\n      1175(19.1%)685(11.1%)562(9.1%)427(6.9%)364(5.9%)326(5.3%)280(4.5%)276(4.5%)260(4.2%)200(3.2%)1605(26.1%)\n      \n      3840\n(38.4%)\n    \n    \n      PRECINCT_NAME\n[character]\n      1. PRECINCT GOSHEN2. HAM2WD33. PRECINCT TWIN TWP. GORDON4. PRECINCT E LIVERPOOL 2-A5. CLEVELAND-03-I6. FAIRVIEW PARK-05-A7. NORTH ROYALTON-06-D8. PRECINCT UNION CITY VILLA9. BATAVIA TOWNSHIP A10. BAY VILLAGE-02-C[ 2373 others ]\n      16(0.2%)15(0.1%)15(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9864(98.6%)\n      \n      0\n(0.0%)\n    \n    \n      PRECINCT_CODE\n[character]\n      1. 09-P-ACR2. 19ABP3. 11AAW4. 15AAF5. 18-P-AYX6. 18-P-BSM7. 18-P-CGK8. 19ABD9. 09-P-AIT10. 09-P-AJL[ 2378 others ]\n      15(0.1%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9866(98.7%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_BOARD_OF_EDUCATION\n[character]\n      1. 012. 023. 034. 055. 076. 087. 098. 109. 11\n      959(9.6%)249(2.5%)1244(12.4%)1176(11.8%)685(6.9%)803(8.0%)406(4.1%)2119(21.2%)2359(23.6%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_REPRESENTATIVE_DISTRICT\n[numeric]\n      Mean (sd) : 48.2 (28.4)min ≤ med ≤ max:13 ≤ 47 ≤ 99IQR (CV) : 55 (0.6)\n      40 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STATE_SENATE_DISTRICT\n[character]\n      1. 212. 243. 234. 045. 146. 197. 128. 109. 1810. 33[ 8 others ]\n      1259(12.6%)1192(11.9%)1173(11.7%)1134(11.3%)889(8.9%)791(7.9%)696(7.0%)509(5.1%)462(4.6%)433(4.3%)1462(14.6%)\n      \n      0\n(0.0%)\n    \n    \n      TOWNSHIP\n[character]\n      1. WEST CHESTER TWP2. MIAMI TWP3. UNION  TWP4. LIBERTY TOWNSHIP5. ORANGE TWP6. LIBERTY TWP7. GENOA TWP8. FAIRFIELD TOWNSHIP9. PERRY TWP10. BATAVIA TWP[ 232 others ]\n      224(4.9%)164(3.6%)157(3.4%)130(2.8%)121(2.6%)116(2.5%)101(2.2%)94(2.1%)86(1.9%)84(1.8%)3295(72.1%)\n      \n      5428\n(54.3%)\n    \n    \n      VILLAGE\n[character]\n      1. MT. ORAB VILLAGE2. EAST PALESTINE VILLAGE3. CRESTLINE VILLAGE4. UNION CITY VILLAGE5. BETHEL VILLAGE6. BLUFFTON VILLAGE7. NEW RICHMOND VILLAGE8. BARNESVILLE VILLAGE9. BELLAIRE VILLAGE10. CARROLLTON VILLAGE[ 119 others ]\n      15(2.9%)14(2.7%)13(2.5%)13(2.5%)12(2.3%)12(2.3%)12(2.3%)11(2.1%)10(1.9%)10(1.9%)400(76.6%)\n      \n      9478\n(94.8%)\n    \n    \n      WARD\n[character]\n      1. DELAWARE CITY - WARD2. HAMILTON CTY WARD 13. CLEVELAND WARD 34. CLEVELAND WARD 45. CLEVELAND WARD 116. CLEVELAND WARD 97. CLEVELAND WARD 88. CLEVELAND WARD 109. CLEVELAND WARD 1510. CLEVELAND WARD 16[ 262 others ]\n      120(2.4%)91(1.9%)84(1.7%)83(1.7%)76(1.5%)75(1.5%)74(1.5%)71(1.4%)70(1.4%)68(1.4%)4095(83.5%)\n      \n      5093\n(50.9%)\n    \n    \n      PRIMARY-03/07/2000\n[character]\n      1. D2. E3. R4. X\n      598(35.9%)1(0.1%)697(41.8%)371(22.3%)\n      \n      8333\n(83.3%)\n    \n    \n      GENERAL-11/07/2000\n[character]\n      1. X\n      3328(100.0%)\n      \n      6672\n(66.7%)\n    \n    \n      SPECIAL-05/08/2001\n[character]\n      1. D2. R3. X\n      6(1.6%)44(11.9%)320(86.5%)\n      \n      9630\n(96.3%)\n    \n    \n      GENERAL-11/06/2001\n[character]\n      1. X\n      1866(100.0%)\n      \n      8134\n(81.3%)\n    \n    \n      PRIMARY-05/07/2002\n[character]\n      1. D2. R3. X\n      401(36.5%)308(28.0%)390(35.5%)\n      \n      8901\n(89.0%)\n    \n    \n      GENERAL-11/05/2002\n[character]\n      1. X\n      2454(100.0%)\n      \n      7546\n(75.5%)\n    \n    \n      SPECIAL-05/06/2003\n[character]\n      1. D2. R3. X\n      44(12.4%)38(10.7%)272(76.8%)\n      \n      9646\n(96.5%)\n    \n    \n      GENERAL-11/04/2003\n[character]\n      1. X\n      1312(100.0%)\n      \n      8688\n(86.9%)\n    \n    \n      PRIMARY-03/02/2004\n[character]\n      1. D2. R3. X\n      924(45.5%)468(23.1%)637(31.4%)\n      \n      7971\n(79.7%)\n    \n    \n      GENERAL-11/02/2004\n[character]\n      1. X\n      4737(100.0%)\n      \n      5263\n(52.6%)\n    \n    \n      SPECIAL-02/08/2005\n[character]\n      1. X\n      304(100.0%)\n      \n      9696\n(97.0%)\n    \n    \n      PRIMARY-05/03/2005\n[character]\n      1. D2. R3. X\n      19(2.0%)33(3.5%)884(94.4%)\n      \n      9064\n(90.6%)\n    \n    \n      PRIMARY-09/13/2005\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2005\n[character]\n      1. X\n      2617(100.0%)\n      \n      7383\n(73.8%)\n    \n    \n      SPECIAL-02/07/2006\n[character]\n      1. X\n      71(100.0%)\n      \n      9929\n(99.3%)\n    \n    \n      PRIMARY-05/02/2006\n[character]\n      1. D2. R3. X\n      707(44.4%)572(35.9%)314(19.7%)\n      \n      8407\n(84.1%)\n    \n    \n      GENERAL-11/07/2006\n[character]\n      1. R2. X\n      2(0.1%)3559(99.9%)\n      \n      6439\n(64.4%)\n    \n    \n      PRIMARY-05/08/2007\n[character]\n      1. D2. R3. X\n      71(16.4%)73(16.8%)290(66.8%)\n      \n      9566\n(95.7%)\n    \n    \n      PRIMARY-09/11/2007\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/06/2007\n[character]\n      1. X\n      1346(100.0%)\n      \n      8654\n(86.5%)\n    \n    \n      PRIMARY-11/06/2007\n[character]\n      1. D2. R3. X\n      13(19.1%)19(27.9%)36(52.9%)\n      \n      9932\n(99.3%)\n    \n    \n      GENERAL-12/11/2007\n[character]\n      1. X\n      53(100.0%)\n      \n      9947\n(99.5%)\n    \n    \n      PRIMARY-03/04/2008\n[character]\n      1. D2. R3. X\n      2073(64.4%)965(30.0%)182(5.7%)\n      \n      6780\n(67.8%)\n    \n    \n      PRIMARY-10/14/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/04/2008\n[character]\n      1. X\n      5352(100.0%)\n      \n      4648\n(46.5%)\n    \n    \n      GENERAL-11/18/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-05/05/2009\n[character]\n      1. D2. R3. X\n      41(7.3%)5(0.9%)517(91.8%)\n      \n      9437\n(94.4%)\n    \n    \n      PRIMARY-09/08/2009\n[character]\n      1. D2. R3. X\n      5(3.9%)2(1.6%)120(94.5%)\n      \n      9873\n(98.7%)\n    \n    \n      PRIMARY-09/15/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/29/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2009\n[character]\n      1. D2. X\n      1(0.0%)3175(100.0%)\n      \n      6824\n(68.2%)\n    \n    \n      PRIMARY-05/04/2010\n[character]\n      1. C2. D3. G4. L5. R6. X\n      2(0.1%)756(43.3%)1(0.1%)3(0.2%)812(46.5%)172(9.9%)\n      \n      8254\n(82.5%)\n    \n    \n      PRIMARY-07/13/2010\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/07/2010\n[character]\n      1. D2. R3. X\n      319(65.5%)153(31.4%)15(3.1%)\n      \n      9513\n(95.1%)\n    \n    \n      GENERAL-11/02/2010\n[character]\n      1. X\n      3868(100.0%)\n      \n      6132\n(61.3%)\n    \n    \n      PRIMARY-05/03/2011\n[character]\n      1. D2. R3. X\n      89(12.9%)61(8.9%)539(78.2%)\n      \n      9311\n(93.1%)\n    \n    \n      PRIMARY-09/13/2011\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2011\n[character]\n      1. X\n      3639(100.0%)\n      \n      6361\n(63.6%)\n    \n    \n      PRIMARY-03/06/2012\n[character]\n      1. D2. G3. L4. R5. X\n      790(38.3%)1(0.0%)4(0.2%)1195(57.9%)73(3.5%)\n      \n      7937\n(79.4%)\n    \n    \n      GENERAL-11/06/2012\n[character]\n      1. X\n      5794(100.0%)\n      \n      4206\n(42.1%)\n    \n    \n      PRIMARY-05/07/2013\n[character]\n      1. D2. R3. X\n      24(5.2%)15(3.2%)423(91.6%)\n      \n      9538\n(95.4%)\n    \n    \n      PRIMARY-09/10/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-10/01/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/05/2013\n[character]\n      1. X\n      2347(100.0%)\n      \n      7653\n(76.5%)\n    \n    \n      PRIMARY-05/06/2014\n[character]\n      1. D2. G3. L4. R5. X\n      668(43.8%)2(0.1%)9(0.6%)703(46.1%)144(9.4%)\n      \n      8474\n(84.7%)\n    \n    \n      GENERAL-11/04/2014\n[character]\n      1. X\n      3433(100.0%)\n      \n      6567\n(65.7%)\n    \n    \n      PRIMARY-05/05/2015\n[character]\n      1. D2. R3. X\n      15(4.4%)43(12.6%)282(82.9%)\n      \n      9660\n(96.6%)\n    \n    \n      PRIMARY-09/15/2015\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2015\n[character]\n      1. X\n      3543(100.0%)\n      \n      6457\n(64.6%)\n    \n    \n      PRIMARY-03/15/2016\n[character]\n      1. D2. G3. R4. X\n      1497(40.0%)2(0.1%)2199(58.7%)46(1.2%)\n      \n      6256\n(62.6%)\n    \n    \n      GENERAL-06/07/2016\n[character]\n      1. X\n      80(100.0%)\n      \n      9920\n(99.2%)\n    \n    \n      PRIMARY-09/13/2016\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2016\n[character]\n      1. X\n      6412(100.0%)\n      \n      3588\n(35.9%)\n    \n    \n      PRIMARY-05/02/2017\n[character]\n      1. D2. R3. X\n      1(0.2%)13(3.2%)388(96.5%)\n      \n      9598\n(96.0%)\n    \n    \n      PRIMARY-09/12/2017\n[character]\n      1. D2. X\n      4(2.4%)163(97.6%)\n      \n      9833\n(98.3%)\n    \n    \n      GENERAL-11/07/2017\n[character]\n      1. X\n      2874(100.0%)\n      \n      7126\n(71.3%)\n    \n    \n      PRIMARY-05/08/2018\n[character]\n      1. D2. G3. R4. X\n      954(46.1%)6(0.3%)1006(48.6%)105(5.1%)\n      \n      7929\n(79.3%)\n    \n    \n      GENERAL-08/07/2018\n[character]\n      1. X\n      246(100.0%)\n      \n      9754\n(97.5%)\n    \n    \n      GENERAL-11/06/2018\n[character]\n      1. X\n      5287(100.0%)\n      \n      4713\n(47.1%)\n    \n    \n      PRIMARY-05/07/2019\n[character]\n      1. D2. R3. X\n      12(2.4%)105(21.0%)383(76.6%)\n      \n      9500\n(95.0%)\n    \n    \n      PRIMARY-09/10/2019\n[character]\n      1. D2. X\n      7(35.0%)13(65.0%)\n      \n      9980\n(99.8%)\n    \n    \n      GENERAL-11/05/2019\n[character]\n      1. X\n      2364(100.0%)\n      \n      7636\n(76.4%)\n    \n    \n      PRIMARY-03/17/2020\n[character]\n      1. D2. L3. R4. X\n      1152(52.2%)2(0.1%)937(42.5%)116(5.3%)\n      \n      7793\n(77.9%)\n    \n    \n      GENERAL-11/03/2020\n[character]\n      1. X\n      7230(100.0%)\n      \n      2770\n(27.7%)\n    \n    \n      PRIMARY-05/04/2021\n[character]\n      1. D2. R3. X\n      2(0.6%)25(7.9%)289(91.5%)\n      \n      9684\n(96.8%)\n    \n    \n      PRIMARY-08/03/2021\n[character]\n      1. D2. R\n      355(91.3%)34(8.7%)\n      \n      9611\n(96.1%)\n    \n    \n      PRIMARY-09/14/2021\n[character]\n      1. D2. X\n      7(2.4%)285(97.6%)\n      \n      9708\n(97.1%)\n    \n    \n      GENERAL-11/02/2021\n[character]\n      1. X\n      2546(100.0%)\n      \n      7454\n(74.5%)\n    \n    \n      PRIMARY-05/03/2022\n[character]\n      1. D2. R3. X\n      732(35.8%)1291(63.1%)23(1.1%)\n      \n      7954\n(79.5%)\n    \n    \n      PRIMARY-08/02/2022\n[character]\n      1. D2. R3. X\n      474(47.3%)525(52.4%)3(0.3%)\n      \n      8998\n(90.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-10-24\n\n\n\nThis sample of 10k is 46.3% and 53.7% Republican.\nOhio actually has open primaries which is a good reason to consider another state for this dataset. I’ll be looking for others."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#next-steps",
    "href": "posts/Final_SteveONeill.html#next-steps",
    "title": "Final Part 1",
    "section": "Next Steps",
    "text": "Next Steps\nNext, I will find the most advantageous state with a closed primary. Ideally, it will be a battleground or solid-Republican state with freely available voter registration data and a downloadable Corporation Search database (or available upon request).\nI look forward to any feedback or refinements to the hypotheses above."
  },
  {
    "objectID": "posts/Homework 1 LJones.html",
    "href": "posts/Homework 1 LJones.html",
    "title": "Homework 1",
    "section": "",
    "text": "First I’ll load the libraries and read in the data.\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlc <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nThe distribution of lung capacity is as follows:\n\n\nCode\nhist(lc$LungCap)\n\n\n\n\n\nThe histogram appears close to the normal distribution.\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data=lc)\n\n\n\n\n\n\n\n\n\n\nCode\nlc %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nInterestingly, the mean lung capacity is higher for smokers than it is for non-smokers.\n\n\n\n\n\nCode\nlcbyagegrp <- lc %>% \n  mutate(age_group = case_when(\n    Age <=13 ~ \"13 and Under\",\n    Age >=14 & Age <=15 ~\"14-15\",\n    Age >=16 & Age <=17 ~\"16 - 17\",\n    Age >=18 ~\"18+\")) %>% \n  arrange(age_group, Age)\n\nggplot(lcbyagegrp, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(age_group ~ Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nCode\nlcbyagegrp %>%\n  group_by(age_group, Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke  Mean\n  <chr>        <chr> <dbl>\n1 13 and Under no     6.36\n2 13 and Under yes    7.20\n3 14-15        no     9.14\n4 14-15        yes    8.39\n5 16 - 17      no    10.5 \n6 16 - 17      yes    9.38\n7 18+          no    11.1 \n8 18+          yes   10.5 \n\n\n\n\nThe mean lung capacity for smokers aged 13 and under is higher than that of non-smokers in the same age group, which defies expectation. The rest of the age groups meet that expectation. There may be an error or extreme outlier in the data for smokers aged 13 and under.\n\n\n\n\n\n\nCode\nlc %>% cov(Age, LungCap)\n\n\nError in pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", : object 'LungCap' not found\n\n\n\n\nCode\n#correlation\ncor(lc$LungCap,lc$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#covariance\ncov(lc$LungCap, lc$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is very close to positive 1, indicating a strong positive correlation between between lung capacity and age. The covariance being a positive number indicates a positive relationship.\n\n\n\n\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(X, Frequency)\n\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\n\n\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\ndf2\n\n\n  X Frequency Probability\n1 0       128  0.15802469\n2 1       434  0.53580247\n3 2       160  0.19753086\n4 3        64  0.07901235\n5 4        24  0.02962963\n\n\nThe probability is about 19.75%.\n\n\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\nsum(b2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability is about 69%.\n\n\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\nsum(c2$Probability)\n\n\n[1] 0.891358\n\n\nThe probability is about 89%.\n\n\n\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\nsum(d2$Probability)\n\n\n[1] 0.108642\n\n\nThe probability is about 10.9%.\n\n\n\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\ne\n\n\n[1] 1.28642\n\n\nThe expected number of prior convictions is about 1.286.\n\n\n\n\n\nCode\n#variance\nvariance <- (sum(Frequency*((X-e)^2)))/(sum(Frequency)-1)\nvariance\n\n\n[1] 0.8572937\n\n\nCode\n#standard deviation\nsd <- sqrt(variance)\nsd\n\n\n[1] 0.9259016\n\n\nThe variance of prior convictions is about 0.857, and the standard deviation (simply, the square root of the variance) is about 0.926."
  },
  {
    "objectID": "posts/Homework 2 LJones.html",
    "href": "posts/Homework 2 LJones.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(readr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAngiography\n\n\nCode\na_mean <- 18\na_sd <- 9\na_ss <- 847\n\na_se <- a_sd/sqrt(a_ss)\n\na_cl <- 0.90  \na_tail <- (1-a_cl)/2\na_tscore <- qt(p = 1-a_tail, df = a_ss-1)\n\na_ci <- c(a_mean - a_tscore * a_se,\n        a_mean + a_tscore * a_se)\nprint(a_ci)\n\n\n[1] 17.49078 18.50922\n\n\nBypass\n\n\nCode\nb_mean <- 19\nb_sd <- 10\nb_ss <- 539\n\nb_se <- b_sd/sqrt(b_ss)\n\nb_cl <- 0.90  \nb_tail <- (1-b_cl)/2\nb_tscore <- qt(p = 1-b_tail, df = b_ss-1)\n\nb_ci <- c(b_mean - b_tscore * b_se,\n        b_mean + b_tscore * b_se)\nprint(b_ci)\n\n\n[1] 18.29029 19.70971\n\n\n\n\nCode\n#assessing which confidence interval is larger\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nThe confidence interval is more narrow for angiographies.\n\n\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = American adults (population), x = sample (surveyed)\nn = 1031\nx = 567\n\n#use prop.test to find p (confidence interval is 95% by default)\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\np (the proportion of adult Americans who believe that college education is essential for success) is 0.5499515. The 95% confidence interval is [0.5189682, 0.5805580], meaning we can be 95% certain this interval captured the true proportion.\n\n\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculating standard deviation using the given\n(200-30)/4\n\n\n[1] 42.5\n\n\nSince our significance level is 5%, our confidence level is 95%. A 95% confidence level corresponds to a z-score of 1.96. From here we can calculate the ideal sample size.\n\n\nCode\n#solve 5 = 1.96((200-30)*.25)/sqrt(x)\n\n(((170*.25)/5)*1.96)^2\n\n\n[1] 277.5556\n\n\nUMass needs a sample of approximately 278 students to estimate the mean cost of textbooks.\n\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\n\nAssumptions: normal distribution, significance level .05 H₀: μ=500 H₁: μ≠500 Test statistic: 410\n\n\nCode\n#given\np_mean = 500\ns_mean = 410\ns_size = 9\ns = 90\n\n#find standard error\nsem = s/sqrt(s_size)\n\n#find t-score\nt_score <- (s_mean - p_mean)/sem\nt_score\n\n\n[1] -3\n\n\nCode\n#find p-value\npvalue = 2 * pt(t_score, df=(s_size - 1))\npvalue\n\n\n[1] 0.01707168\n\n\nBecause the p-value is less than the significance level (.05), we reject the null hypothesis that μ=500.\n\n\n\n\n\nCode\n#calculate the p-value for lower tail only\nltail <- pt(t_score, df=(s_size - 1), lower.tail = TRUE)\nltail\n\n\n[1] 0.008535841\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we have evidence that μ < 50.\n\n\n\n\n\nCode\n#calculate the p-value for upper tail only\nutail <- pt(t_score, df=(s_size - 1), lower.tail = FALSE)\nutail\n\n\n[1] 0.9914642\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we do not have evidence that μ > 500.\nChecking my work:\n\n\nCode\nltail + utail\n\n\n[1] 1\n\n\n\n\n\n\nJones and Smith separately conduct studies to test H₀: μ = 500 against H₁ : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n\nJones\n\n\nCode\n#t-score\nJt_score = (519.5 - 500)/10\nJt_score\n\n\n[1] 1.95\n\n\nCode\n#p-value\nJp = 2 * pt(Jt_score, df=(1000 - 1), lower.tail = FALSE)\nJp\n\n\n[1] 0.05145555\n\n\nSmith\n\n\nCode\n#t-score\nSt_score = (519.7 - 500)/10\nSt_score\n\n\n[1] 1.97\n\n\nCode\n#p-value\nSp = 2 * pt(St_score, df=(1000 - 1), lower.tail = FALSE)\nSp\n\n\n[1] 0.04911426\n\n\n\n\n\nAt α = 0.05, Jones’ result is statistically significant (because the p-value is greater than α) but Smith’s result is not.\n\n\n\nJones’ p-value is only just barely greater than 0.05, and Smith’s p-value is only just barely less than 0.05. It is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected, leading to very different conclusions based on data that is extremely similar.\n\n\n\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\nt.test(gas_taxes, mu = 45.0, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nSince the p-value is 0.03827, we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. However, we do not know from what year(s) the data was collected. Therefore we can conclude with certainty that the average tax per gallon in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/Homework 3 LJones.html",
    "href": "posts/Homework 3 LJones.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(ggplot2)\n\n\n\n\n\n\n\nCode\ndata(UN11)\n\n\n\n\nIdentify the predictor and the response.\nSince we’re studying the dependence of fertility on ppgdp, the predictor is ppgdp and the response is fertility.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nscatterplot(fertility ~ ppgdp, UN11)\n\n\n\n\n\nThe data appears curvilinear, so a straight-line function would be inaccurate.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nscatterplot (log(fertility) ~ log(ppgdp), UN11)\n\n\n\n\n\nThe logarithm helps adjust the plots on the graph, so this model is much more plausible.\n\n\n\n\n\n\nHow, if at all, does the slope of the prediction equation change?\nThe slope of the equation increases by 1.33.\n\n\n\nHow, if at all, does the correlation change?\nThe correlation should not change because the ratio of the values is constant.\n\n\n\n\nDraw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nThere appears to be a strong positive correlation between stream runoff and precipitation at OPBPC, OPRC, and OPSLAKE, so you could potentially predict water supply near those sites. Correlation between the two at the other sites seems loosely positively correlated, if at all.\n\n\n\nCreate a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\ndata(\"Rateprof\")\n\nrp <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\n\n\nError in select(., quality, helpfulness, clarity, easiness, raterInterest): could not find function \"select\"\n\n\nCode\npairs(rp)\n\n\nError in pairs(rp): object 'rp' not found\n\n\n\nRater interest appears to have no correlation (or possibly a very weak positive correlation) with any other variable.\nQuality has a strong positive correlation with helpfulness and clarity, a weak positive correlation with easiness.\nHelpfulness has a strong positive correlation with clarity and a week positive correlation with easiness.\nClarity has a weak positive correlation with easiness (easiness has a weak positive correlation with every variable).\n\n\n\n\n\n\nCode\ndata(\"student.survey\")\n\nss <- student.survey\n\n\n\n\n\n\n\n\nCode\nlm(pi ~ re, data = ss)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\n\nCall:\nlm(formula = pi ~ re, data = ss)\n\nCoefficients:\n(Intercept)         re.L         re.Q         re.C  \n     3.5253       2.1864       0.1049      -0.6958  \n\n\n\n\n\nI could not make my code work for the categorical variables in this particular regression.\n\n\n\n\n\n\n\n\nCode\nfit2 <- lm(hi ~ tv, data = ss)\n\nplot(hi ~ tv, data = ss)\nabline(fit2)\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(lm(hi ~ tv, data = ss))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = ss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThe p-value and the plot both suggest that the negative correlation between hours spent watching TV and high school GPA is not strong. R-squared is not very close to 1, which also demonstrates the weakness of this relationship."
  },
  {
    "objectID": "posts/Homework 4 LJones.html",
    "href": "posts/Homework 4 LJones.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\n\n\n\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x₁ = size of home (in square feet), and x₂ = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x₁ + 2.84x₂.\n\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\n#plug variables into equation\nx1 = 1240\nx2 = 18000\ny = 145000\n\nybar1 <- (-10536)+(53.8*x1)+(2.84*x2)\n\nybar1\n\n\n[1] 107296\n\n\n\n\nCode\n#subtract predicted from actual to find residual\ny - ybar1\n\n\n[1] 37704\n\n\nThe house sold for $37,704 greater than predicted.\n\n\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\n\n\nCode\n#plug variables into equation\nx1f = 1241 #increased size by 1 square foot\nx2f = 18000\nyf = 145000\n\nybar1f <- (-10536)+(53.8*x1f)+(2.84*x2f)\n\n#subtract old predicted price from new predicted price\nybar1f-ybar1 \n\n\n[1] 53.8\n\n\nIn the given equation ŷ = −10,536 + 53.8x₁ + 2.84x₂, the slope coefficient for x₁ (size of home) is 53.8, meaning for every 1-foot increase in square footage the house selling price increases by $53.80. In the code above I demonstrate this by taking my code from part A and increasing square footage by 1, then subtracting part A’s solution from the new solution.\n\n\n\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nCode\n53.8/2.84\n\n\n[1] 18.94366\n\n\nThe slope coefficient for x₂ (lot size) is 2.84, meaning for every 1-foot increase in lot size the house selling price increases by $2.84. We can use the equation 2.84 * x₂ = 53.8 to calculate what increase in lot size is needed to have the same value increase in home size. We can simplify this equation as 53.8/2.84, which equals about 18.94 square feet.\n\n\n\n\nThe data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nCode\nhead(salary)\n\n\n   degree rank    sex year ysdeg salary\n1 Masters Prof   Male   25    35  36350\n2 Masters Prof   Male   13    22  35350\n3 Masters Prof   Male   10    23  28200\n4 Masters Prof Female    7    27  26775\n5     PhD Prof   Male   19    30  33696\n6 Masters Prof   Male   16    21  28516\n\n\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\nThe null hypothesis is that the mean salary for men is the same as the mean salary for women, controlling for other variables. The alternative hypothesis is that the mean salary for men is NOT the same as the mean salary for women, controlling for other variables.\n\n\nCode\nt.test(salary ~ sex, data = salary, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  salary by sex\nt = 1.8474, df = 50, p-value = 0.0706\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -291.257 6970.550\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe t-test shows that the mean salaries for each sex are different, but since the p-value is greater than .05, we fail to reject the null hypothesis.\n\n\n\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\n#run multiple linear regression\nfit = lm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\nsummary(fit)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\n#obtain a 95% confidence interval\nconfint(fit, \"sexFemale\")\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\n\nThe above confidence interval suggests that a female professor may earn between approximately $697.82 less and $3030.57 more than a male professor.\n\n\n\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\n\n\nCode\nsummary(fit)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWe can see from the p-values that the only statistically significant predictors are rank (both associate and professor) and year. The estimator column gives us the coefficient/slope of each variable, so we see that all variables except years since last degree are associated with an increase in salary. Years since last degree is actually associated with a decrease in salary (but again, this value is not statistically significant).\n\n\n\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nrank2 = relevel(salary$rank, \"Prof\")\n\nfit2 = lm(formula = salary ~ degree + rank2 + sex + year + ysdeg, data = salary)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank2 + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrank2Asst   -11118.76    1351.77  -8.225 1.62e-10 ***\nrank2Assoc   -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nBy changing the baseline category for the rank to “Prof,” I made the coefficients for the rank completely different- now rank is associated with a decrease in salary. This just means that if a professor were to become an assistant or associate professor, their salary would be expected to decrease.\n\n\n\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “a variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nfit3 = lm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nEliminating rank from the regression completely changes the direction for having a PhD, being female, and years since last degree.\n\n\n\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\n#create a dummy variable\nsalary$hired <- ifelse(salary$ysdeg <= 15, 1, 0)\n\ndean <- lm(salary ~ hired + rank + sex + degree, data = salary)\nsummary(dean)\n\n\n\nCall:\nlm(formula = salary ~ hired + rank + sex + degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  17585.6     1621.2  10.847 2.88e-14 ***\nhired          319.0     1303.8   0.245 0.807777    \nrankAssoc     4825.3     1276.0   3.781 0.000448 ***\nrankProf     11925.7     1512.4   7.885 4.37e-10 ***\nsexFemale     -829.2      997.6  -0.831 0.410113    \ndegreePhD     1126.2     1018.4   1.106 0.274532    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI removed the variable ysdegree since it would be highly correlated (collinear) with the dummy variable I created, which is based on those who earned their most recent degree in the last 15 years. In this model, those hired by the new dean make $319 more than those hired by the previous dean. This is only supported at the highest significance level, so I would not say that the evidence to support this theory is strong.\n\n\n\n\n\n\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\ndata(\"house.selling.price\")\n\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nSize and “newnesss” of the homes both seem to be a predictor, as they are both statistically significant at the given value. The coefficient for size indicates that the price is expected to increase by about $116.13 for every 1 foot increase in square footage. The coefficient for newness indicates that, controlling for everything else, a new home is expected to sell for about $57,736.28 more than an older home.\n\n\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nThe prediction equation is ŷ = -40230.867 + 116.132x + 57736.283z, where x = the size of the home; and z = 1 if the home is new or z = 0 if the home is older.\nOld: price = -40230.867 + 116.132x New: price = -40230.867 + 116.132 + 57736.283 or 17505.133 + 116.132x\n\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n#old\n-40230.867 + 116.132*3000\n\n\n[1] 308165.1\n\n\nCode\n#new\n17505.133 + 116.132*3000\n\n\n[1] 365901.1\n\n\n\n\n\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nnewfit = lm(Price ~ Size*New, data = house.selling.price)\nsummary(newfit)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nThe interaction between size and newness appears statistically significant.\n\n\n\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nOld: ŷ = -22227.808 + 104.438x New: ŷ = -22227.808 + 104.438x + -78527.502 + 61.916xz\n\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n\n\nCode\n#new\n-22227.808 + 104.438*3000 + -78527.502 + 61.916*3000\n\n\n[1] 398306.7\n\n\n\n\n\n\n\nCode\n#old\n-22227.808 + 104.438*3000\n\n\n[1] 291086.2\n\n\n\n\n\n\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\n#new\n-22227.808 + 104.438*1500 + -78527.502 + 61.916*1500\n\n\n[1] 148775.7\n\n\n\n\n\n\nCode\n#old\n-22227.808 + 104.438*1500\n\n\n[1] 134429.2\n\n\nWe can see that the price difference for larger homes differs much more dramatically based on age than for smaller homes.\n\n\n\n\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nBased on the p-values and the R-squared for both models, I think the model with interaction is a slightly better fit."
  },
  {
    "objectID": "posts/Homework 4_Kaushika Potluri.html",
    "href": "posts/Homework 4_Kaushika Potluri.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\n#1(a) Prediction equation: ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\nCode\n#Plugging in the values of home and lot size into prediction equation\npredicted_sellingprice <- -10536 + 53.8*1240 + 2.84*18000\npredicted_sellingprice\n\n\n[1] 107296\n\n\n$107,297 is the predicticted selling price\n\n\nCode\n#Residual = Actual-predicted\nresidual <- 145000-107296\nresidual\n\n\n[1] 37704\n\n\nTherfore, the Residual is $37,704.\n#1(b)\nThe prediction equation is ŷ = -10536 + 53.8x1 + 2.84x2. Using this, where x2= lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is due to a fixed lot size would make 2.84x2 a set number in the prediction equation. Which means in the prediction equation y = -10356 + 53.8x1 + 2.84x2 x1 would have input values making it increase.\n#1(c) For fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nx2 <- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price.\n#2\n\n\nCode\ndata(\"salary\")\nsalary\n\n\n    degree  rank    sex year ysdeg salary\n1  Masters  Prof   Male   25    35  36350\n2  Masters  Prof   Male   13    22  35350\n3  Masters  Prof   Male   10    23  28200\n4  Masters  Prof Female    7    27  26775\n5      PhD  Prof   Male   19    30  33696\n6  Masters  Prof   Male   16    21  28516\n7      PhD  Prof Female    0    32  24900\n8  Masters  Prof   Male   16    18  31909\n9      PhD  Prof   Male   13    30  31850\n10     PhD  Prof   Male   13    31  32850\n11 Masters  Prof   Male   12    22  27025\n12 Masters Assoc   Male   15    19  24750\n13 Masters  Prof   Male    9    17  28200\n14     PhD Assoc   Male    9    27  23712\n15 Masters  Prof   Male    9    24  25748\n16 Masters  Prof   Male    7    15  29342\n17 Masters  Prof   Male   13    20  31114\n18     PhD Assoc   Male   11    14  24742\n19     PhD Assoc   Male   10    15  22906\n20     PhD  Prof   Male    6    21  24450\n21     PhD  Asst   Male   16    23  19175\n22     PhD Assoc   Male    8    31  20525\n23 Masters  Prof   Male    7    13  27959\n24 Masters  Prof Female    8    24  38045\n25 Masters Assoc   Male    9    12  24832\n26 Masters  Prof   Male    5    18  25400\n27 Masters Assoc   Male   11    14  24800\n28 Masters  Prof Female    5    16  25500\n29     PhD Assoc   Male    3     7  26182\n30     PhD Assoc   Male    3    17  23725\n31     PhD  Asst Female   10    15  21600\n32     PhD Assoc   Male   11    31  23300\n33     PhD  Asst   Male    9    14  23713\n34     PhD Assoc Female    4    33  20690\n35     PhD Assoc Female    6    29  22450\n36 Masters Assoc   Male    1     9  20850\n37 Masters  Asst Female    8    14  18304\n38 Masters  Asst   Male    4     4  17095\n39 Masters  Asst   Male    4     5  16700\n40 Masters  Asst   Male    4     4  17600\n41 Masters  Asst   Male    3     4  18075\n42     PhD  Asst   Male    3    11  18000\n43 Masters Assoc   Male    0     7  20999\n44 Masters  Asst Female    3     3  17250\n45 Masters  Asst   Male    2     3  16500\n46 Masters  Asst   Male    2     1  16094\n47 Masters  Asst Female    2     6  16150\n48 Masters  Asst Female    2     2  15350\n49 Masters  Asst   Male    1     1  16244\n50 Masters  Asst Female    1     1  16686\n51 Masters  Asst Female    1     1  15000\n52 Masters  Asst Female    0     2  20300\n\n\n#2(a)\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nHere the null hypothesis would be: mean salary for men and women are equal The Alternative hypothesis would be: the salaries are not equal for men and women. Here, the female coefficient is -3340, which can imply that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, hence we fail to reject the null hypothesis. Therefore, we cannot conclude that there is a difference between mean salaries for men and women.\n#2(b)\n\n\nCode\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% confidence interval for the difference in salary between male and females is -697.82 and 3030.56.\n#2(d)\nDegreeePhD: For a faculty member that has a PhD degree their predicted salary is $1388.61 than other faculty members who don’t have a PhD degree.\nRank: The baseline category is asst prof.For an associate professor their predicted salary is $5,292.36. For a professor their predicted salary is $11,118.76. These salary differences are statistically significant at the 0.0001 alpha level for both Asst and Professor rank.\nSex: For a faculty member who is female their predicted salary is $1166.37 more than a male. However, his coefficient is not statistically significant at any alpha level.\nYear: Every year a faculty member’s salary is expected to increase by $478.31.The coeffiticent is significant at the 0.0001 alpha level.\nysdegree: For every year after degree completion they can expect to have their slary decrese by $124.57. However this coefficient is not significant at any alpha level.\n#2(d)\n\n\nCode\nsalary$rank<- relevel(salary$rank, ref = 'Assoc')\nsummary(lm(salary ~ degree + rank + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe baseline category is now Assoc. According to these coefficients, faculty of rank asst are expected to make $5292.36 less than Associate professors. Faculty of rank Professor are expected to make $5826.40 more than Associate professors.\n#2(e)\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nRemoving the rank variable reveals a difference between male and female salaries with females making $1286.54 less than men. However, this difference is not signficant at any standard alpha levels.\n#2(f)\n\n\nCode\nsalary<-mutate(salary, hired= case_when(ysdeg<15 ~ \"new\", ysdeg>=15 ~ \"old\"))\nsummary(lm( salary ~ degree + sex + rank +  hired + year, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + rank + hired + year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3588.0 -1532.2  -232.2   565.7  9132.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20468.7      951.7  21.507  < 2e-16 ***\ndegreePhD     1073.5      843.3   1.273   0.2096    \nsexFemale     1046.7      858.0   1.220   0.2289    \nrankAsst     -5012.5     1002.3  -5.001 9.16e-06 ***\nrankProf      6213.3     1045.0   5.946 3.76e-07 ***\nhiredold     -2421.6     1187.9  -2.038   0.0474 *  \nyear           450.7       81.5   5.530 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2360 on 45 degrees of freedom\nMultiple R-squared:  0.8597,    Adjusted R-squared:  0.841 \nF-statistic: 45.95 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nAccording to this equation, faculty hired by the old dean make $2421.60 less than new faculty when we control for other factors. This is significant at the 0.05 alpha level. Excluded ysdegree after creating the new variable to avoid multicollinearity.\n#3(a)\n\n\nCode\ndata(house.selling.price)\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\n\n\nCode\nsummary(lm(Price ~ Size + New, data= house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nAccording to the coefficient for size, the price of a house is expected to increase by $116.132 for each square foot increase in size. The coefficient is significant at the 0.0001 alpha level, meaning there is a strong correlation between size and price when the age status (new/old) is held fixed.\nAccording to the coefficient for new, a new house is expected to cost $57,736.283 more than an old house. This variable is significant at the 0.001 level, meaning that whether a house is old or new has a strong positive impact on price of the house.\n#3(b)\nY = -40230.867 + 116.132(X1) + 57736.283 (X2) where X1 represents size and X2 represents new/old.\nFor a new house: Y = -40230.867 + 116.132(size) + 57736.283\nFor an old house: Y = -40230.867 + 116.132(size)\n#3(c)\n\n\nCode\nsize<- 3000\n-40230.867 + (116.132 * size) + 57736.283 \n\n\n[1] 365901.4\n\n\n\n\nCode\nsize<- 3000\n-40230.867 + (116.132 *size)\n\n\n[1] 308165.1\n\n\n#3(d)\n\n\nCode\nsummary(lm( Price~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n#3(e) The predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nFor a new house: Y = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld: Y = -22227.81 + 104.44 * Size\n#3(f)\n\n\nCode\n#new: \n-2227.808 + 166.354*3000 - 78527.502\n\n\n[1] 418306.7\n\n\n\n\nCode\n#not new:\n-2227.808 + 104.438*3000\n\n\n[1] 311086.2\n\n\nNew: $418,306.70 Not new: $311,086.20\n#3(g)\n\n\nCode\n#new: \n-2227.808 + 166.354*1500 - 78527.502\n\n\n[1] 168775.7\n\n\n\n\nCode\nSize <- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger. Houses that are larger are much greater in price, especially when comparing new large houses to small new houses.\n#3(h)\nI prefer the second model with the interaction term which provides a clearer picture of how increased square footage makes a larger difference in bigger sized houses. The model with the interaction term also has a larger adjusted R squared.\n\n\nCode\n#for a 1000 sq foot home:\n#New:\n-2227.808 + 166.354*1000 - 78527.502\n\n\n[1] 85598.69\n\n\n\n\nCode\n#Not new:\n-2227.808 + 104.438*1000\n\n\n[1] 102210.2\n\n\nI wouldn’t use this model for small homes: for a home that is 1000 square feet, the predicted price for a new house is greater than for an old house. So I dont think this model would be good at predicting tiny house prices."
  },
  {
    "objectID": "posts/Homework 5 LJones.html",
    "href": "posts/Homework 5 LJones.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\n\n\n\n\n\nCode\ndata(house.selling.price.2)\nhead(house.selling.price.2)\n\n\n      P    S Be Ba New\n1  48.5 1.10  3  1   0\n2  55.0 1.01  3  2   0\n3  68.0 1.45  3  2   0\n4 137.0 2.40  3  3   0\n5 309.4 3.30  4  3   1\n6  17.5 0.40  1  1   0\n\n\n\n\nThe BEDS variable should be eliminated first because it has the highest p-value and is therefore the weakest predictor of price.\n\n\n\nIn forward selection, we add based on the strongest significance. The p-value for SIZE is 0 and the p-value for NEW is 0.000. I am inclined to believe that NEW appears this way because it has a value slightly higher than 0, so I would estimate that SIZE is the most statistically significant and should be added first.\n\n\n\nI think the correlation between BEDS and PRICE is influenced by another variable, perhaps SIZE. On its own, BEDS may just be a weak indicator of PRICE,\n\n\n\n\n\n\n\n\n\n\nCode\nm1 <- lm(P ~ S, house.selling.price.2)\nm2 <- lm(P ~ Be, house.selling.price.2)\nm3 <- lm(P ~ Ba, house.selling.price.2)\nm4 <- lm(P ~ New, house.selling.price.2)\nm5 <- lm(P~., house.selling.price.2)\nm6 <- lm(P~.-Be, house.selling.price.2)\nstargazer(m1, m2, m3, m4, m5, m6, type = 'text')\n\n\n\n================================================================================================================================================================\n                                                                                Dependent variable:                                                             \n                    --------------------------------------------------------------------------------------------------------------------------------------------\n                                                                                         P                                                                      \n                              (1)                    (2)                    (3)                    (4)                     (5)                     (6)          \n----------------------------------------------------------------------------------------------------------------------------------------------------------------\nS                          75.607***                                                                                    64.761***               62.263***       \n                            (3.865)                                                                                      (5.630)                 (4.335)        \n                                                                                                                                                                \nBe                                                42.969***                                                              -2.766                                 \n                                                   (6.160)                                                               (3.960)                                \n                                                                                                                                                                \nBa                                                                       76.026***                                      19.203***               20.072***       \n                                                                          (7.822)                                        (5.650)                 (5.495)        \n                                                                                                                                                                \nNew                                                                                             34.158***               18.984***               18.371***       \n                                                                                                 (9.383)                 (3.873)                 (3.761)        \n                                                                                                                                                                \nConstant                  -25.194***               -37.229*              -49.248***             89.249***              -41.795***              -47.992***       \n                            (6.688)                (19.955)               (15.644)               (5.148)                (12.104)                 (8.209)        \n                                                                                                                                                                \n----------------------------------------------------------------------------------------------------------------------------------------------------------------\nObservations                  93                      93                     93                     93                     93                      93           \nR2                           0.808                  0.348                  0.509                  0.127                   0.869                   0.868         \nAdjusted R2                  0.806                  0.341                  0.504                  0.118                   0.863                   0.864         \nResidual Std. Error    19.473 (df = 91)        35.861 (df = 91)       31.119 (df = 91)       41.506 (df = 91)       16.360 (df = 88)        16.313 (df = 89)    \nF Statistic         382.628*** (df = 1; 91) 48.660*** (df = 1; 91) 94.473*** (df = 1; 91) 13.254*** (df = 1; 91) 145.763*** (df = 4; 88) 195.313*** (df = 3; 89)\n================================================================================================================================================================\nNote:                                                                                                                                *p<0.1; **p<0.05; ***p<0.01\n\n\nFor parts a and b above, I compare several models in one chart (I did not run all possible models; just a few that made sense based on previous information). Because a higher R² indicates a stronger fit, the model including all variables (m5) should be selected if using R² and the model including all except BEDS (m6) should be selected if using adjusted R².\n\n\n\n\n\nCode\n#use PRESS function to test all models from previous questions\n\nPRESS <- function(linear.model) {\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n  PRESS <- sum(pr^2)\n  return(PRESS)\n}\n\ncat('       ', 'PRESS', '\\n',\n'm1:  ', PRESS(m1), '\\n',\n'm2:  ', PRESS(m2), '\\n',\n'm3:  ', PRESS(m3), '\\n',\n'm4:  ', PRESS(m4), '\\n',\n'm5:  ', PRESS(m5), '\\n',\n'm6:  ', PRESS(m6), '\\n')\n\n\n        PRESS \n m1:   38203.29 \n m2:   122984.3 \n m3:   95732.14 \n m4:   164039.3 \n m5:   28390.22 \n m6:   27860.05 \n\n\nIf using PRESS as the primary criterion, model m6 is the best choice because it has the lowest PRESS score.\n\n\n\n\n\nCode\ncat('       ', 'AIC', '\\n',\n'm1:  ', AIC(m1), '\\n',\n'm2:  ', AIC(m2), '\\n',\n'm3:  ', AIC(m3), '\\n',\n'm4:  ', AIC(m4), '\\n',\n'm5:  ', AIC(m5), '\\n',\n'm6:  ', AIC(m6), '\\n')\n\n\n        AIC \n m1:   820.1439 \n m2:   933.7168 \n m3:   907.3327 \n m4:   960.908 \n m5:   790.6225 \n m6:   789.1366 \n\n\nm6 is the best choice here, but m5 is close.\n\n\n\n\n\nCode\ncat('       ', 'BIC', '\\n',\n'm1:  ', BIC(m1), '\\n',\n'm2:  ', BIC(m2), '\\n',\n'm3:  ', BIC(m3), '\\n',\n'm4:  ', BIC(m4), '\\n',\n'm5:  ', BIC(m5), '\\n',\n'm6:  ', BIC(m6), '\\n')\n\n\n        BIC \n m1:   827.7417 \n m2:   941.3146 \n m3:   914.9305 \n m4:   968.5058 \n m5:   805.8181 \n m6:   801.7996 \n\n\nSimilar to AIC, m6 is again the best choice here, but m5 is close.\n\n\n\nIt seems to me that m6, which includes all variables but BEDS, is the best fit. It meets the criterion for best fit in most tests. Model m5, which includes all variables, would be a close 2nd choice.\n\n\n\n\n\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\n\n\n\n\nCode\ntree <- lm(Volume ~ Girth + Height, data=trees)\ntree\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nCoefficients:\n(Intercept)        Girth       Height  \n   -57.9877       4.7082       0.3393  \n\n\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(tree, which = 1:6)\n\n\n\n\n\nThe Residuals vs Fitted plot violates the assumption of linearity. The line in the Scale-Location plot is not quite horizontal, but it is unclear which assumption this violates- perhaps homoskedasticity.\n\n\n\n\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.\n\n\n\n\nCode\nvote <- lm(Buchanan ~ Bush, data=florida)\npar(mfrow = c(2,3)); plot(vote, which = 1:6)\n\n\n\n\n\nPalm Beach does appear to be an outlier as it falls way outside the lines on most plots. Dade is also an outlier on a few plots.\n\n\n\n\n\nCode\nvote_log <- lm(log(Buchanan)~log(Bush),data=florida)\npar(mfrow = c(2,3)); plot(vote_log, which = 1:6)\n\n\n\n\n\nPalm Beach still appears to be a significant outlier, but now there are other counties which also stand out. Dade no longer appears to be an outlier on any plot."
  },
  {
    "objectID": "posts/Homework 5.html",
    "href": "posts/Homework 5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\n\n\n\n\n\n\nCode\ndata(house.selling.price.2)\nsummary(lm(P ~ ., data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\n\n\nQuestion: For backward elimination, which variable would be deleted first? Why?\nAnswer: For backward elimination, the first variable to be deleted would be the beds variable, as it is the weakest indicator with the highest p-value (0.486763) in the regression analysis.\n\n\n\nQuestion: For forward selection, which variable would be added first? Why?\nAnswer: For forward selection, the first variable to be added would be size, as it has the highest t-value (11.504) in the regression analysis.\n\n\n\nQuestion: Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nAnswer: I think that the beds variable is not statistically significant, which has little to do with the correlation (correlation does not equal causation). There could be another, stronger indicator that is influencing the high correlation with price, such as the size of the house.\n\n\n\nQuestion: Using software with these four predictors, find the model that would be selected using each criterion:\n\n\n\n\nCode\nm1 <- lm(P ~ S, house.selling.price.2)\nstargazer(m1, type = 'text')\n\n\nError in stargazer(m1, type = \"text\"): could not find function \"stargazer\"\n\n\nCode\nm2 <- lm(P ~ Be, house.selling.price.2)\nstargazer(m2, type = 'text')\n\n\nError in stargazer(m2, type = \"text\"): could not find function \"stargazer\"\n\n\nCode\nm3 <- lm(P ~ Ba, house.selling.price.2)\nstargazer(m3, type = 'text')\n\n\nError in stargazer(m3, type = \"text\"): could not find function \"stargazer\"\n\n\nCode\nm4 <- lm(P ~ New, house.selling.price.2)\nstargazer(m4, type = 'text')\n\n\nError in stargazer(m4, type = \"text\"): could not find function \"stargazer\"\n\n\nCode\nm5 <- lm(P~., house.selling.price.2)\nstargazer(m5, type = 'text')\n\n\nError in stargazer(m5, type = \"text\"): could not find function \"stargazer\"\n\n\nCode\nm6 <- lm(P~.-Be, house.selling.price.2)\nstargazer(m6, type = 'text')\n\n\nError in stargazer(m6, type = \"text\"): could not find function \"stargazer\"\n\n\nm1 = 0.808\nm2 = 0.348\nm3 = 0.509\nm4 = 0.127\nm5 = 0.869\nm6 = 0.868\n\n\n\nm1 = 0.806\nm2 = 0.341\nm3 = 0.504\nm4 = 0.118\nm5 = 0.863\nm6 = 0.864\n\n\n\n\n\nCode\nPRESS <- function(linear.model) {\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n  PRESS <- sum(pr^2)\n  return(PRESS)\n}\n\ncat('       ', 'PRESS', '\\n',\n'm1:  ', PRESS(m1), '\\n',\n'm2:  ', PRESS(m2), '\\n',\n'm3:  ', PRESS(m3), '\\n',\n'm4:  ', PRESS(m4), '\\n',\n'm5:  ', PRESS(m5), '\\n',\n'm6:  ', PRESS(m6), '\\n')\n\n\n        PRESS \n m1:   38203.29 \n m2:   122984.3 \n m3:   95732.14 \n m4:   164039.3 \n m5:   28390.22 \n m6:   27860.05 \n\n\nm1 = 38203.29\nm2 = 122984.3\nm3 = 95732.14\nm4 = 164039.3\nm5 = 28390.22\nm6 = 27860.05\n\n\n\n\n\nCode\ncat('       ', 'AIC', '\\n',\n'm1:  ', AIC(m1), '\\n',\n'm2:  ', AIC(m2), '\\n',\n'm3:  ', AIC(m3), '\\n',\n'm4:  ', AIC(m4), '\\n',\n'm5:  ', AIC(m5), '\\n',\n'm6:  ', AIC(m6), '\\n')\n\n\n        AIC \n m1:   820.1439 \n m2:   933.7168 \n m3:   907.3327 \n m4:   960.908 \n m5:   790.6225 \n m6:   789.1366 \n\n\nm1 = 820.1439\nm2 = 933.7168\nm3 = 907.3327\nm4 = 960.908\nm5 = 790.6225\nm6 = 789.1366\n\n\n\n\n\nCode\ncat('       ', 'BIC', '\\n',\n'm1:  ', BIC(m1), '\\n',\n'm2:  ', BIC(m2), '\\n',\n'm3:  ', BIC(m3), '\\n',\n'm4:  ', BIC(m4), '\\n',\n'm5:  ', BIC(m5), '\\n',\n'm6:  ', BIC(m6), '\\n')\n\n\n        BIC \n m1:   827.7417 \n m2:   941.3146 \n m3:   914.9305 \n m4:   968.5058 \n m5:   805.8181 \n m6:   801.7996 \n\n\nm1 = 827.7417\nm2 = 941.3146\nm3 = 914.9305\nm4 = 968.5058\nm5 = 805.8181\nm6 = 801.7996\n\n\n\n\nQuestion: Explain which model you prefer and why.\nAnswer: The model that I prefer would be m6, which includes all variables but the beds variable. It was the best fit model in most of the tests. A second choice would be m5, which includes all variables, as it also performed similarly to m6.\n\n\n\n\n\n\nFit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables.\n\n\nCode\ntree <- lm(Volume ~ Girth + Height, data=trees)\ntree\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nCoefficients:\n(Intercept)        Girth       Height  \n   -57.9877       4.7082       0.3393  \n\n\n\n\n\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\nCode\npar(mfrow = c(2,3)); plot(tree, which = 1:6)\n\n\n\n\n\nAnswer: In the first plot, residuals vs fitted, the assumption of linearity is violated as the line is a curved line rather than linear. The other plot that has a violation is the scale-location plot, as it is not a horizontal line, but rather, interestingly skewed.\n\n\n\n\n\n\nRun a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\nCode\nvote <- lm(Buchanan ~ Bush, data=florida)\npar(mfrow = c(2,3)); plot(vote, which = 1:6)\n\n\n\n\n\nYes, Palm Beach does appear to be an outlier on the plots. It falls outside the line of best fit in all the plots.\n\n\n\nTake the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (a). Does your findings change?\n\n\nCode\nvote_log <- lm(log(Buchanan)~log(Bush),data=florida)\npar(mfrow = c(2,3)); plot(vote_log, which = 1:6)\n\n\n\n\n\nPalm Beach is still an outlier, but additional counties appear as outliers in several plots, such as Calhoun and Liberty."
  },
  {
    "objectID": "posts/Homework1QH.html",
    "href": "posts/Homework1QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework1QH.html#a",
    "href": "posts/Homework1QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/Homework1QH.html#d",
    "href": "posts/Homework1QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/Homework1QH.html#e",
    "href": "posts/Homework1QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/Homework1QH.html#f",
    "href": "posts/Homework1QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/Homework1QH.html#a-1",
    "href": "posts/Homework1QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/Homework1QH.html#b",
    "href": "posts/Homework1QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/Homework1QH.html#c",
    "href": "posts/Homework1QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/Homework1QH.html#d-1",
    "href": "posts/Homework1QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/Homework1QH.html#e-1",
    "href": "posts/Homework1QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/Homework1QH.html#f-1",
    "href": "posts/Homework1QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/Homework2.html",
    "href": "posts/Homework2.html",
    "title": "Homework 2 - Emily Duryea",
    "section": "",
    "text": "Uploading packages to be used for this assignment:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\n\n\n\nBelow is the code used for setting up the degrees of freedom (df = sample size - 1). Thus, Bypass df would be 539 - 1 = 538, and, Angiography would be 847 - 1 = 846.\n\n\nCode\nBypass_df = 538\nAngio_df = 846\n\n\nBelow is the code for setting up the mean and standard deviation.\n\n\nCode\nBypass_mean = 19\nBypass_sd = 10\nAngio_mean = 18\nAngio_sd = 9\n\n\nThe code for calculating t-score with 90% confidence interval is below.\n\n\nCode\nBypass_tscore <- qt(p = 0.9, df = Bypass_df)\nAngio_tscore <- qt(p = 0.9, df = Angio_df)\n\n\nThe code for calculating the standard error (sd/sqrt(sample size)) is below.\n\n\nCode\nBypass_se <- Bypass_sd/sqrt(539)\nAngio_se <- Angio_sd/sqrt(847)\n\n\nThe code for calculating the margin of error (T-score multiplied by the standard error) is below.\n\n\nCode\nBypass_me <- Bypass_tscore*Bypass_se\nAngio_me <- Angio_tscore*Angio_se\n\n\nBelow is the code for calculating the upper and lower ranges (add mean to margin of error for upper, subtract for lower).\n\n\nCode\nBypass_low <- Bypass_mean - Bypass_me\nBypass_up <- Bypass_mean + Bypass_me\nAngio_low <- Angio_mean - Angio_me\nAngio_up <- Angio_mean + Angio_me\nBypass <- c(Bypass_low, Bypass_up)\nAngio <- c(Angio_low, Angio_up)\nBypass\n\n\n[1] 18.44732 19.55268\n\n\nCode\nAngio\n\n\n[1] 17.60338 18.39662\n\n\nThe 90% confidence interval for Bypass is [18.45, 19.55], and for Angio it is [17.60, 18.40]. Thus, Angio has the narrower confidence interval, which is logical to conclude because it has a larger sample size (847 > 539), which reduces the margin of error. Additionally, the standard deviation is smaller (9<10), which signifies less variance.\n\n\n\n\n\nCode\n# Number who believed college ed is essential for success\nN <- 567\n\n# Total sample size\nS <- 1031\n\n# Calculating point estimate\nPE <- N/S\n\n# Using the function prop.test() to find the confidence interval range & p-value\nprop.test(N, S, PE)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  N out of S, null probability PE\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval is [0.519, 0.580], with a p-value of 0.550.\n\n\n\n\n\nCode\n# Calculating confidence interval of 95%\nCI95 <- qnorm(0.025, lower.tail = F)\n# Calculating sample size needed using confidence interval equation\nStudent_sample <- ((170*0.25/5)*CI95)^2\nStudent_sample\n\n\n[1] 277.5454\n\n\nBased on these calculations, the needed sample size would be 278 students for a significance level of 5%.\n\n\n\n\n\n\n\nCode\n# Calculating the standard error (sd = 90 sample = 9)\ncompany_se <- 90/sqrt(9)\ncompany_se\n\n\n[1] 30\n\n\nCode\n# Calculating the t-score\ncompany_tscore <- (410-500)/company_se\ncompany_tscore\n\n\n[1] -3\n\n\nCode\n# Calculating the p-value (df = 9-1 = 8)\ncompany_pvalue <- (pt(q=-3, df=8))*2\ncompany_pvalue\n\n\n[1] 0.01707168\n\n\nIt is possible to reject the null hypothesis, as the p-value is statistically significant (0.017), less than 0.05.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or less\nless_company <- pt(-3, 8)\nless_company\n\n\n[1] 0.008535841\n\n\nThe p-value for the lower tail is 0.00854.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or more\nmore_company <- pt(-3, 8, lower.tail = F)\nmore_company\n\n\n[1] 0.9914642\n\n\nThe p-value for the upper tail is 0.991.\n\n\nCode\ntotal_company <- less_company + more_company\ntotal_company\n\n\n[1] 1\n\n\nThe total of both tails is equal to 1.\n\n\n\n\n\n\n\n\nCode\n# Calculating t-scores\nJones_tscore <- (519.5-500)/10\nJones_tscore\n\n\n[1] 1.95\n\n\nCode\nSmith_tscore <- (519.7-500)/10\nSmith_tscore\n\n\n[1] 1.97\n\n\nCode\n# Calculating p-values\nJones_pvalue <- (pt(q=1.95, df=999, lower.tail=FALSE))*2\nJones_pvalue\n\n\n[1] 0.05145555\n\n\nCode\nSmith_pvalue <- (pt(q=1.97, df=999, lower.tail=FALSE))*2\nSmith_pvalue\n\n\n[1] 0.04911426\n\n\n\n\n\nIf the significance level is 0.05, then Smith has statistically significant study findings, while Jones does not.\n\n\n\nBoth of the studies could be statistically significant, depending on the significance level. For example, a 0.1 significance level would mean Jones also had statistically significant results. In this case, since the t-scores were so similar (0.2 off), it would not be unreasonable for both studies to reject the null hypothesis.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# Getting the mean\nmean_gtaxes <- mean(gas_taxes)\nmean_gtaxes\n\n\n[1] 40.86278\n\n\nCode\n# Conducting t-test\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nBecause the p-value is 0.03827 on a 95% confidence interval, on the 0.05 significance level, it is possible the null hypothesis that gas prices are equal to or greater than $0.45."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html",
    "href": "posts/Homework2_Kaushika Potluri.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "title": "Homework 2",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stats)\n\n\n##Question 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for Angiography or Bypass surgery?\n\nAngiography\n\n\nCode\nang_mean <- 18\nang_sd <- 9\nang_ss <- 847\n\nang_se <- ang_sd/sqrt(ang_ss)\n\nang_cl <- 0.90  \nang_tail <- (1-ang_cl)/2\nang_tscore <- qt(p = 1-ang_tail, df = ang_ss-1)\n\nang_ci <- c(ang_mean - ang_tscore * ang_se,\n        ang_mean + ang_tscore * ang_se)\nprint(ang_ci)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nCode\n#assessing Confidence interval\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_ang <- ang_tscore * ang_se\nMargin_of_error_ang * 1.01\n\n\n[1] 0.5143103\n\n\nWe can be 90% confident that the population mean wait time for the Angiography procedure is between 17.49078 and 18.50922 days with margin of error +/-0.51\n\n\nBypass\n\n\nCode\nbypass_mean <- 19\nbypass_sd <- 10\nbypass_ss <- 539\n\nbypass_se <- bypass_sd/sqrt(bypass_ss)\n\nbypass_cl <- 0.90  \nbypass_tail <- (1-bypass_cl)/2\nbypass_tscore <- qt(p = 1-bypass_tail, df = bypass_ss-1)\n\nbypass_ci <- c(bypass_mean - bypass_tscore * bypass_se,\n        bypass_mean + bypass_tscore * bypass_se)\nprint(bypass_ci)\n\n\n[1] 18.29029 19.70971\n\n\nWe can be 90% confident that the population mean wait time for the Bypass procedure is between 18.29029 and 19.70971 days.\n\n\nCode\n#assessing Confidence interval\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_bypass <- bypass_tscore * bypass_se\nMargin_of_error_bypass * 1.41\n\n\n[1] 1.000692\n\n\nTherefore, the confidence interval is more narrow for Angiographies."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-2",
    "href": "posts/Homework2_Kaushika Potluri.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = Number of American adults (population), x = sample (surveyed)\nn = 1031\nx = 567 #(believed that college education is essential for success)\n\n#Using prop.test to find p (The CI is 95% by default)\n#This  function will return the range for the point estimate at 95% CI.\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe percentage of adult Americans who think a college education is necessary for success is p, which is 0.5499515. We have a confidence interval of 95 percent confidence interval that equals, [0.5189682, 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-3",
    "href": "posts/Homework2_Kaushika Potluri.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#Evaluating standard deviation using the given values\nUMassSD <- (200-30)/4\nUMassSD\n\n\n[1] 42.5\n\n\nSince the significance level is at 5% our Confidence level is 95%. A 95% confidence level has a z-score of 1.96. With this ideal sample size can be calculated.\n\n\nCode\n#samplesize = ((UMassSD * zscore)/5)^2\nsamplesize <- ((UMassSD * 1.96)/5)^2\nprint(samplesize)\n\n\n[1] 277.5556\n\n\nThe size necessary for the sample is 278."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-4",
    "href": "posts/Homework2_Kaushika Potluri.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\np_mean <- 500\ns_meanfemale <- 410\ns_sizefemale = 9\nsd = 90\n\n#find standard error\nstandarderrorfemale<- sd/sqrt(s_sizefemale)\nstandarderrorfemale\n\n\n[1] 30\n\n\n\n\nCode\n#calculating t-score\nt_stat<- (s_meanfemale-p_mean)/standarderrorfemale\nt_stat\n\n\n[1] -3\n\n\n\n\nCode\n#calculating p value\ndf <- 9-1\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nSince the p value is less than .05 we can reject the null hypothesis\n\nReport the P-value for Ha : μ < 500. Interpret.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\npvalue_lower <- pt(-t_stat, df, lower.tail = FALSE)\npvalue_lower\n\n\n[1] 0.008535841\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500.\n\nReport and interpret the P-value for H a: μ > 500.\n\n\n\nCode\npvalue_upper <- pt(t_stat, df, lower.tail = FALSE)\npvalue_upper\n\n\n[1] 0.9914642\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500.\n\n\nCode\n#checking if sum = 1\npvalue_upper + pvalue_lower\n\n\n[1] 1"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-5",
    "href": "posts/Homework2_Kaushika Potluri.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nJones\n\n\nCode\n#first calculating t-value for Jones\nt_stat_Jones <- (519.5 - 500)/(10)\nt_stat_Jones\n\n\n[1] 1.95\n\n\nCode\ndf <- 1000-1\n#now we calculate p value for Jones\n\n\np_value_Jones <- 2*pt(t_stat_Jones,df, lower.tail = FALSE)\np_value_Jones\n\n\n[1] 0.05145555\n\n\n\n\nSmith\n\n\nCode\n#first calculating t-value for Smith\nt_stat_Smith <- (519.7 - 500)/(10)\nt_stat_Smith\n\n\n[1] 1.97\n\n\nCode\ndf <- 1000-1\n\n#now we calculate p value for Smith\np_value_Smith <- 2*pt(t_stat_Smith,df, lower.tail = FALSE)\np_value_Smith\n\n\n[1] 0.04911426\n\n\nb)Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAnswer : When they say ‘statistically significant’ it means the p-value is smaller than the 0.05. For Jones, the p-value is 0.051 which is greater than the 0.05 significance level. This means that it is not statistically significant and we cannot reject the null hypothesis.\nFor Smith, the p-value is 0.049 which is smaller than the significance level. This means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\nAnswer : One cannot assess the validity of the result if we do not provide the P-value and you cannot tell how close the p-value is to being significant. Since the values of Jones and Smith’s is barely greater and lesser than 0.05 respectively, it is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected. This could draw very different conclusions."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-6",
    "href": "posts/Homework2_Kaushika Potluri.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer :\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\n#Mean of taxes\nMean_gastaxes <- mean(gas_taxes)\nMean_gastaxes\n\n\n[1] 40.86278\n\n\n\n\nCode\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value is 0.03 at 95% confidence level. This is lesser than the 5% significance level. Therefore, this proves that we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. We can say that the average tax per gallon of gas in the US in 2005 was less than 45 cents with 95% confidence."
  },
  {
    "objectID": "posts/Homework3.html",
    "href": "posts/Homework3.html",
    "title": "Homework 3 - Emily Duryea",
    "section": "",
    "text": "United Nations (Data file: UN11in alr4)\nThe data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nQuestion: Identify the predictor and the response.\nAnswer: The predictor is ppgdp, and the response is ferility, since we are looking at how ppgdp (the independent variable) is affecting fertility (dependent variable).\n\n\n\nQuestion: Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\n# Importing needed libraries\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\n# Importing the UN11 dataset\ndata(UN11)\n\n# Creating a scatterplot\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point(color = 'black') +\n  labs(title = \"PPGDP and Fertility\")\n\n\n\n\n\nAnswer: This graph does not look like it could represented by a linear function. Rather, it looks like it would be represented by a nonlinear (curvilinear) function.\n\n\n\nQuestion: Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\n# Creating a scatterplot\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point(color = 'black') +\n  geom_smooth(method = lm) +\n  labs(title = \"PPGDP and Fertility\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAnswer: After taking the logarithm of each variable, based on the graph, it is now plausible to use a simple linear regression.\n\n\n\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\nQuestion: How, if at all, does the slope of the prediction equation change?\n\n\nCode\n# Creating a variable for the British pound\nUN11$Britishpound <- 1.33*UN11$ppgdp\n\n# Examining the slope\nsummary(lm(fertility ~ Britishpound, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ Britishpound, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.178e+00  1.048e-01  30.331  < 2e-16 ***\nBritishpound -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nggplot(data = UN11, aes(x = log(Britishpound), y = log(fertility))) +\n  geom_point(color = 'black') +\n  geom_smooth(method = lm) +\n  labs(title = \"British Pound and Fertility\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCode\n# Comparing the slope\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nAnswer: The slope has changed slightly due to the 1.33 increase adjustment for British pounds, but according to the results of the summary function, the adjusted R-squared is the same for both (0.1895).\n\n\n\nQuestion: How, if at all, does the correlation change?\n\n\nCode\n# Finding the correlation with US dollars\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\n# Finding the correlation with British pounds\ncor(UN11$Britishpound, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nAnswer: The correlations of fertility with US dollars AND British pounds are the same, because, although British pounds are of a different value from US dollars, the values are multiplied by a constant (1.33).\n\n\n\n\nWater runoff in the Sierras (Data file: water in alr4)\nCan Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\n# Loading dataset\ndata(water)\n\n# Creating scatterplots\npairs(water)\n\n\n\n\n\nCode\n# Conducting regression analysis\nwater1 <- lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\nsummary(water1)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nAnswer: This graph does not look like it could represented by a linear function. Rather, it looks like it would be represented by a nonlinear (curvilinear) function.\n\n\n\nProfessor ratings (Data file: Rateprof in alr4)\nIn the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\n# Importing dataset\ndata(Rateprof)\n\n# Creating a subset of the dataset with the five variables of interest\nRateprof5 <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# Creating the scatterplots\npairs(Rateprof5)\n\n\n\n\n\nAnswer: All 5 of the variables of interest have positive correlations. However, some relationships are stronger than others. Quality, helpfulness, and clarity all have stronger positive relationships, while easiness and raterInterest are very weak positive relationships.\n\n\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching. (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\n\nQuestion: Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\nCode\n# Importing dataset\ndata(student.survey)\nstudentsurvey <- student.survey\n\n# Creating subset of data with variables needed\nstudentsurvey <- studentsurvey %>%\n  select(hi, tv, pi, re)\n\n# Creating a plot to compare political ideology with religious service attendance\nplot(pi ~ re, data = studentsurvey)\n\n\n\n\n\nCode\n# Creating a plot comparing High School GPA (hi) and average number of hours watching tb a week (tv)\nggplot(data = studentsurvey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAnswer: Based on the plots generated, religious service attendance is correlated with conservatism, and hours of TV watched per week has a negative relationship with high school GPA.\n\n\n\nQuestion: Summarize and interpret results of inferential analyses.\n\n\nCode\n# Changing the pi variable to a numeric one\nstudentsurvey$pi <- as.numeric(studentsurvey$pi)\n\n# Removing ordering from the re variable\nlevels(studentsurvey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudentsurvey$re <- factor(studentsurvey$re, ordered = FALSE)\n\n# Conducting regression analyses for pi and re\nsummary(lm(pi ~ re, studentsurvey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = studentsurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nCode\n# Conducting regression analyses for hi and tv\nsummary(lm(hi ~ tv, studentsurvey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = studentsurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nAnswer: According to this dataset, people who attended religious services most weeks or every week are significantly more likely to report as conservative (p < 0.001). Additionally, people who watch less hours of tv are significantly more likely to have a higher GPA (p < 0.05)."
  },
  {
    "objectID": "posts/Homework3_Kaushika Potluri.html",
    "href": "posts/Homework3_Kaushika Potluri.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n##Question 1\nLoading data :\n1.1.1\nThe Predicted variable here is ppgdp (GDP per capita) The response is fertility.\n1.1.2\nYes, linear regression seems plausible.\n##Question 2 a.\n2b"
  },
  {
    "objectID": "posts/Homework3_Kaushika Potluri.html#question-3",
    "href": "posts/Homework3_Kaushika Potluri.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\n\n#generating scatterplots.\npairs(water)\n\n\n\n\n\nLooking at the plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/Homework3_Kaushika Potluri.html#question-4",
    "href": "posts/Homework3_Kaushika Potluri.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\n# load dataset.\ndata(Rateprof)\n\n# create subset.\nrateprof <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# generate scatterplots.\npairs(rateprof)\n\n\n\n\n\nQuality, helpfulness and clarity have the clearest linear relationships with one another. Easiness and raterInterest do not seem to have linear relationships with the other variables.\n##Question 5 a.\n\n\nCode\n# load dataset.\ndata(student.survey)\nglimpse(student.survey)\n\n\nRows: 60\nColumns: 18\n$ subj <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ ge   <fct> m, f, f, f, m, m, m, f, m, m, m, f, m, m, f, f, f, m, m, f, f, f,…\n$ ag   <int> 32, 23, 27, 35, 23, 39, 24, 31, 34, 28, 23, 27, 36, 28, 28, 25, 4…\n$ hi   <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.5, 3.3, …\n$ co   <dbl> 3.5, 3.5, 3.0, 3.2, 3.5, 3.5, 3.7, 3.0, 3.0, 3.1, 2.6, 3.6, 3.5, …\n$ dh   <int> 0, 1200, 1300, 1500, 1600, 350, 0, 5000, 5000, 900, 253, 190, 245…\n$ dr   <dbl> 5.0, 0.3, 1.5, 8.0, 10.0, 3.0, 0.2, 1.5, 2.0, 2.0, 1.5, 3.0, 1.5,…\n$ tv   <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, 6, 25, 4…\n$ sp   <int> 5, 7, 4, 5, 6, 5, 12, 3, 5, 1, 15, 3, 15, 10, 3, 6, 7, 9, 12, 0, …\n$ ne   <int> 0, 5, 3, 6, 3, 7, 4, 3, 3, 2, 1, 7, 12, 1, 1, 1, 3, 6, 2, 0, 4, 7…\n$ ah   <int> 0, 6, 0, 3, 0, 0, 2, 1, 0, 1, 1, 0, 5, 2, 0, 0, 10, 10, 2, 2, 1, …\n$ ve   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ pa   <fct> r, d, d, i, i, d, i, i, i, i, r, d, d, i, d, i, i, d, i, d, i, i,…\n$ pi   <ord> conservative, liberal, liberal, moderate, very liberal, liberal, …\n$ re   <ord> most weeks, occasionally, most weeks, occasionally, never, occasi…\n$ ab   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ aa   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ld   <lgl> FALSE, NA, NA, FALSE, FALSE, NA, FALSE, FALSE, NA, FALSE, FALSE, …\n\n\n\n\nCode\n# generate plots.\nboxplot(pi ~ re, student.survey)\n\n\n\n\n\n\n\nCode\nscatterplot(hi ~ tv, student.survey)\n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship. High school GPA and TV-watching seem to have a negative relationship.\n5b\n\n\nCode\n# change pi to numeric variable.\nstudent.survey$pi <- as.numeric(student.survey$pi)\n\n# removing ordering in re and rename it.\nlevels(student.survey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudent.survey$re <- factor(student.survey$re, ordered = FALSE)\n\n# run regression models.\nsummary(lm(pi ~ re, student.survey))\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\n\n\nCode\nsummary(lm(hi ~ tv, student.survey))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWhen compared to those who never went, those who regularly attended religious services were much more likely to be conservative, p<.001.Between those who occasionally attended religious services and those who never did, there was little variation in political orientation.\nWatching less hours of TV per week was associated with higher high-school GPAs, p < .05. That being said, as the R2 is fairly low, hours of TV watching is not a great predictor of high school GPA."
  },
  {
    "objectID": "posts/Homework4.html",
    "href": "posts/Homework4.html",
    "title": "Homework 4 - Emily Duryea",
    "section": "",
    "text": "For recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is \nŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\nCode\n# Importing needed libraries\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\n\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\n# Plugging given values into the equation to solve\n-10536 + 53.8*1240 + 2.84*18000\n\n\n[1] 107296\n\n\nCode\n# Calculating the residual\n145000-107296\n\n\n[1] 37704\n\n\nThe predicted selling price of the house would be $107,296, and the residual would be $37,704.\n\n\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nThe house selling price predicted to increase for each square-foot increase is $53.80, based on the multiplier in the prediction equation.\n\n\n\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nCode\n# Calculating using square foot home size multiplier and square foot lot size multipler\n53.8/2.84\n\n\n[1] 18.94366\n\n\nThe lot size of a fixed size home would have to increase by 18.94366 feet to have the same impact as a one square foot increase in home size.\n\n\n\n\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\n# Importing the data\ndata(salary)\n\n# Conducting a t-test to test hypothesis\nt.test(salary ~ sex, data=salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nBased on the results of the t-test, there is no statistically significant difference in the salary of male and female professors. This is because the p-value (0.09009) is greater than 0.05.\n\n\n\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\n# Creating the model\nsummary(lm(salary ~ degree + rank + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\n# Putting model data into a variable\nprofessor_salary <- lm(salary ~ degree + rank + sex + year + ysdeg, data=salary)\n\n#Creating a confidence interval for the variables in the model\nconfint(professor_salary)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe confidence interval for salary difference based on sex is between -697.8183 and 3,030.56452.\n\n\n\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\n\n\nThe p-value is not statistically significant, as the p-value is greater than 0.05. The results indicate that professors with a PhD make $1,388.61 than professors without a PhD.\n\n\n\nThe p-value is statistically significant, as it is less than 0.05. The results indicate that associate professors make $5,292.36 than assistant professors.\n\n\n\nThe p-value is statistically significant. The results indicate that faculty professors make $11,118.76 more than assistant professors.\n\n\n\nThe p-value is not statistically significant. The results indicate that female professors make $1,166.37 more than male professors.\n\n\n\nThe p-value is statistically significant. The results indicate that with each year as a professor, there is a salary increase of $476.31.\n\n\n\nThe p-value is not statistically significant. The results indicate that for each year after completing their highest degree, there is a salary decrease of $124.57.\n\n\n\n\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\n# Changing the baseline category for the rank variable\nsalary$rank<- relevel(salary$rank, ref = 'Assoc')\n\n# Redoing the model from Part B\nsummary(lm(salary ~ degree + rank + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe baseline category has been changed to “Assoc.” The results indicate that assistant professors make $5,292.36 less than Associate professors. Faculty professor are expected to make $5,826.40 more than Associate professors. These results are both still statistically significant (p-value < 0.05).\n\n\n\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, \"[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be 'tainted.' \" Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\n# Creating a model without rank\nsummary(lm(salary ~ degree + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nAfter removing rank from the model, the results indicate that there is a difference between male and female salaries, with females making $1,286.54 less than men. However, this difference is not statistically significant.\n\n\n\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\n# Creating a variable with categories \"new\" for less than 15 years, and \"old\" for 15 or more years\n#creating a dummy variable new and old dean\nsalary<-mutate(salary, dean= case_when(ysdeg < 15 ~\"new\",\n                               ysdeg >=15 ~\"old\"))\n\n# Rerunning the model with new variable\nsummary(lm(salary ~ dean + degree + sex + rank +year, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ dean + degree + sex + rank + year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3588.0 -1532.2  -232.2   565.7  9132.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20468.7      951.7  21.507  < 2e-16 ***\ndeanold      -2421.6     1187.9  -2.038   0.0474 *  \ndegreePhD     1073.5      843.3   1.273   0.2096    \nsexFemale     1046.7      858.0   1.220   0.2289    \nrankAsst     -5012.5     1002.3  -5.001 9.16e-06 ***\nrankProf      6213.3     1045.0   5.946 3.76e-07 ***\nyear           450.7       81.5   5.530 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2360 on 45 degrees of freedom\nMultiple R-squared:  0.8597,    Adjusted R-squared:  0.841 \nF-statistic: 45.95 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nBased on the new variable and controlling for other predictors, faculty hired by the old dean make $2,421.60 less than new faculty.\n\n\n\n\n(Data file: house.selling.price in smss R package)\n\n\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of sizeof home (in square feet) and whether the home is new(1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\n# Importing needed data\ndata(house.selling.price)\n\n# Creating model based on size and age status\nsummary(lm(Price ~ Size + New, data= house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBased on the size variable, the house price will increase by ~$116.132 for each square foot increase in size. This finding is statistically significant.\nBased on the age of the house variable, a new house is projected to cost ~$57,736.283 more than an old house. This finding is also statistically significant.\n\n\n\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nPrediction equation:\ny = -40230.867 + 116.13*x1 + 57736.283*x2\nx1 = size\nx2 = age of house (old vs. new)\nPrediction equation for a new house:\ny = -40230.867 + 116.132*x1 + 57736.283\nPrediction equation for an old house:\ny = -40230.867 + 116.132*x1\n\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n# Using the prediction model to predict the cost of a new house\n-40230.867 + 116.132*3000 +  57736.283\n\n\n[1] 365901.4\n\n\nCode\n# Using the prediction model to predict the cost of an old house\n-40230.867 + 116.132*3000\n\n\n[1] 308165.1\n\n\nBased on the prediction model, a new house of 3,000 square feet would cost $365,901.40, while an old house of the same size would cost $308,165.10.\n\n\n\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\n# Creating a model for an interaction between size and new\nsummary(lm(Price ~ Size + New + Size*New, data=house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nBased on this model, the prediction equation would be y = -2227.808 + 104.438*x1 + 61.916*x2 -78527.502*x3, where x1 is the size of the house in square feet, x2 is the size and house age interaction variable, and x3 is the house age variable (old vs new). Both size and the interaction between size and house age are statistically significant. However, the house age variable is no longer statistically significant.\n\n\n\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nPrediction equation for a new house:\ny = -2227.808 + 166.354*x1 - 78527.502\nPrediction equation for an old house:\ny = -2227.808 + 104.438*x1\n\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n# Using the prediction model to predict the cost of a new house\n-2227.808 + 166.354*3000 - 78527.502\n\n\n[1] 418306.7\n\n\nCode\n# Using the prediction model to predict the cost of an old house\n-2227.808 + 104.438*3000\n\n\n[1] 311086.2\n\n\nBased on the new prediction model, a new house of 3,000 square feet would cost $418,306.70, while an old house of the same size would cost $311,086.20.\n\n\n\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\n# Using the prediction model to predict the cost of a new house\n-2227.808 + 166.354*1500 - 78527.502\n\n\n[1] 168775.7\n\n\nCode\n# Using the prediction model to predict the cost of an old house\n-2227.808 + 104.438*1500\n\n\n[1] 154429.2\n\n\nBased on the new prediction model, a new house of 1,500 square feet would cost $168,775.70, while an old house of the same size would cost $154,429.20. This model demonstrates that small houses vary less in price, regardless of age, than larger houses, where new vs old houses makes much more of a difference.\n\n\n\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\n\nCode\n# Calculating predicted house cost of a large new house with first model\n-40230.867 + 116.132*5000 +  57736.283\n\n\n[1] 598165.4\n\n\nCode\n# Calculating predicted house cost of a large old house with first model\n-40230.867 + 116.132*5000\n\n\n[1] 540429.1\n\n\nCode\n# Calculating predicted house cost of a small new house with first model\n-40230.867 + 116.132*1000 +  57736.283\n\n\n[1] 133637.4\n\n\nCode\n# Calculating predicted house cost of a small old house with the first model\n-40230.867 + 116.132*1000\n\n\n[1] 75901.13\n\n\nCode\n# Calculating predicted house cost of a large new house with second model\n-2227.808 + 166.354*5000 - 78527.502\n\n\n[1] 751014.7\n\n\nCode\n# Calculating predicted house cost of a large old house with second model\n-2227.808 + 104.438*5000\n\n\n[1] 519962.2\n\n\nCode\n# Calculating predicted house cost of a small new house with second model\n-2227.808 + 166.354*1000 - 78527.502\n\n\n[1] 85598.69\n\n\nCode\n# Calculating predicted house cost of a small old house with the second model\n-2227.808 + 104.438*1000\n\n\n[1] 102210.2\n\n\nI think that it depends. For large houses, I would use the second model, because it shows the difference the price a new house vs an old house more greatly. However, for smaller houses, the second model is inaccurate, since it predicts that a new small house would be cheaper than an old small house. I think the first model would be a better predictor for small houses."
  },
  {
    "objectID": "posts/HW 1 DACSS.html",
    "href": "posts/HW 1 DACSS.html",
    "title": "Graphs and Probz",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ readr   2.1.2     ✓ stringr 1.4.0\n✓ purrr   0.3.4     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\n\nlibrary(readxl)\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)                                                              \n\n\nm_lung<-mean(LungCapData$LungCap)\nsd_lung<-sd(LungCapData$LungCap)\n\nhist(LungCapData$LungCap, prob= TRUE, xlim = c(0, 20))\ncurve(dnorm(x, m_lung, sd_lung), add= TRUE,lwd= 2,col= \"blue\")"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-5",
    "href": "posts/HW 1 DACSS.html#question-5",
    "title": "Graphs and Probz",
    "section": "Question 5",
    "text": "Question 5\nDoesnt look like its good being a smoker under the age of 18, or any age. Lung capacity is smaller for these groups\n\nggplot(LungCapData, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-6",
    "href": "posts/HW 1 DACSS.html#question-6",
    "title": "Graphs and Probz",
    "section": "Question 6",
    "text": "Question 6\n\ncovar<-cov(LungCapData$LungCap, LungCapData$Age)\nprint(covar)\n\n[1] 8.738289\n\ncorre<-cor(LungCapData$LungCap, LungCapData$Age, method = \"pearson\")\nprint(corre)\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW 2 - Eris Dodds.html",
    "href": "posts/HW 2 - Eris Dodds.html",
    "title": "",
    "section": "",
    "text": "Question 1\nSee Code\n\nbypass_mean<-19\nbypass_sd<-10\nbypass_size<-539\nstandard_error_bypass<-bypass_sd/sqrt(bypass_size)\nconfidence_level<-0.9\ntail_area<-(1-confidence_level)/2\nt_score<-qt(p = 1-tail_area, df = bypass_size-1)\nCI_bypass<-c(bypass_mean - t_score * standard_error_bypass, bypass_mean + t_score * standard_error_bypass)\nprint(CI_bypass)\n\n[1] 18.29029 19.70971\n\nanio_mean<-18\nanio_sd<-9\nanio_size<-847\nstandard_error_anio<-anio_sd/sqrt(anio_size)\ntail_area_anio<-(1-confidence_level)/2\nt_score_anio<-qt(p = 1-tail_area_anio, df = anio_size)\nCI_anio<-c(anio_mean - t_score_anio * standard_error_anio, anio_mean + t_score_anio * standard_error_anio)\nprint(CI_anio)\n\n[1] 17.49078 18.50922\n\n\n\n\nQuestion 2\nPE<-.55 Lower Limit = .52 Upper Limit = .58\n\npop<-567\nsize<-1031\nPE<-pop/size\nprop.test(pop,size)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  pop out of size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n\n\nQuestion 3\n\n(1.96)^2 *(42.5)^2/ (5)^2\n\n[1] 277.5556\n\n\n\n\nQuestion 4\nThe p value is .049, showing it is statisitcally significant and we can reject the null. Other aspects to this question are verified within the code\n\nfem_size<-9\nfem_mean<-410\nnull<-500\nsd<-90\n\nSE<-sd/sqrt(fem_size)\nt_score<-(fem_mean-null)/SE\np_value<-(pt(t_score, df=9))*2\nupper_p<-(pt(t_score, df=8, lower.tail = FALSE))\nupper_p\n\n[1] 0.9914642\n\nlower_p<-(pt(t_score, df=8, lower.tail = FALSE))\nlower_p\n\n[1] 0.9914642\n\n\n\n\nQuestion 5\n\nsee code\nJones = .051 not significant, cannot reject null. Smith = .049 significant, reject null.\nBeing broad about the direction of the p value, in this case, would overshadow how marginally significant and insignificant the p values actually came out to in this case.\n\n\njones_mean<-519.5\nsmith_mean<-519.7\njones_se<-10\nsmith_se<-10\nnull<-500\njones_t<-(jones_mean-null)/jones_se\njones_t\n\n[1] 1.95\n\nsmith_t<-(smith_mean-null)/smith_se\nsmith_t\n\n[1] 1.97\n\njones_p<-pt(jones_t, df=999, lower.tail = FALSE)*2\njones_p\n\n[1] 0.05145555\n\nsmith_p<-pt(smith_t, df=999, lower.tail = FALSE)*2\nsmith_p\n\n[1] 0.04911426\n\n\n\n\nQuestion 6\nThe results of a t test show that the mean is less that 45, with a relatively small p value. We can have more confidence, then, that the average gas tax per gallon was less than .45 cents. ::: {.cell}\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n:::"
  },
  {
    "objectID": "posts/hw1.html",
    "href": "posts/hw1.html",
    "title": "Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot)\n\n\nError in library(ggplot): there is no package called 'ggplot'\n\n\nCode\nlungcap<- read_excel(\"LungCapData.xls\") \n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/hw1.html#lungcapdata",
    "href": "posts/hw1.html#lungcapdata",
    "title": "Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n``\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n \n### 1b. Compare the probability distribution of the LungCap with respect to Males and Females? \n\n\nError: attempt to use zero-length variable name\n\n\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Gender): object 'lungcap' not found\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Smoke): object 'lungcap' not found\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\nError in ggplot(lungcap, aes(x = LungCap)): object 'lungcap' not found\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nLung capacity for those under age 13 is 6.36 for non-smokers and 7.20 for smokers.\n\n\nCode\nlungcap %>%\n+ filter(Age <= 13)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 14 to 15\nlungcap%>%\n+ filter(Age=<15 & Age >=14)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 16 to 17\nlungcap%>%\n+     filter(Age=<17>=16)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those 18 and older\nlungcap%>%\n+ filter(Age>=18)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\n\nError: <text>:6:6: unexpected symbol\n5: \n6: Lung capacity\n        ^\n\n\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/hw1.html#inmate-data",
    "href": "posts/hw1.html#inmate-data",
    "title": "Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\nx<- c(0, 1, 2, 3, 4)\ny<- c(128, 434, 160, 64, 24)\nprison <-data.frame(x,y)\nView(prison)\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20% 2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69% 2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89% 2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11% 2e. What is the expected value for the number of prior convictions? 84% 2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison, y= NULL)\nsd(rnorm(810))\nThe standard deviation is 1.02.\n\n\nError: <text>:3:5: unexpected symbol\n2: sd(rnorm(810))\n3: The standard\n       ^"
  },
  {
    "objectID": "posts/HW1answers_DonnySnyder.html",
    "href": "posts/HW1answers_DonnySnyder.html",
    "title": "Homework 1 - Donny Snyder",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = Gender, y = LungCap)) + geom_boxplot()\n\n\n\n\n\nThe probability distribution suggests that the lung capacity of males tends to be higher.\n##c\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity of smokers vs nonsmokers appears to be higher for smokers. This doesn’t really make sense because I’ve been taught to think smokers tend to have reduced lung capacity.\n##d and e\n\n\nCode\nx=1\ndf$AgeGroup <- rep(c(\"NA\"),times=725)\nwhile(x <= 725){\n  if(df$Age[x] <= 13){\n    df$AgeGroup[x] = \"less than or equal to 13\"\n  }\n  else if((df$Age[x] >= 14)&&(df$Age[x] <= 15)){\n    df$AgeGroup[x] = \"14 to 15\"\n  }\n  else if((df$Age[x] >= 16)&&(df$Age[x] <= 17)){\n    df$AgeGroup[x] = \"16 to 17\"\n  }\n  else if(df$Age[x] >= 18){\n    df$AgeGroup[x] = \"greater than 18\"\n  }\nx = x + 1\n}\naggregate(data = df, LungCap~AgeGroup+Smoke, mean)\n\n\n                  AgeGroup Smoke   LungCap\n1                 14 to 15    no  9.138810\n2                 16 to 17    no 10.469805\n3          greater than 18    no 11.068846\n4 less than or equal to 13    no  6.358746\n5                 14 to 15   yes  8.391667\n6                 16 to 17   yes  9.383750\n7          greater than 18   yes 10.513333\n8 less than or equal to 13   yes  7.201852\n\n\nCode\naggregate(data = df,LungCap~AgeGroup+Smoke,length)\n\n\n                  AgeGroup Smoke LungCap\n1                 14 to 15    no     105\n2                 16 to 17    no      77\n3          greater than 18    no      65\n4 less than or equal to 13    no     401\n5                 14 to 15   yes      15\n6                 16 to 17   yes      20\n7          greater than 18   yes      15\n8 less than or equal to 13   yes      27\n\n\nCode\naggregate(data = df,Age~Smoke,mean)\n\n\n  Smoke      Age\n1    no 12.03549\n2   yes 14.77922\n\n\nIt seems like people tend to have a lung capacity that increases with age. However, nonsmokers have a higher lung capacity for each age break down besides less than or equal to 13. It seems like smokers just might tend to be older. I confirmed this by looking at the length and mean ages per group, where you can see a majority of smokers are older, whereas non smokers tend to be younger. The mean age for smokers also tends to be older.\n##f\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity appears to be quite correlated with age. This means that Lung capacity tends to go up as age goes up, and vice versa. This is confirmed also by the covariance.\n#Question 2\n##a\n\n\nCode\nprint((160/810) * 100)\n\n\n[1] 19.75309\n\n\nThe probability is 19.75309% that a randomly selected inmate has exactly 2 prior convictions.\n##b\n\n\nCode\nprint(((434+128)/810) * 100)\n\n\n[1] 69.38272\n\n\nThe probability is 69.38272% that a randomly selected inmate has fewer than 2 prior convictions.\n##c\n\n\nCode\nprint(((160+434+128)/810) * 100)\n\n\n[1] 89.1358\n\n\nThe probability is 89.1358% that a randomly selected inmate has 2 or fewer prior convictions.\n##d\n\n\nCode\nprint(((64+24)/810) * 100)\n\n\n[1] 10.8642\n\n\nThe probability is 10.8642% that a randomly selected inmate has more than 2 prior convictions.\n##e\n\n\nCode\nnewDf <- NA\nnewDf[1:128] <- 0\nnewDf[129:562] <- 1\nnewDf[563:722] <- 2\nnewDf[723:786] <- 3\nnewDf[787:810] <- 4\nnewDf <- as.data.frame(newDf)\nmean(newDf$newDf)\n\n\n[1] 1.28642\n\n\nThe expected value, known as the “mean” when it deals in data that are not probability distributions, is 1.28642. Because I created a vector here, I took the mean, though I also could have calculated the expected value by multiplying the probabilities by the numbers. They are both the same value in this case.\n##f\n\n\nCode\nsd(newDf$newDf)\n\n\n[1] 0.9259016\n\n\nCode\nvar(newDf$newDf)\n\n\n[1] 0.8572937\n\n\nThe variance of prior convictions is 0.8572937, the standard deviation of prior convictions is 0.9259016."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-1",
    "href": "posts/HW1_603_Niharikapola.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-data",
    "href": "posts/HW1_603_Niharikapola.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a",
    "href": "posts/HW1_603_Niharikapola.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b",
    "href": "posts/HW1_603_Niharikapola.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c",
    "href": "posts/HW1_603_Niharikapola.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d",
    "href": "posts/HW1_603_Niharikapola.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e",
    "href": "posts/HW1_603_Niharikapola.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f",
    "href": "posts/HW1_603_Niharikapola.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-2",
    "href": "posts/HW1_603_Niharikapola.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "href": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a-1",
    "href": "posts/HW1_603_Niharikapola.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b-1",
    "href": "posts/HW1_603_Niharikapola.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c-1",
    "href": "posts/HW1_603_Niharikapola.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d-1",
    "href": "posts/HW1_603_Niharikapola.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e-1",
    "href": "posts/HW1_603_Niharikapola.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f-1",
    "href": "posts/HW1_603_Niharikapola.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/hw1_boonstra.html",
    "href": "posts/hw1_boonstra.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a",
    "href": "posts/hw1_boonstra.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlungcap <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(lungcap$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b",
    "href": "posts/hw1_boonstra.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThese are the boxplots of the distributions for the lung capacity of males and females in the sample:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Gender,y=LungCap)) +\n  geom_boxplot()\n\n\n\n\n\nAccording to these boxplots, it appears that males and females have similar median lung capacities, but that males may be more likely to have a higher lung capacity than females."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c",
    "href": "posts/hw1_boonstra.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\nlungcap %>% \n  group_by(Smoke) %>% \n  summarise(mean_lungcap=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nAccording to this sample, it would appear that smokers have a higher lung capacity than non-smokers. This would appear to be counter-intuitive, as one would likely expect smoking to reduce lung functionality and, by extension, capacity."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d",
    "href": "posts/hw1_boonstra.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nIn order to complete this examination by group, we must create a new nominal variable that groups observations by age; this can be accomplished fairly simply using the mutate() and case_when() functions:\n\n\nCode\nlungcap_age <- lungcap %>% \n  mutate(age_group = case_when(\n    Age <= 13 ~ \"13 and under\",\n    Age == 14 | Age == 15 ~ \"14 to 15\",\n    Age == 16 | Age == 17 ~ \"16 to 17\",\n    Age >= 18 ~ \"18 and older\"\n  ))\n\n\nWith this new dataframe, we can use the group_by() function to calculate mean lung capacity by age group and smoker status:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group,Smoke) %>% \n  summarise(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke `mean(LungCap)`\n  <chr>        <chr>           <dbl>\n1 13 and under no               6.36\n2 13 and under yes              7.20\n3 14 to 15     no               9.14\n4 14 to 15     yes              8.39\n5 16 to 17     no              10.5 \n6 16 to 17     yes              9.38\n7 18 and older no              11.1 \n8 18 and older yes             10.5 \n\n\nAccording to these data, it appears that lung capacity generally increases with age. Interestingly, lung capacity is worse for smokers than it is for non-smokers in every age group except for “13 and under”. This is surprising on the surface, given that, when the data are ungrouped, smokers have a higher lung capacity than non-smokers (see part c). However, this begins to make more sense when we see how much better the “13 and under” group is represented compared to the others in this dataset:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group) %>% \n  count()\n\n\n# A tibble: 4 × 2\n# Groups:   age_group [4]\n  age_group        n\n  <chr>        <int>\n1 13 and under   428\n2 14 to 15       120\n3 16 to 17        97\n4 18 and older    80\n\n\nThis high number of observations compared to other age groups likely plays a significant role in skewing the mean of the entire dataset."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e",
    "href": "posts/hw1_boonstra.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nIt is not clear to me how this part is different from part d; from what I do understand, I believe the question being asked here is addressed in that part."
  },
  {
    "objectID": "posts/hw1_boonstra.html#f",
    "href": "posts/hw1_boonstra.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nIt would appear that lung capacity and age covary together positively, such that a higher age means a higher lung capacity. We can confirm this with a simple visualization:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Age,y=LungCap)) +\n  geom_point() +\n  geom_smooth(method='lm')"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a-1",
    "href": "posts/hw1_boonstra.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 160 / 810 = 0.1975309."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b-1",
    "href": "posts/hw1_boonstra.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThe probability that a randomly selected inmate has less than 2 prior convictions is (128+434) / 810 = 0.6938272."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c-1",
    "href": "posts/hw1_boonstra.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is (128+434+160) / 810 = 0.891358."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d-1",
    "href": "posts/hw1_boonstra.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe probability that a randomly selected inmate has more than 2 prior convictions is (64+24) / 810 = 0.108642."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e-1",
    "href": "posts/hw1_boonstra.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nBefore calculating expected value, we should put together a probability mass function for the prisoners data.\n\n\nCode\nprisoners <- prisoners %>% \n  mutate(prob=freq/810) %>% \n  mutate(expect=prob*priors)\n\nprisoners %>% \n  summarise(sum(expect))\n\n\n  sum(expect)\n1     1.28642\n\n\nThe expected value for the number of prior convictions is about 1.29 priors.\nEDIT: There is a much simpler way to compute this! Rather than using the dataframe I created, storing values and their frequencies, I can create one vector that stores each value a certain number of times, according to the given frequencies:\n\n\nCode\nprisoners_full <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nprisoners_full\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n\nBecause each value now appears as frequently as its “probability” of appearing, taking the mean of this vector also provides the correct expected value.\n\n\nCode\nmean(prisoners_full)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/hw1_boonstra.html#f-1",
    "href": "posts/hw1_boonstra.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCreating this numerical vector also makes the standard deviation calculation extremely simple in R.\n\n\nCode\nsd(prisoners_full)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW1_EmmaRasmussen.html",
    "href": "posts/HW1_EmmaRasmussen.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlungcap<-read_excel(\"_data/LungCapData.xls\")\nhead(lungcap)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\n#saving a copy of original dataset\nlungcap_orig<-lungcap\n\n#checking for missing values in LungCap\nwhich(is.na(lungcap$LungCap))\n\n\ninteger(0)\n\n\n\n1a.\nThe distribution of LungCapData is plotted as a histogram below.\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_histogram()\n\n\n\n\n\nThe histogram looks approximately normally distributed\n\n\n1b.\nThe probability distribution of LungCap data for males and females is compared using the boxplots below:\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Gender))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity of males appears slightly higher than that of females. The IQR and range for males and females appears similarly spread with a higher average for males.\n\n\n1c.\nBelow the mean and standard deviation of smokers and non-smokers is compared. They are also plotted as a boxplot to help visualize the distribution.\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity for smokers (8.645) in this sample is higher than that of non-smokers (7.770). This does not make sense. However, the standard deviation of non-smokers (2.726) is much higher than smokers (1.883) so there might be something else going on (see boxplot).\n\n\n1d.\nBelow, means are taken by age groups of smokers/non-smokers. I also created a new age category variable (“AgeCat”) to plot the data by smoking status and age category.\n\n\nCode\n#Mean under 13 and nonsmoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n#Mean under 13 and smoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 7.201852\n\n\nCode\n#Mean 14-15 and nonsmoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.068018\n\n\nCode\n#Mean 14-15 and smoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.689231\n\n\nCode\n#Mean 16-17 and nonsmoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.30523\n\n\nCode\n#Mean 16-17 and smoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.850385\n\n\nCode\n#Mean over 18 and nonsmoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n#Mean over 18 and smoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\n#creating new variable AgeCat to create boxplots\nlungcap<-lungcap %>% \n  mutate(AgeCat= as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\n\n#new Category AgeCat is the last column\nlungcap\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeCat      \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_boxplot()+facet_grid(Smoke ~ AgeCat)\n\n\n\n\n\n\n\n1e.\nComparing the lung capacities for smokers and non-smokers in different age categories:\nNow we can see that the mean lung capacity for smokers by age group is generally lower than that of nonsmokers. This is true in all categories except for Under 13, which is likely because smokers in that category are going to be older than nonsmokers in that category (i.e. it is more likely that a 12 year old smokes than a 6 year old, and a 12 year old has a larger lung capacity than a 6 year old regardless of smoking status)\nThis explains the first calculation of mean by smoking status (before finding the mean by age categories). Smokers are generally going to be older than non-smokers for this sample (the oldest participant in the sample is 19- see code below), which explains why the mean for smokers versus non-smokers (not separated by age categories) makes it look like smokers have a higher average lung capacity.\n\n\nCode\n#checking how old participants in the sample are\nlungcap %>% \n  summarize(range(Age))\n\n\n# A tibble: 2 × 1\n  `range(Age)`\n         <dbl>\n1            3\n2           19\n\n\n\n\n1f.\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\n#Creating vectors of Age and Lung Capacity from df (lungcap) to apply cov() and cor() functions to\nx<-c(lungcap$Age)\ny<-c(lungcap$LungCap)\n\n\n#Calculating covariance\ncov(x, y)\n\n\n[1] 8.738289\n\n\nCode\n#calculating correlation\ncor(x, y)\n\n\n[1] 0.8196749\n\n\nThe covariance, 8.738 is fairly high and positive, meaning as age increases, so does lung capacity (i.e. age and lung capacity co-vary). The correlation (0.82) is fairly close to one and positive, indicating they correlate fairly closely.\n\n\n2a-f.\nPrior Conviction Data\n\n\nCode\n#creating a data frame\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nprison<- data.frame(X, Frequency)\nprison\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nprison<-rename(prison, PriorConvictions=X)\nprison\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(prison, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\nAnswering the Questions\n\n\nCode\n#creating a vector of probabilities\nprobs<-Frequency/810\nprobs\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, probs)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions? 19.75% probability (or 0.1975)\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69.38% probability\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89.14% probability\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions? 10.86% probability\nWhat is the expected value for the number of prior convictions? 1.28642 prior convictions\nCalculate the variance and the standard deviation for the Prior Convictions. variance: 0.8572937 standard deviation: 0.9259016 prior convictions"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html",
    "href": "posts/HW1_EthanCampbell.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.8     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n(The distribution of LungCap looks as follows:)\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 x 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n\n\n\n(Comparing lung cap by gender)\nHere we notice that males tend to have a higher lung cap compared to females. Females average tends to sit around 8 while males seems to sit closer to 9\n\n\nCode\nboxplot(df$LungCap~df$Gender)\n\n\n\n\n\n\n\n\n(smoker vs non-smoker lung cap)\nInterestingly, none smokers tend to have a lower lung capacity however, I believe this might be due to age. No this does not make sense at first glance and does betray my expectation.\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 x 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\n\n\n\n(relation between smoking and lung cap at different age groups)\nThe lung cap starts off higher but takes and dip then rises as the age continues to grow. I believe the trend is the higher age grows the higher the lung cap until it reaches a certain point.\n\n\nCode\n# lung cap is 9.62\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 13) %>%\n  colMeans()\n\n\n      Age   LungCap \n15.609290  9.628757 \n\n\nCode\n# lung cap is 9.04\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  colMeans()\n\n\n      Age   LungCap \n14.533333  9.045417 \n\n\nCode\n# lung cap is 10.24\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  colMeans()\n\n\n     Age  LungCap \n16.44330 10.24588 \n\n\nCode\n# lung cap is 11.26\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age > 18) %>%\n  colMeans()\n\n\n     Age  LungCap \n19.00000 11.26149 \n\n\n\n\n\n(lung cap for smokers and non smokers broken into age groups)\nWe notice a clear trend that smokers have a lower lung capacity compared to non-smokers\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 13) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 x 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.71\n2 yes    9.21\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 x 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 x 2\n  Smoke  mean\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age > 18) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 x 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     11.3\n2 yes    11.3\n\n\n\n\n\n(correlation and covariance between lung capacity and age)\ncorrelation is at .819 meaning they have a positive correlation of about 82%. This means that there is a connection between the two and when one goes up so does the other.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#a-1",
    "href": "posts/HW1_EthanCampbell.html#a-1",
    "title": "Homework 1",
    "section": "2.a",
    "text": "2.a\nprobability of exactly 2 convictions probability = 19.7%\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  filter(X == 2)\n\n\n# A tibble: 1 x 3\n      X  Freq Probability\n  <dbl> <dbl>       <dbl>\n1     2   160       0.197"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#b-1",
    "href": "posts/HW1_EthanCampbell.html#b-1",
    "title": "Homework 1",
    "section": "2.b",
    "text": "2.b\nprobability of fewer than 2 convictions probability = 69.2%\n\n\nCode\nsum(df1$Probability[1:2])\n\n\n[1] 0.6921182"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#c-1",
    "href": "posts/HW1_EthanCampbell.html#c-1",
    "title": "Homework 1",
    "section": "2.c",
    "text": "2.c\nProbability of having 2 or fewer convictions probability = 88.9%\n\n\nCode\nsum(df1$Probability[1:3])\n\n\n[1] 0.8891626"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#d-1",
    "href": "posts/HW1_EthanCampbell.html#d-1",
    "title": "Homework 1",
    "section": "2.d",
    "text": "2.d\nprobability of having more than 2 convictions probability = 11.08%\n\n\nCode\nsum(df1$Probability[4:5])\n\n\n[1] 0.1108374"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#e-1",
    "href": "posts/HW1_EthanCampbell.html#e-1",
    "title": "Homework 1",
    "section": "2.e",
    "text": "2.e\nWhat is the expected value expected value is 1.29 convictions\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  mutate(expected_value = (0*0.15763547)+(1*0.53448276)+(2*0.19704433)+(3*0.07881773)+(4*0.03201970))\n\n\n# A tibble: 5 x 4\n      X  Freq Probability expected_value\n  <dbl> <dbl>       <dbl>          <dbl>\n1     0   128      0.158            1.29\n2     1   434      0.534            1.29\n3     2   160      0.197            1.29\n4     3    64      0.0788           1.29\n5     4    26      0.0320           1.29"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#f-1",
    "href": "posts/HW1_EthanCampbell.html#f-1",
    "title": "Homework 1",
    "section": "2.f",
    "text": "2.f\nWhat is the variance and standard deviation of the prior convictions Variance = 25810.8 standard deviation = 160.6574\n\n\nCode\nvar(df$Freq)\n\n\n[1] 25810.8\n\n\nCode\nsd(df$Freq)\n\n\n[1] 160.6574"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html",
    "href": "posts/HW1_KarenDetter.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "href": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "title": "Homework 1",
    "section": "Plot histogram with probability density on the y axis",
    "text": "Plot histogram with probability density on the y axis\n\n\nCode\nhist(LungCapData$LungCap, freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution - most of the observations are close to the mean, with very few close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "href": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "title": "Homework 1",
    "section": "Create boxplots separated by gender",
    "text": "Create boxplots separated by gender\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, horizontal = TRUE)\n\n\n\n\n\nThe boxplots show that male lung capacity has a wider range than that of females; however, the minimum, median, and maximum values are all higher than those of females. This implies that, as a group, men are likely to have higher lung capacity than women."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "href": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "title": "Homework 1",
    "section": "Group by smoking status and summarize mean lung capacities",
    "text": "Group by smoking status and summarize mean lung capacities\n\n\nCode\nlibrary(dplyr)\nLungCapData %>%\ngroup_by(Smoke) %>%\nsummarize(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\nIn this dataset, the mean lung capacity of smokers is actually higher than that of non-smokers. Since this is counter to what would be expected, there is likely another variable exerting a confounding effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "href": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "title": "Homework 1",
    "section": "Create new data frame with age group category variables",
    "text": "Create new data frame with age group category variables\n\n\nCode\nLungCapData_AgeGroups <- LungCapData %>%\nmutate(AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\", \n            Age == 14 | Age == 15 ~ \"14 to 15\",\n            Age == 16 | Age == 17 ~ \"16 to 17\",\n            Age >= 18 ~ \"greater than or equal to 18\"))"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "href": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "title": "Homework 1",
    "section": "Summarize mean lung capacities by age group and smoking status",
    "text": "Summarize mean lung capacities by age group and smoking status\n\n\nCode\nLungCapData_AgeGroups %>%\ngroup_by(AgeGroup, Smoke) %>%\nsummarize(MeanLungCap = mean(LungCap), n = n())\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   AgeGroup [4]\n  AgeGroup                    Smoke MeanLungCap     n\n  <chr>                       <chr>       <dbl> <int>\n1 14 to 15                    no           9.14   105\n2 14 to 15                    yes          8.39    15\n3 16 to 17                    no          10.5     77\n4 16 to 17                    yes          9.38    20\n5 greater than or equal to 18 no          11.1     65\n6 greater than or equal to 18 yes         10.5     15\n7 less than or equal to 13    no           6.36   401\n8 less than or equal to 13    yes          7.20    27"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "href": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "title": "Homework 1",
    "section": "Calculate correlation and covariance between lung capacity and age",
    "text": "Calculate correlation and covariance between lung capacity and age\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 8.738289\n\n\nSince the correlation coefficient is close to 1, there is a high degree of correlation between lung capacity and age. The covariance of 8.7, being a positive number, indicates that as age increases, lung capacity increases."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-data-frame",
    "href": "posts/HW1_KarenDetter.html#create-data-frame",
    "title": "Homework 1",
    "section": "Create data frame",
    "text": "Create data frame\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nFreq <- c(128,434,160,64,24)\nPrisonerData <- data.frame (PriorConv, Freq)\nPrisonerData\n\n\n  PriorConv Freq\n1         0  128\n2         1  434\n3         2  160\n4         3   64\n5         4   24"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has == 2 prior convictions",
    "text": "Calculate probability that an inmate has == 2 prior convictions\nprobability = frequency/n\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has < 2 prior convictions",
    "text": "Calculate probability that an inmate has < 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n\n\n\nCode\n(128/810) + (434/810)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has <= 2 prior convictions",
    "text": "Calculate probability that an inmate has <= 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n + frequency(2)/n\n\n\nCode\n(128/810) + (434/810) + (160/810)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has > 2 prior convictions",
    "text": "Calculate probability that an inmate has > 2 prior convictions\nprobability = frequency(3)/n + frequency(4)/n\n\n\nCode\n(64/810) + (24/810)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate expected value for number of prior convictions",
    "text": "Calculate expected value for number of prior convictions"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "href": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "title": "Homework 1",
    "section": "Create a matrix of prior conviction values and their probabilities",
    "text": "Create a matrix of prior conviction values and their probabilities\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nProbs <- c(0.1580247, 0.5358025, 0.1975309, 0.07901235, 0.02962963)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "title": "Homework 1",
    "section": "Calculate expected value",
    "text": "Calculate expected value\n\n\nCode\nc(PriorConv %*% Probs)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate variance and standard deviation for prior convictions",
    "text": "Calculate variance and standard deviation for prior convictions\n\n\nCode\nvar(PriorConv)\n\n\n[1] 2.5\n\n\nCode\nsd(PriorConv)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#double-check-values",
    "href": "posts/HW1_KarenDetter.html#double-check-values",
    "title": "Homework 1",
    "section": "Double-check values",
    "text": "Double-check values\n\n\nCode\nsqrt(var(PriorConv)) == sd(PriorConv)\n\n\n[1] TRUE"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html",
    "href": "posts/HW1_ManiShankerKamarapu.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"_data/LungCapData.xls\")\nLc\n\n\n\n\n  \n\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a",
    "href": "posts/HW1_ManiShankerKamarapu.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\n\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b",
    "href": "posts/HW1_ManiShankerKamarapu.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c",
    "href": "posts/HW1_ManiShankerKamarapu.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d",
    "href": "posts/HW1_ManiShankerKamarapu.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e",
    "href": "posts/HW1_ManiShankerKamarapu.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f",
    "href": "posts/HW1_ManiShankerKamarapu.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\nCorrelation <- cor(Lc$LungCap, Lc$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html",
    "href": "posts/HW1_PrahithaMovva.html",
    "title": "Homework 1 - Prahitha Movva",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data = df)\n\n\n\n\n\nFrom the boxplots for gender above, we can see that males seem to have (slightly) higher lung capacity than females.\n\n\n\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity for smokers and nonsmokers seems to be higher for smokers. This does not make sense as we generally expect smokers to have a reduced lung capacity due to the damage from smoking.\n\n\n\n\n\nCode\ndf_ageGroups <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n\nError in mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == : could not find function \"mutate\"\n\n\nCode\nggplot(df_ageGroups, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\nError in ggplot(df_ageGroups, aes(x = LungCap)): could not find function \"ggplot\"\n\n\nWe see non-smokers to have a higher lung capacity than smokers, as expected.\n\n\n\nLung capacity seems to be directly proportional to age and after breaking down the data by age groups, we see that the lung capacities for non-smokers are higher than those of smokers in the same age group (except for less than or equal to 13). This could be because of the total number of observations in each age group. The age group less than or equal to 13 has the highest number of observations - thereby skewing the results (here, mean) for the entire distribution.\n\n\n\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity seems to be positively correlated with age i.e., as age increases, lung capacity increases. Same is the case with covariance."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#a-1",
    "href": "posts/HW1_PrahithaMovva.html#a-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\na <- 160/810\n\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#b-1",
    "href": "posts/HW1_PrahithaMovva.html#b-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\nb <- (128+434)/810\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#c-1",
    "href": "posts/HW1_PrahithaMovva.html#c-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\nc <- (128+434+160)/810\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#d-1",
    "href": "posts/HW1_PrahithaMovva.html#d-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "d",
    "text": "d\n\n\nCode\nd <- (64+24)/810\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#e-1",
    "href": "posts/HW1_PrahithaMovva.html#e-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "e",
    "text": "e\n\n\nCode\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\n\nThe expected value for the number of prior convictions is 1.2864198 or 1, as the number of convictions cannot be a float."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#f-1",
    "href": "posts/HW1_PrahithaMovva.html#f-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "f",
    "text": "f\n\n\nCode\nvar_0 <- ((0-e)^2) * (128/810)\nvar_1 <- ((1-e)^2) * (434/810)\nvar_2 <- ((2-e)^2) * (160/810)\nvar_3 <- ((3-e)^2) * (64/810)\nvar_4 <- ((4-e)^2) * (24/810)\n\nvar <- var_0 + var_1 + var_2 + var_3 + var_4\n\nsd <- sqrt(var)\n\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html",
    "href": "posts/HW1_Saaradhaa.html",
    "title": "Homework 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a",
    "href": "posts/HW1_Saaradhaa.html#a",
    "title": "Homework 1",
    "section": "1 (a)",
    "text": "1 (a)\nReading in the data:\n\n# load packages.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(lsr)\n\n# read in data.\ndf <- read_excel(\"_data/LungCapData.xls\")\n\nDistribution of LungCap:\n\nhist(df$LungCap)\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b",
    "href": "posts/HW1_Saaradhaa.html#b",
    "title": "Homework 1",
    "section": "1 (b)",
    "text": "1 (b)\nThe boxplots below show the probability distributions grouped by gender.\n\nboxplot(LungCap~Gender, data = df)\n\n\n\n\nMales appear to have a slightly greater lung capacity than females."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c",
    "href": "posts/HW1_Saaradhaa.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n# check class of Smoke.\nclass(df$Smoke)\n\n[1] \"character\"\n\n# convert Smoke to factor type.\ndf$Smoke <- as.factor(df$Smoke)\n\n# mean lung capacity for smokers.\ndf %>% select(Smoke, LungCap) %>% group_by(Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nIt does not make sense, as I did not expect smokers to have greater mean lung capacities than non-smokers."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d",
    "href": "posts/HW1_Saaradhaa.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n# check class of Age.\nclass(df$Age)\n\n[1] \"numeric\"\n\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n\n\nMost people seem to be non-smokers, and non-smokers seem to have greater lung capacity."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e",
    "href": "posts/HW1_Saaradhaa.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n# check class of AgeGroup.\nclass(df$AgeGroup)\n\n[1] \"character\"\n\n# convert AgeGroup to factor.\ndf$AgeGroup <- as.factor(df$AgeGroup)\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nNon-smokers have greater mean lung capacity for ages 14-15, 16-17 and 18 and above. Smokers have greater mean lung capacity for age 13 and below, which is different from 1(d). There might be some extreme outliers affecting the results for those age 13 and below."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f",
    "href": "posts/HW1_Saaradhaa.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n# correlation.\ncor(df$LungCap,df$Age)\n\n[1] 0.8196749\n\n# covariance.\ncov(df$LungCap,df$Age)\n\n[1] 8.738289\n\n\nThe value of 0.82 for correlation indicates a strong positive relationship between lung capacity and age - as age increases, lung capacity increases. The covariance is a little harder to interpret - the positive value reflects a positive relationship between lung capacity and age, but it is hard to assess the strength of the relationship, given that covariance ranges from negative infinity to infinity. I would prefer to use correlation in most cases."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a-1",
    "href": "posts/HW1_Saaradhaa.html#a-1",
    "title": "Homework 1",
    "section": "2 (a)",
    "text": "2 (a)\n\na <- 160/810\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b-1",
    "href": "posts/HW1_Saaradhaa.html#b-1",
    "title": "Homework 1",
    "section": "2 (b)",
    "text": "2 (b)\n\nb <- (128+434)/810\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c-1",
    "href": "posts/HW1_Saaradhaa.html#c-1",
    "title": "Homework 1",
    "section": "2 (c)",
    "text": "2 (c)\n\nc <- (128+434+160)/810\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d-1",
    "href": "posts/HW1_Saaradhaa.html#d-1",
    "title": "Homework 1",
    "section": "2 (d)",
    "text": "2 (d)\n\nd <- (64+24)/810\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e-1",
    "href": "posts/HW1_Saaradhaa.html#e-1",
    "title": "Homework 1",
    "section": "2 (e)",
    "text": "2 (e)\n\n# multiply each value of X by its probability and add the products.\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\nThe expected value for the number of prior convictions is 1.2864198. To be more precise, since number of prior convictions should not have decimal places, we can round this down to 1, which is what the line graph showed us as well."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f-1",
    "href": "posts/HW1_Saaradhaa.html#f-1",
    "title": "Homework 1",
    "section": "2 (f)",
    "text": "2 (f)\n\n# calculate required formula for each value of X.\nf1_0 <- ((0-e)^2) * (128/810)\nf1_1 <- ((1-e)^2) * (434/810)\nf1_2 <- ((2-e)^2) * (160/810)\nf1_3 <- ((3-e)^2) * (64/810)\nf1_4 <- ((4-e)^2) * (24/810)\n\n# sum up the above for variance.\nf1 <- f1_0 + f1_1 + f1_2 + f1_3 + f1_4\n\n# square root for SD.\nf2 <- sqrt(f1)\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298. In general, I think it might be more meaningful to calculate mode and proportions when generating descriptive statistics for number of prior convictions."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html",
    "href": "posts/HW1_Solutions_OmerYalcin.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males.\n\n\n\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity.\n\n\n\n\nLess than or equal to 13\n\n\n\nCode\ndf %>%\n  filter(Age <= 13) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %>%\n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %>%\n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %>%\n  filter(Age >= 18) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       11.1\n2 yes      10.5\n\n\n\n\n\nFor three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is.\n\n\n\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nBoth the correlation and the covariance are positive (when one of them is the other has to). Positive values suggest that people who are older tend to have higher lung capacity, confirming what we found. Since correlation is standardized (needs to be between -1 and 1), its absolute value tells us about the strength of the relationship. 0.82 suggests a pretty strong relationship."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\n\n\nCode\ntb %>%\n  filter(X == 2) %>%\n  pull(Frequency) %>%\n  divide_by(n)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\n\n\nCode\ntb %>%\n  filter(X < 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\ntb %>%\n  filter(X <= 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\n\n\nCode\ntb %>%\n  filter(X > 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nExpected number of prior convictions is just a weighted average of the number of prior convictions.\n\nMethod 1: Multiply every value with their frequency, then divide by total frequency i.e. (0 * 128 + 1 * 434 + 2 * 160 ……) / 810.\n\n\n\nCode\nsum(tb$X * tb$Frequency) / n\n\n\n[1] 1.28642\n\n\n\nMethod 2: Multiply every value with their probility, sum them up.\n\n\n\nCode\ntb %>%\n  mutate(probability = Frequency / n) -> tb\n\nprint(tb)\n\n\n# A tibble: 5 × 3\n      X Frequency probability\n  <dbl>     <dbl>       <dbl>\n1     0       128      0.158 \n2     1       434      0.536 \n3     2       160      0.198 \n4     3        64      0.0790\n5     4        24      0.0296\n\n\n\n\nCode\nsum(tb$X * tb$probability)\n\n\n[1] 1.28642\n\n\n\nMethod 3: Recreate the whole sample (a vector that has 128 zeroes, 434 ones, 160 twos, ….) with a total length/size of 810. Take the mean.\n\n\n\nCode\nsample <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\nmean(sample)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\nMethod 1: Let’s start from the end: we have the sample, just call var() and sd()\n\n\n\nCode\ncat('Variance:', var(sample))\n\n\nVariance: 0.8572937\n\n\nCode\ncat('\\nStandard Deviation:', sd(sample))\n\n\n\nStandard Deviation: 0.9259016\n\n\nMethod 2: Manually apply the formula using weights.\nStandard deviation is square root of variance. So let’s calculate variance first. For that we need the mean. Let’s pull the expected value from the previous section:\n\n\nCode\nm <- sum(tb$X * tb$Frequency) / n\n\n\nFor every observation, we’ll need the squared difference from mean (squared deviation from mean).\n\n\nCode\ntb %>%\n  mutate(sq_deviation = (X - m)^2) -> tb \nprint(tb)\n\n\n# A tibble: 5 × 4\n      X Frequency probability sq_deviation\n  <dbl>     <dbl>       <dbl>        <dbl>\n1     0       128      0.158        1.65  \n2     1       434      0.536        0.0820\n3     2       160      0.198        0.509 \n4     3        64      0.0790       2.94  \n5     4        24      0.0296       7.36  \n\n\nThen, we can now multiply them with probability.\n\n\nCode\nsum(tb$sq_deviation * tb$probability)\n\n\n[1] 0.8562353\n\n\nThis gives us the ‘population’ variance. If we wanted the ‘sample’ variance, what the var() function does, we could manually apply the Bessel’s correction:\n\n\nCode\nvariance <- sum(tb$sq_deviation * tb$probability) * (n / (n-1))\nprint(variance)\n\n\n[1] 0.8572937\n\n\nStandard deviation is then just the square root:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9259016\n\n\nThis replicated what we found directly using the sample."
  },
  {
    "objectID": "posts/HW1_StephRoberts.html",
    "href": "posts/HW1_StephRoberts.html",
    "title": "Homework 1",
    "section": "",
    "text": "Homework 1\n##1. Use the LungCapData to answer the following questions. (Hint: Using dplyr, especiallygroup_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nCode\ndf<- read_excel(\"_data/LungCapData.xls\")\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n#Summarize\n\n\nCode\nsummary(df)\n\n\n    LungCap            Age            Height         Smoke          \n Min.   : 0.507   Min.   : 3.00   Min.   :45.30   Length:725        \n 1st Qu.: 6.150   1st Qu.: 9.00   1st Qu.:59.90   Class :character  \n Median : 8.000   Median :13.00   Median :65.40   Mode  :character  \n Mean   : 7.863   Mean   :12.33   Mean   :64.84                     \n 3rd Qu.: 9.800   3rd Qu.:15.00   3rd Qu.:70.30                     \n Max.   :14.675   Max.   :19.00   Max.   :81.80                     \n    Gender           Caesarean        \n Length:725         Length:725        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n\n\n\nCode\nmean(df$LungCap)\n\n\n[1] 7.863148\n\n\n\n\nCode\nmedian(df$LungCap)\n\n\n[1] 8\n\n\n\n\nCode\nvar(df$LungCap)\n\n\n[1] 7.086288\n\n\n\n\nCode\nsd(df$LungCap)\n\n\n[1] 2.662008\n\n\n\n\nCode\nmin(df$LungCap)\n\n\n[1] 0.507\n\n\nCode\nmax(df$LungCap)\n\n\n[1] 14.675\n\n\n#a. What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=0.5,col='black',fill='gray')\n\n\n\n\n\nThe histogram follows a distribution close to normal distibution. In fact, if we change binwidth slightly, it appears even closer to normal distribution.\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')\n\n\n\n\n\nThis helps illustrate the importance of binwidth and what it can do to our visualization interpretations.\n#b. Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nggplot(df, aes(x = LungCap, y = Gender)) +        \n  geom_boxplot()\n\n\n\n\n\nThe distribution of male lung capacity is larger and longer than females’.\n#c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean() \n\n\n[1] 8.645455\n\n\nCode\ndf %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\n\n[1] 7.770188\n\n\nIt does not make sense at face value. In this sample, smokers have a higher mean lung capacity than non-smokers. Let’s check how big each subsample is.\n\n\nCode\nlength(which(df$Smoke == 'yes'))\n\n\n[1] 77\n\n\nCode\nlength(which(df$Smoke == 'no'))\n\n\n[1] 648\n\n\nAs suspected, there are far more, almost 10 times as many, non-smokers. If we could gather data from all the smokers, perhaps our means would look a lot different. Maybe our sample was taken from young people whose lungs have not been long affected by the smoking.\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(Age) %>%\n  median() \n\n\n[1] 15\n\n\nAgain, as suspected, our sample of smokers is a young age. Therefore, the lack of difference in lung capacity between smokers and non-smokers is not too surprising.\n#d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Create age groups\ndf <- df %>% \n  mutate(agegroup = case_when(\n    Age <= 13  ~ \"less than or equal to 13\",\n    Age >= 14 & Age <= 15 ~ \"14 to 15\",\n    Age >= 16 & Age <= 17 ~ \"16 TO 17\",\n    Age >= 18 ~ \"greater than or equal to 18\"))\n\ntable(df$agegroup)\n\n\n\n                   14 to 15                    16 TO 17 \n                        120                          97 \ngreater than or equal to 18    less than or equal to 13 \n                         80                         428 \n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  ggplot(aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')+\n  facet_wrap(~agegroup)\n\n\n\n\n\nThese histograms suggest that participants 13 or younger have smaller lung capacity. The Lung capacity seems to generally increase with age as children grow.\n#e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(df, aes(x = LungCap, \n           fill = agegroup)) +\n  geom_density(alpha = 0.4)+\n  facet_wrap(~Smoke)\n\n\n\n\n\nThis visualization starts to explain furthermore why there is an unexpected result for lung capacity in smokers vs. non-smokers. As we have deducted, lung capacity generally improves with age (in growing years). However, teenagers approaching adulthood are also a group more likely to have access or influence to smoking cigarettes. It is likely that our smokers account for some of the older participants, who happen to be closer to normal smoking age.\n#f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\ncov(df$LungCap, df$Age) #calculate covariance\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age) #calculate correlation\n\n\n[1] 0.8196749\n\n\nA positive coraviance (8.74) indicates lung capacity and age tend to increase together. The positive correlation relatively close to 1 (0.82) indicates there is a fairly strong correlation between the variables.\n##2. Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n#create the sample\nx<-rep(c(0,1,2,3,4),times=c(128, 434, 160, 64, 24))\nsample(x, 10)\n\n\n [1] 2 1 1 0 1 1 2 1 0 1\n\n\nCode\n#Verify n of sample\nsum(128, 434, 160, 64, 24)\n\n\n[1] 810\n\n\n\n\nCode\n#Calculate the mean\nmean(x)\n\n\n[1] 1.28642\n\n\nCode\n#Verify the mean\nsample_mean <- (((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/810)\nprint(sample_mean)\n\n\n[1] 1.28642\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016\n\n\n#a. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n#probability of 2 convictions?\ndnorm.convict <- dnorm(2, mean(x), sd(x))\nprint(dnorm.convict)\n\n\n[1] 0.3201613\n\n\nThe probability of 2 convications in 0.32.\n#b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#probability of <2 convictions\nless.than <- pnorm(2, mean(x), sd(x)) - dnorm.convict\nprint(less.than)\n\n\n[1] 0.4593924\n\n\nThe probability of <2 convictions is 0.46.\n#c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n#probability of =<2 convictions?\npnorm.convict <- pnorm(2, mean(x), sd(x))\nprint(pnorm.convict)\n\n\n[1] 0.7795537\n\n\nThe probability of less than or equal to 2 convictions is 0.78.\n#d. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n#probability of >2 convictions?\ngreater.than <- 1 - pnorm.convict\nprint(greater.than)\n\n\n[1] 0.2204463\n\n\nThe probability of greater than 2 convictions is 0.22.\n\n\nCode\n#Verify all probabilities add to 1\nless.than + dnorm.convict + greater.than\n\n\n[1] 1\n\n\n#e. What is the expected value for the number of prior convictions?\n\n\nCode\n# Expected value of a probability distribution  can be found with μ = Σx * P(x), where x = data value and P(x) = probability of data. \n\n#Calculate probabilities of data\np0 <- dnorm(0, mean(x), sd(x))\np0\n\n\n[1] 0.1641252\n\n\nCode\np1 <- dnorm(1, mean(x), sd(x))\np1\n\n\n[1] 0.410739\n\n\nCode\np2 <- dnorm(2, mean(x), sd(x))\np2\n\n\n[1] 0.3201613\n\n\nCode\np3 <- dnorm(3, mean(x), sd(x))\np3\n\n\n[1] 0.07772916\n\n\nCode\np4 <- dnorm(4, mean(x), sd(x))\np4\n\n\n[1] 0.005877753\n\n\nCode\n#Calculate expected value\nev <- sum((0*p0), (1*p1), (2*p2), (3*p3), (4*p4))\nev\n\n\n[1] 1.30776\n\n\nCode\n#The expected value should be close to the mean in a normal distribution\nmean(x)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.31.\n#f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n#Calculate variance\nvar(x)\n\n\n[1] 0.8572937\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW1_SteveONeill.html",
    "href": "posts/HW1_SteveONeill.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 2\nI will make a dataframe from the values provided:\n\n\nCode\nprior_convictions=c(0,1,2,3,4)\nfreq=c(128, 434, 160, 64, 24)\nprisondata <- data.frame(prior_convictions, freq)\nprisondata\n\n\n  prior_convictions freq\n1                 0  128\n2                 1  434\n3                 2  160\n4                 3   64\n5                 4   24\n\n\nAnd add a probability column:\n\n\nCode\nprison_prob <- prisondata %>% mutate(prob = freq/sum(freq))\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\n\n2a.\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nFrom the table above, the probability is 0.19753086, nearly 20 percent.\n\n\n2b.\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nhead(prison_prob,2) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.6938272\n\n\nThe probability a randomly selected inmate has has fewer than 2 prior convictions is ~69%.\n\n\n2c.\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nhead(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1  0.891358\n\n\nThe probability a randomly selected inmate has 2 or fewer convictions is ~89%\n\n\n2d.\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ntail(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.3061728\n\n\nThe probability a randomly selected inmate has more than 2 prior convictions is ~30.6%\n\n\n2e.\nWhat is the expected value of the number of prior convictions?\n\n\nCode\nsum(prison_prob$prior_convictions*prison_prob$prob)\n\n\n[1] 1.28642\n\n\nCode\n#Or another way,\n\nweighted.mean(prison_prob$prior_convictions,prison_prob$prob)\n\n\n[1] 1.28642\n\n\nThe expected value of prior convictions is 1.28642\n\n\n2f\n\n\nCode\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\nCode\nvar(prison_prob$freq)\n\n\n[1] 25948\n\n\nCode\nsd(prison_prob$freq)\n\n\n[1] 161.0838\n\n\nThe variance among all prior convictions is 25948. The standard deviation among all prior convictions is 161.0838."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html",
    "href": "posts/HW1_ToryBartelloni.html",
    "title": "DACSS 603: Homework 1",
    "section": "",
    "text": "First, let’s load our packages and read in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\nlcdata <- read_xls(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1a",
    "href": "posts/HW1_ToryBartelloni.html#q1a",
    "title": "DACSS 603: Homework 1",
    "section": "Q1a",
    "text": "Q1a\nWhat does the distribution of LungCap look like?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_histogram(bins=45) +\n  theme_bw() +\n  labs(x=\"Lung Capacity\", y=\"Frequency\", \n       title = \"Lung Capacity Distribution\")\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1b",
    "href": "posts/HW1_ToryBartelloni.html#q1b",
    "title": "DACSS 603: Homework 1",
    "section": "Q1b",
    "text": "Q1b\nCompare the probability density of the LungCap with respect to Males and Females.\n\n\nCode\nlcdata %>%\n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(group=Gender, fill=Gender)) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(x=\"Lung Capacity\", title = \"Lung Capacity Distribution\",\n       subtitle = \"Comparing lung capacity between genders\")\n\n\n\n\n\nThe boxplot comparison indicates that on average males have larger lung capacity, but it also shows that the range and IQR for each gender are similar and have a significant amount of overlap."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1c",
    "href": "posts/HW1_ToryBartelloni.html#q1c",
    "title": "DACSS 603: Homework 1",
    "section": "Q1c",
    "text": "Q1c\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers\")\n\n\n\n\n\nComparing the distributions shows that Smokers have a higher mean lung capacity and a significantly smaller range and IQR. This does not make sense intuitively so I would want to investigate the data a bit more to understand the possible reasons."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1d",
    "href": "posts/HW1_ToryBartelloni.html#q1d",
    "title": "DACSS 603: Homework 1",
    "section": "Q1d",
    "text": "Q1d\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nlc_with_age_groups <- lcdata %>%\n  mutate(Age_Group = factor(case_when(\n    Age <= 13 ~ \"<=13\",\n    Age %in% c(14,15) ~ \"14-15\",\n    Age %in% c(16,17) ~ \"16-17\",\n    Age >= 18 ~ \">=18\"\n      ),\n    levels = c(\"<=13\",\"14-15\",\"16-17\",\">=18\")\n    )\n  )\n\nlc_with_age_groups %>% \n  ggplot(aes(x=Age_Group,y=LungCap)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nComparing age groups shows a consistent and clear increase in lung capacity as ages increase up to and over 18 years old."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1e",
    "href": "posts/HW1_ToryBartelloni.html#q1e",
    "title": "DACSS 603: Homework 1",
    "section": "Q1e",
    "text": "Q1e\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\n\n\nCode\nlc_with_age_groups %>%\n  ggplot(aes(x=Smoke, y=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  facet_wrap(~Age_Group) +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers within age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nOutside of ages 13 and under all ages groups show higher average, range, and IRQ for non-smokers. It seems likely that the youngest age group, 13 and under, has the largest number of observations of non-smokers which is bringing down the overall average and lower end of the range. This effect is what is causing us to see the higher lung capacity in smokers overall, but we can infer that the causal factor is more likely age than smoking."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1f",
    "href": "posts/HW1_ToryBartelloni.html#q1f",
    "title": "DACSS 603: Homework 1",
    "section": "Q1f",
    "text": "Q1f\nCalculate the correlation and covariance between Lung Capacity and Age (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nknitr::kable(\n  lcdata %>% summarise(Covariance = cov(LungCap, Age),\n                     Correlation = cor(LungCap, Age)),\n  caption = \"Relationship between lung capacity and age.\"\n)\n\n\n\nRelationship between lung capacity and age.\n\n\nCovariance\nCorrelation\n\n\n\n\n8.738289\n0.8196749\n\n\n\n\n\nThe covariance shows us that the relationship is positive and the correlation coefficient shows us that the relationship is a strong, positive relationship. So the older the people in the data the larger the lung capacity was observed, on average."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2a",
    "href": "posts/HW1_ToryBartelloni.html#q2a",
    "title": "DACSS 603: Homework 1",
    "section": "Q2a",
    "text": "Q2a\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nprison_props <-  prison_data %>% group_by(X) %>%\n    summarise(Frequency = Frequency,\n      Proportion = Frequency / sum(prison_data$Frequency))\nknitr::kable(prison_props,\n  caption=\"Proportion of Inmates\"\n\n)\n\n\n\nProportion of Inmates\n\n\nX\nFrequency\nProportion\n\n\n\n\n0\n128\n0.1580247\n\n\n1\n434\n0.5358025\n\n\n2\n160\n0.1975309\n\n\n3\n64\n0.0790123\n\n\n4\n24\n0.0296296\n\n\n\n\n\nBy calculating the proportion of inmates with each number of prior convictions we can see that the probability of randomly selecting an inmate with 2 prior convictions is 0.1975 or about 19.8%."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2b",
    "href": "posts/HW1_ToryBartelloni.html#q2b",
    "title": "DACSS 603: Homework 1",
    "section": "Q2b",
    "text": "Q2b\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nprint(paste(\"Probability of fewer than 2 prior convictions:\",\nsum(filter(prison_props, X < 2)$Proportion)))\n\n\n[1] \"Probability of fewer than 2 prior convictions: 0.693827160493827\"\n\n\nSumming prisoners with zero and one prior conviction provides us a probability that 0.6938 or about 69.4% chance that a randomly selected inmate would have less than 2 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2c",
    "href": "posts/HW1_ToryBartelloni.html#q2c",
    "title": "DACSS 603: Homework 1",
    "section": "Q2c",
    "text": "Q2c\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nprint(paste(\"Probability of 2 or fewer prior convictions:\",\n            sum(filter(prison_props, X <=2)$Proportion)))\n\n\n[1] \"Probability of 2 or fewer prior convictions: 0.891358024691358\"\n\n\nSumming the prisoners with two or fewer prior convictions gives us the probability that 0.89 or about 89% probability that a randomly selected inmate would have two prior convictions or fewer."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2d",
    "href": "posts/HW1_ToryBartelloni.html#q2d",
    "title": "DACSS 603: Homework 1",
    "section": "Q2d",
    "text": "Q2d\nWhat is the probability that a randomly selected inmate has more than two prior convictions?\n\n\nCode\nprint(paste(\"Probability of more than 2 prior convictions:\",\nsum(filter(prison_props, X > 2)$Proportion)))\n\n\n[1] \"Probability of more than 2 prior convictions: 0.108641975308642\"\n\n\nThe probability found for either 3 or 4 prior convictions (there is no inmate with more than 4 prior convictions) is 0.1084 or about 10.8% probability."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2e",
    "href": "posts/HW1_ToryBartelloni.html#q2e",
    "title": "DACSS 603: Homework 1",
    "section": "Q2e",
    "text": "Q2e\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nprint(paste(\"The expected value for prior convictions:\", mean(prison_indi_data$X)))\n\n\n[1] \"The expected value for prior convictions: 1.28641975308642\"\n\n\nBy taking a weighted average or an average of all possible observations to select from the expected value is 1.28642 or about 1.3 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2f",
    "href": "posts/HW1_ToryBartelloni.html#q2f",
    "title": "DACSS 603: Homework 1",
    "section": "Q2f",
    "text": "Q2f\nCalculate the variance and standard deviation for Prior Convictions.\n\n\nCode\nknitr::kable(\n  prison_indi_data %>% summarise(Variance = var(X),\n                     \"Standard Deviation\" = sd(X)),\n  caption = \"Spread of Inmate Prior Convictions\"\n)\n\n\n\nSpread of Inmate Prior Convictions\n\n\nVariance\nStandard Deviation\n\n\n\n\n0.8572937\n0.9259016"
  },
  {
    "objectID": "posts/HW1_Yakub Rabiutheen.html",
    "href": "posts/HW1_Yakub Rabiutheen.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap,freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nComparison of the Genders for both Men and Women using a Boxplot.\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\n\n\n\nHere is the capacity of Smokers vs Non-Smokers\n\n\nCode\nboxplot(df$LungCap~df$Smoke,\n        ylab = \"Capacity\", \n        main = \"Lung Capacity of Smokers Vs Non-Smokers\",\n        las = 1)\n\n\n\n\n\n\n\n\nLet’s break it down even further, this is the Lung Capacity by Age Group\n\n\nCode\ndf$Agegroups<-cut(df$Age,breaks=c(-Inf, 13, 15, 17, 20), labels=c(\"0-13 years\", \"14-15 years\", \"16-17 years\", \"18+ years\"))\n\n\nBelow is the overall Lung Capacity of Age Groups without including Smokers.\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Gender)) +\n          geom_bar(stat = \"identity\") +\n          coord_flip() +\n          theme_classic()\n\n\n\n\n\n#e\nHere is a comparision of AgeGroup Lung Capacity in comparison with Smoker vs Non-Smoker.\n\n\nCode\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\n\nBased on the comparison of lung capacities between Smoker and Non-Smoker the results are pretty similar.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nQuestion 2\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\ndf <- data.frame(X, Frequency)\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nAs shown below, the most common Prior Convictions is 1.\n\n\nCode\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nDividing by the total among 810 we can determine the probability for each. 810 is the Sum of the Frequency which I checked manually.\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\n\n\nError in mutate(df, Probability = Frequency/sum(Frequency)): could not find function \"mutate\"\n\n\nCode\ndf2\n\n\nError in eval(expr, envir, enclos): object 'df2' not found\n\n\n\nFilter for Probability of 2 Convictions\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\n\nError in df2 %>% filter(X < 2): could not find function \"%>%\"\n\n\nCode\nsum(b2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'b2' not found\n\n\n\nFilter for Probability of Less than 2 Convictions\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\n\nError in df2 %>% filter(X <= 2): could not find function \"%>%\"\n\n\nCode\nsum(c2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'c2' not found\n\n\n\n\n\nFilter for Probability of greater than 2 convictions.\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\n\nError in df2 %>% filter(X > 2): could not find function \"%>%\"\n\n\nCode\nsum(d2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'd2' not found\n\n\nWhat is the expected value of the number of prior convictions?\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\n\n\nError in weighted.mean(df2$X, df2$Probability): object 'df2' not found\n\n\nCode\ne\n\n\nError in eval(expr, envir, enclos): object 'e' not found\n\n\n\n\n\nVariance and Standard Deviation for Question.\n\n\nCode\nvar(df$X)\n\n\n[1] 2.5\n\n\n\n\nCode\nsd(df$X)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/HW2answers-DonnySnyder.html",
    "href": "posts/HW2answers-DonnySnyder.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\n\n\nCode\nbyPassConfInt <- NA\nbyPassConfInt[1] <- 19 + .9*(10/(sqrt(539)))\nbyPassConfInt[2] <- 19 - .9*(10/(sqrt(539)))\n\nangConfInt <- NA\nangConfInt[1] <- 18 + .9*(9/(sqrt(847)))\nangConfInt[2] <- 18 - .9*(9/(sqrt(847)))\n\nprint(byPassConfInt)\n\n\n[1] 19.38766 18.61234\n\n\nCode\nprint(angConfInt)\n\n\n[1] 18.27832 17.72168\n\n\nThe confidence interval is narrower than for angiography than for bypass surgery.\n\n\nQuestion 2\n\n\nCode\npointEstData <- NA\npointEstData[1:567] <- 1\npointEstData[568:1031] <- 0\npointSD <- sd(pointEstData)\npointEst <- 567/1031\npointConfInt <- NA\npointConfInt[1] <- pointEst + .95*(pointSD/(sqrt(1031)))\npointConfInt[2] <- pointEst - .95*(pointSD/(sqrt(1031)))\nprint(pointConfInt)\n\n\n[1] 0.5646779 0.5352251\n\n\nThe confidence interval here suggests that we can assume with 95% confidence that between 56.5% of adult Americans and 53.5% believe that college education is essential for success.\n\n\nQuestion 3\n\n\nCode\npopSD <- (200 - 30)/4\ncriticalVal <- 1.96\nsampSize <- ((popSD * criticalVal)/5)^2\nprint(sampSize)\n\n\n[1] 277.5556\n\n\nThe size of the sample should be 278.\n\n\nQuestion 4a\nNull hypothesis: Womens income does not deviate from the mean income of senior-level workers.\nAlternative hypothesis: Womens income does deviate from the mean income of senior-level workers.\n\n\nCode\ntStat <- (410 - 500)/(90/(sqrt(9)))\ndegreeFree <- 9-1\n2*pt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.01707168\n\n\nCode\npt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.008535841\n\n\nCode\npt(tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\nThe p value of this test statistic and degrees of freedom is 0.017.\n#4b The p-value for the one-tailed test h0 < 500 is 0.0085. This is half because it is only measuring half of the distribution.\n#4c The p-value for the one-tailed test h0 > 500 is 0.9915. This is because this is measuring in the opposite direction of the actual mean.\n#Question 5\n\n\nCode\ntStatJones <- (519.5 - 500)/(10)\ntStatSmith <- (519.7 - 500)/(10)\ndegreeFree <- 1000-1\n\n2*pt(tStatJones,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.05145555\n\n\nCode\n2*pt(tStatSmith,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.04911426\n\n\n#Question 5b As you can see from the printed values, the Smith study is statistically significant while the Jones study is not.\n#Question 5c If you do not report the p-value, you cannot tell how close the p-value is to being significant, so it can get rid of the value of running the study to not report it.\n#Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ntStatGas <- (mean(gas_taxes) - 45)/(sqrt(sd(gas_taxes)/length(gas_taxes)))\ndegreeFree <- length(gas_taxes) - 1\n\n2*pt(-tStatGas,degreeFree, lower.tail = FALSE)\n\n\n[1] 2.341428e-05\n\n\nYes there is enough evidence. The p-value is far below p = 0.05, at 0.0000234."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html",
    "href": "posts/HW2_603_Niharikapola.html",
    "title": "Homework-2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-1",
    "href": "posts/HW2_603_Niharikapola.html#question-1",
    "title": "Homework-2",
    "section": "Question 1:",
    "text": "Question 1:\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-2",
    "href": "posts/HW2_603_Niharikapola.html#question-2",
    "title": "Homework-2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-3",
    "href": "posts/HW2_603_Niharikapola.html#question-3",
    "title": "Homework-2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-4",
    "href": "posts/HW2_603_Niharikapola.html#question-4",
    "title": "Homework-2",
    "section": "Question-4",
    "text": "Question-4\n\nA\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9\n\n\n\n\nCalculating test-statistic\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 1.861970 1.861755\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500.\n\n\nB\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.9309851 0.9308776\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500.\n\n\nC\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.06901494 0.06912239\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-5",
    "href": "posts/HW2_603_Niharikapola.html#question-5",
    "title": "Homework-2",
    "section": "Question 5",
    "text": "Question 5\n\nA\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCalculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555\n\n\n\n\nCalculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith.\n\n\nB\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones.\n\n\nC\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as *P ≤ 0.05* versus *P > 0.05* will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_603_Niharikapola.html#question-6",
    "href": "posts/HW2_603_Niharikapola.html#question-6",
    "title": "Homework-2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/hw2_boonstra.html",
    "href": "posts/hw2_boonstra.html",
    "title": "Homework 2",
    "section": "",
    "text": "This question involves calculating 90% confidence intervals for data on mean wait time between heart surgery procedures being scheduled and the procedures being conducted for individuals in Ontario, Canada.\nThe equation for calculating confidence intervals using the Student’s t-distribution is as follows:\n\\(CI = \\overline{x} \\pm (t \\times \\frac{\\sigma}{\\sqrt{n}})\\)\n\n\nStarting with the Bypass subset, we can fill in some of these values:\n\\(CI_{bypass} = 19 \\pm (t \\times \\frac{10}{\\sqrt{539}})\\)\nThe t-quantile for the 90% confidence interval at 538 degrees of freedom is equal to 1.6476908, leaving us with the equation:\n\\(CI_{bypass} = 19 \\pm (1.648 \\times \\frac{10}{\\sqrt{539}})\\)\nThis maths out to 18.2902893 and 19.7097107.\n\n\n\nSimilarly, for the Angiography subset:\n\\(CI_{angiography} = 18 \\pm (t \\times \\frac{9}{847} )\\)\nThe t-quantile for the 90% confidence interval at 84 degrees of freedom is equal to 1.6466568, leaving us with the equation:\n\\(CI_{angiography} = 18 \\pm (1.647 \\times \\frac{9}{847} )\\)\nThis maths out to 17.4907818 and 18.5092182.\n\n\n\nBetween these two subsets, the 90% confidence interval is narrower for the Angiography subset:\n\\(CI_{bypass\\_range} = CI_{bypass\\_high} - CI_{bypass\\_low}=\\) 1.4194214\n\\(CI_{angiography\\_range}=CI_{angiography\\_high} - CI_{angiography\\_low}=\\) 1.0184363"
  },
  {
    "objectID": "posts/hw2_boonstra.html#t-tests",
    "href": "posts/hw2_boonstra.html#t-tests",
    "title": "Homework 2",
    "section": "t-tests",
    "text": "t-tests\nThese tests operate under the assumption that female employees’ pay data are randomly sampled and normally distributed.\n\nt.test(fem_pay,mu=500) # two-sided\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.01707\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 340.8199 479.1801\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"less\") # one-sided, H_0 mean is not less\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.008536\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 465.7864\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"greater\") # one-sided, H_0 mean is not greater\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.9915\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 354.2136      Inf\nsample estimates:\nmean of x \n      410 \n\n\n\nPart A\nFrom the first test, we can reject the null hypothesis that mean pay for female employees is equal to $500 per week. This holds at the 5% significance level, with a p-value of less than 0.02, and a t-statistic of -3.\n\n\nPart B\nFrom the second test, we get a p-value of less than 0.009, which enables us at the 5% significance level to reject the null hypothesis that mean pay for female employees is not less than $500, and accept the alternative hypothesis that mean pay is less than $500.\n\n\nPart C\nFrom the third test, we get a p-value of greater than 0.99, which mean that we fail to reject the null hypothesis that mean pay for female employees is not greater than $500."
  },
  {
    "objectID": "posts/HW2_CalebHill.html",
    "href": "posts/HW2_CalebHill.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s load the relevant libraries.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nFor question 1, we need to construct the 90% confident interval to estimate the actual mean wait time for eahc of the two procedures.\n\n\nCode\ns_mean_b <- 19\ns_sd_b <- 10\ns_size_b <- 539\nstandard_error_b <- s_sd_b / sqrt(s_size_b)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_b <- qt(p = 1 - tail_area, df = s_size_b - 1)\nCI_b <- c(s_mean_b - t_score_b * standard_error_b,\n        s_mean_b + t_score_b * standard_error_b)\nprint(CI_b)\n\n\n[1] 18.29029 19.70971\n\n\nThis is the CI for bypass. The following code chunk is for angiography.\n\n\nCode\ns_mean_a <- 18\ns_sd_a <- 9\ns_size_a <- 847\nstandard_error_a <- s_sd_a / sqrt(s_size_a)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_a <- qt(p = 1 - tail_area, df = s_size_a - 1)\nCI_a <- c(s_mean_a - t_score_a * standard_error_a,\n        s_mean_a + t_score_a * standard_error_a)\nprint(CI_a)\n\n\n[1] 17.49078 18.50922\n\n\nIs the confidence interval narrower for angiograpy or bypass survey? Answer = angiography."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a",
    "href": "posts/HW2_CalebHill.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\n\n\nCode\ns_mean_4a <- 410\ns_sd_4a <- 90\ns_size_4a <- 9\nstandard_error_4a <- s_sd_4a / sqrt(s_size_4a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\nCI_4a <- c(s_mean_4a - t_score_4a * standard_error_4a,\n        s_mean_4a + t_score_4a * standard_error_4a)\nprint(CI_4a)\n\n\n[1] 340.8199 479.1801\n\n\nBased upon the data provided, we can be within a 95% CI that mean income for female employees is less than $500 per week. If Ha : μ < 500, then we can accept the hypothesis, based upon the CI. However, for section B, we’ll report the P-value via the t-score."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b",
    "href": "posts/HW2_CalebHill.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value=pt(q = t_score_4a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.025\n\n\nWith a P-value of 0.025, we can accept the Ha : μ < 500. However, let’s change the lower.tail value to TRUE to see about Ha : μ > 500."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c",
    "href": "posts/HW2_CalebHill.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value = pt(q = t_score_4a, df = 8, lower.tail = TRUE)\nprint(p_value)\n\n\n[1] 0.975\n\n\nJust as I thought. We have to reject the second hypothesis, that Ha : μ > 500, as the P-value is 0.975, outside of statistical significance minimum of 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a-1",
    "href": "posts/HW2_CalebHill.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nFor Jones:\n\n\nCode\ns_mean_5a <- 519.5\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 499.8766 539.1234\n\n\nFor Smith:\n\n\nCode\ns_mean_5a <- 519.7\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 500.0766 539.3234"
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b-1",
    "href": "posts/HW2_CalebHill.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nCode for Section B are the P-values shown for each code chunk. Are they statistically significant? At 0.043 for both, yes as they are below 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c-1",
    "href": "posts/HW2_CalebHill.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nThe P-value is the likelihood of finding the particular set of observations if the null hypothesis were true. As the P-value is traditionally use in frequentist statistics, we are only able to ascribe probability to this specific set of observations – which are themselves a set amount of observations.\nTherefore, it can sometimes be misleading to report a P-value as 0.05. CI levels allow a range within the set of observations. We can see this problem best with the above results via Jones and Smith. They do not get the same sample mean, even with similar observations."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html",
    "href": "posts/HW2_EmmaRasmussen.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section",
    "href": "posts/HW2_EmmaRasmussen.html#section",
    "title": "Homework 2",
    "section": "1.",
    "text": "1.\n\n\nCode\n#Bypass\n#calculating t-score for 90% confidence interval\ntscoreb<- qt(p=1-.05, df=539-1)\n\n#calculating standard error\nseb<- 10/sqrt(539)\n\nmeanb<- 19\n\nCIb<- c(meanb- (tscoreb*seb), meanb+ (tscoreb*seb))\nCIb\n\n\n[1] 18.29029 19.70971\n\n\nCode\n#Angiography\n#calculating t-score for 90% confidence interval\ntscorea<- qt(p=1-.05, df=847-1)\n\n#calculating standard error\nsea<- 9/sqrt(847)\n\nmeana<- 18\n\nCIa<- c(meana- (tscorea*sea), meana+ (tscorea*sea))\nCIa\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days. The 90% confidence interval for angiography is [17.49, 18.51] days. The confidence interval for angiography is narrower, which makes sense given it has a (slightly) smaller standard deviation and a larger sample size (larger sample size reduces margin of error)."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-1",
    "href": "posts/HW2_EmmaRasmussen.html#section-1",
    "title": "Homework 2",
    "section": "2.",
    "text": "2.\n\n\nCode\n#assigning n= number of trials\nn<- 1031\n#assigning k= number agree\nk<- 567\n\n#calculating point estimate\np<- 567/1031\np\n\n\n[1] 0.5499515\n\n\nCode\n#calculating margin of error for 95% CI. I have no idea how to calculate a confidence interval without a sd. I found this formula online.\nmargin<- qnorm(0.975)*sqrt(p*(1-p)/n)\nmargin\n\n\n[1] 0.03036761\n\n\nCode\nCI<- c(p-margin, p+ margin)\nCI\n\n\n[1] 0.5195839 0.5803191\n\n\nThe 95% confidence interval for American’s agreeing that college education is essential for success is [51.96, 58.03]%. The point estimate for this value based on the survey is 55%."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-2",
    "href": "posts/HW2_EmmaRasmussen.html#section-2",
    "title": "Homework 2",
    "section": "3.",
    "text": "3.\n\n\nCode\n#estimating population standard deviation\n(200-30)/4\n\n\n[1] 42.5\n\n\nCode\n#solving for n in the equation for confidence intervals 5=1.96*(42.5/sqrt(n))\n#n= 277.56 or 278\n\n\nBy plugging in 5 to the formula for confidence intervals (the t-value for 95%, and the standard deviation estimate of 42.5) we get a value of n=277.56 or 278 need to be in the sample to retrieve a confidence interval of width 10."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-3",
    "href": "posts/HW2_EmmaRasmussen.html#section-3",
    "title": "Homework 2",
    "section": "4.",
    "text": "4.\n\na.\n\nAssume data is normally distributed\nHo: μ =500\nHa: μ not equal to 500 μ < 500 μ > 500\nalpha level =0.05\n\n\n\nCode\n#calculating the standard error\nsef<- 90/sqrt(9)\nsef\n\n\n[1] 30\n\n\nCode\n#calculating t-score\ntf<-(410-500)/(sef)\ntf \n\n\n[1] -3\n\n\nCode\n#calculating the p-value from the test statistic (multiply times two because we are doing a two-sided test)\n(pt(q=-3, df=8))*2\n\n\n[1] 0.01707168\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or lower, as the default calculates the lower tail)\npt(-3, 8)\n\n\n[1] 0.008535841\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or higher, as we included lower.tail=FALSE)\npt(-3, 8, lower.tail=FALSE)\n\n\n[1] 0.9914642\n\n\nCode\n# since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than 500 a week\n\n\nTwo-sided: Since our p-value is 0.017 we can reject the null hypothesis at alpha level=0.05. We have evidence that the mean weekly earnings for women at this company is different from $500. ### b. Lower tail: Since our p-value (0.0085) is less than the alpha level of 0.05, we can reject the null. We have evidence that the mean weekly earnings at this company for women is less than $500. ### c.  Upper tail: Since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than $500 a week"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-4",
    "href": "posts/HW2_EmmaRasmussen.html#section-4",
    "title": "Homework 2",
    "section": "5.",
    "text": "5.\n\na.\n\n\nCode\n#Jones\n#calculating t-score\n(519.5-500)/(10)\n\n\n[1] 1.95\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.95, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n##calculating t-score\n(519.7-500)/(10)\n\n\n[1] 1.97\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.97, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.04911426\n\n\n\n\nb.\nAt the .05 significance level, Jones’ findings are not significant but Smith’s findings are.\n\n\nc.\nThis example shows that there is a very find line between rejecting and not rejecting the null hypothesis. Their findings were extremely similar, the means are different by only 0.2. In this way, reporting the p-value retrieved is actually really important to make this distinction. Similarly, Jones’ findings would have been significant at the 0.1 significance level, so rejecting or not rejecting the null hypothesis based on a p-value can be fairly arbitrary."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-5",
    "href": "posts/HW2_EmmaRasmussen.html#section-5",
    "title": "Homework 2",
    "section": "6.",
    "text": "6.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nUsing a 95% confidence level we get a p-value of 0.038. We reject the null hypothesis. We have evidence that the average tax per gallon of gas in the U.S. is less than 45 cents."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html",
    "href": "posts/HW2_EthanCampbell.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\n\n\nWarning: package 'readxl' was built under R version 4.1.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.8     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a",
    "href": "posts/HW2_EthanCampbell.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nHere we can get the t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\n# Calculating the t statistic\nT_statistic = (410-500)/(90/(sqrt(9)))\nT_statistic\n\n\n[1] -3\n\n\nCode\n# calculating the p value\n\npvalue = 2* pt(T_statistic, df=8)\n\npvalue\n\n\n[1] 0.01707168\n\n\nCode\n# the p value is showing evidence that we would reject the null hypothesis here since it is < .05."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b",
    "href": "posts/HW2_EthanCampbell.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nTesting to see p value of it being less than 500\n\n\nCode\npvalue_left <- pt(T_statistic, df = 8, lower.tail = TRUE)\npvalue_left\n\n\n[1] 0.008535841\n\n\nCode\n# this is also showing a value smaller than the 5% given which means it is more evidence to reject the null hypothesis"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c",
    "href": "posts/HW2_EthanCampbell.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nTesting to see the p value greater than 500\n\n\nCode\npvalue_right <- pt(T_statistic, df = 8, lower.tail = FALSE)\n\npvalue_right\n\n\n[1] 0.9914642\n\n\nCode\n# Making sure the two values equal 1\nsum(pvalue_left, pvalue_right)\n\n\n[1] 1\n\n\nthis is showing a 99.14% chance of observing if the population mean was less than that 500 mark. This is interesting and we would fail to reject the null hypothesis here since it exceeds the amount specified. This would indicate that they are not getting paid the same amount."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a-1",
    "href": "posts/HW2_EthanCampbell.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n# Lets run the test and see whats going on\n\nJones <- (519.5-500)/(10)\nSmith= (519.7-500)/(10)\n\n# Here we can see the t stat they are both getting so looking good so far\nJones\n\n\n[1] 1.95\n\n\nCode\nSmith\n\n\n[1] 1.97\n\n\nCode\n# Now to get the P-value\n\nJones_p <- 2*pt(Jones, df= 999, lower.tail = FALSE)\nSmith_p <- 2*pt(Smith, df= 999, lower.tail = FALSE)\n\n# Observing the p values\n\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b-1",
    "href": "posts/HW2_EthanCampbell.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBased on the basic test of this with a CI of 95% we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c.",
    "href": "posts/HW2_EthanCampbell.html#c.",
    "title": "Homework 2",
    "section": "C.",
    "text": "C.\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html",
    "href": "posts/HW2_KarenKimble.html",
    "title": "Kimble HW 2",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(stats)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-1",
    "href": "posts/HW2_KarenKimble.html#question-1",
    "title": "Kimble HW 2",
    "section": "Question 1",
    "text": "Question 1\n\nAngiography\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 9 / sqrt(847)\n\nt_score <- qt(p = 1-tail_area, df = 846)\n\nCI_1 <- c(18 - t_score * standard_error, 18 + t_score * standard_error)\n\nprint(CI_1)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nBypass Surgery\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 10 / sqrt(539)\n\nt_score <- qt(p = 1-tail_area, df = 538)\n\nCI_2 <- c(19 - t_score * standard_error, 19 + t_score * standard_error)\n\nprint(CI_2)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval for angiography (range of about 1.1) is smaller than the confidence interval for bypass surgery (range of about 1.5) at a confidence level of 90%."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-2",
    "href": "posts/HW2_KarenKimble.html#question-2",
    "title": "Kimble HW 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\n# Point estimate\n\np <- 567/1031\n\np\n\n\n[1] 0.5499515\n\n\nThe point estimate p for the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n# Confidence Interval\n\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_deviation <- sqrt((p * (1-p))/1031)\n\nstandard_error <- standard_deviation / sqrt(1031)\n\nt_score <- qt(p = 1-tail_area, df = 1030)\n\nCI_3 <- c(p - t_score * standard_error, p + t_score * standard_error)\n\nprint(CI_3)\n\n\n[1] 0.5490046 0.5508984\n\n\nThrough this test, I am 95% confident that the true proportion of all adult Americans who believe a college education is essential for success lies between 0.549 and 0.551."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-3",
    "href": "posts/HW2_KarenKimble.html#question-3",
    "title": "Kimble HW 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nsd <- (200-30)/4\n\nx <- 5/(2.26 * sd)\n\nsample_size = (1/x)^2\n\nsample_size\n\n\n[1] 369.0241\n\n\nThe size of the sample should be at least 370 people. Since they want a confidence interval within 5 dollars and they assume the standard deviation is a quarter of the range of 30 dollars to 200 dollars, I was able to use the confidence interval equation to find the missing variable of sample size. The confidence level is 95%, meaning that with a large sample size, the t-score would be around 2.26, allowing me to use the equation and isolate the sample size."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-4",
    "href": "posts/HW2_KarenKimble.html#question-4",
    "title": "Kimble HW 2",
    "section": "Question 4",
    "text": "Question 4\n\nPart A\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\n\n\nCode\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 90 / sqrt(9)\n\nt_score <- qt(p = 1-tail_area, df = 8)\n\nCI_4A <- c(410 - t_score * standard_error, 410 + t_score * standard_error)\n\np_value = 2 * pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05\n\n\nCode\nprint(CI_4A)\n\n\n[1] 340.8199 479.1801\n\n\nThe p-value is exactly 0.05, meaning it is not smaller than the alpha value of 0.05 and thus is not statistically significant. We do not have enough evidence to reject the null hypothesis. The confidence interval shows that we are 95% confident the true mean income of female employees lies between 340.82 dollars/week and 479.18 dollars/week.\n\n\nPart B\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = TRUE)\n\np_value\n\n\n[1] 0.975\n\n\nThe p-value for the alternate hypothesis that the true mean income of female employees is greater than 500 dollars/week is 0.975. This value is extremely large and greater than the 0.05 alpha level, meaning there is not statistically significant evidence and thus do not reject the null hypothesis.\n\n\nPart C\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.025\n\n\nThe p-value for the alternative hypothesis that the true mean income of female employees is less than 500 dollars/week is 0.025. This value is less than the alpha value of 0.05, meaning it is statistically significant and we can reject the null hypothesis. The true mean income of female employees is likely less than 500 dollars/week."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-5",
    "href": "posts/HW2_KarenKimble.html#question-5",
    "title": "Kimble HW 2",
    "section": "Question 5",
    "text": "Question 5\n\nPart A\n\n\nCode\n# Jones\n\nt_score <- (519.5-500)/(10)\n\nt_score\n\n\n[1] 1.95\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Smith\n\nt_score <- (519.7-500)/(10)\n\nt_score\n\n\n[1] 1.97\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.04911426\n\n\n\n\nPart B\nFor Jones’s study, the results were not statistically significant because the p-value of 0.51 is greater than the alpha value of 0.05. For Smith’s study, the results were statistically signficiant because the p-value of 0.49 is less than the alpha value of 0.05.\n\n\nPart C\nReporting the result of a test as P being greater or less than the alpha value can be misleading if the p value is not reported. In both studies, the p-value was .01 away from 0.05, yet in only one study was the result statistically significant. A small p-value may still be meaningful to report because it still shows that there was a relatively small probability of getting the result that one did. Not reporting the p-value when reporting the result and whether or not a hypothesis is rejected leaves an important part of the study out. Someone simply reading that a hypothesis was not rejected without knowing the p-value may assume the p-value was very large even when it was small (such as in the case of Smith vs. Jones), thus leaving out a major aspect of the study."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-6",
    "href": "posts/HW2_KarenKimble.html#question-6",
    "title": "Kimble HW 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, alternative = c(\"less\"), mu = 45)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value of this test is 0.038, which is less than the alpha value of 0.05. This means that there is statistically significant evidence and we can reject the null hypothesis, that the true average gas tax per gallon in the United States is 45 cents. There is significant evidence to suggest that the true average gas tax per gallon in the United States is less than 45 cents."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html",
    "href": "posts/HW2_ManiGogula.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-1",
    "href": "posts/HW2_ManiGogula.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-2",
    "href": "posts/HW2_ManiGogula.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-3",
    "href": "posts/HW2_ManiGogula.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-4",
    "href": "posts/HW2_ManiGogula.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a",
    "href": "posts/HW2_ManiGogula.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "href": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-p-value",
    "href": "posts/HW2_ManiGogula.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b",
    "href": "posts/HW2_ManiGogula.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c",
    "href": "posts/HW2_ManiGogula.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-5",
    "href": "posts/HW2_ManiGogula.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a-1",
    "href": "posts/HW2_ManiGogula.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b-1",
    "href": "posts/HW2_ManiGogula.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c-1",
    "href": "posts/HW2_ManiGogula.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-6",
    "href": "posts/HW2_ManiGogula.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html",
    "href": "posts/HW2_ManiShankerKamarapu.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "href": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "title": "Homework 2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a",
    "href": "posts/HW2_ManiShankerKamarapu.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b",
    "href": "posts/HW2_ManiShankerKamarapu.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c",
    "href": "posts/HW2_ManiShankerKamarapu.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html",
    "href": "posts/HW2_PrahithaMovva.html",
    "title": "Homework 2 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(stats)\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#angiography",
    "href": "posts/HW2_PrahithaMovva.html#angiography",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Angiography",
    "text": "Angiography\n\n\nCode\nsample.mean <- 18\nsample.n <- 847\nsample.sd <- 9\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 17.49078 18.50922\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.018436"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#bypass",
    "href": "posts/HW2_PrahithaMovva.html#bypass",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Bypass",
    "text": "Bypass\n\n\nCode\nsample.mean <- 19\nsample.n <- 539\nsample.sd <- 10\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 18.29029 19.70971\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.419421\n\n\nThe 90% confidence interval for angiography is [17.49, 18.51] wait days (1.02) and for bypass is [18.29, 19.71] wait days (1.42). The confidence interval for angiography is slightly narrower (by 0.4)."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a",
    "href": "posts/HW2_PrahithaMovva.html#a",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\nAssumptions:\n\nThe data is normally distributed\nHo is true\n95% CI\n\n\n\nCode\nt.numerator <- sample.mean - population.mean\nt.denominator <- sample.s/sqrt(sample.n)\nt.statistic <- t.numerator/t.denominator\n\np.value <- pt(q=abs(t.statistic), df=sample.n-1, lower.tail=F)*2\nprint(t.statistic)\n\n\n[1] -3\n\n\nCode\nprint(p.value)\n\n\n[1] 0.01707168\n\n\nThe t statistic is -3 and the p-value at 5% significance level is 0.017. Since the p-value is less than 0.05, it is evidence against Ho, i.e., the mean income of female employees differ significantly from $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b",
    "href": "posts/HW2_PrahithaMovva.html#b",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\np.value_less <- pt(q=t.statistic, df=sample.n-1, lower.tail=T)\nprint(p.value_less)\n\n\n[1] 0.008535841\n\n\nHere too, the p-value at 5% significance level is less than 0.05. So we reject Ho and can say that the mean income of female employees is significantly less than $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c",
    "href": "posts/HW2_PrahithaMovva.html#c",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\np.value_greater <- pt(q=t.statistic, df=sample.n-1, lower.tail=F)\nprint(p.value_greater)\n\n\n[1] 0.9914642\n\n\nHere, the p-value at 5% significance level is higher than 0.05 and we fail to reject Ho. This means, we do not have evidence that the mean income of female employees is more than $500 per week.\n\n\nCode\np.value_greater + p.value_less\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a-1",
    "href": "posts/HW2_PrahithaMovva.html#a-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\njones.t <- ((jones.mean-population.mean)/jones.se)\njones.t\n\n\n[1] 1.95\n\n\nCode\njones.p <- pt(q=abs(jones.t), df=sample.n-1, lower.tail=F)*2\njones.p\n\n\n[1] 0.05145555\n\n\nCode\nsmith.t <- ((smith.mean-population.mean)/smith.se)\nsmith.t\n\n\n[1] 1.97\n\n\nCode\nsmith.p <- pt(q=abs(smith.t), df=sample.n-1, lower.tail=F)*2\nsmith.p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b-1",
    "href": "posts/HW2_PrahithaMovva.html#b-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\nAt 5% significance level, the result for Smith is statistically significant but that of Jones is not"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c-1",
    "href": "posts/HW2_PrahithaMovva.html#c-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\nThis example shows using P > 0.05 or P <= 0.05 to see if we can the reject the null hypothesis or not is misleading, if the actual p-value is not reported. Both the p-values are only 0.1 significance level away from 0.05 but only one is significant, so the experiment might not be meaningful without the actual p-values."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html",
    "href": "posts/HW2_RoyYoon.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Bypass Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Bypass Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nbypass_sample_size <- 539\nbypass_mean <- 19\nbypass_sd <- 10"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Standard Error for Bypass",
    "text": "Standard Error for Bypass\n\n\nCode\nbypass_standard_error <- bypass_sd / sqrt(bypass_sample_size)\n\nbypass_standard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "title": "Homework 2",
    "section": "T-Value Steps for Bypass",
    "text": "T-Value Steps for Bypass\n\n\nCode\nbypass_confidence_level <- 0.90\nbypass_tail_area <- (1 - bypass_confidence_level)/2\n\nbypass_tail_area\n\n\n[1] 0.05\n\n\nCode\nbypass_t_score <- qt(p = 1 - bypass_tail_area, df = bypass_sample_size - 1)\n\nbypass_t_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Bypass Confidence Interval",
    "text": "Calculate Bypass Confidence Interval\n\n\nCode\nbypass_confidence_interval <- c(bypass_mean - (bypass_t_score * bypass_standard_error),\n                                bypass_mean + (bypass_t_score * bypass_standard_error))\n\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Margin of Error for bypass",
    "text": "Margin of Error for bypass\n\n\nCode\nbypass_margin_of_error <- bypass_t_score * bypass_standard_error\n\nbypass_margin_of_error\n\n\n[1] 0.7097107"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Angiography Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Angiography Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nangiography_sample_size <- 847\nangiography_mean <- 18\nangiography_sd <- 9"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "title": "Homework 2",
    "section": "Standard Error for Angiography",
    "text": "Standard Error for Angiography\n\n\nCode\nangiography_standard_error <- angiography_sd / sqrt(angiography_sample_size)\n\nangiography_standard_error\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "title": "Homework 2",
    "section": "T-Value Steps for Angiography",
    "text": "T-Value Steps for Angiography\n\n\nCode\nangiography_confidence_level <- 0.90\nangiography_tail_area <- (1 - angiography_confidence_level)/2\n\nangiography_tail_area\n\n\n[1] 0.05\n\n\nCode\nangiography_t_score <- qt(p = 1 - angiography_tail_area, df = angiography_sample_size - 1)\n\nangiography_t_score\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Angiography Confidence Interval",
    "text": "Calculate Angiography Confidence Interval\n\n\nCode\nangiography_confidence_interval <- c(angiography_mean - (angiography_t_score * angiography_standard_error),\n                                angiography_mean + (angiography_t_score * angiography_standard_error))\n\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Margin of Error for Angiography",
    "text": "Margin of Error for Angiography\n\n\nCode\nangiography_margin_of_error <- angiography_t_score * angiography_standard_error\n\nangiography_margin_of_error\n\n\n[1] 0.5092182"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "href": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "title": "Homework 2",
    "section": "Comparing Bypass and Angiogrpahy Confidence Intervals",
    "text": "Comparing Bypass and Angiogrpahy Confidence Intervals\n\n\nCode\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922\n\n\nCode\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nAngiography has a more narrow confidence interval"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#point-estimate",
    "href": "posts/HW2_RoyYoon.html#point-estimate",
    "title": "Homework 2",
    "section": "Point Estimate",
    "text": "Point Estimate\n\n\nCode\nsample_size <- 1031\n\neducation_essential <- 567\n\npoint_estimate <- education_essential / sample_size\n\npoint_estimate \n\n\n[1] 0.5499515\n\n\nCode\nprop.test(education_essential, sample_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  education_essential out of sample_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate for adult Americans who believe college education is essential is 0.5499515. The 95 percent confidence interval is 0.5189682, 0.5805580. 95% of confidence intervals calculated with this procedure would contain the true mean. With the sampling method repeated, about 95% of the intervals would contain the true mean."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a.",
    "href": "posts/HW2_RoyYoon.html#a.",
    "title": "Homework 2",
    "section": "A.",
    "text": "A.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\n\nData is normally distributed; significance level is 5%\nNull Hypothesis (H0): μ = 500\nAlternative Hypothesis(H1): μ < 500, μ > 500"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-standard-error",
    "href": "posts/HW2_RoyYoon.html#a-standard-error",
    "title": "Homework 2",
    "section": "A: Standard Error",
    "text": "A: Standard Error\n\n\nCode\nfemale_standard_dev <- 90\nfemale_sample_size <- 9\nfemale_sample_mean <- 410 \nnull_hypothesis_mean <- 500\n\nfemale_standard_error <- female_standard_dev / sqrt(female_sample_size)\n\nfemale_standard_error\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-t-test",
    "href": "posts/HW2_RoyYoon.html#a-t-test",
    "title": "Homework 2",
    "section": "A: t-test",
    "text": "A: t-test\n\n\nCode\nt_stat <- (female_sample_mean - null_hypothesis_mean) / female_standard_error\n\nt_stat\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-p-value",
    "href": "posts/HW2_RoyYoon.html#a-p-value",
    "title": "Homework 2",
    "section": "A: p-value",
    "text": "A: p-value\n\n\nCode\np_value <- (pt(t_stat, df = 8)) * 2\n\np_value\n\n\n[1] 0.01707168\n\n\np-value (0.01707168) is smaller than the 5% significance level, so we are able to reject the null hypothesis and favor the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "href": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "title": "Homework 2",
    "section": "B. Report the P-value for Ha : μ < 500. Interpret.",
    "text": "B. Report the P-value for Ha : μ < 500. Interpret.\n\n\nCode\nlow_p_value <- (pt(t_stat, df = 8, lower.tail = TRUE))\nlow_p_value\n\n\n[1] 0.008535841"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "href": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "title": "Homework 2",
    "section": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)",
    "text": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nup_p_value <- (pt(t_stat, df = 8, lower.tail = FALSE))\nup_p_value\n\n\n[1] 0.9914642\n\n\nCode\n#sanity check from hint: The P-values for the two possible one-sided tests must sum to 1\nlow_p_value + up_p_value\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a",
    "href": "posts/HW2_RoyYoon.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#Jones\n\njones_t_stat <- (jones_sample_mean - h0_mean) / jones_standard_error\n\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- (pt(jones_t_stat, df = 999, lower.tail=FALSE)) * 2\n\njones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n\nsmith_t_stat <- (smith_sample_mean - h0_mean) / smith_standard_error\n\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- (pt(smith_t_stat, df = 999, lower.tail=FALSE) ) * 2\n\nsmith_p_value\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b",
    "href": "posts/HW2_RoyYoon.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nif the p value is less than the significance level, the null hypothesis is rejected.\nsignificance level: α = 0.05\nJones p-value: 0.05145555; Jones p-value > significance level, so the study is not statistically significant\nsmith p-value: 0.04911426; Smith p-value < significance level, so the study is statistically significant"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c",
    "href": "posts/HW2_RoyYoon.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIf the p value is less than the significance level, the null hypothesis is rejected and we can consider the finding/result statistically insignificant. So blanketing Jone’s study as not statistically significant and and Smith’s study as significant may make sense. However, when you look at the sample means from Jones and Smith, there is not an outstanding difference between them. By the way the mathematics of calculating the p-value worked out, it can seem as though there is a great difference between the numbers reported by Jones and Smith, when their reported numbers are actually very narrow to each other."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#question-6",
    "href": "posts/HW2_RoyYoon.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nWith a sample mean of 40.86278 and a p-value of 0.03827 which is less than the assumes 0.05 significance level, the null hypothesis is rejected and the alternative hypothesis is favored.\nThus, there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/HW2_Saaradhaa.html",
    "href": "posts/HW2_Saaradhaa.html",
    "title": "Homework 2",
    "section": "",
    "text": "Qn 2\n\nset.seed(0)\nprop <- prop.test(x=567, n=1031)\nprop\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% CI is [0.5189682, 0.580558], which includes the point estimate 0.5499515 and excludes 0.5. Hence, we can reject the null hypothesis that the true probability is 0.5 at the 5% significance level, p = 0.0014898.\n\n\nQn 3\n\n# calculate population SD.\nPSD <- (200-30)/4\n\n# calculate sample size.\nn <- round(((1.96*PSD)/5)^2)\n\nThe minimum required sample size is 278.\n\n\nQn 4a\nAssumptions: H0 is true, observations are independent of one another, y is continuous and sample is approximately normally distributed. H0: μ = 500 Ha: μ ≠ 500\n\n# calculate t-statistic.\nt <- (410-500)/(90/sqrt(9))\n\n# calculate p-value.\np <- 2*pt(q=abs(t), df=8, lower.tail=FALSE)\np\n\n[1] 0.01707168\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8) = 3, p = 0.0170717. Female employees’ mean income significantly differs from $500 per week.\n[I have a question - I am confused on whether I was right to use the absolute value here, and when we should use absolute values.]\n\n\nQn 4b\n\n# calculate p-value.\np2 <- pt(q=t, df=8, lower.tail=TRUE)\np2\n\n[1] 0.008535841\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.0085358. Female employees’ mean income is significantly less than $500 per week.\n\n\nQn 4c\n\n# calculate p-value.\np3 <- pt(q=t, df=8, lower.tail=FALSE)\np3\n\n[1] 0.9914642\n\n\nWe fail to reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.9914642. Female employees’ mean income is not significantly more than $500 per week.\n\n\nQn 5a\n\n# calculate SD for Jones and Smith.\nSD <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_j <- ((519.5-500)/SD) * sqrt(1000)\nt_j\n\n[1] 1.95\n\n# calculate p-value for Jones.\np_j <- 2*(pt(q=t_j, df=999, lower.tail=FALSE))\np_j\n\n[1] 0.05145555\n\n# calculate t for Smith.\nt_s <- ((519.7-500)/SD) * sqrt(1000)\nt_s\n\n[1] 1.97\n\n# calculate p-value for Smith.\np_s <- 2*(pt(q=t_s, df=999, lower.tail=FALSE))\np_s\n\n[1] 0.04911426\n\n\n\n\nQn 5b\nThe result is statistically significant for Smith, but not Jones.\n\n\nQn 5c\nIt is useful to report the exact p-value in cases like this, when the p-value is very close to alpha. It helps the reader to understand (1) why it was/was not rejected, and (2) how much evidence there is against the null hypothesis.\n\n\nQn 6\n\n#create variable.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# do t-test.\ntax <- t.test(gas_taxes, alternative=\"less\",mu=45)\ntax\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% CI is [-, 44.6794598], which includes the estimated mean 40.8627778 and excludes 45. Hence, we can reject the null hypothesis at the 5% significance level, t(17)= -1.8857058, p = 0.0382708. The average tax per gallon in the US in 2005 was significantly less than 45 cents."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html",
    "href": "posts/HW2_ShoshanaBuck.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Tail area and standard error for Bypass",
    "text": "Tail area and standard error for Bypass\n\n\nCode\ntail_area<- (1-.90)/2\ntail_area\n\n\n[1] 0.05\n\n\nCode\nstandard_error<- 10/sqrt(539)\nstandard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "title": "Homework 2",
    "section": "t-value for Bypass",
    "text": "t-value for Bypass\n\n\nCode\nt_score<- qt(p= 1-tail_area, df= 538)\nt_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for Bypass",
    "text": "Confidence interval and margin of error for Bypass\n\n\nCode\nCI<- c(19 - t_score * standard_error, 19 + t_score * standard_error)\nCI\n\n\n[1] 18.29029 19.70971\n\n\nCode\nMOE<- t_score *standard_error\nMOE *1.41\n\n\n[1] 1.000692"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "title": "Homework 2",
    "section": "Standard error of angiography",
    "text": "Standard error of angiography\n\n\nCode\nstandard_error2<- 9/sqrt(847)\nstandard_error2\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "title": "Homework 2",
    "section": "t-score for angiography",
    "text": "t-score for angiography\n\n\nCode\nt_score2<- qt(p= 1-.05, df= 846)\nt_score2\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for angiography",
    "text": "Confidence interval and margin of error for angiography\n\n\nCode\nCI<- c(18 - t_score2 * standard_error2, 18 + t_score2 * standard_error2)\nCI\n\n\n[1] 17.49078 18.50922\n\n\nCode\nMOE2<- t_score2 *standard_error2\nMOE2 *1.01\n\n\n[1] 0.5143103\n\n\nThe Bypass points are [18.29029 & 19.70971] days and has a margin of error of +/- 0.7. Whereas the angiography is [17.49 & 18.50] days with a margin of error of +/- 0.5. Angigography is more narrower because it has a larger sample size and the range between the high and low end of the confidence interval is smaller."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "href": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "title": "Homework 2",
    "section": "Point estimate P",
    "text": "Point estimate P\n\n\nCode\ns_size<- 1031\nb<- 567\n\npoint_estimate<- b/s_size\npoint_estimate\n\n\n[1] 0.5499515"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "title": "Homework 2",
    "section": "95% confidence interval for P",
    "text": "95% confidence interval for P\n\n\nCode\nprop.test(b,s_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  b out of s_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nBased off the point estimate, 54% of the adult Americans that were surveyed by the National Center for Public Policy believe that college education is essential for success. 95% confidence interval of adult Americans who believe that college education is essential for success is [0.5189682 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "href": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "title": "Homework 2",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nCode\nsd<- (200-30)/4\nsd\n\n\n[1] 42.5"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "href": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "title": "Homework 2",
    "section": "Solving for n",
    "text": "Solving for n\n\n\nCode\n#steps for the equation\n#1. 5 = 1.96 * (42.5/sqrt(n))\n\n#2. 5 = 8.3/sqrt(n)\n\n#3. 5*sqrt(n)= 83.3\n\n#4. sqrt(n) = 83.3/5\n\n#5. n= (83.3/5)^2\n\n#6. n= 278.89\n\n\nThe standard deviation from the data is 42.5. Since we have solved for the standard deviation we can plug it into the CI equation and solve for n."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-4",
    "href": "posts/HW2_ShoshanaBuck.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nA\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. ## Assumptions\nWe are assuming there is normal distribution, the null hypothesis is: μ= 500 and the alternative hypothesis is 500> μ <500."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error",
    "title": "Homework 2",
    "section": "Standard error",
    "text": "Standard error\n\n\nCode\ns_sizef<- 9\nsd<-90\ns_meanf<- 410\nnull_hypo_mean<- 500\n\nstandard_errorf<- sd/sqrt(s_sizef)\nstandard_errorf\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score",
    "href": "posts/HW2_ShoshanaBuck.html#t-score",
    "title": "Homework 2",
    "section": "t-score",
    "text": "t-score\n\n\nCode\nt_stat<- (s_meanf-null_hypo_mean)/standard_errorf\nt_stat\n\n\n[1] -3\n\n\nI took the sample mean of 410 subtracted that from the mu = 500 and then divided it by the standard error = 30."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#p-value",
    "href": "posts/HW2_ShoshanaBuck.html#p-value",
    "title": "Homework 2",
    "section": "p-value",
    "text": "p-value\n\n\nCode\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nThe p-value than the 5% significance level so we can reject the null hypothesis in favor of the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#b-c",
    "href": "posts/HW2_ShoshanaBuck.html#b-c",
    "title": "Homework 2",
    "section": "B +C",
    "text": "B +C\nReport the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500.\n\n\nCode\nupper_p_value<- (pt(t_stat, df=8, lower.tail = FALSE))\nupper_p_value\n\n\n[1] 0.9914642\n\n\nCode\nlower_p_value<- (pt(t_stat, df=8, lower.tail = TRUE))\nlower_p_value\n\n\n[1] 0.008535841\n\n\nThe upper-tailed p-value is 0.99 and the lower-tailed p-value is 0.008. If you add the two tails together they will equal 1."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-5",
    "href": "posts/HW2_ShoshanaBuck.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\njones_sample_mean<- 519.5\nsmith_sample_mean<-519.7\nnull_hyp<- 500\njones_se<- 10\nsmith_se<- 10\n\n\n\nA: Jones t-score and p-value\n\n\nCode\njones_t_stat<- (jones_sample_mean-null_hyp)/jones_se\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value<- pt(jones_t_stat, df=999, lower.tail = FALSE) *2\njones_p_value\n\n\n[1] 0.05145555\n\n\n\n\nA: Smith t-score and p-value\n\n\nCode\nsmith_t_stat<-(smith_sample_mean-null_hyp)/smith_se\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value<- pt(smith_t_stat, df=999, lower.tail = FALSE)*2\nsmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\nB\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe results are “statistically significant when the p-value is smaller than the 0.05. Jones p-value is 0.051 which is greater than the 0.05 significance level which means it is not statistically significant and we cannot reject the null hypothesis. Smith’s p-value is 0.49 which is smaller than the significance level which means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\n\nC\n“P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” is a misleading statement without providing the p-values because it makes it seem that there is a drastic difference between Jones and Smith that caused one hypothesis to be statistically significant and the other one not to be. However, when looking at the actual p-value it can be noted that there is a very small difference between the values."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-6",
    "href": "posts/HW2_ShoshanaBuck.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents?\nAt the 95% confidence level the p-value is 0.03 which is less than the 5% significance level. This proves that we can reject the null hypothesis and that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html",
    "href": "posts/HW2_Solutions_OmerYalcin.html",
    "title": "Homework 2",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\nbypass_n = 539\nangio_n = 847\n\nbypass_sample_mean = 19\nangio_sample_mean = 18\n\nbypass_sample_sd = 10\nangio_sample_sd = 9\n\nbypass_se = bypass_sample_sd/sqrt(bypass_n)\nangio_se = angio_sample_sd/sqrt(angio_n)\n\nbypass_me = qt(0.95, df = bypass_n - 1)*bypass_se\nangio_me = qt(0.95, df = angio_n - 1)*angio_se\n\nThe confidence intervals:\n\nprint(bypass_sample_mean + c(-bypass_me, bypass_me))\n\n[1] 18.29029 19.70971\n\nprint(angio_sample_mean + c(-angio_me, angio_me))\n\n[1] 17.49078 18.50922\n\n\nThe size of the confidence intervals, which is twice the margin of error:\n\n2 * bypass_me\n\n[1] 1.419421\n\n2 * angio_me\n\n[1] 1.018436\n\n\nThe confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\none-step solution:\n\nn = 1031\nk = 567\nprop.test(k, n)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  k out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nAlternatively:\n\np_hat <- k/n # point estimate\nse = sqrt((p_hat*(1-p_hat))/n) # standard error\ne = qnorm(0.975)*se # margin of error\np_hat + c(-e, e) # confidence interval \n\n[1] 0.5195839 0.5803191\n\n\nAlternatively, we can use the exact binomial test. In large samples like the one we have, the results should essentially be the same as prop.test().\n\nbinom.test(k, n)\n\n\n    Exact binomial test\n\ndata:  k and n\nnumber of successes = 567, number of trials = 1031, p-value = 0.001478\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5189927 0.5806243\nsample estimates:\nprobability of success \n             0.5499515"
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nrange = 200-30\npopulation_sd = range/4\n\nRemember:\n\\[CI_{95} = \\bar x \\pm z \\frac{s}{\\sqrt n}\\]\n(We can use \\(z\\) because we assume population standard deviation is known.)\nWe want the number \\(n\\) that ensures:\n\\[ z \\frac{s}{\\sqrt n} = 5 \\]\n\\[ zs = 5 \\sqrt n\\]\n\\[ \\frac{zs}{5} = \\sqrt n\\]\n\\[  (\\frac{zs}{5})^2 = n\\]\nIn our case:\n\nz = qnorm(.975)\ns = population_sd\nn = ((z *s) / 5)^2\nprint(n)\n\n[1] 277.5454\n\n\nRounding up, we need a sample of 278."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-4",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nWe can write a function to find the t-statistic, and then do all the tests in a, b, and c using that.\n\\[t = \\frac{\\bar x - \\mu}{s / \\sqrt n}\\]\nwhere \\(\\bar x\\) is them sample mean, \\(\\mu\\) is the hypothesizes population mean, \\(s\\) is the sample standard deviation, and \\(n\\) is the sample size.\nWriting this in R:\n\nget_t_stat <- function(x_bar, mu, sd, n){\n  return((x_bar - mu) / (sd / sqrt(n)))\n}\n\nFind the t-statistic:\n\nt_stat <- get_t_stat(x_bar = 410, mu = 500, sd = 90, n = 9)\n\n\nA\nTwo-tailed test\n\nn = 9\npval_two_tail = 2*pt(t_stat, df = n-1)\npval_two_tail\n\n[1] 0.01707168\n\n\nWe can reject the hypothesis that population mean is 500.\n\n\nB\n\npval_lower_tail = pt(t_stat, df = n-1)\npval_lower_tail\n\n[1] 0.008535841\n\n\nWe can reject the hypothesis that population mean is greater than 500.\n\n\nC\n\npval_upper_tail = pt(t_stat, df = n-1, lower.tail=FALSE)\npval_upper_tail\n\n[1] 0.9914642\n\n\nWe fail to reject the hypothesis that population mean is less than 500.\nAlternatively for C, we could just subtract the answer in B from 1:\n\n1 - pval_lower_tail\n\n[1] 0.9914642"
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-5",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nt_jones = ((519.5 - 500)/ 10)\nt_smith = ((519.7 - 500)/ 10)\ncat(\"t value for Jones:\", t_jones, '\\n')\n\nt value for Jones: 1.95 \n\ncat(\"t value for Smith:\", t_smith, '\\n')\n\nt value for Smith: 1.97 \n\ncat('p value for Jones:', round(2*pt(t_jones, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Jones: 0.0515 \n\ncat('p value for Smith:', round(2*pt(t_smith, df = 999, lower.tail=FALSE), 4), '\\n')\n\np value for Smith: 0.0491 \n\n\nAt 0.05 level Smith’s result is statistically significant but Jones’s is not. The result show the arbitrariness of the 0.05 demarcation line and the importance of reporting actual p-values to better make sense of results."
  },
  {
    "objectID": "posts/HW2_Solutions_OmerYalcin.html#question-6",
    "href": "posts/HW2_Solutions_OmerYalcin.html#question-6",
    "title": "Homework 2",
    "section": "Question 6:",
    "text": "Question 6:\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nIn the one sided test, we are able to reject the null in favor of the alternative that the gas taxes are less than 45 cents.\nNote that a two-sided test at the same level would not have resulted in the rejection of the null.\nHowever, a two-sided 90% confidence interval gives the same upper bound, since now there is a 5% rejection are on two sides:\n\nt.test(gas_taxes, mu = 45, alternative = 'two.sided', conf.level = 0.9)\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.07654\nalternative hypothesis: true mean is not equal to 45\n90 percent confidence interval:\n 37.04610 44.67946\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/HW2_StephRoberts.html",
    "href": "posts/HW2_StephRoberts.html",
    "title": "HW2",
    "section": "",
    "text": "###Homework 2\n##Question 1\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population\nSurgical Procedure Sample Size Mean Wait Time Standard Deviation Bypass 539 19 10 Angiography 847 18 9\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#Assign values\nbypass_ss <- 539\nbypass_mean <- 19\nbypass_sd <- 10\nangio_ss <- 847\nangio_mean <- 18\nangio_sd <- 9\n\n#calculate bypass t-score\nalpha <- 0.1\nt_score_b <- qt(p = 1-alpha/2, df = bypass_ss-1)\nprint(t_score_b)\n\n\n[1] 1.647691\n\n\nThe t-score for bypass wait time is 1.65.\n\n\nCode\n#Calculate 90% confidence interval of bypass wait time\nCI_bypass <- c(bypass_mean - t_score_b * bypass_sd ,\n               bypass_mean + t_score_b * bypass_sd)\nprint(CI_bypass)\n\n\n[1]  2.523092 35.476908\n\n\nThe 90% confidence interval for bypass wait time is 2.53 to 35.48 days.\n\n\nCode\n#calculate angiography t-score\nalpha <- 0.1\nt_score_a <- qt(p = 1-alpha/2, df = angio_ss-1)\nprint(t_score_a)\n\n\n[1] 1.646657\n\n\nThe t-score for bypass wait time is 1.65.\n\n\nCode\n#Calculate 90% confidence interval of bypass wait time\nCI_angio <- c(angio_mean - t_score_a * angio_sd ,\n               angio_mean + t_score_a * angio_sd)\nprint(CI_angio)\n\n\n[1]  3.180089 32.819911\n\n\nThe 90% confidence interval for angiography wait time is 3.18 to 32.82 days.\n##Question 2 A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#Sample size\nn <- 1031\n\n#Number of those who believed that college education is essential for success\nk <- 567\n\n#find sample proportion\np <- k/n\np\n\n\n[1] 0.5499515\n\n\nCode\nprop.test(x=k, n=n, p=p, alternative=\"two.sided\")\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  k out of n, null probability p\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThe proportion of all adult Americans who believe that a college education is essential for success is 0.55.The confidence interval for the proportion is 0.52 to 0.58.\n##Question 3 Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\nThis information gives us a confidence interval of 10, a confidence level of 95%, and an unknown population size.\n\n\nCode\nz <- 1.96\nbook_sd <- (200-30)/4\nmargin <- 5\n\nbook_ss <- ((z*book_sd)/margin)^2\nbook_ss\n\n\n[1] 277.5556\n\n\nThe sample size should be at least 278 students.\n##Question 4 According to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. Hint: The P-values for the two possible one-sided tests must sum to 1.\nThe null hypothesis is that female employees income is $500, H0 : μ = 500. The alternative hypothesis is that female employees income is not, H1 : μ ≠ 500. A second alternative is that income is under $500, H2 : μ < 500. A third alternative is that income is greater than $500, H3 : μ > 500. We assume that the sample is random and that the population has a normal distribution. We will use a p-value of 0.05 and reject the null if it is any less.\n\n\nCode\nsample <- 9\nmean_income <- 410\ns <- 90\nμ <- 500\n\n#Calculate t-score\nt_score_income <- (mean_income-μ)/(s/sqrt(sample))\nt_score_income\n\n\n[1] -3\n\n\n\n\nCode\n#Calculate p-value\nincome_p <- (pt(t_score_income, sample-1))*2\nincome_p\n\n\n[1] 0.01707168\n\n\nWe find a p-value of 0.017. With a p-value of less than 0.05, we reject the null hypothesis. The female employee mean income is not equal to that of all senior-level workers.\n\n\nCode\n#Calculate p-value <500\nincome_p_lower <- pt(t_score_income, sample-1, lower.tail = TRUE)\nincome_p_lower\n\n\n[1] 0.008535841\n\n\nWe find a p-value of 0.009 when assessing the lower limits of the distribution. The smaller number suggests a stronger correlation. Again, we reject the null hypothesis. The mean female income is likely less than $500.\n\n\nCode\n#Calculate p-value >500\nincome_p_upper <- pt(t_score_income, sample-1, lower.tail = FALSE)\nincome_p_upper\n\n\n[1] 0.9914642\n\n\nWe find a p-value of 0.99. Here the p-value is much greater than 0.05, so we fail to reject the null hypothesis.\n##Question 5 Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith. Using α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nNull hypothesis: H0: μ = 500 Alternative hypothesis: Ha: μ ≠ 500 We will reject the null hypothesis at a p-value less than 0.05 We assume that the sample is random and that the population has a normal distribution.\n\n\nCode\njmean <- 519.5\njse <- 10\nss <- 1000\nμ <- 500\nsmean <- 519.7\nsse <- 10\n\n#Calculate t-score for Jones study\nt_score_j <- (jmean-μ)/jse\nt_score_j\n\n\n[1] 1.95\n\n\nCode\n#Calculate p-value for Jones study\njp <- 2*pt(t_score_j, ss-1, lower.tail = FALSE)\njp\n\n\n[1] 0.05145555\n\n\nThe p-value for Jones’s study is 0.051, which exceeds our threshold of 0.05. Therefore, we reject the null.\n\n\nCode\n#Calculate t-score for Smith study\nt_score_s <- (smean-μ)/sse\nt_score_s\n\n\n[1] 1.97\n\n\nCode\n#Calculate p-value for Smith study\nsp <- 2*pt(t_score_s, ss-1, lower.tail = FALSE)\nsp\n\n\n[1] 0.04911426\n\n\nThe p-value for Smith’s study is 0.049, which falls under our threshold of 0.05. Therefore, we fail to reject the null.\nThese studies have nearly identical means and p-values. It illustrates the importance of reporting the actually calculated p-value along with its interpretation. Without the number, we might conclude Jones’s study indicates strong statistical significance while Smith’s did not, when in fact they were almost identical.\n##Question 6 Are the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence level of state and local taxes per gallon is 36.23 to 45.49. We can interpret this by saying that if we took another 100 samples, 95 of them would have a mean that falls within our confidence interval. We cannot conclude, however, that the mean is less than 45 cents per gallon, because that number falls within our 95% confidence interval range."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html",
    "href": "posts/HW2_SteveONeill.html",
    "title": "Homework 2",
    "section": "",
    "text": "“The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (”Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population”\n\n\n\nCode\nbypass <- data.frame(sample_size = 539,\n                     mean_wait_time = 19,\n                     standard_dev = 10)\nbypass \n\n\n  sample_size mean_wait_time standard_dev\n1         539             19           10\n\n\nCode\nangiography <- data.frame(sample_size = 847,\n                     mean_wait_time = 18,\n                     standard_dev = 9)\nangiography\n\n\n  sample_size mean_wait_time standard_dev\n1         847             18            9\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-2",
    "href": "posts/HW2_SteveONeill.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\n\nCode\ncollege_n <- 1031\ncollege_k <- 567\ncollege_p <- college_k/college_n\ncollege_p\n\n\n[1] 0.5499515\n\n\nThe point estimate for the proportion of all adult Americans who believe that a college education is essential for success is simply 0.5499515.\n\n\nCode\ncollege_moe <- qnorm(0.975)*sqrt(college_p*(1-college_p)/college_n)\ncollege_moe\n\n\n[1] 0.03036761\n\n\nCode\ncollege_CI_low <- college_p - college_moe\ncollege_CI_high <- college_p + college_moe\ncollege_CI_low\n\n\n[1] 0.5195839\n\n\nCode\ncollege_CI_high\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the point estimate is [0.5195839, 0.5803191]."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-3",
    "href": "posts/HW2_SteveONeill.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\n\nCode\n#The estimate will be useful if it is within $5 of the true population mean\nbooks_moe <- 5\n#The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200.\nbooks_range <- (200-30)\n#They think that the population standard deviation is about a quarter of this range\nbooks_sd <- books_range / 4\nbooks_sd\n\n\n[1] 42.5\n\n\nA 5% alpha means a 95% confidence level. Conventionally we know the ‘critical value’ for the 95% confidence interval is 1.96\nUnfortunately, I’m missing the equation that lets us find the sample size from these values!"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-4",
    "href": "posts/HW2_SteveONeill.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\n\nCode\n#the mean income for all senior-level workers in a large service company equals $500 per week.\nunion_pop_mean = 500\n#For a random sample of nine female employees,\nunion_sample_size = 9\n# ȳ = $410\nunion_sample_mean = 410\n# and s = 90\nunion_sample_sd = 90\n\n\n\nT-statistic\nAssuming the sample is normally distributed, the formula for a t-statistic is: (sample mean - population mean) / (sample standard deviation / square root of sample size)\n\n\nCode\ntstatistic <- (union_sample_mean - union_pop_mean) / (union_sample_sd / sqrt(union_sample_size))\ntstatistic\n\n\n[1] -3\n\n\nThe t-statistic is -3.\n\n\nHypotheses\nIn this case, the null hypothesis is that the mean income for female employees matches $500/wk. The alternate hypothesis is that it does not equal $500/wk.\npt takes the T-statistic and degrees of freedom and returns the p-value to the left. If we multiply it by two, it becomes ‘two-tailed’:\n\n\nCode\n2 * pt(-3, (union_sample_size - 1))\n\n\n[1] 0.01707168\n\n\nThis two-tailed test returns a p-value of 0.01707168 his is well under the significance level of 5%, meaning that there is only a 1.7 percent chance of receiving as extreme a result as this under the null hypothesis. Therefore the null hypothesis is said to be rejected."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-5",
    "href": "posts/HW2_SteveONeill.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0\n\n\n\nCode\njones_smith_pop_mean = 500\njones_smith_sample_size = 10000\n\njones_sample_mean = 519.5\nsmith_sample_mean = 519.7\n\njones_smith_se = 10\n\n\n\n\nCode\njones_t_statistic <- (jones_sample_mean - jones_smith_pop_mean) / jones_smith_se\njones_t_statistic\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- 2*pt(jones_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\njones_p_value\n\n\n[1] 0.05120403\n\n\nCode\nsmith_t_statistic <- (smith_sample_mean - jones_smith_pop_mean) / jones_smith_se\nsmith_t_statistic\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- 2*pt(smith_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\nsmith_p_value\n\n\n[1] 0.04886592\n\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\n\nUsing an alpha of .05, Jones’ p value is not ‘statistically significant’, but Smith’s is.\nBoth Jones and Smith’s results should be presented with the P-values included so that knowledgeable people can see how borderline they are. Alternately, different significance levels could be adopted - or the sample size could be increased."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-6",
    "href": "posts/HW2_SteveONeill.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\ngas_mean <- gas_taxes %>% mean()\n\n\nError in gas_taxes %>% mean(): could not find function \"%>%\"\n\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\n\nCode\nt.test(gas_taxes, mu = gas_mean, alternative = 'less')\n\n\nError in t.test.default(gas_taxes, mu = gas_mean, alternative = \"less\"): object 'gas_mean' not found\n\n\nWith the information we have, I am not sure if we have enough evidence to conclude about the average tax per gallon in 2005. Otherwise, the p-value is within bounds at .5 and yes, this would be statistically significant."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html",
    "href": "posts/HW2_ToryBartelloni.html",
    "title": "DACSS 603: Homework 2",
    "section": "",
    "text": "Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nlibrary(dplyr)\nsurgery <- data.frame(Procedure=c(\"Bypass\",\"Angiography\"), \n                      Sample_Size=c(539,847),\n                      Mean_Wait_Time=c(19,18),\n                      Standard_Deviation=c(10,9))\n\nbypass <- surgery %>% filter(Procedure == \"Bypass\")\n\nt_score_bypass <- qt(p=1-0.05, df=bypass$Sample_Size-1)\n\nse_bypass <- bypass$Standard_Deviation / sqrt(bypass$Sample_Size)\n\nCI_bypass <- c(bypass$Mean_Wait_Time - t_score_bypass * se_bypass,\n    bypass$Mean_Wait_Time + t_score_bypass * se_bypass)\n\nangio <- surgery %>% filter(Procedure == \"Angiography\")\n\nt_score_angio <- qt(p=1-0.05, df=angio$Sample_Size-1)\n\nse_angio <- angio$Standard_Deviation / sqrt(angio$Sample_Size)\n\nCI_angio <- c(angio$Mean_Wait_Time - t_score_angio * se_angio,\n    angio$Mean_Wait_Time + t_score_angio * se_angio)\n\nCI <- data.frame(Procedure = c(\"Bypass\", \"Angiography\"),\n                 Lower_Limit = c(CI_bypass[1],CI_angio[1]),\n                 Upper_Limit = c(CI_bypass[2],CI_angio[2]))\n\nknitr::kable(CI, caption = \"90% Confidence Levels for Cardiac Procedures\")\n\n\n\n90% Confidence Levels for Cardiac Procedures\n\n\nProcedure\nLower_Limit\nUpper_Limit\n\n\n\n\nBypass\n18.29029\n19.70971\n\n\nAngiography\n17.49078\n18.50922\n\n\n\n\n\nThe confidence interval is more narrow for angiogrphy because the larger sample size reduces t-score, and the larger sample and lower standard deviation together reduce the standard error."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#a.",
    "href": "posts/HW2_ToryBartelloni.html#a.",
    "title": "DACSS 603: Homework 2",
    "section": "A.",
    "text": "A.\nAbove we have shown that the difference in the results leads to the t-statistics and p-values provided."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#b.",
    "href": "posts/HW2_ToryBartelloni.html#b.",
    "title": "DACSS 603: Homework 2",
    "section": "B.",
    "text": "B.\nThe p-values observed show that at a 95% confidence level Jones’ results are non-significant while Smith’s results are significant."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#c.",
    "href": "posts/HW2_ToryBartelloni.html#c.",
    "title": "DACSS 603: Homework 2",
    "section": "C.",
    "text": "C.\nThis is a good example of why not reporting p-values can be insufficient or misleading because the results we observed are extremely close, but the arbitrary boundary we agreed upon prior to the test distinguishes them into different categories. This would not be so important if the difference between those categories were not important. Without the context of the specific results we could see the two extremely similar results treated and acted upon in starkly different ways.\nI would argue this is one good reason why we should be reporting p-values up until .001 so researchers and users of the data can fully understand the context they would be applying the result within. Good to note that reporting extremely small p-values (<.001) has it’s drawbacks as well and we do not want to overemphasize results that may be the result of methodology rather than a real and important distinction for instance."
  },
  {
    "objectID": "posts/hw3.html",
    "href": "posts/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/hw3.html#question-1",
    "href": "posts/hw3.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/hw3.html#a",
    "href": "posts/hw3.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/hw3.html#b",
    "href": "posts/hw3.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first, then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/hw3.html#c",
    "href": "posts/hw3.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/hw3.html#question-2",
    "href": "posts/hw3.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/hw3.html#a-1",
    "href": "posts/hw3.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/hw3.html#b-1",
    "href": "posts/hw3.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change."
  },
  {
    "objectID": "posts/hw3.html#question-3",
    "href": "posts/hw3.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/hw3.html#question-4",
    "href": "posts/hw3.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/hw3.html#question-5",
    "href": "posts/hw3.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/hw3.html#a-2",
    "href": "posts/hw3.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/hw3.html#b-2",
    "href": "posts/hw3.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW3answers-DonnySnyder.html",
    "href": "posts/HW3answers-DonnySnyder.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 1\n\n\nCode\ndata <- UN11\n\nggplot(data, aes(x = ppgdp, y = fertility)) + geom_point()\n\n\n\n\n\nCode\nggplot(data, aes(x = log(ppgdp), y = log(fertility))) + geom_point()\n\n\n\n\n\n#Question 1.1 The predictor is ppgdp and the response is fertility.\n#Question 1.2 A straight-line mean function does not seem to be plausible for this graph.\n#Question 1.3 A simple linear regression model does seem plausible for a summary of the log log graph.\n\n\nCode\ndata$ppgdp2 <- data$ppgdp*0.75\n\nmodel1 <- lm(fertility ~ ppgdp, data)\nsummary(model1)\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nmodel2 <- lm(fertility ~ ppgdp2, data)\nsummary(model2)\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp2      -4.268e-05  6.206e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\ncor(data$fertility, data$ppgdp)\n\n\n[1] -0.4399891\n\n\nCode\ncor(data$fertility, data$ppgdp2)\n\n\n[1] -0.4399891\n\n\n\n\nQuestion 2(a)\nThe slope of the prediction equation will increase, as the units of the explanatory variable are decreased.\n\n\nQuestion 2(b)\nThe correlation will stay the same.\n\n\nQuestion 3\n\n\nCode\nwatData <- water\npairs(watData)\n\n\n\n\n\nCode\nggpairs(watData)\n\n\nError in ggpairs(watData): could not find function \"ggpairs\"\n\n\nOPBPC, OPRC, and OPSLAKE all seem to be highly correlated with BSAAM.\n\n\nQuestion 4\n\n\nCode\nprofData <- Rateprof\nprofData <- data.frame(profData$quality, profData$helpfulness, profData$clarity, profData$easiness, profData$raterInterest)\npairs(profData)\n\n\n\n\n\nIt seems as if quality, helpfulness and clarity are all highly interrelated. easiness and raterInterest are not as highly correlated.\n#Question 5\n\n\nCode\nstud <- as.data.frame(student.survey)\n\n\nError in as.data.frame(student.survey): object 'student.survey' not found\n\n\nCode\nstud$piNum <- NA\n\n\nError in stud$piNum <- NA: object 'stud' not found\n\n\nCode\nstud$reNum <- NA\n\n\nError in stud$reNum <- NA: object 'stud' not found\n\n\nCode\nx = 1\nwhile(x <= 60){\n  if(stud$pi[x] == \"very liberal\"){\n    stud$piNum[x] = -3\n  }\n  if(stud$pi[x] == \"liberal\"){\n    stud$piNum[x] = -2\n  }\n  if(stud$pi[x] == \"slightly liberal\"){\n    stud$piNum[x] = -1\n  }\n  if(stud$pi[x] == \"moderate\"){\n    stud$piNum[x] = 0\n  }\n  if(stud$pi[x] == \"very conservative\"){\n    stud$piNum[x] = 3\n  }\n  if(stud$pi[x] == \"conservative\"){\n    stud$piNum[x] = 2\n  }\n  if(stud$pi[x] == \"slightly liberal\"){\n    stud$piNum[x] = 1\n  }\n  \n  \n  if(stud$re[x] == \"never\"){\n    stud$reNum[x] = 0\n  }\n  if(stud$re[x] == \"occasionally\"){\n    stud$reNum[x] = 1\n  }\n  if(stud$re[x] == \"most weeks\"){\n    stud$reNum[x] = 2\n  }\n  if(stud$re[x] == \"every week\"){\n    stud$reNum[x] = 3\n  }\n  x = x + 1\n}\n\n\nError in eval(expr, envir, enclos): object 'stud' not found\n\n\nCode\nmodel3 <- lm(piNum~reNum, stud)\n\n\nError in is.data.frame(data): object 'stud' not found\n\n\nCode\nsummary(model3)\n\n\nError in summary(model3): object 'model3' not found\n\n\nCode\nmodel4 <- lm(hi~tv, stud)\n\n\nError in is.data.frame(data): object 'stud' not found\n\n\nCode\nsummary(model4)\n\n\nError in summary(model4): object 'model4' not found\n\n\nCode\nggplot(stud, aes(x = reNum, y = piNum)) + geom_jitter()\n\n\nError in ggplot(stud, aes(x = reNum, y = piNum)): object 'stud' not found\n\n\nCode\nggplot(stud, aes(x = tv, y = hi)) + geom_jitter()\n\n\nError in ggplot(stud, aes(x = tv, y = hi)): object 'stud' not found\n\n\nIt seems like the results are that political ideology tends to be more right-leaning as religiosity increases. As hours of tv watching tends to go down, high school GPA tends to go up. These relationships are both statistically significant."
  },
  {
    "objectID": "posts/hw3_boonstra.html",
    "href": "posts/hw3_boonstra.html",
    "title": "Homework 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(alr4)\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nWarning: package 'smss' was built under R version 4.2.2\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw3_boonstra.html#section",
    "href": "posts/hw3_boonstra.html#section",
    "title": "Homework 3",
    "section": "1.1.1",
    "text": "1.1.1\nIn this model, ppgdp would be the predictor variable, and fertility would be the response variable."
  },
  {
    "objectID": "posts/hw3_boonstra.html#section-1",
    "href": "posts/hw3_boonstra.html#section-1",
    "title": "Homework 3",
    "section": "1.1.2",
    "text": "1.1.2\n\nun11 %>% \n  ggplot(aes(x=ppgdp,y=fertility)) +\n  geom_point() +\n  geom_smooth(method=lm,se=F)\n\n\n\n\nA linear OLS regression of a linear-linear model of the data does not seem to fit the data very well. While the regression does capture the general downward trend of the data, it is plain to see in the above visualization that it is not a good fit, particularly for larger values of the response variable (ppgdp)."
  },
  {
    "objectID": "posts/hw3_boonstra.html#section-2",
    "href": "posts/hw3_boonstra.html#section-2",
    "title": "Homework 3",
    "section": "1.1.3",
    "text": "1.1.3\n\nun11 %>% \n  ggplot(aes(x=log(ppgdp),y=log(fertility))) +\n  geom_point() +\n  geom_smooth(method=lm,se=F)\n\n\n\n\nThis log-log model of the data is much better suited to a linear OLS regression fit."
  },
  {
    "objectID": "posts/HW3_CalebHill.html",
    "href": "posts/HW3_CalebHill.html",
    "title": "Homework 3",
    "section": "",
    "text": "First, let’s load the relevant libraries and set all the graph themes to minimal.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\ntheme_minimal()\n\n\nList of 93\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : NULL\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ size         : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : NULL\n $ panel.grid.minor          :List of 6\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.5\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\nFor Question 1, let’s load in the UN11 data-set and view the first few variables to verify we have loaded in the data.\n\n\nCode\ndata(UN11)\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\nFor the sake of the following two subsections, the predictor is “ppgdp” and the response is “fertility.” We are attempting to predict fertility but a country’s PPGDP.\n\n\n\n\n\nCode\nggplot(UN11, aes(ppgdp, fertility)) +\n  geom_point()\n\n\n\n\n\nA straight-line mean function does seem possible if you remove ppgdp that is less than $5,000. Otherwise, there is a sharp concentration of points at this range that might distort a straight-line mean.\n\n\n\n\n\nCode\nggplot(UN11, aes(log(ppgdp), log(fertility))) +\n  geom_point() + \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\nThe simple linear regression would be much more plausible with this graph. We now see a negative association between fertility and ppgdp.The higher the fertility, the lower the ppgdp, and vice-versa."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#a",
    "href": "posts/HW3_CalebHill.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nggplot(UN11, aes(log(ppgdp*1.33), log(fertility))) +\n  geom_point() +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\n\nI don’t think the slope has changed, but we can call a quick lm to see.\n\n\nCode\nlm(log(ppgdp) ~ log(fertility), UN11)\n\n\n\nCall:\nlm(formula = log(ppgdp) ~ log(fertility), data = UN11)\n\nCoefficients:\n   (Intercept)  log(fertility)  \n        10.780          -2.539  \n\n\nCode\nlm(log(ppgdp*1.33) ~ log(fertility), UN11)\n\n\n\nCall:\nlm(formula = log(ppgdp * 1.33) ~ log(fertility), data = UN11)\n\nCoefficients:\n   (Intercept)  log(fertility)  \n        11.065          -2.539  \n\n\nA little for the intercept, but not when mapped onto the outcome variable (ppgdp does not change its effect on fertility when currency changes)."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#b",
    "href": "posts/HW3_CalebHill.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nTherefore, the correlation does not change when adjusted from US dollars to British pounds."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#a-1",
    "href": "posts/HW3_CalebHill.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\ndata(student.survey)\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\nCode\nggplot(student.survey, aes(re, pi)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCode\nggplot(student.survey, aes(tv, hi)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", se=FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nDue to the categorical nature for (i), it is very difficult to draw conclusions from this graph on how the explanatory variable relates to the outcome variable. There is a hint of a positive relationship, but that is just how the graph is coded. If political ideology was flipped (liberal to conservative from top to bottom), we would see an inverse relationship. Conservatives seem more likely to attend every week, while liberals never do.\nFor (ii), we see that as an individual observes more tv per week, the lower the high school GPA is. If we removed the outliers, we might see a regression line where the explanatory variable explains more for the outcome variable, but that is not within the scope of this homework, nor is it immediately a best practice."
  },
  {
    "objectID": "posts/HW3_CalebHill.html#b-1",
    "href": "posts/HW3_CalebHill.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nlm(re ~ pi, student.survey)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\n\nCall:\nlm(formula = re ~ pi, data = student.survey)\n\nCoefficients:\n(Intercept)         pi.L         pi.Q         pi.C         pi^4         pi^5  \n     2.6071       2.0552       0.4501       0.1361      -0.2283      -0.3046  \n       pi^6  \n     0.4565  \n\n\nCode\nlm (tv ~ hi, student.survey)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nCoefficients:\n(Intercept)           hi  \n     20.200       -3.909  \n\n\nThe LM models are a little difficult to interpret, but I shall attempt to explain.\nFor (i), the outcome variable (pi), is explained by the intercept codes. So a change in re (religiosity), accounts for varying differences for pi (political ideology). This can be a massive change, such as 2.05 for L or -0.30 for pi^5. I attempted to switch the variables in the LM model, as I was unsure if this was correct, and the issue persisted due to the categorical nature of the simple linear regression. Either I am still completing this wrong or the model is incorrect for the data provided - better a logistic regression perhaps.\nFor (ii), one hour of TV on average explains a change of -3.91 in high school GPA. This shows a strong negative association between the two variables."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html",
    "href": "posts/HW3_EmmaRasmussen.html",
    "title": "HW3_EmmaRasmussen",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(alr4)\nlibrary(smss)\nlibrary(stargazer)\n\nError in library(stargazer): there is no package called 'stargazer'"
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section",
    "href": "posts/HW3_EmmaRasmussen.html#section",
    "title": "HW3_EmmaRasmussen",
    "section": "1.1.1",
    "text": "1.1.1\nThe predictor is ppgdp, and the response variable is fertility. ## 1.1.2\n\ndata(list=\"UN11\")\n\nplot(x=UN11$ppgdp, y=UN11$fertility)\n\n\n\n\nA straight line function would not be a good model for this graph. It appears that ppgdp has the biggest impact on fertility towards the left side of the graph (closer to x=0). In other words, ppgd has the biggest impact on fertility in lower ppgdp values and then does not change as drastically as ppgdp gets even larger (right side of the graph). ## 1.1.3\n\nplot(x=log(UN11$ppgdp), y=log(UN11$fertility))\n\n\n\n\nA logarithmic function makes a lot more sense for this data frame in order to apply a linear regression model. When both variables are logged, the data appears more linear and has a negative trend."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#a",
    "href": "posts/HW3_EmmaRasmussen.html#a",
    "title": "HW3_EmmaRasmussen",
    "section": "2a",
    "text": "2a\nI created an example data frame to explore this question. According to the output of the lm() function, the slopes are different (the one multiplied by 1.33 has a greater slope). The plots appear to have the same slope, however the y scale is different which likely explains why the lm() function gives different slopes.\n\ndfexample<-data.frame(col1=c(2004, 2005, 2006, 2007, 2008, 2009, 2010),\ncol2=c(50000, 56000, 70000, 68000, 58000, 72000, 80000),\ncol3=col2*1.33)\n\nError in data.frame(col1 = c(2004, 2005, 2006, 2007, 2008, 2009, 2010), : object 'col2' not found\n\ndfexample\n\nError in eval(expr, envir, enclos): object 'dfexample' not found\n\nlm(col2~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nlm(col3~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nfit2<-lm(col2~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nfit3<-lm(col3~ col1, data=dfexample)\n\nError in is.data.frame(data): object 'dfexample' not found\n\nsummary(fit2)\n\nError in summary(fit2): object 'fit2' not found\n\nsummary(fit3)\n\nError in summary(fit3): object 'fit3' not found\n\nplot(x=dfexample$col1, y=dfexample$col2)\n\nError in plot(x = dfexample$col1, y = dfexample$col2): object 'dfexample' not found\n\nplot(x=dfexample$col1, y=dfexample$col3)\n\nError in plot(x = dfexample$col1, y = dfexample$col3): object 'dfexample' not found"
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#b",
    "href": "posts/HW3_EmmaRasmussen.html#b",
    "title": "HW3_EmmaRasmussen",
    "section": "2b",
    "text": "2b\nThe correlation (adjusted R squared) is the same for both models. See above."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section-1",
    "href": "posts/HW3_EmmaRasmussen.html#section-1",
    "title": "HW3_EmmaRasmussen",
    "section": "3",
    "text": "3\n\ndata(list=\"water\")\npairs(water[2:8])\n\n\n\n\nI don’t know if I am interpreting this correctly but using this matrix we can see which site correlates most closely with stream runoff (BSAAM). Using this matrix, we see there is a strong correlation between OPSLAKE, OPRC and OPBPC site precipitation and runoff. Perhaps precipitation measured at these sites could predict runoff. Moving forward, I might fit a models using those three sites to predict runoff at the site near bishop and figure out which model creates the best prediction (has the highest F statistic)."
  },
  {
    "objectID": "posts/HW3_EmmaRasmussen.html#section-2",
    "href": "posts/HW3_EmmaRasmussen.html#section-2",
    "title": "HW3_EmmaRasmussen",
    "section": "4",
    "text": "4\n\ndata(list=\"Rateprof\")\npairs(Rateprof[8:12])\n\n\n\n\nThere is a strong positive correlation between quality and helpfulness, quality and clarity, and clarity and helpfulness. In other words, professors that rate high in one of these areas are likely to rate high in the others. easiness is less strongy correlated with quality, helpfulness and clarity, but there is still a positive relationship (i.e. professors with “easy” courses are more likely to rate higher in other categories but this trend is less strong). Finally, raterInterest does not predict the other ratings very well. Easiness does not appears to have much of a correlation with rater interest. There is a positive relationship between rater interest and quality, helpfulness, and clarity, but again it is not a strong relationship.\n##5a\n\ndata(list=\"student.survey\")\nggplot(student.survey, aes(x=re, y=pi)) +geom_point()\n\n\n\nggplot(student.survey, aes(x= tv, y=hi))+geom_point()+geom_smooth(method=\"lm\")\n\n\n\n\nPolitical Affiliation and Religiosity: This graph is not super useful given there are multiple observations contained in each point on the graph but even so, it appears that more frequently attending religious services correlates with more conservative political ideology.\nTV and GPA: There appears to be a negative correlation between time spent watching tv and high school gpa.\n##5b\n\nstudent.survey\n\n   subj ge ag  hi  co   dh    dr   tv sp ne ah    ve pa                    pi\n1     1  m 32 2.2 3.5    0  5.00  3.0  5  0  0 FALSE  r          conservative\n2     2  f 23 2.1 3.5 1200  0.30 15.0  7  5  6 FALSE  d               liberal\n3     3  f 27 3.3 3.0 1300  1.50  0.0  4  3  0 FALSE  d               liberal\n4     4  f 35 3.5 3.2 1500  8.00  5.0  5  6  3 FALSE  i              moderate\n5     5  m 23 3.1 3.5 1600 10.00  6.0  6  3  0 FALSE  i          very liberal\n6     6  m 39 3.5 3.5  350  3.00  4.0  5  7  0 FALSE  d               liberal\n7     7  m 24 3.6 3.7    0  0.20  5.0 12  4  2 FALSE  i               liberal\n8     8  f 31 3.0 3.0 5000  1.50  5.0  3  3  1 FALSE  i               liberal\n9     9  m 34 3.0 3.0 5000  2.00  7.0  5  3  0 FALSE  i          very liberal\n10   10  m 28 4.0 3.1  900  2.00  1.0  1  2  1 FALSE  i      slightly liberal\n11   11  m 23 2.3 2.6  253  1.50 10.0 15  1  1 FALSE  r slightly conservative\n12   12  f 27 3.5 3.6  190  3.00 14.0  3  7  0 FALSE  d               liberal\n13   13  m 36 3.3 3.5  245  1.50  6.0 15 12  5 FALSE  d          very liberal\n14   14  m 28 3.2 3.2  500  6.00  3.0 10  1  2 FALSE  i              moderate\n15   15  f 28 3.0 3.5 3500  1.00  4.0  3  1  0 FALSE  d          very liberal\n16   16  f 25 3.8 3.3  210 10.00  7.0  6  1  0 FALSE  i               liberal\n17   17  f 41 4.0 3.0 1000 15.00  6.0  7  3 10 FALSE  i      slightly liberal\n18   18  m 50 3.8 3.8    0  3.00  5.0  9  6 10 FALSE  d               liberal\n19   19  m 71 4.0 3.5 5000  3.00  6.0 12  2  2 FALSE  i               liberal\n20   20  f 28 3.0 3.8  120  1.00 25.0  0  0  2 FALSE  d          very liberal\n21   21  f 26 3.7 3.7 8000  8.00  4.0  4  4  1 FALSE  i              moderate\n22   22  f 27 4.0 3.7    2  2.50  4.0  2  7  0 FALSE  i               liberal\n23   23  m 31 2.7 3.5 1700  5.00  7.0  7  2  0 FALSE  r     very conservative\n24   24  f 23 3.7 3.7    2  2.00  7.0  4  2  0 FALSE  i              moderate\n25   25  m 23 3.2 3.8  450  4.00  0.0  7  7  3 FALSE  i          very liberal\n26   26  f 44 3.0 3.0    0  2.00  2.0  3  2  3 FALSE  i      slightly liberal\n27   27  m 26 3.7 3.0 1000  3.00  8.0  2  7  0 FALSE  d               liberal\n28   28  f 31 3.7 3.8  850 10.00 10.0  3  7  0 FALSE  r slightly conservative\n29   29  m 24 3.3 3.1  420  2.00 10.0  6  5  0 FALSE  d              moderate\n30   30  f 26 3.3 3.3 1200  0.75 10.0  0  3  0 FALSE  r               liberal\n31   31  m 26 3.3 3.5 1000  1.50  0.0  3  3  3 FALSE  d               liberal\n32   32  f 32 3.5 3.9  150 12.00 10.0  2  0  0 FALSE  d               liberal\n33   33  m 26 3.4 3.4 2000  1.50  2.0  7 14  0 FALSE  d               liberal\n34   34  f 22 3.2 2.8  316  2.00 10.0  3  5  2 FALSE  i               liberal\n35   35  f 24 3.5 3.9  900  1.75  8.0  0  0  1 FALSE  d          very liberal\n36   36  m 24 3.6 3.3  250  2.00  4.0  6  3  1 FALSE  r slightly conservative\n37   37  m 23 3.8 3.7  180  0.50 10.0  5  7  0 FALSE  i               liberal\n38   38  m 33 3.4 3.4 6000  1.50  8.0  5  6  2 FALSE  i               liberal\n39   39  m 23 2.8 3.2  950  2.00 37.0 10  5  0 FALSE  r slightly conservative\n40   40  m 31 3.8 3.5 1100  0.75  0.5  3  5  2 FALSE  r          conservative\n41   41  m 26 3.4 3.4 1300  1.20  0.0  8  2  0 FALSE  i               liberal\n42   42  m 28 2.0 3.0  360  0.25 10.0  8  3  0 FALSE  d      slightly liberal\n43   43  f 24 3.8 3.9 1800  2.00  2.0  5  4  1 FALSE  r          conservative\n44   44  m 23 3.0 3.6  900 15.00 12.0  0  5  0 FALSE  r slightly conservative\n45   45  f 25 3.0 4.0 5000  5.00  1.5  0  4  0 FALSE  i              moderate\n46   46  f 24 3.0 3.5  300  1.00 10.0  5  5  0 FALSE  d               liberal\n47   47  f 27 3.0 3.8 2000 20.00 28.0  7 14  2 FALSE  r      slightly liberal\n48   48  m 24 3.3 3.8  630  1.30  2.0  3  5  0 FALSE  r     very conservative\n49   49  f 26 3.8 4.0 1200  1.00  0.0  4  3  1 FALSE  d               liberal\n50   50  f 27 3.0 4.0  580  2.00  5.0 15  1  2 FALSE  d          very liberal\n51   51  m 32 3.0 3.0 2000  5.00  5.0  5  2  1 FALSE  r slightly conservative\n52   52  f 41 4.0 4.0    0  8.00  8.0  4  2  2 FALSE  r              moderate\n53   53  f 29 3.0 3.9  300  3.70  2.0  5  1 11 FALSE  d               liberal\n54   54  f 50 3.5 3.8    6  6.00  7.0  3  7  0 FALSE  d               liberal\n55   55  f 22 3.4 3.7   80  7.00 10.0  1  2  2 FALSE  i               liberal\n56   56  f 23 3.6 3.2  375  1.50  5.0 10  5  0 FALSE  r          conservative\n57   57  m 26 3.5 3.6 2000  0.30 16.0  8  3  0 FALSE  d              moderate\n58   58  m 30 3.0 3.0    1  1.10  1.0  4  3  0 FALSE  i      slightly liberal\n59   59  f 23 3.0 3.0  112  0.50 15.0  3  3  0 FALSE  i              moderate\n60   60  f 22 3.4 3.0  650  4.00  8.0 16  7  1 FALSE  i              moderate\n             re    ab    aa    ld\n1    most weeks FALSE FALSE FALSE\n2  occasionally FALSE FALSE    NA\n3    most weeks FALSE FALSE    NA\n4  occasionally FALSE FALSE FALSE\n5         never FALSE FALSE FALSE\n6  occasionally FALSE FALSE    NA\n7  occasionally FALSE FALSE FALSE\n8  occasionally FALSE FALSE FALSE\n9  occasionally FALSE FALSE    NA\n10        never FALSE FALSE FALSE\n11 occasionally FALSE FALSE FALSE\n12 occasionally FALSE FALSE    NA\n13 occasionally FALSE FALSE FALSE\n14 occasionally FALSE FALSE FALSE\n15        never FALSE FALSE FALSE\n16   every week FALSE FALSE FALSE\n17   every week FALSE    NA FALSE\n18        never FALSE FALSE FALSE\n19        never FALSE FALSE FALSE\n20 occasionally FALSE FALSE FALSE\n21 occasionally FALSE FALSE FALSE\n22 occasionally FALSE FALSE FALSE\n23   every week FALSE FALSE FALSE\n24        never FALSE FALSE FALSE\n25        never FALSE FALSE FALSE\n26   most weeks FALSE FALSE FALSE\n27 occasionally FALSE FALSE    NA\n28   most weeks FALSE FALSE FALSE\n29 occasionally FALSE FALSE    NA\n30 occasionally FALSE FALSE    NA\n31 occasionally FALSE FALSE FALSE\n32 occasionally FALSE FALSE FALSE\n33        never FALSE FALSE FALSE\n34 occasionally FALSE FALSE    NA\n35 occasionally FALSE FALSE    NA\n36   every week FALSE FALSE FALSE\n37        never FALSE FALSE    NA\n38        never FALSE FALSE FALSE\n39   most weeks FALSE FALSE FALSE\n40   most weeks FALSE FALSE    NA\n41 occasionally FALSE FALSE FALSE\n42        never FALSE FALSE    NA\n43   every week FALSE FALSE FALSE\n44        never FALSE FALSE FALSE\n45 occasionally FALSE FALSE FALSE\n46        never FALSE FALSE FALSE\n47 occasionally FALSE FALSE FALSE\n48   every week FALSE FALSE FALSE\n49        never FALSE FALSE FALSE\n50 occasionally FALSE FALSE FALSE\n51   every week FALSE FALSE FALSE\n52 occasionally FALSE FALSE FALSE\n53 occasionally FALSE FALSE FALSE\n54 occasionally FALSE FALSE    NA\n55        never FALSE FALSE    NA\n56   every week FALSE FALSE FALSE\n57 occasionally FALSE FALSE    NA\n58   every week FALSE FALSE FALSE\n59   most weeks FALSE FALSE FALSE\n60 occasionally FALSE FALSE FALSE\n\nlm(pi ~ re, data=student.survey)\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nCoefficients:\n(Intercept)         re.L         re.Q         re.C  \n     3.5253       2.1864       0.1049      -0.6958  \n\nlm(formula= hi ~ tv, data=student.survey)\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nCoefficients:\n(Intercept)           tv  \n    3.44135     -0.01831  \n\n\nPolitical Affiliation and Religiosity: I have no idea what re.Q re.L and re.C is. Both categorical variables take on more than three possible values so I am guessing I have the wrong code.\nTV and GPA: For every +1 hr spend watching tv per week, gpa decreases by 0.018. For a student that watches no tv in the week, their predicted gpa is 3.44."
  },
  {
    "objectID": "posts/HW3_EthanCampbell.html",
    "href": "posts/HW3_EthanCampbell.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n(a)\nHow, if at all, does the slope of the prediction equation change?\nYes, the slope will change by 1.33 since this is rate at which it is changing and the conversion between the two values. The US version will increase by 1.33 times compared to the British version.\n\n\n(b)\nHow, if at all, does the correlation change?\nThe correlation should not change as the scale changes in relation to the amount.\n\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\ndata(water)\n\nhead(water)\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n# here is the pairs of each variable and we notice some correlation however, it is really hard to get a closer look here. \npairs(water,\n      bg = 'blue')\n\n\n\n# creating the regression to look closer \nbsaam_water <- lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\n\n# summary\nsummary(bsaam_water)\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\n\nSolution\nHere we see that there are two variables that have a statistically significant relationship with BSAAM. These are OPRC and OPSLAKE each less than .05 however, questions regarding multicollinarity arise with this strong corrleation. The other variables such as the AP variables seem to also be correlated however, it does not appear as strongly as the two before. When looking at the range of residuals we notice a very large difference and this indicates that there may be some large and small outliers. This will effect the bests fitted line and lead to less robust analysis. When we are looking at the R^2 we notice it is very high which means it is fairly well fitted as it is close to 1.00.\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\ndata(Rateprof)\nhead(Rateprof)\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\nRates <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\n\npairs(Rates)\n\n\n\n\n\nSolution\nHere we see a vary of different strengths of correlations these will be discussed one by one. These range from very weak correlation to very strong correlation which is interesting to see. These are all linear and positive.\nQuality ~ Helpfulness - Here we notice a strong correlation as the data is very linear. This data is also showing a very strong correlation in a positive direction.\nQuality ~ Clarity- Here we notice a strong correlation that is positive and linear.\nQuality ~ Easiness- Here we notice a weaker correlation compared to the last too but still a positive linear correlation.\nQuality ~ RaterInterest- Here we notice a weak correlation but still a positive linear correlation.\nEasiness ~ RaterInterst - Shows a very flat line which shows a very weak correlation.\nClarity ~ Easiness- Shows a weak linear correlation.\n\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\ndata(student.survey)\nSS <- student.survey\nhead(SS)\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n# getting the data that we will be working with and checking to see which each variable means\n#?student.survey\n\nSS <- SS %>%\n  select(hi, tv, pi, re)\n\n\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n# plotting the comparison between political ideology and religiosity\nSS_plot <- plot(pi~re, data = SS)\n\n\n\n# plotting the comparison between high school gpa and hours of tv watching\n\nggplot(data = SS, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n# making a table comparing these variabels\nxtabs(~pi+re, SS)\n\n                       re\npi                      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nsummary(SS)\n\n       hi              tv                             pi                re    \n Min.   :2.000   Min.   : 0.000   very liberal         : 8   never       :15  \n 1st Qu.:3.000   1st Qu.: 3.000   liberal              :24   occasionally:29  \n Median :3.350   Median : 6.000   slightly liberal     : 6   most weeks  : 7  \n Mean   :3.308   Mean   : 7.267   moderate             :10   every week  : 9  \n 3rd Qu.:3.625   3rd Qu.:10.000   slightly conservative: 6                    \n Max.   :4.000   Max.   :37.000   conservative         : 4                    \n                                  very conservative    : 2                    \n\n\n\n\n(b)\nSummarize and interpret results of inferential analyses.\nHere we notice that the more conservative you are the more likely you are to visit religious service on more occasions. However, this is not a very significant trend and it is hard to say with the graph alone. When looking into the xtabs comparing these two results we notice a similar shift there tends to be a higher number of liberals in the never and occasionally sections and then as it gets into most weeks and every week that number drops off quickly and slowly increases on the conservative side.\nWe are also notice a very clear correlation with the hours of watching tv and the highschool gpa. Here we notice that the more hours spent watching tv the less likelu you are to have a higher gpa. There is a negative slope as it drops off between the 10-15 hour mark."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html",
    "href": "posts/HW3_KarenKimble.html",
    "title": "DACSS 603 HW 3",
    "section": "",
    "text": "Code\n# Setup\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-1",
    "href": "posts/HW3_KarenKimble.html#question-1",
    "title": "DACSS 603 HW 3",
    "section": "Question 1",
    "text": "Question 1\n\nPart 1\nThe predictor is the ppgdp, the gross national product per person in U.S. dollars, and the response is fertility, the birth rate per 1000 females.\n\n\nPart 2\n\n\nCode\ndata(UN11)\noptions(scipen = 999)\nplot(fertility ~ ppgdp, data = UN11, main=\"Fertility by GDP per Capita\", xlab = \"GDP per Capita\", ylab = \"Fertility Rate per 1000 Females\")\n\n\n\n\n\nA straight-line function does not seem applicable to this graph because the trend of the data appears closer to a quadratic or exponential function. There are very many high values of fertility rate on the very low end of the x axis, but then this sharply changes between 10,000 and 20,000.\n\n\nPart 3\n\n\nCode\nplot(log(fertility) ~ log(ppgdp), data = UN11, main=\"Log of Fertility by Log of GDP per Capita\", xlab = \"Log of GDP per Capita\", ylab = \"Log of Fertility Rate per 1000 Females\")\n\n\n\n\n\nUsing the logs of each variable results in a different graph where a straight line would fit better than in the previous graph. A simple linear regression model seems plausible here."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-2",
    "href": "posts/HW3_KarenKimble.html#question-2",
    "title": "DACSS 603 HW 3",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\nThe slope of the prediction equation might decrease because the British pound is worth more U.S. dollars, so the slope would be increasing by the same number of units (for both 1.33 U.S. dollars and 1 British pound) but over a larger span of the x-axis.\n\n\nPart B\nThe correlation would not change because the relationship between annual income and the dependent/response variable is the same regardless of units of measurement."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-3",
    "href": "posts/HW3_KarenKimble.html#question-3",
    "title": "DACSS 603 HW 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\npairs(water, upper.panel = NULL)\n\n\n\n\n\nIt looks like there has been a lot more runoff at the OPRC, OPSLAKE, BSAAM, and APSLAkE in recent years. Runoff in the earliest years seemed to me more random and spread out."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-4",
    "href": "posts/HW3_KarenKimble.html#question-4",
    "title": "DACSS 603 HW 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\npairs(Rateprof[,8:12], upper.panel = NULL)\n\n\n\n\n\nIt looks like as the ratings of professors’ quality of teaching increased, helpfulness and clarity ratings also increased. However, easiness and interest ratings don’t seem to be as correalted with these variables."
  },
  {
    "objectID": "posts/HW3_KarenKimble.html#question-5",
    "href": "posts/HW3_KarenKimble.html#question-5",
    "title": "DACSS 603 HW 3",
    "section": "Question 5",
    "text": "Question 5\n\nPolitical Ideology vs Religiosity\n\nPart A\n\n\nCode\ndata(student.survey)\nstudent.survey$pi <- unclass(student.survey$pi)\nstudent.survey$re <- unclass(student.survey$re)\n\n# In the political ideology variable, very conservative = 7 and very liberal = 1\n\n# In the religiosity variable, attending religious services every week = 4, and never = 1\n\nplot(pi ~ re, data = student.survey, main = \"Political Ideology by Religiosity\", xlab = \"Religiousity\", ylab = \"Political Ideology\")\n\n\n\n\n\n\n\nPart B\n\n\nCode\nfit1 <- lm(pi ~ re, data = student.survey)\nsummary(fit1)\n\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189     0.0327 *  \nre            0.9704     0.1792   5.416 0.00000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 0.000001221\n\n\nFor the variables political ideology and religiosity, there seems to be a correlation between them. The p-value for religiosity is much smaller than the alpha value of 0.05, indicating that there is statistically significant evidence showing religiosity affects students’ political ideology. The two are positively related, meaning that, within the regression model, for an increase in religiosity (attending more religious services) by 1, there is a 0.97 increase in political ideology (more conservative or less liberal).\n\n\n\nHigh School GPA versus TV Watching\n\nPart A\n\n\nCode\nplot(hi ~ tv, data = student.survey, main = \"High School GPA by Hours of TV Watched\", xlab = \"Avg Hours/Week Watching TV\", ylab = \"High School GPA\")\n\n\n\n\n\n\n\nPart B\n\n\nCode\nfit2 <- lm(hi ~ tv, data = student.survey)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323 <0.0000000000000002 ***\ntv          -0.018305   0.008658  -2.114              0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThere also seems to be a correlation between the variables high school GPA and hours of TV watched per week. The p-value for hours of TV is less than the alpha value of 0.05, indicating that there is statistically significant evidence showing TV watching affects students’ high school GPA. The variables are negatively related: for every hour of TV watched per week (on average), there is a 0.018 decrease in high school GPA based on the regression model."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html",
    "href": "posts/HW3_ManiShankerKamarapu.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a",
    "href": "posts/HW3_ManiShankerKamarapu.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b",
    "href": "posts/HW3_ManiShankerKamarapu.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first, then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#c",
    "href": "posts/HW3_ManiShankerKamarapu.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW3_ManiShankerKamarapu.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-4",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#question-5",
    "href": "posts/HW3_ManiShankerKamarapu.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#a-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/HW3_ManiShankerKamarapu.html#b-2",
    "href": "posts/HW3_ManiShankerKamarapu.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html",
    "href": "posts/HW3_PrahithaMovva.html",
    "title": "Homework 3 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(stats)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-1",
    "href": "posts/HW3_PrahithaMovva.html#question-1",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 1",
    "text": "Question 1\n\n1.1.1\nThe predictor is ppgdp and the response is fertility because we are studying the dependence of fertility on ppgdp.\n\n\n1.1.2\n\n\nCode\nggplot(data=UN11, aes(x=ppgdp, y=fertility))+geom_point()\n\n\n\n\n\nThere are very few observations with gross national product per person between 20,000 and 100,000 when compared to those below 20,000 (in addition to the huge dip in fertility). Since the above graph does not seem to exhibit a linear relationship, it is not plausible for it to have a straight-line mean function.\n\n\n1.1.3\n\n\nCode\nggplot(data=UN11, aes(x=log(ppgdp), y=log(fertility)))+geom_point()\n\n\n\n\n\nWe can see that as log(ppgdp) increases, log(fertility) decreases linearly. So, we can say that the relationship between these two variables is linear and a simple linear regression model would be plausible."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-2",
    "href": "posts/HW3_PrahithaMovva.html#question-2",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 2",
    "text": "Question 2\n\na\nThe slope gets divided by 1.33 due to the conversion from US Dollar to British Pound. So the slope of the British prediction equation will be ~25% less than the US prediction equation. We can also observe this from the below graph, where the British equation is represented in red and the US in blue.\n\n\nCode\nUN11$british.ppgdp <- UN11$ppgdp/1.33\n\nggplot() +\n  geom_smooth(data=UN11, aes(x = british.ppgdp, y = fertility), \n              method = \"lm\", se = FALSE, color = \"red\") + \n  geom_smooth(data=UN11, aes(x = ppgdp, y = fertility), \n              method = \"lm\", se = FALSE, color = \"blue\") + \n  geom_point(data=UN11, aes(x = british.ppgdp, y = fertility), color = \"red\") + \n  geom_point(data=UN11, aes(x = ppgdp, y = fertility), color = \"blue\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nb\nThe correlation between the explanatory variable and the response will not change as both will increase 1.33x\n\n\nCode\nUN11$british.ppgdp <- UN11$ppgdp/1.33\n\ncor(UN11$british.ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-3",
    "href": "posts/HW3_PrahithaMovva.html#question-3",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the matrix, we can see that the first three locations - APMAM, APSAB and APSLAKE - are strongly correlated with each other. Similarly, the next three location - OPBPC, OPRC and OPSLAKE - are also correlated with each other. However, the first three locations do not share a strong correlation with the response - BSAAM, whereas the next three locations do. This implies that using one of the last three locations will give a better fit/ predictions for the stream runoff volume. On close inspection, we can further say that OPSLAKE might be a better choice among the last three locations to predict BSAAM.\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPBPC, data=water))$adj.r.squared\n\n\n[1] 0.7792942\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPRC, data=water))$adj.r.squared\n\n\n[1] 0.8419507\n\n\nCode\nsummary(lm(water$BSAAM ~ water$OPSLAKE, data=water))$adj.r.squared\n\n\n[1] 0.8777515"
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-4",
    "href": "posts/HW3_PrahithaMovva.html#question-4",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\nrates <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rates)\n\n\n\n\n\nAll of them have a positive correlation but with different magnitudes. Quality, helpfulness and clarity seem to have a linear relationship with one another with a strong positive correlation. Quality, with easiness and rater interest has a positive correlation but is weak (weaker for rater interest). The plot for easiness and rater interest seems flat amongst all, implying almost no (very weak) correlation."
  },
  {
    "objectID": "posts/HW3_PrahithaMovva.html#question-5",
    "href": "posts/HW3_PrahithaMovva.html#question-5",
    "title": "Homework 3 - Prahitha Movva",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\npi.clean <- as.numeric(student.survey$pi)\nre.clean <- as.numeric(student.survey$re)\nmodel.i <- lm(pi.clean ~ re.clean, data=student.survey)\nsummary(model.i)\n\n\n\nCall:\nlm(formula = pi.clean ~ re.clean, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre.clean      0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nmodel.ii <- lm(hi ~ tv, data = student.survey)\nsummary(model.ii)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\na\n\n\nCode\nggplot(data=student.survey, aes(x=re, y=pi)) +\n  geom_jitter(data=student.survey, aes(x=re, y=pi), color=\"blue\") +\n  geom_abline(intercept=0.9308, slope=0.9704) +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFrom the above visualization, we can also say that as religiosity increases, political ideology becomes more conservative.\n\n\nCode\nggplot(data=student.survey, aes(x=tv, y=hi)) +\n  geom_point(data=student.survey, aes(x=tv, y=hi), color=\"blue\") +\n  geom_abline(intercept=3.441353, slope=-0.018305) +\n  geom_smooth(method='lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFrom the above visualization, we can say that as the number of hours of TV watching increases, the high school GPA of the student decreases.\n\n\nb\nIn (i), ~33.6% of the variance in political ideology is explained by religiosity and with 1 unit increase in religiosity, political ideology increases approximately by 0.97 units. We can also see that the p-value for (i) is way below the significance threshold of 0.05 and is therefore statistically significant. Similarly, in (ii), only ~7.2% of the variance in high school GPA is explained by the hours of TV watching. As expected, the p-value for (ii) is less than the usual 0.05 significance threshold but is close to it so it does not exhibit strong statistical significance."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html",
    "href": "posts/HW3_RoyYoon.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section",
    "href": "posts/HW3_RoyYoon.html#section",
    "title": "Homework 3",
    "section": "1.1.1",
    "text": "1.1.1\nIdentify the predictor and the response.\nPredictor: gross national product per person in U.S. dollars (ppdgp)\nResponse: fertility\n\n\nCode\nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section-1",
    "href": "posts/HW3_RoyYoon.html#section-1",
    "title": "Homework 3",
    "section": "1.1.2",
    "text": "1.1.2\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nfertiity_ppgddp <- UN11 %>%\n  select(fertility,ppgdp )\n\nfertiity_ppgddp\n\n\n                                 fertility    ppgdp\nAfghanistan                       5.968000    499.0\nAlbania                           1.525000   3677.2\nAlgeria                           2.142000   4473.0\nAngola                            5.135000   4321.9\nAnguilla                          2.000000  13750.1\nArgentina                         2.172000   9162.1\nArmenia                           1.735000   3030.7\nAruba                             1.671000  22851.5\nAustralia                         1.949000  57118.9\nAustria                           1.346000  45158.8\nAzerbaijan                        2.148000   5637.6\nBahamas                           1.877000  22461.6\nBahrain                           2.430000  18184.1\nBangladesh                        2.157000    670.4\nBarbados                          1.575000  14497.3\nBelarus                           1.479000   5702.0\nBelgium                           1.835000  43814.8\nBelize                            2.679000   4495.8\nBenin                             5.078000    741.1\nBermuda                           1.760000  92624.7\nBhutan                            2.258000   2047.2\nBolivia                           3.229000   1977.9\nBosnia and Herzegovina            1.134000   4477.7\nBotswana                          2.617000   7402.9\nBrazil                            1.800000  10715.6\nBrunei Darussalam                 1.984000  32647.6\nBulgaria                          1.546000   6365.1\nBurkina Faso                      5.750000    519.7\nBurundi                           4.051000    176.6\nCambodia                          2.422000    797.2\nCameroon                          4.287000   1206.6\nCanada                            1.691000  46360.9\nCape Verde                        2.279000   3244.0\nCayman Islands                    1.600000  57047.9\nCentral African Republic          4.423000    450.8\nChad                              5.737000    727.4\nChile                             1.832000  11887.7\nChina                             1.559000   4354.0\nColombia                          2.293000   6222.8\nComoros                           4.742000    736.6\nCongo                             4.442000   2665.1\nCook Islands                      2.530806  12212.1\nCosta Rica                        1.812000   7703.8\nCote dIvoire                      4.224000   1154.1\nCroatia                           1.501000  13819.5\nCuba                              1.451000   5704.4\nCyprus                            1.458000  28364.3\nCzech Republic                    1.501000  18838.8\nDemocratic Republic of the Congo  5.485000    200.6\nDenmark                           1.885000  55830.2\nDjibouti                          3.589000   1282.6\nDominica                          3.000000   7020.8\nDominican Republic                2.490000   5195.4\nEast Timor                        5.918000    706.1\nEcuador                           2.393000   4072.6\nEgypt                             2.636000   2653.7\nEl Salvador                       2.171000   3425.6\nEquatorial Guinea                 4.980000  16852.4\nEritrea                           4.243000    429.1\nEstonia                           1.702000  14135.4\nEthiopia                          3.848000    324.6\nFiji                              2.602000   3545.7\nFinland                           1.875000  44501.7\nFrance                            1.987000  39545.9\nFrench Polynesia                  2.033000  24669.0\nGabon                             3.195000  12468.8\nGambia                            4.689000    579.1\nGeorgia                           1.528000   2680.3\nGermany                           1.457000  39857.1\nGhana                             3.988000   1333.2\nGreece                            1.540000  26503.8\nGreenland                         2.217000  35292.7\nGrenada                           2.171000   7429.0\nGuatemala                         3.840000   2882.3\nGuinea                            5.032000    427.5\nGuinea-Bissau                     4.877000    539.4\nGuyana                            2.190000   2996.0\nHaiti                             3.159000    612.7\nHonduras                          2.996000   2026.2\nHong Kong                         1.137000  31823.7\nHungary                           1.430000  12884.0\nIceland                           2.098000  39278.0\nIndia                             2.538000   1406.4\nIndonesia                         2.055000   2949.3\nIran                              1.587000   5227.1\nIraq                              4.535000    888.5\nIreland                           2.097000  46220.3\nIsrael                            2.909000  29311.6\nItaly                             1.476000  33877.1\nJamaica                           2.262000   4899.0\nJapan                             1.418000  43140.9\nJordan                            2.889000   4445.3\nKazakhstan                        2.481000   9166.7\nKenya                             4.623000    801.8\nKiribati                          3.500000   1468.2\nKuwait                            2.251000  45430.4\nKyrgyzstan                        2.621000    865.4\nLaos                              2.543000   1047.6\nLatvia                            1.506000  10663.0\nLebanon                           1.764000   9283.7\nLesotho                           3.051000    980.7\nLiberia                           5.038000    218.6\nLibya                             2.410000  11320.8\nLithuania                         1.495000  10975.5\nLuxembourg                        1.683000 105095.4\nMacao                             1.163000  49990.2\nMadagascar                        4.493000    421.9\nMalawi                            5.968000    357.4\nMalaysia                          2.572000   8372.8\nMaldives                          1.668000   4684.5\nMali                              6.117000    598.8\nMalta                             1.284000  19599.2\nMarshall Islands                  4.384466   3069.4\nMauritania                        4.361000   1131.1\nMauritius                         1.590000   7488.3\nMexico                            2.227000   9100.7\nMicronesia                        3.307000   2678.2\nMoldova                           1.450000   1625.8\nMongolia                          2.446000   2246.7\nMontenegro                        1.630000   6509.8\nMorocco                           2.183000   2865.0\nMozambique                        4.713000    407.5\nMyanmar                           1.939000    876.2\nNamibia                           3.055000   5124.7\nNauru                             3.300000   6190.1\nNepal                             2.587000    534.7\nNeth Antilles                     1.900000  20321.1\nNetherlands                       1.794000  46909.7\nNew Caledonia                     2.091000  35319.5\nNew Zealand                       2.135000  32372.1\nNicaragua                         2.500000   1131.9\nNiger                             6.925000    357.7\nNigeria                           5.431000   1239.8\nNorth Korea                       1.988000    504.0\nNorway                            1.948000  84588.7\nOman                              2.146000  20791.0\nPakistan                          3.201000   1003.2\nPalau                             2.000000  10821.8\nPalestinian Territory             4.270000   1819.5\nPanama                            2.409000   7614.0\nPapua New Guinea                  3.799000   1428.4\nParaguay                          2.858000   2771.1\nPeru                              2.410000   5410.7\nPhilippines                       3.050000   2140.1\nPoland                            1.415000  12263.2\nPortugal                          1.312000  21437.6\nPuerto Rico                       1.757000  26461.0\nQatar                             2.204000  72397.9\nRepublic of Korea                 1.389000  21052.2\nRomania                           1.428000   7522.4\nRussian Federation                1.529000  10351.4\nRwanda                            5.282000    532.3\nSaint Lucia                       1.907000   6677.1\nSamoa                             3.763000   3343.3\nSao Tome and Principe             3.488000   1283.3\nSaudi Arabia                      2.639000  15835.9\nSenegal                           4.605000   1032.7\nSerbia                            1.562000   5123.2\nSeychelles                        2.340000  11450.6\nSierra Leone                      4.728000    351.7\nSingapore                         1.367000  43783.1\nSlovakia                          1.372000  15976.0\nSlovenia                          1.477000  23109.8\nSolomon Islands                   4.041000   1193.5\nSomalia                           6.283000    114.8\nSouth Africa                      2.383000   7254.8\nSpain                             1.504000  30542.8\nSri Lanka                         2.235000   2375.3\nSt Vincent and Grenadines         1.995000   6171.7\nSudan                             4.225000   1824.9\nSuriname                          2.266000   7018.0\nSwaziland                         3.174000   3311.2\nSweden                            1.925000  48906.2\nSwitzerland                       1.536000  68880.2\nSyria                             2.772000   2931.5\nTajikistan                        3.162000    816.0\nTanzania                          5.499000    516.0\nTFYR Macedonia                    1.397000   4434.5\nThailand                          1.528000   4612.8\nTogo                              3.864000    524.6\nTonga                             3.783000   3543.1\nTrinidad and Tobago               1.632000  15205.1\nTunisia                           1.909000   4222.1\nTurkey                            2.022000  10095.1\nTurkmenistan                      2.316000   4587.5\nTuvalu                            3.700000   3187.2\nUganda                            5.901000    509.0\nUkraine                           1.483000   3035.0\nUnited Arab Emirates              1.707000  39624.7\nUnited Kingdom                    1.867000  36326.8\nUnited States                     2.077000  46545.9\nUruguay                           2.043000  11952.4\nUzbekistan                        2.264000   1427.3\nVanuatu                           3.750000   2963.5\nVenezuela                         2.391000  13502.7\nViet Nam                          1.750000   1182.7\nYemen                             4.938000   1437.2\nZambia                            6.300000   1237.8\nZimbabwe                          3.109000    573.1\n\n\n\n\nCode\nfertility_ppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= ppgdp, y = fertility)) + geom_point()\n\nfertility_ppgdp_plot\n\n\n\n\n\nA straight line mean function does not seem plausible for a summary of this graph as a there does not seem to be a plausible predictive straight line that would be able to describe the tendencies in the scatter plot. Rather the shape seems to be asymptotic."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#section-2",
    "href": "posts/HW3_RoyYoon.html#section-2",
    "title": "Homework 3",
    "section": "1.1.3",
    "text": "1.1.3\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nfertility_logppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= log(ppgdp), y = fertility)) + geom_point()\n\nfertility_logppgdp_plot\n\n\n\n\n\nA simple linear regression model seems plausible for a summary of this graph as a there seems to be a plausible predictive straight line(negative slope) that would be able to describe the tendencies in the scatter plot of log(ppgdp)\n\n\nCode\nlogfertility_ppgdp_plot <- ggplot(data = fertiity_ppgddp, aes(x= ppgdp, y = log(fertility))) + geom_point()\n\nlogfertility_ppgdp_plot\n\n\n\n\n\nA simple linear regression model does not seem plausible for a summary of this graph as a there does not seem to be a plausible predictive straight line that would be able to describe the tendencies in the scatter plot of log(fertility)"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#a",
    "href": "posts/HW3_RoyYoon.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nHow, if at all, does the slope of the prediction equation change?\nsuppose there is 10, 20, and 30 British pounds sterling and 13.3, 26.6, and 39.9 USD.\nThe difference between the 10, 20, and 30 British pounds sterling is 10 as the values increment.\nThe difference between 13.3, 26.6, and 39.9 USD is 13.3 as the values increment.\n13.3 is a greater rate of change than 10. Thus looking at the slope as the rate of change, there is a difference in the rate of change."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#b",
    "href": "posts/HW3_RoyYoon.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nHow, if at all, does the correlation change?\nCorrelation is standardized version of the slope. The unit of measurement does not affect the correlation value.\nThere will be no change to the correlation. The correlation observes how close the dad points are to the line of the data. If all points are converted to the same scale, then how close the points remain to the line should remain constant."
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#a-1",
    "href": "posts/HW3_RoyYoon.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\n(i) y = political ideology and x = religiosity\n\n\nCode\nstudent.survey\n\n\nError in eval(expr, envir, enclos): object 'student.survey' not found\n\n\nCode\na <- ggplot(data = student.survey, aes(x= as.numeric(pi), y = as.numeric(re))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(pi), y = as.numeric(re))): object 'student.survey' not found\n\n\nCode\na\n\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\n\n\n(ii) y = high school GPA and x = hours of TV watching\n\n\nCode\nb <- ggplot(data = student.survey, aes(x= as.numeric(tv), y = as.numeric(hi))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(tv), y = as.numeric(hi))): object 'student.survey' not found\n\n\nCode\nb\n\n\nError in eval(expr, envir, enclos): object 'b' not found"
  },
  {
    "objectID": "posts/HW3_RoyYoon.html#b-1",
    "href": "posts/HW3_RoyYoon.html#b-1",
    "title": "Homework 3",
    "section": "B",
    "text": "B\nSummarize and interpret results of inferential analyses.\n\ny = political ideology and x = religiosity\ny = high school GPA and x = hours of TV watching\n\nthe relationship between the average number of hours per week that one watches tv and the high school gpa on a four point square is negative. The more hours of tvs one watches per week, the lower the gpa is.\n\n\nCode\nsummary(lm(hi ~ tv, data = student.survey))\n\n\nError in is.data.frame(data): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW3_Saaradhaa.html",
    "href": "posts/HW3_Saaradhaa.html",
    "title": "Homework 3",
    "section": "",
    "text": "Qn 1.1.2\n\n# load libraries.\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(alr4)\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\n# load dataset.\ndata(UN11)\n\n# draw scatterplot.\nscatterplot(fertility ~ ppgdp, UN11)\n\n\n\n\nNo, the graph seems curvilinear.\n\n\nQn 1.1.3\n\n# draw scatterplot.\nscatterplot (log(fertility) ~ log(ppgdp), UN11)\n\n\n\n\nYes, the simple linear regression model now seems plausible.\n\n\nQn 2a\nWe can test this using the UN11 dataset since ppgdp is in US dollars.\n\n# create new variable.\nUN11$british <- 1.33*UN11$ppgdp\n\n# check slope.\nsummary(lm(fertility ~ british, UN11))\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, although adjusted R^2 has not.\n\n\nQn 2b\nWe can test this too.\n\n# correlation with US dollars.\ncor(UN11$ppgdp, UN11$fertility)\n\n[1] -0.4399891\n\n# correlation with British pounds.\ncor(UN11$british, UN11$fertility)\n\n[1] -0.4399891\n\n\nSince we multiplied by a constant, the correlation remains the same.\n\n\nQn 3\n\n# load dataset.\ndata(water)\n\n# generate scatterplots.\npairs(water)\n\n\n\n\nStream runoff (BSAAM) seems to have a positive linear relationship with precipitation at OPSLAKE, OPRC and OPBPC; but not with precipitation at APMAM, APSAB or APSLAKE. Stream runoff also seems to be fairly constant (?) over the years.\n\n\nQn 4\n\n# load dataset.\ndata(Rateprof)\n\n# create subset.\nrateprof <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# generate scatterplots.\npairs(rateprof)\n\n\n\n\nQuality, helpfulness and clarity have the clearest linear relationships with one another. Easiness and raterInterest do not seem to have linear relationships with the other variables.\n\n\nQn 5a\n\n# load dataset.\ndata(student.survey)\nglimpse(student.survey)\n\nRows: 60\nColumns: 18\n$ subj <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ ge   <fct> m, f, f, f, m, m, m, f, m, m, m, f, m, m, f, f, f, m, m, f, f, f,…\n$ ag   <int> 32, 23, 27, 35, 23, 39, 24, 31, 34, 28, 23, 27, 36, 28, 28, 25, 4…\n$ hi   <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.5, 3.3, …\n$ co   <dbl> 3.5, 3.5, 3.0, 3.2, 3.5, 3.5, 3.7, 3.0, 3.0, 3.1, 2.6, 3.6, 3.5, …\n$ dh   <int> 0, 1200, 1300, 1500, 1600, 350, 0, 5000, 5000, 900, 253, 190, 245…\n$ dr   <dbl> 5.0, 0.3, 1.5, 8.0, 10.0, 3.0, 0.2, 1.5, 2.0, 2.0, 1.5, 3.0, 1.5,…\n$ tv   <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, 6, 25, 4…\n$ sp   <int> 5, 7, 4, 5, 6, 5, 12, 3, 5, 1, 15, 3, 15, 10, 3, 6, 7, 9, 12, 0, …\n$ ne   <int> 0, 5, 3, 6, 3, 7, 4, 3, 3, 2, 1, 7, 12, 1, 1, 1, 3, 6, 2, 0, 4, 7…\n$ ah   <int> 0, 6, 0, 3, 0, 0, 2, 1, 0, 1, 1, 0, 5, 2, 0, 0, 10, 10, 2, 2, 1, …\n$ ve   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ pa   <fct> r, d, d, i, i, d, i, i, i, i, r, d, d, i, d, i, i, d, i, d, i, i,…\n$ pi   <ord> conservative, liberal, liberal, moderate, very liberal, liberal, …\n$ re   <ord> most weeks, occasionally, most weeks, occasionally, never, occasi…\n$ ab   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ aa   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ld   <lgl> FALSE, NA, NA, FALSE, FALSE, NA, FALSE, FALSE, NA, FALSE, FALSE, …\n\n# generate plots.\nboxplot(pi ~ re, student.survey)\n\n\n\nscatterplot(hi ~ tv, student.survey)\n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\nHigh school GPA and TV-watching seem to have a negative relationship.\n\n\n\nQn 5b\n\n# change pi to numeric variable.\nstudent.survey$pi <- as.numeric(student.survey$pi)\n\n# removing ordering in re and rename it.\nlevels(student.survey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudent.survey$re <- factor(student.survey$re, ordered = FALSE)\n\n# run regression models.\nsummary(lm(pi ~ re, student.survey))\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\nsummary(lm(hi ~ tv, student.survey))\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\nThose who attended religious services most weeks/every week were significantly more likely to be conservative than those who never did, p < .001. There was no significant difference in political ideology between those who occasionally attended religious services and those who never did.\nWatching less hours of TV per week was associated with higher high-school GPAs, p < .05. That being said, as the R2 is fairly low, hours of TV watching is not a great predictor of high school GPA."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html",
    "href": "posts/HW3_ShoshanaBuck.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-1",
    "href": "posts/HW3_ShoshanaBuck.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\n\n1.1.1\nThe predictor is the ppgdp and the response is fertility.\n\n\n1.1.2\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nfertility_ppgdp<- ggplot(data = UN11, aes(x=ppgdp, y= fertility)) + geom_point() + labs(title = \"Fertility vs. ppgdp,United Nations (2011)\")\nfertility_ppgdp\n\n\n\n\n\nThe scatterplot shows the relationship between fertility and the ppgdp. There is a strong correlation between fertility and ppgdp when fertility is high, however, the more ppgdp increases fertility decreases. As a result, a straight-line mean function doesn’t seem plausible for a summary of this graph.\n\n\n1.1.3\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nlog_ppgdp<- ggplot(data = UN11, aes(x = log(ppgdp), y= fertility)) + geom_point() + labs(title = \"Natural Log of fertility vs. ppgdp, United Nations (2011)\")\nlog_ppgdp\n\n\n\n\n\nNow using the log() function with ppgdp the relationship between ppgdp vs. fertility seems to look more like a straight line, making plausible to use a straight-line mean function to summarize the graph.\n\n\nCode\nlog_fertility<- ggplot(data = UN11, aes(x =ppgdp, y= log(fertility))) + geom_point() + labs(title = \"Natural Log of fertility vs. ppgdp, United Nations (2011)\")\nlog_fertility\n\n\n\n\n\nUsing the log() function for fertility shows a similar relationship to the first graph, not making it plausible to use a straight-line mean function to summarize the graph."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-2",
    "href": "posts/HW3_ShoshanaBuck.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nA\nHow, if at all, does the slope of the prediction equation change?\nThe slope of the prediction equation would increase by 1.33 because that is the conversion rate between British pounds sterling and US dollars.\n\n\nB\nHow, if at all, does the correlation change?\nNo, the correlation does not change because it is the standardized version of the slope and the unit of measurement does not affect the slope."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-3",
    "href": "posts/HW3_ShoshanaBuck.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\nwater\n\n\n   Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1  1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2  1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3  1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4  1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5  1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6  1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n7  1954  5.02  1.45    1.77 13.57 12.45   13.32  65356\n8  1955  6.70  7.44    6.51  9.28  9.65    9.80  67909\n9  1956 10.50  5.85    3.38 21.20 18.55   17.42  92715\n10 1957  9.10  6.13    4.08  9.55  9.20    8.25  70024\n11 1958  8.75  5.23    5.90 15.25 14.80   17.48  99216\n12 1959  8.10  3.77    4.56  9.05  6.85    9.56  55786\n13 1960  3.75  1.47    1.78  4.57  6.10    7.65  46153\n14 1961 10.15  5.09    4.86  8.90  7.15    9.00  47947\n15 1962  6.15  3.52    3.30 16.90 14.75   17.68  76877\n16 1963 12.75  8.17   10.16 16.75 11.55   15.53  88443\n17 1964  7.35  4.33    4.85  5.25  7.45    8.20  54634\n18 1965 11.25  6.56    7.60  8.40 13.20   13.29  78806\n19 1966  4.05  1.90    2.00 10.85  8.25   12.56  56542\n20 1967 12.65  6.62    7.14 23.25 17.00   23.66 116244\n21 1968  4.65  3.84    3.34  7.10  6.80    8.28  60857\n22 1969  5.35  3.62    4.62 43.37 24.85   33.07 146345\n23 1970  4.05  1.98    2.94  8.95 11.25   11.00  73726\n24 1971  5.90  5.72    5.42  8.45 10.90   10.82  65530\n25 1972  9.45  4.82    6.79  7.90  7.60    8.06  60772\n26 1973  3.45  2.63    2.88 14.80 14.70   15.86  91696\n27 1974  4.25  2.54    2.36 18.05 16.90   16.42  87377\n28 1975  7.90  4.42    6.78 11.50  9.55   12.56  77306\n29 1976  9.38  8.30    9.70  6.80  5.25    4.73  44756\n30 1977  7.08  4.40    3.90  4.05  4.35    4.60  41785\n31 1978 11.92  5.78    6.70 25.30 20.55   21.94 112653\n32 1979  3.88  2.26    3.10 15.97 11.83   13.88  79975\n33 1980  5.80  3.10    3.34 24.40 19.15   23.78 106821\n34 1981  2.70  2.22    2.48  8.99  9.45   12.14  69177\n35 1982 18.08 11.96   13.02 18.55 18.40   19.45 120463\n36 1983  8.20  4.98    5.76 19.25 22.90   23.86 135043\n37 1984  7.65  5.30    5.74 14.45 13.15   14.42 102001\n38 1985  5.22  4.42    4.04 11.45 10.16   13.06  77790\n39 1986  4.93  3.26    4.58 26.47 15.33   26.46 118144\n40 1987  5.99  2.76    3.98  4.80  6.85    6.36  61229\n41 1988  6.83  6.82    5.18  7.20  9.01    9.88  58942\n42 1989  8.80  5.06    4.92  8.05  9.60    9.58  53965\n43 1990  7.10  5.06    6.05  5.80  6.50    8.41  49774\n\n\n\n\nCode\npairs(water)\n\n\n\n\n\nThere seems to be a positive correlation between the stream runoff (BSAAM) and water availability in three of the perceptitaion measurement sites (OPSLAKE, OPRC, and OPBPC). The other three precipitation measurement sites- (APSLAKE, APSAB, and APMAM ) show a distribution of values without having a positive or negative correlation between the stream runoff and water availability as the graph ."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-4",
    "href": "posts/HW3_ShoshanaBuck.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\nRateprof\n\n\n    gender numYears numRaters numCourses pepper discipline\n1     male        7        11          5     no        Hum\n2     male        6        11          5     no        Hum\n3     male       10        43          2     no        Hum\n4     male       11        24          5     no        Hum\n5     male       11        19          7     no        Hum\n6     male       10        15          9     no        Hum\n7     male        7        17          3     no        Hum\n8     male       11        16          3     no        Hum\n9     male       11        12          4     no        Hum\n10    male        7        18          4     no        Hum\n11    male       11        11          4     no        Hum\n12    male        6        33          4    yes        Hum\n13    male        4        23          5     no        Hum\n14    male       10        34          2    yes        Hum\n15    male       11        23          8    yes        Hum\n16    male        3        27          5    yes        Hum\n17    male        1        14          2    yes        Hum\n18    male       11        19          7     no        Hum\n19    male        6        11          5     no        Hum\n20    male       11        14          5     no        Hum\n21    male        7        59          9     no        Hum\n22    male       10        28          5     no        Hum\n23    male        1        13          3     no        Hum\n24    male       10        15          5     no        Hum\n25    male        7        16          2     no        Hum\n26    male       11        35          6     no        Hum\n27    male        7        11          2     no        Hum\n28    male       11        23         10     no        Hum\n29    male       11        27          6     no        Hum\n30    male       11        42          5    yes        Hum\n31    male       11        38          4     no        Hum\n32    male       11        20          8     no        Hum\n33    male       11        44          6     no        Hum\n34    male       11        57          3    yes        Hum\n35    male       10        15          3     no        Hum\n36    male        9        46          5     no        Hum\n37    male       11        10          4     no        Hum\n38    male        3        18          2     no        Hum\n39    male       11        79          7     no        Hum\n40    male       11        18          2     no        Hum\n41    male        3        26          5     no        Hum\n42    male       11        52          2     no        Hum\n43    male       11        10          2     no        Hum\n44    male        7        26          5     no        Hum\n45    male       10        15          3     no        Hum\n46    male       11        45          8     no        Hum\n47    male        3        12          3    yes        Hum\n48    male        7        12          2     no        Hum\n49    male       11        16          3     no        Hum\n50    male        9        36          8     no        Hum\n51    male       11        24          5     no        Hum\n52    male        3        29          4     no        Hum\n53    male       11        65          4     no        Hum\n54    male       11        11          4     no        Hum\n55    male        2        14          4    yes        Hum\n56    male       11        14          7     no        Hum\n57    male       11        29          6     no        Hum\n58    male       11        54          6    yes        Hum\n59    male        4        12          3     no        Hum\n60    male       11        57          5     no        Hum\n61    male        9        13          6     no        Hum\n62    male        5        33          3     no        Hum\n63    male        8        46          7     no        Hum\n64    male       11        19          4     no        Hum\n65  female       11        18          6    yes        Hum\n66  female        9        28          3     no        Hum\n67  female       11        21          7     no        Hum\n68  female        8        26          8    yes        Hum\n69  female       11        10          2     no        Hum\n70  female        2        14          4     no        Hum\n71  female        7        27          6     no        Hum\n72  female       10        22          3     no        Hum\n73  female        4        36          6     no        Hum\n74  female        2        14          4    yes        Hum\n75  female       11        19          6     no        Hum\n76  female       11        86          8     no        Hum\n77  female        8        25          8     no        Hum\n78  female        8        26          7    yes        Hum\n79  female       10        16          2     no        Hum\n80  female        9        20          3     no        Hum\n81  female       11        30          4     no        Hum\n82  female        7        12          3     no        Hum\n83  female       10        21          3     no        Hum\n84  female        1        19          4    yes        Hum\n85  female       11        12          3    yes        Hum\n86  female        4        31          3    yes        Hum\n87  female        4        22          2    yes        Hum\n88  female        4        13          5     no        Hum\n89  female       11        22          5     no        Hum\n90  female       11        26          8     no        Hum\n91  female       11        15          3     no        Hum\n92  female        2        20          5     no        Hum\n93  female       11        24          7     no        Hum\n94  female       11        69          3     no        Hum\n95  female       10        37          6     no        Hum\n96  female        9        36          7     no        Hum\n97  female        7        32          3     no        Hum\n98  female        2        11          3     no        Hum\n99  female       11        41          2     no        Hum\n100 female        7        34          7     no        Hum\n101 female        5        15          4     no        Hum\n102 female       11        38          3     no        Hum\n103 female        2        10          4    yes        Hum\n104 female       11        29          5     no        Hum\n105 female        1        10          4    yes        Hum\n106 female       11        27          5     no        Hum\n107 female       11        36          4     no        Hum\n108 female        9        19          3     no        Hum\n109 female       11        14          2     no        Hum\n110 female        8        17          5     no        Hum\n111 female       10        54          3     no        Hum\n112 female       11        21          5     no        Hum\n113 female        5        33          6     no        Hum\n114 female       11        47          8     no        Hum\n115 female       11        58          5     no        Hum\n116 female        7        14          4    yes        Hum\n117 female       11        10          5     no        Hum\n118 female        9        36          6     no        Hum\n119 female        7        14          5    yes        Hum\n120 female       11        26          5     no        Hum\n121 female        8        39          5     no        Hum\n122 female       11        16          6     no        Hum\n123 female        5        67          3     no        Hum\n124 female        9        29          4     no        Hum\n125 female       11        26          6     no        Hum\n126 female        6        24          7     no        Hum\n127 female        7        16          5    yes        Hum\n128 female        5        20          2     no        Hum\n129 female       10        35          6     no        Hum\n130 female        5        36          9     no        Hum\n131 female        4        28          3     no        Hum\n132 female       11        10          4     no        Hum\n133 female        9        53          7     no        Hum\n134 female        2        13          2     no        Hum\n135   male        7        10          2     no     SocSci\n136   male        7        21          2     no     SocSci\n137   male        2        33          2     no     SocSci\n138   male        7        11          2     no     SocSci\n139   male        6        18          3     no     SocSci\n140   male       11        30          2     no     SocSci\n141   male        7        14          2     no     SocSci\n142   male        4        15          5    yes     SocSci\n143   male       11        50          4     no     SocSci\n144   male        3        10          3     no     SocSci\n145   male       11        30          5     no     SocSci\n146   male       11        53          3     no     SocSci\n147   male       11        46          4    yes     SocSci\n148   male       11        30          5     no     SocSci\n149   male       11        42          7     no     SocSci\n150   male       11        74          8     no     SocSci\n151   male       10        51          4     no     SocSci\n152   male       11        31          4     no     SocSci\n153   male       11        19          2     no     SocSci\n154   male       11        56          4     no     SocSci\n155   male       11        72          7     no     SocSci\n156   male        8        67          4     no     SocSci\n157   male       11        85          7     no     SocSci\n158   male        9        14          1     no     SocSci\n159   male        9        47          7     no     SocSci\n160   male       10        69          2     no     SocSci\n161   male        2        11          2     no     SocSci\n162   male       11        56          6     no     SocSci\n163   male       10        14          5     no     SocSci\n164   male        7        23          7     no     SocSci\n165   male        6        36          5     no     SocSci\n166   male       11        44          8     no     SocSci\n167 female        6        19          4     no     SocSci\n168 female        4        11          5     no     SocSci\n169 female       11        10          1    yes     SocSci\n170 female       10        56          2     no     SocSci\n171 female       11        85          3     no     SocSci\n172 female        2        16          1    yes     SocSci\n173 female        7        39          5    yes     SocSci\n174 female       11        31          4     no     SocSci\n175 female        8        24          3     no     SocSci\n176 female       11        67          3     no     SocSci\n177 female        2        15          3     no     SocSci\n178 female        6        46          3     no     SocSci\n179 female        9        13          6     no     SocSci\n180 female        4        27          3     no     SocSci\n181 female       11        35          3     no     SocSci\n182 female        1        32          4     no     SocSci\n183 female        3        22          2    yes     SocSci\n184 female       11        51          3     no     SocSci\n185 female        8        11          2     no     SocSci\n186 female       10        52          3     no     SocSci\n187 female       10        62          5     no     SocSci\n188 female        3        13          1     no     SocSci\n189 female        2        20          2     no     SocSci\n190 female       11        32          4     no     SocSci\n191 female        5        23          3     no     SocSci\n192 female       11        62          1     no     SocSci\n193 female        5        42          6    yes     SocSci\n194 female       11        24          2     no     SocSci\n195 female        5        26          6     no     SocSci\n196 female        3        12          4    yes     SocSci\n197 female       11        32          4     no     SocSci\n198 female        9        50          5     no     SocSci\n199 female       11        57          4     no     SocSci\n200 female       11        15          4     no     SocSci\n201   male        8        32          4    yes       STEM\n202   male       11        17          9     no       STEM\n203   male       11        23          5     no       STEM\n204   male       11        13          4     no       STEM\n205   male        1        13          2     no       STEM\n206   male        6        18          7     no       STEM\n207   male        1        11          3    yes       STEM\n208   male       10        26          4     no       STEM\n209   male        6        15          4     no       STEM\n210   male       11        25          2     no       STEM\n211   male       11        21          3     no       STEM\n212   male       11        43          3     no       STEM\n213   male       11        51          6     no       STEM\n214   male       11        11          2     no       STEM\n215   male       11        27          2     no       STEM\n216   male       11        37          5     no       STEM\n217   male       11        35          3     no       STEM\n218   male       11        29          2     no       STEM\n219   male        4        30          2     no       STEM\n220   male       11        28          6    yes       STEM\n221   male       11        33          3     no       STEM\n222   male       11        28          6     no       STEM\n223   male       11        62          5     no       STEM\n224   male        5        38          7     no       STEM\n225   male       11        14          1     no       STEM\n226   male        5        25          3     no       STEM\n227   male       11        53          8     no       STEM\n228   male        7        20          3     no       STEM\n229   male       11        21         10     no       STEM\n230   male        6        40          6     no       STEM\n231   male       11        13          3     no       STEM\n232   male       11        67          4     no       STEM\n233   male       11        19          5     no       STEM\n234   male       10        17          2     no       STEM\n235   male       11        22          7     no       STEM\n236   male       11        16          3     no       STEM\n237   male        6        35          6     no       STEM\n238   male        8        38          5     no       STEM\n239   male       11        67          6     no       STEM\n240   male       11        35          4     no       STEM\n241   male       11        32          5     no       STEM\n242   male       11        58          4     no       STEM\n243   male       11        21          3     no       STEM\n244   male       10        12          4     no       STEM\n245   male       11        45          7     no       STEM\n246   male        3        14          2     no       STEM\n247   male        9        41          5     no       STEM\n248   male        1        10          3     no       STEM\n249   male       11        30          3     no       STEM\n250   male       11        25         12     no       STEM\n251   male       11        37          7     no       STEM\n252   male       11        23          4     no       STEM\n253   male       11        65          3     no       STEM\n254   male       11        54          7     no       STEM\n255   male        8        41          5     no       STEM\n256   male        9        13          6     no       STEM\n257   male       11        52          6     no       STEM\n258   male        4        33          8    yes       STEM\n259   male        8        20          6     no       STEM\n260   male        2        13          2     no       STEM\n261   male       11        52          5     no       STEM\n262   male        6        20          3     no       STEM\n263   male        5        39          3     no       STEM\n264   male       11        53          8     no       STEM\n265   male       11        49          7     no       STEM\n266   male       11        49          6     no       STEM\n267   male       11        57          7     no       STEM\n268   male       11        17          5     no       STEM\n269   male       11        30         10     no       STEM\n270   male       11        13          3     no       STEM\n271   male       11        29          6     no       STEM\n272   male       11        29          3     no       STEM\n273   male       11        11          6     no       STEM\n274   male       10        12          2     no       STEM\n275   male       11        18          5     no       STEM\n276   male        3        15          3     no       STEM\n277 female       11        25          3     no       STEM\n278 female        8        10          4     no       STEM\n279 female       11        13          3    yes       STEM\n280 female       11        47          8     no       STEM\n281 female        5        11          6     no       STEM\n282 female       11        18          5     no       STEM\n283 female        4        21          1     no       STEM\n284 female        7        42          2     no       STEM\n285 female        9        33          1     no       STEM\n286 female        3        27          3     no       STEM\n287 female       11        12          3     no       STEM\n288 female       11        45          4     no       STEM\n289 female       11        49          3     no       STEM\n290 female       11        36          3    yes       STEM\n291 female       11        54          4     no       STEM\n292 female       10        31          6     no       STEM\n293 female        6        17          4     no       STEM\n294 female        7        30          4     no       STEM\n295 female        4        11          3     no       STEM\n296 female        2        17          5     no       STEM\n297 female        8        80          8     no       STEM\n298 female       11        39          4     no       STEM\n299 female       11        26          4     no       STEM\n300 female       11        29          4     no       STEM\n301 female       11        60          7     no       STEM\n302 female        1        16          3     no       STEM\n303 female        3        10          5     no       STEM\n304   male        3        14          2     no   Pre-prof\n305   male        6        14          7     no   Pre-prof\n306   male        7        12          4     no   Pre-prof\n307   male       11        13          3     no   Pre-prof\n308   male       11        11          5     no   Pre-prof\n309   male        8        39          2     no   Pre-prof\n310   male        8        32          3    yes   Pre-prof\n311   male        2        10          2     no   Pre-prof\n312   male       10        21          5    yes   Pre-prof\n313   male       11        10          3     no   Pre-prof\n314   male        7        12          3     no   Pre-prof\n315   male        5        15          3    yes   Pre-prof\n316   male       11        53          6     no   Pre-prof\n317   male        8        68          3     no   Pre-prof\n318   male        7        48          4     no   Pre-prof\n319   male        2        10          1     no   Pre-prof\n320   male       11        36          3     no   Pre-prof\n321   male        8        15          2    yes   Pre-prof\n322   male        3        32          3    yes   Pre-prof\n323   male        9        23          5     no   Pre-prof\n324   male        4        14          4     no   Pre-prof\n325   male        8        12          4     no   Pre-prof\n326   male       11        31          2     no   Pre-prof\n327   male       11        24          3    yes   Pre-prof\n328   male        8        33          4     no   Pre-prof\n329   male        6        50          2    yes   Pre-prof\n330   male        8        11          6     no   Pre-prof\n331   male       11        19          3     no   Pre-prof\n332   male        6        19          1     no   Pre-prof\n333   male       11        57          5     no   Pre-prof\n334   male       11        34          3     no   Pre-prof\n335   male        6        16          2     no   Pre-prof\n336   male        8        29          3     no   Pre-prof\n337   male       11        45          6     no   Pre-prof\n338   male       11        21          4     no   Pre-prof\n339 female        9        10          1     no   Pre-prof\n340 female        7        10          4     no   Pre-prof\n341 female       11        24          3     no   Pre-prof\n342 female        5        10          2     no   Pre-prof\n343 female        7        10          2     no   Pre-prof\n344 female       11        16          4     no   Pre-prof\n345 female       11        16          2     no   Pre-prof\n346 female       11        10          5     no   Pre-prof\n347 female        2        15          1     no   Pre-prof\n348 female        2        29          2    yes   Pre-prof\n349 female        3        24          2     no   Pre-prof\n350 female        2        11          4     no   Pre-prof\n351 female       11        22          9    yes   Pre-prof\n352 female       11        62          3     no   Pre-prof\n353 female       11        21          3     no   Pre-prof\n354 female        9        53          4     no   Pre-prof\n355 female        5        12          2     no   Pre-prof\n356 female       11        11          2     no   Pre-prof\n357 female       11        38          2     no   Pre-prof\n358 female       11        46          4     no   Pre-prof\n359 female       11        10          3     no   Pre-prof\n360 female        3        24          4     no   Pre-prof\n361 female       11        27          4     no   Pre-prof\n362 female        5        21          3     no   Pre-prof\n363 female        4        15          2     no   Pre-prof\n364 female        2        10          3     no   Pre-prof\n365 female        9        11          5     no   Pre-prof\n366 female        2        11          4     no   Pre-prof\n                           dept  quality helpfulness  clarity easiness\n1                       English 4.636364    4.636364 4.636364 4.818182\n2             Religious Studies 4.318182    4.545455 4.090909 4.363636\n3                           Art 4.790698    4.720930 4.860465 4.604651\n4                       English 4.250000    4.458333 4.041667 2.791667\n5                       Spanish 4.684211    4.684211 4.684211 4.473684\n6                       Spanish 4.233333    4.266667 4.200000 4.533333\n7                       Spanish 4.382353    4.352941 4.411765 4.117647\n8                       English 2.062500    2.062500 2.062500 1.437500\n9                         Music 2.041667    2.166667 2.000000 1.750000\n10                      English 4.111111    4.222222 4.000000 3.666667\n11                   Philosophy 4.727273    4.909091 4.545455 4.000000\n12                   Philosophy 3.724242    3.848485 3.606060 4.242424\n13                        Music 2.804348    2.695652 2.913043 2.217391\n14                        Music 4.838235    4.823529 4.852941 4.676471\n15                      Spanish 4.565217    4.565217 4.565217 2.826087\n16            Religious Studies 4.944444    4.962963 4.925926 3.703704\n17                      English 4.464286    4.714286 4.214286 3.214286\n18                      History 4.184211    4.368421 4.000000 3.631579\n19                          Art 3.909091    4.090909 3.727273 2.272727\n20                          Art 3.500000    3.285714 3.714286 3.285714\n21                   Philosophy 3.474576    3.542373 3.406780 2.355932\n22                      English 3.696429    3.714286 3.678571 3.642857\n23                      English 3.576923    3.461538 3.692308 4.615385\n24                   Philosophy 1.633333    1.600000 1.666667 2.266667\n25                      History 3.531250    3.312500 3.750000 2.562500\n26                      History 3.114286    3.057143 3.171429 3.857143\n27                        Music 4.909091    5.000000 4.818182 4.181818\n28                      English 4.239130    4.434783 4.043478 2.826087\n29                      Spanish 3.981481    4.037037 3.925926 3.148148\n30                      English 4.392857    4.500000 4.285714 3.166667\n31                      History 4.526316    4.473684 4.578947 3.131579\n32                       German 4.075000    4.150000 4.000000 3.250000\n33                   Philosophy 2.829546    3.022727 2.636364 2.045454\n34            Religious Studies 4.552632    4.561404 4.543860 3.614035\n35                        Music 3.700000    3.600000 3.800000 3.133333\n36                      English 1.891304    2.239130 1.543478 3.869565\n37                       French 4.277778    4.222222 4.333333 3.888889\n38                      English 4.416667    4.611111 4.222222 3.111111\n39                      History 2.569620    2.696203 2.443038 2.658228\n40                      Theater 3.472222    3.111111 3.833333 3.388889\n41                      English 2.442308    2.692308 2.192308 2.230769\n42            Religious Studies 3.067308    3.403846 2.692308 3.076923\n43            Religious Studies 3.600000    3.600000 3.600000 3.200000\n44                      Spanish 4.115385    4.230769 4.000000 2.884615\n45                      English 2.900000    3.200000 2.600000 3.800000\n46                      English 3.388889    3.711111 3.111111 2.844444\n47                   Philosophy 4.708334    4.750000 4.666666 3.666667\n48                         FLTR 3.291667    3.333333 3.250000 3.333333\n49                      History 2.250000    2.062500 2.437500 2.812500\n50                      English 3.555556    3.722222 3.388889 2.777778\n51                       French 3.854167    3.875000 3.833333 3.000000\n52                      History 3.913793    3.965517 3.862069 2.965517\n53                      History 3.469231    3.353846 3.584615 3.061538\n54                          Art 3.045455    3.090909 3.000000 3.363636\n55                      Spanish 3.464286    3.285714 3.642857 3.357143\n56                       German 4.000000    4.000000 4.000000 2.500000\n57                        Music 3.982759    3.862069 4.103448 3.310345\n58                      History 4.518519    4.518519 4.518519 4.074074\n59                      Theater 2.958333    2.833333 3.083333 2.500000\n60                      History 2.085965    2.280702 1.894737 2.631579\n61                      Theater 3.538462    3.461538 3.615385 3.230769\n62                      English 2.924242    2.696970 3.151515 3.454545\n63                      English 2.293478    2.282609 2.304348 2.456522\n64                      English 3.684211    3.684211 3.684211 3.157895\n65                       German 4.666666    4.888889 4.444445 3.055556\n66               Womens Studies 4.732143    4.857143 4.607143 4.500000\n67               Womens Studies 4.880952    4.952381 4.809524 4.809524\n68                      English 4.653846    4.615385 4.692308 4.076923\n69                      History 3.100000    3.500000 2.700000 3.000000\n70                      Spanish 2.107143    1.928571 2.285714 2.000000\n71                      Spanish 4.180000    4.320000 4.040000 3.720000\n72                      English 3.500000    3.500000 3.500000 2.272727\n73                      English 4.625000    4.666667 4.583333 4.527778\n74                      Spanish 4.428571    4.642857 4.214286 4.000000\n75                      English 2.394737    2.263158 2.526316 1.736842\n76                      English 1.674419    1.686047 3.360465 1.558140\n77                      English 3.760000    3.840000 3.680000 2.920000\n78                      English 4.538462    4.769231 4.307692 3.500000\n79            Religious Studies 1.875000    2.187500 1.562500 2.687500\n80            Religious Studies 4.025000    3.950000 4.100000 2.650000\n81                      Spanish 3.550000    3.666667 3.433333 1.966667\n82                      Spanish 3.166667    3.333333 3.000000 3.916667\n83                      English 2.523810    2.428571 2.619048 1.857143\n84                      Spanish 3.710526    3.894737 3.526316 2.789474\n85                          Art 4.458333    4.500000 4.416667 4.166667\n86                      Spanish 4.709677    4.806452 4.612903 3.645161\n87                      English 4.500000    4.545455 4.454545 3.636364\n88                       French 3.461538    3.307692 3.615385 2.923077\n89                       French 4.545455    4.545455 4.545455 3.500000\n90                      English 3.500000    3.807692 3.192308 3.692308\n91                      English 2.200000    2.000000 2.400000 2.400000\n92                      English 3.925000    4.000000 3.500000 3.850000\n93                      History 3.958333    3.916667 4.000000 2.750000\n94            Religious Studies 2.782609    3.000000 2.565218 3.768116\n95                      English 1.756757    1.810811 1.702703 1.783784\n96                      English 4.851351    4.945946 4.756757 3.729730\n97                      English 3.218750    3.343750 3.093750 2.156250\n98                      Spanish 3.312500    3.416667 3.208333 2.458333\n99               Womens Studies 3.695122    3.731707 3.658537 3.390244\n100                 Art History 3.794118    3.794118 3.794118 4.088235\n101                     Spanish 3.233333    3.266667 3.200000 3.400000\n102                     Spanish 3.236842    3.631579 2.842105 3.421053\n103                     English 4.700000    4.700000 4.700000 3.700000\n104                     English 1.482759    1.517241 1.448276 2.068966\n105                     Spanish 3.950000    3.900000 4.000000 3.400000\n106                     English 3.537037    3.629630 3.444444 2.814815\n107                     English 3.208333    3.222222 3.194444 2.083333\n108                     English 2.394737    2.684211 2.105263 1.789474\n109                       Dance 3.500000    3.714286 3.285714 3.500000\n110                     English 3.470588    3.529412 3.411765 3.294118\n111                     English 4.305556    4.259259 4.351852 3.203704\n112                     English 3.095238    3.142857 3.047619 3.857143\n113                  Philosophy 2.469697    2.454545 2.484848 2.666667\n114                     English 2.904255    3.021277 2.787234 3.361702\n115                     History 3.189655    3.206897 3.172414 3.672414\n116                     Spanish 4.678571    4.714286 4.642857 4.071429\n117              Art and design 3.200000    2.800000 3.600000 2.000000\n118                     History 3.833333    3.805556 3.861111 2.638889\n119                       Music 3.785714    3.500000 4.071429 3.285714\n120                       Music 3.442308    3.346154 3.500000 4.038462\n121                     History 2.589744    2.641026 2.538462 3.923077\n122                     History 2.437500    2.375000 2.500000 3.187500\n123                     English 2.977612    2.955224 3.000000 2.373134\n124                     English 3.224138    3.172414 3.275862 2.965517\n125                      German 2.750000    2.961539 2.538461 3.038461\n126                     English 3.729167    4.041666 3.416667 3.166667\n127                    Japanese 4.625000    4.812500 4.437500 3.062500\n128                     English 3.825000    4.000000 3.650000 2.550000\n129                     History 3.200000    3.171429 3.228571 3.028571\n130                     English 3.347222    3.333333 3.361111 3.500000\n131                     English 3.000000    3.074074 2.851852 3.592593\n132                       Music 4.100000    4.100000 4.100000 2.600000\n133                     English 2.792453    2.698113 2.886792 2.735849\n134                     Spanish 3.076923    3.076923 3.076923 3.461538\n135                  Psychology 4.800000    4.900000 4.700000 4.400000\n136               Communication 4.571429    4.571429 4.571429 4.238095\n137                  Psychology 4.757576    4.818182 4.696970 3.969697\n138               Communication 3.636364    3.545455 3.727273 3.727273\n139               Communication 3.583333    3.222222 3.944444 3.833333\n140                  Psychology 4.400000    4.366667 4.433333 4.133333\n141               Communication 4.071429    4.071429 4.142857 3.142857\n142                  Psychology 4.333333    4.333333 4.333333 2.933333\n143                   Sociology 4.250000    4.220000 4.280000 2.480000\n144                   Geography 4.150000    4.100000 4.200000 2.800000\n145           Political Science 3.366667    3.566667 3.166667 2.366667\n146                Anthropology 4.490566    4.283019 4.698113 3.603774\n147           Political Science 4.130435    3.934783 4.326087 3.217391\n148                  Psychology 2.700000    2.866667 2.533333 1.633333\n149           Political Science 3.142857    3.119048 3.166667 2.547619\n150                   Geography 2.621622    2.662162 2.581081 2.175676\n151                  Psychology 4.372549    4.176471 4.568627 3.588235\n152                  Psychology 1.467742    1.483871 1.451613 2.483871\n153                  Psychology 3.421053    3.578947 3.263158 4.368421\n154                Anthropology 2.419643    2.375000 2.464286 2.142857\n155           Political Science 3.833333    3.805556 3.861111 2.611111\n156           Political Science 4.216418    4.104477 4.328358 2.537314\n157           Political Science 2.494118    2.764706 2.223529 2.082353\n158                  Psychology 4.178571    4.142857 4.214286 2.428571\n159                   Sociology 2.872340    3.340426 2.404255 3.808511\n160                   Sociology 3.086957    3.420290 2.753623 3.579710\n161                Anthropology 3.409091    3.454545 3.363636 3.818182\n162           Political Science 2.535714    2.446429 2.625000 2.500000\n163               Communication 1.604167    1.500000 1.708333 2.083333\n164               Communication 2.630435    3.173913 2.086957 3.173913\n165               Communication 2.916667    3.138889 2.694444 3.500000\n166                   Sociology 2.733333    2.622222 2.844444 3.177778\n167     Communication Disorders 4.684211    4.894737 4.473684 4.210526\n168               Communication 4.000000    4.272727 3.727273 3.636364\n169               Communication 4.600000    4.700000 4.500000 4.500000\n170                   Sociology 4.544643    4.517857 4.571429 4.303571\n171                  Psychology 4.311765    4.305882 4.317647 4.435294\n172                  Psychology 4.937500    4.937500 4.937500 4.000000\n173                  Psychology 4.115385    4.179487 4.051282 2.410256\n174               Communication 2.435484    2.451613 2.419355 1.870968\n175                  Psychology 4.574074    4.703704 4.444444 2.888889\n176                   Sociology 3.880597    3.746269 4.029851 3.910448\n177                   Sociology 3.666667    3.866667 3.466667 3.533333\n178                   Sociology 4.195652    4.413043 3.978261 3.869565\n179                Anthropology 4.153846    4.384615 3.923077 4.076923\n180               Communication 2.000000    1.962963 2.037037 3.185185\n181               Communication 3.742857    3.714286 3.771429 2.885714\n182                   Sociology 2.171875    2.437500 1.906250 4.093750\n183                  Psychology 4.295455    4.318182 4.272727 3.954545\n184                  Psychology 3.686275    3.686275 3.686275 2.764706\n185               Communication 3.636364    4.090909 3.181818 3.363636\n186                  Psychology 4.076923    4.076923 4.076923 3.423077\n187                Anthropology 3.435484    3.274194 3.596774 2.322581\n188               Communication 2.730769    2.230769 3.230769 3.461538\n189                  Psychology 3.625000    3.550000 3.700000 3.500000\n190                  Psychology 4.562500    4.468750 4.656250 3.875000\n191                   Sociology 3.173913    3.130435 3.217391 3.173913\n192               Communication 3.741935    3.612903 3.870968 3.483871\n193                  Psychology 4.535714    4.547619 4.523810 2.500000\n194               Communication 4.083333    3.958333 4.208333 3.333333\n195     Communication Disorders 2.807692    2.730769 2.884615 3.576923\n196                   Geography 3.791667    3.500000 4.083334 3.000000\n197               Communication 3.343750    3.218750 3.468750 2.718750\n198                  Psychology 2.580000    2.600000 2.560000 2.860000\n199           Political Science 2.728070    2.526316 2.929825 2.684210\n200               Communication 3.966667    3.800000 4.133333 3.400000\n201                        Math 4.921875    5.000000 4.843750 3.812500\n202                        Math 2.529412    2.411765 2.647059 2.235294\n203                   Chemistry 2.565218    2.565218 2.565218 1.391304\n204                   Chemistry 4.269231    4.461538 4.076923 2.615385\n205                        Math 3.730769    3.692308 3.769231 3.846154\n206                        Math 2.638889    2.833333 2.444444 2.055556\n207                     Geology 4.954545    4.909091 5.000000 4.363636\n208                   Chemistry 4.480769    4.384615 4.576923 3.038462\n209                     Biology 2.646667    2.800000 2.533333 2.000000\n210                     Physics 4.720000    4.760000 4.680000 2.560000\n211                   Chemistry 2.619048    2.904762 2.333333 1.571429\n212           Astronomy/Physics 4.918605    5.000000 4.837209 3.279070\n213                        Math 4.549020    4.627451 4.470588 4.039216\n214                     Biology 4.045455    4.090909 4.000000 3.000000\n215                   Chemistry 4.796296    4.925926 4.666667 3.148148\n216                   Chemistry 3.067568    3.108108 3.027027 2.189189\n217                     Biology 3.385714    3.028571 3.742857 1.628571\n218                   Chemistry 4.000000    3.655172 4.344828 2.137931\n219                   Chemistry 1.750000    2.166667 1.333333 2.666667\n220            Computer Science 4.321429    4.392857 4.250000 2.714286\n221                     Physics 4.666667    4.757576 4.575758 3.212121\n222                        Math 2.714286    2.892857 2.535714 2.750000\n223                        Math 3.338710    3.645161 3.048387 3.193548\n224                        Math 3.986842    4.026316 3.947368 3.842105\n225       Physics and Astronomy 3.821429    4.071429 3.571429 3.214286\n226                     Geology 3.482759    3.482759 3.482759 3.206897\n227                   Chemistry 3.490566    3.924528 3.056604 3.000000\n228                     Geology 4.625000    4.600000 4.650000 3.700000\n229         Physics & Astronomy 3.452381    3.523810 3.380952 2.238095\n230                     Biology 2.287500    2.450000 2.125000 1.900000\n231                     Biology 3.038462    3.384615 2.692308 2.230769\n232                     Biology 4.365672    4.447761 4.283582 1.820896\n233                     Physics 3.684211    3.947368 3.421053 2.157895\n234                     Geology 3.500000    3.294118 3.705882 2.823529\n235                     Physics 4.159091    4.363636 3.954545 3.090909\n236                        Math 3.562500    3.437500 3.687500 3.375000\n237                   Chemistry 3.562500    3.437500 3.687500 3.375000\n238                     Geology 3.960526    4.000000 3.921053 2.578947\n239                        Math 4.231343    4.164179 4.298507 4.164179\n240                     Biology 2.671429    2.885714 2.457143 1.971429\n241                        Math 4.015625    3.937500 4.093750 4.406250\n242                     Biology 3.103448    3.275862 2.931034 2.224138\n243       Physics and Astronomy 4.166667    4.428571 3.904762 3.095238\n244                     Biology 2.791667    2.750000 2.833333 1.750000\n245                        Math 3.966667    4.088889 3.844444 2.533333\n246                     Geology 4.107143    4.142857 4.142857 3.071429\n247                   Chemistry 3.329268    3.512195 3.146341 3.000000\n248                     Geology 2.800000    2.900000 2.700000 2.400000\n249                     Biology 4.066667    3.900000 4.233333 2.933333\n250            Computer Science 4.620000    4.720000 4.520000 2.480000\n251                        Math 3.000000    3.081081 2.918919 3.054054\n252                   Chemistry 2.586957    2.608696 2.565217 2.695652\n253                        Math 1.915385    2.076923 1.723077 2.107692\n254                     Geology 4.527778    4.537037 4.518519 2.888889\n255                   Chemistry 3.890244    4.073171 3.707317 2.634146\n256                        Math 4.384615    4.461538 4.307692 3.846154\n257                        Math 3.451923    3.596154 3.269231 3.692308\n258                        Math 3.878788    4.030303 3.727273 3.515152\n259                     Physics 3.950000    3.900000 4.000000 3.200000\n260                   Chemistry 4.846154    4.923077 4.769231 3.769231\n261                        Math 2.865385    2.826923 2.903846 2.250000\n262                   Chemistry 3.000000    3.400000 2.600000 2.300000\n263                     Biology 3.166667    3.333333 3.000000 2.153846\n264                        Math 3.264151    3.339623 3.188679 3.528302\n265                        Math 2.928571    3.081633 2.775510 3.591837\n266         Physics & Astronomy 4.265306    4.265306 4.265306 3.204082\n267                        Math 2.508772    2.561404 2.473684 3.017544\n268                   Chemistry 4.235294    4.235294 4.235294 2.705882\n269            Computer Science 2.850000    2.933333 2.766667 2.200000\n270                     Geology 3.307692    3.461539 3.153846 2.769231\n271                        Math 2.620690    2.758621 2.482759 3.034483\n272                   Chemistry 3.103448    3.172414 3.034483 2.758621\n273            Computer Science 3.727273    4.000000 3.454545 3.272727\n274                     Geology 2.375000    2.416667 2.333333 2.750000\n275                        Math 3.722222    3.944444 3.500000 3.500000\n276                   Chemistry 2.500000    2.733333 2.266667 2.733333\n277                        Math 4.940000    5.000000 4.880000 4.800000\n278                     Biology 2.900000    3.100000 2.700000 1.500000\n279                     Biology 4.692308    4.769231 4.615385 4.230769\n280                        Math 3.436170    3.531915 3.340425 4.744681\n281                        Math 3.772727    3.818182 3.727273 3.727273\n282                   Chemistry 2.750000    2.777778 2.722222 2.444444\n283                   Chemistry 2.619048    3.190476 2.047619 2.523810\n284                     Geology 4.285714    4.452381 4.119048 3.619048\n285                     Biology 4.015152    4.181818 3.848485 1.878788\n286                        Math 4.981481    5.000000 4.962963 3.962963\n287                        Math 4.583333    4.500000 4.666667 3.666667\n288                     Biology 2.422222    2.533333 2.311111 2.022222\n289                        Math 2.336735    2.591837 2.081633 2.183673\n290                     Biology 4.500000    4.555556 4.444444 3.305556\n291                     Geology 3.555556    3.555556 3.555556 2.500000\n292                        Math 4.064516    4.096774 4.032258 3.258065\n293                     Biology 3.529412    3.470588 3.588235 2.117647\n294                     Biology 4.233333    4.366667 4.100000 2.933333\n295                     Biology 3.454545    3.363636 3.545455 2.636364\n296                        Math 2.352941    2.411765 2.294118 2.000000\n297                        Math 2.087500    2.375000 1.800000 2.387500\n298                     Physics 3.653846    4.076923 3.230769 2.794872\n299                     Biology 2.153846    2.346154 1.961538 2.115385\n300            Computer Science 2.655172    2.689655 2.620690 2.655172\n301                        Math 2.608333    2.716667 2.466667 3.483333\n302       Physics and Astronomy 2.406250    2.375000 2.437500 1.937500\n303                        Math 2.900000    3.200000 2.600000 3.100000\n304                        Kins 2.392857    2.142857 2.642857 1.428572\n305                        Kins 4.892857    4.928571 4.857143 4.857143\n306                        Kins 2.958333    3.000000 2.916667 3.833333\n307                        Kins 2.000000    2.000000 2.000000 2.846154\n308                    Business 3.954545    4.363636 3.545455 3.636364\n309                   Economics 4.294872    4.384615 4.205128 3.769231\n310                        Kins 4.453125    4.343750 4.562500 4.625000\n311                  Accounting 4.200000    4.600000 4.200000 2.400000\n312                        Kins 4.904762    4.857143 4.952381 4.809524\n313                     Finance 3.100000    3.500000 2.700000 1.900000\n314 Environmental Public Health 3.166667    3.166667 3.250000 3.583333\n315         Information Systems 3.433333    3.533333 3.333333 3.600000\n316                   Economics 4.811321    4.849057 4.773585 4.132075\n317                   Economics 3.919118    4.250000 3.588235 2.911765\n318                  Accounting 4.062500    4.229167 3.895833 1.750000\n319                    Business 2.600000    3.100000 2.200000 4.300000\n320                    Business 2.791667    2.750000 2.777778 2.194444\n321                     Finance 4.166667    4.266667 4.066667 3.000000\n322         Information Systems 4.250000    4.218750 4.281250 4.125000\n323                   Economics 4.260870    4.478261 4.043478 4.130435\n324                    Business 4.178571    4.071429 4.285714 3.142857\n325          Managerial Science 2.916667    3.000000 2.833333 2.333333\n326                  Accounting 2.919355    2.645161 3.193548 2.322581\n327                   Marketing 4.562500    4.791667 4.333333 3.666667\n328                   Economics 4.393939    4.424242 4.363636 3.454545\n329            Criminal Justice 4.640000    4.560000 4.720000 3.940000\n330  Curriculum and Instruction 2.590909    2.636364 2.545454 2.272727\n331             Library Science 4.394737    4.473684 4.315789 3.368421\n332                     Finance 3.447368    3.105263 3.789474 2.473684\n333                   Economics 3.719298    3.912281 3.526316 2.561404\n334                    Business 2.720588    2.764706 2.676471 2.647059\n335                    Business 3.687500    3.812500 3.562500 3.437500\n336           Special Education 3.136364    3.000000 3.272727 2.090909\n337                   Economics 4.288889    4.466667 4.111111 3.155556\n338                 Social Work 3.095238    2.809524 3.380952 2.666667\n339                    Business 4.050000    3.900000 4.200000 4.900000\n340           Special Education 4.850000    4.900000 4.800000 4.900000\n341                  Accounting 4.861111    4.833333 4.888889 4.722222\n342                        Kins 4.850000    4.800000 4.900000 4.000000\n343                  Management 3.350000    3.600000 3.100000 3.700000\n344                   Marketing 3.562500    3.687500 3.437500 4.125000\n345                    Business 4.687500    4.625000 4.750000 3.500000\n346                   Marketing 3.600000    3.600000 3.600000 2.700000\n347          Managerial Science 3.433333    3.666667 3.200000 3.200000\n348                   Economics 4.120690    4.344828 3.896552 4.034483\n349                   Economics 4.083330    4.583330 3.583330 4.166670\n350                 Social Work 3.181818    3.272727 3.090909 3.545455\n351                        Kins 4.340909    4.545455 4.090909 4.454545\n352                  Accounting 3.491935    3.612903 3.370968 2.467742\n353                  Management 4.500000    4.428571 4.571429 3.619048\n354                   Economics 3.584906    3.415094 3.754717 3.037736\n355                        Kins 3.708333    3.583333 3.833333 4.250000\n356                  Accounting 3.318182    3.090909 3.545455 2.181818\n357                   Economics 3.513158    3.657895 3.368421 3.868421\n358                   Economics 4.228261    4.304348 4.152174 3.369565\n359                        Kins 1.900000    1.900000 1.900000 3.800000\n360                   Economics 1.937500    2.250000 1.625000 2.333333\n361                    Business 3.462963    3.333333 3.592593 3.111111\n362                 Social Work 2.619048    2.714286 2.523810 3.619048\n363                  Accounting 2.966667    3.066667 2.866667 2.666667\n364                  Accounting 3.250000    3.200000 3.300000 3.000000\n365                     Nursing 1.909091    1.909091 1.909091 2.272727\n366                   Marketing 1.409091    1.363636 1.454545 2.636364\n    raterInterest  sdQuality sdHelpfulness sdClarity sdEasiness sdRaterInterest\n1        3.545455 0.55185640     0.6741999 0.5045250  0.4045199       1.1281521\n2        4.000000 0.90201795     0.9341987 0.9438798  0.5045250       1.0744356\n3        3.432432 0.45293432     0.6663898 0.4129681  0.5407021       1.2369438\n4        3.181818 0.93250483     0.9315329 0.9990938  0.5882300       1.3322506\n5        4.214286 0.65001124     0.8200699 0.5823927  0.6117753       0.9749613\n6        3.916667 0.86327170     1.0327956 0.7745967  0.6399405       0.6685579\n7        3.812500 0.94421613     0.9963167 0.9393364  0.6966305       1.2230427\n8        2.937500 1.19547760     1.3400871 1.1236103  0.7274384       1.6111590\n9        3.750000 1.07573090     1.3371159 1.0444659  0.7537783       1.2880570\n10       4.176471 0.90025413     1.0032627 0.9074852  0.7669650       1.5381123\n11       2.900000 0.34377584     0.3015113 0.5222330  0.7745967       1.1972190\n12       3.333333 1.23870660     1.3257359 1.2975793  0.7917663       1.2909944\n13       3.217391 0.98556780     1.0632191 1.1246431  0.7952428       1.1660548\n14       4.718750 0.51816386     0.6262243 0.4357058  0.8060600       0.5811210\n15       3.952381 0.99206334     1.0368697 0.9920634  0.8340577       1.0712698\n16       3.592593 0.16012815     0.1924501 0.2668803  0.8688992       1.1522306\n17       2.357143 0.60333323     0.4688072 0.8925824  0.8925824       1.1507284\n18       3.526316 0.69142620     0.6839856 0.8164966  0.8950808       1.1239030\n19       4.000000 1.26131250     1.4459976 1.1908744  0.9045340       1.0000000\n20       3.230769 1.25575600     1.3827827 1.2666473  0.9138736       1.2351685\n21       3.537037 1.59585600     1.6433989 1.5878705  0.9240599       1.3418231\n22       3.250000 1.23482860     1.2724180 1.3348206  0.9511898       1.3228756\n23       2.923077 1.57911040     1.6641006 1.6012815  0.9607689       1.4978617\n24       3.400000 0.71879530     0.9102590 1.0465362  0.9611501       1.2983506\n25       3.800000 0.93930380     1.1383468 0.9309493  0.9639329       0.9411239\n26       3.000000 1.27236300     1.3271566 1.3823619  0.9744639       1.5811388\n27       4.909091 0.20225996     0.0000000 0.4045199  0.9816498       0.3015113\n28       3.333333 1.12683750     1.1609591 1.2605288  0.9840627       1.3284223\n29       4.153846 1.08735290     1.1596247 1.1410496  0.9885383       0.8338972\n30       3.516129 1.00932060     1.1529390 0.9947598  1.0101115       1.3384311\n31       4.029412 0.62544435     0.6872130 0.7580765  1.0179750       1.0294245\n32       3.631579 1.38862860     1.3484884 1.4867839  1.0195458       1.1160708\n33       3.475000 1.35519330     1.5017607 1.3655378  1.0332731       1.3772417\n34       3.901961 0.72999640     0.8867586 0.8252743  1.0480319       1.1533413\n35       3.933333 1.26491100     1.2983506 1.3201731  1.0600988       0.7988086\n36       2.347826 1.03769530     1.4634022 0.8084697  1.0668478       1.1588767\n37       3.882353 0.84404874     1.0602750 0.7669650  1.0786096       0.9926198\n38       3.500000 0.57522374     0.9785276 0.7320845  1.0786096       0.9851844\n39       2.956522 1.17597260     1.3141676 1.2273260  1.0846963       1.3109806\n40       4.000000 1.18162680     1.4095844 1.0981267  1.0921586       1.0954451\n41       2.692308 1.37351320     1.5942203 1.2967415  1.1066234       1.2890068\n42       3.442308 1.14630200     1.2408033 1.2452020  1.1175280       1.1784580\n43       4.300000 1.50554530     1.5055453 1.5776213  1.1352924       0.4830459\n44       4.320000 0.86380196     0.8629110 1.0954451  1.1428709       0.9882645\n45       2.733333 1.28452320     1.3732131 1.3522468  1.1464230       1.3870146\n46       3.185185 1.42577290     1.4866408 1.5109031  1.1472409       1.3877773\n47       3.416667 0.54181236     0.4522670 0.6513389  1.1547005       1.4433757\n48       3.416667 1.33923880     1.3026779 1.5447860  1.1547005       1.4433757\n49       3.200000 0.96609180     1.1236103 1.0935416  1.1672617       1.2649110\n50       3.290322 1.40802690     1.4660171 1.4595063  1.1737878       1.2556325\n51       4.227273 1.27244340     1.2958965 1.3405601  1.1795356       0.9725675\n52       4.320000 1.40196910     1.4010904 1.4814119  1.1796718       0.9451631\n53       3.280702 1.28049350     1.3855366 1.3450407  1.1842313       1.3059532\n54       3.636364 1.42222620     1.6403991 1.2649111  1.2060454       0.9244163\n55       2.785714 1.51231200     1.7288756 1.3926810  1.2157393       1.4238934\n56       3.875000 1.19292780     1.4142136 1.0377490  1.2247449       0.6408699\n57       4.333333 1.24987690     1.4814119 1.2633523  1.2846191       1.2038585\n58       4.340909 0.70015470     0.7458246 0.8182065  1.2863852       1.0330173\n59       3.500000 1.23322070     1.4034589 1.3113722  1.3142575       1.0000000\n60       3.166667 1.15177480     1.2642670 1.2054076  1.3447196       1.4241846\n61       3.800000 1.16299780     1.5607362 0.8697185  1.3634421       1.6193277\n62       2.696970 1.28160800     1.3803271 1.3257359  1.3713795       1.2370542\n63       3.806452 1.33988430     1.4089089 1.3311576  1.3777038       1.3271361\n64       2.933333 1.18098970     1.2495613 1.2495613  1.3849652       1.1629192\n65       3.937500 0.42008403     0.3233808 0.7047922  1.0556415       0.7719024\n66       3.333333 0.44058344     0.4483951 0.5669467  0.5091751       1.2089410\n67       3.428571 0.21821790     0.2182179 0.4023739  0.5117663       1.2478553\n68       3.550000 0.62879616     0.6972473 0.6793662  0.6275716       1.0505954\n69       2.800000 1.12546290     1.3540064 1.0593499  0.6666667       1.2292726\n70       3.166667 1.04105290     1.2066665 1.1387288  0.6793662       1.4034589\n71       3.840000 0.77567180     0.8524475 0.8888194  0.7371115       0.9865766\n72       3.000000 1.38873020     1.5039630 1.4392458  0.7672969       1.4832397\n73       3.722222 0.59009683     0.6324555 0.7319251  0.7740842       1.0852547\n74       3.214286 0.26726124     0.4972452 0.5789342  0.7844645       1.1217138\n75       3.066667 1.51454940     1.6276126 1.6113632  0.8056816       1.4375906\n76       2.814815 1.07585110     1.1808435 1.0584168  0.8059288       1.2258784\n77       3.631579 1.20864940     1.2330483 1.2489996  0.8124038       1.2115429\n78       3.730769 0.54631630     0.4296689 0.7358930  0.8602325       1.1156233\n79       2.666667 0.82663980     1.1086779 0.8139410  0.8732125       1.2909944\n80       3.812500 1.15251450     1.2343760 1.1652874  0.8750940       1.1672618\n81       4.000000 1.15482500     1.0933445 1.2507469  0.8899180       1.1126973\n82       3.777778 1.05169420     1.0730867 1.2060454  0.9003366       1.7159384\n83       2.450000 1.25972410     1.4342743 1.3219754  0.9102590       1.3562720\n84       4.157895 1.26178660     1.2864567 1.3485968  0.9176629       1.2588865\n85       3.750000 0.68947720     0.9045340 0.6685579  0.9374369       1.1381804\n86       3.870968 0.49622230     0.4774484 0.6672041  0.9503819       1.4081178\n87       3.545455 0.80178374     0.8578641 0.9116846  0.9534626       1.0107646\n88       3.153846 0.98871840     1.1094004 1.1929279  0.9540736       1.2142318\n89       3.818182 0.87163080     1.0568269 0.8004328  0.9636241       1.2960145\n90       3.153846 1.20830460     1.3272296 1.2006409  0.9703290       1.3473621\n91       2.769231 1.34695420     1.3093073 1.4540584  0.9856108       1.4232502\n92       3.250000 1.05475120     1.1697953 1.0894228  0.9880869       1.2926920\n93       3.666667 1.13172300     1.4116492 0.9780193  0.9890707       1.3904436\n94       2.903226 1.24699130     1.3612279 1.2541491  1.0021291       1.3755437\n95       2.545455 1.06472230     1.1263964 1.1514451  1.0037467       1.5429458\n96       3.750000 0.30878040     0.2292434 0.4947168  1.0178586       1.2181424\n97       2.937500 1.24393690     1.3102419 1.4223872  1.0194678       1.1341474\n98       3.125000 1.46563900     1.5012072 1.5316705  1.0206207       1.2618999\n99       3.358974 1.24939010     1.3968606 1.2571745  1.0217154       1.1582014\n100      2.794118 1.14228400     1.2499554 1.2739681  1.0259555       1.2739681\n101      3.400000 1.34783780     1.4864467 1.3732131  1.0555973       1.3522468\n102      3.457143 1.42722960     1.4597492 1.5338754  1.0560400       1.2912114\n103      3.400000 0.78881060     0.9486833 0.6749486  1.0593499       1.0749677\n104      2.400000 0.66120505     0.7847060 0.6316762  1.0667385       1.3844373\n105      3.900000 0.89597870     1.4491377 0.6666667  1.0749677       0.8755950\n106      3.166667 1.31504550     1.3414650 1.3681355  1.0754976       1.4345630\n107      2.647059 1.15495820     1.3117297 1.0907257  1.0790207       1.2524486\n108      2.842105 1.03519920     1.2495613 1.1002392  1.0841765       1.3849652\n109      3.846154 1.17669680     1.2666474 1.2043876  1.0919284       1.7246330\n110      2.875000 1.15204420     1.3284223 1.1757351  1.1048024       1.6683325\n111      2.920000 1.01133510     1.1190492 0.9935150  1.1053836       1.2590894\n112      2.850000 1.51343190     1.6212870 1.5961263  1.1084094       1.0021622\n113      3.363636 1.36324180     1.4809395 1.4603341  1.1086779       1.0552897\n114      3.380952 1.35389260     1.5671819 1.3011061  1.1117071       1.1032630\n115      3.333333 1.21694030     1.3861295 1.2860647  1.1299423       0.9712859\n116      4.071429 0.63872296     0.6112498 0.8418974  1.1411388       1.1411388\n117      3.000000 1.41813650     1.6193277 1.3498971  1.1547005       1.2472191\n118      3.166667 1.06904500     1.2608261 1.0994226  1.1748016       1.2761549\n119      3.714286 0.87077080     1.0919285 0.7300459  1.2043875       1.2043875\n120      1.098039 1.57052170     1.7191232 1.7191232  1.2159200       1.1972190\n121      3.105263 1.33215200     1.4232502 1.3542556  1.2222631       1.4292216\n122      3.062500 1.22304260     1.5438048 1.0954451  1.2230427       1.4361407\n123      2.552239 1.47307360     1.6554120 1.4142136  1.2288923       1.1452614\n124      2.827586 1.36660560     1.5369024 1.3600565  1.2387424       1.4159541\n125      3.277778 1.19373370     1.3705698 1.1740791  1.2483835       1.2307339\n126      3.333333 1.33025940     1.3376712 1.5012072  1.2740442       1.1671841\n127      4.533333 0.64549720     0.5439056 0.8920949  1.2893797       0.6399405\n128      3.235294 1.29039980     1.3764944 1.3869694  1.3168943       1.1472473\n129      3.657143 1.31842150     1.3823619 1.3303187  1.3169866       1.1867617\n130      3.500000 1.53910660     1.5856499 1.6062873  1.3416408       1.3471506\n131      3.148148 1.27097790     1.3846945 1.3503192  1.3660516       1.0635103\n132      3.300000 1.24275680     1.3703203 1.1972190  1.4298407       1.6363917\n133      2.913043 1.35317990     1.3948381 1.4366143  1.4431662       1.2441166\n134      2.615385 1.32045050     1.4978617 1.3204506  1.4500221       1.0439078\n135      4.500000 0.25819890     0.3162278 0.4830459  0.5163978       0.5477226\n136      4.095238 0.63807750     0.6761234 0.6761234  0.7003401       0.9952267\n137      4.000000 0.28287620     0.3916747 0.4666937  0.7699370       0.7905694\n138      3.333333 1.22659910     1.3684763 1.1908744  0.7862454       1.3228757\n139      3.000000 1.15363870     1.2628425 1.2113300  0.8574929       1.2649111\n140      4.000000 0.75885576     0.7648905 0.8583598  0.8603661       0.9325048\n141      3.500000 1.10692130     1.3847680 1.0271052  0.8644378       1.0801235\n142      3.933333 0.91936830     1.0465362 0.8997354  0.8837151       0.5936168\n143      3.312500 0.66432410     0.7899884 0.8091316  0.8861750       1.1697654\n144      3.800000 1.17968920     1.1972190 1.2292726  0.9189366       1.3165612\n145      3.296296 1.22427560     1.3047217 1.3412124  0.9278575       1.0308628\n146      3.428571 0.62396500     0.9482242 0.5401177  0.9474586       1.3844373\n147      3.057143 0.87834935     0.9752988 0.9440892  0.9640895       1.4939655\n148      3.192308 1.36836170     1.5698305 1.2793677  0.9643055       1.2335066\n149      3.428571 1.23607220     1.2916690 1.3954048  0.9678334       1.5324595\n150      3.218750 1.30806460     1.4456077 1.3447692  0.9702155       1.2657735\n151      4.083334 0.76709280     1.0675591 0.6963624  0.9900296       0.7810788\n152      3.225806 0.88445354     0.9956896 0.8500474  0.9956896       1.5429339\n153      3.277778 1.49316180     1.6095475 1.5578513  1.0116283       1.6379886\n154      3.111111 1.40057380     1.4962073 1.4008346  1.0344708       1.3963114\n155      3.606557 1.17485390     1.2631159 1.1904024  1.0421478       1.2685778\n156      3.500000 0.91793525     1.0020332 0.9752709  1.0777130       1.2460464\n157      3.222222 1.24521490     1.4528056 1.2571365  1.0824549       1.3862730\n158      3.785714 1.01160850     1.0994504 1.1217138  1.0894096       1.3688047\n159      2.857143 1.13476720     1.4337030 1.0766206  1.0962049       1.2010448\n160      3.019608 1.21557450     1.3438947 1.1931890  1.1298854       1.2726381\n161      3.090909 1.39316510     1.5075567 1.4333686  1.1677484       1.3003496\n162      2.365854 1.39106610     1.4259879 1.5203170  1.1908744       1.3182583\n163      2.454545 0.90864410     0.9780193 0.9990938  1.2128539       1.4384937\n164      3.650000 1.17953560     1.5270939 1.0406748  1.2303796       1.0399899\n165      2.861111 1.33897610     1.5147424 1.3484265  1.2305632       1.2224747\n166      3.097561 1.33399600     1.4027390 1.4027390  1.3973279       1.0721450\n167      4.368421 0.34199280     0.3153018 0.5129892  0.6306035       0.7608859\n168      3.636364 0.80622580     0.9045340 0.9045340  0.6741999       0.9244163\n169      3.400000 0.65828060     0.6749486 0.7071068  0.7071068       1.5776213\n170      3.285714 0.51589980     0.6321988 0.6566344  0.7608522       1.0738933\n171      3.705882 0.91604894     0.9883070 0.9905718  0.7935313       1.1070847\n172      2.187500 0.17078250     0.2500000 0.2500000  0.8164966       1.5585784\n173      3.828571 1.23789690     1.2746887 1.2343487  0.8181477       1.0427823\n174      3.357143 1.19542130     1.2606535 1.2589465  0.8462441       1.3666473\n175      3.833333 0.58348596     0.5417078 0.7510676  0.8473185       1.0494995\n176      3.566667 1.06986810     1.2103504 1.0726652  0.8656989       1.1404232\n177      3.466667 1.34518540     1.5055453 1.4573296  0.9154754       1.1872337\n178      3.369565 0.98589080     1.1070699 0.9997584  0.9335403       1.1226694\n179      3.692308 1.16161910     1.1929279 1.2557560  0.9540736       1.1094004\n180      2.740741 1.05611770     1.2854655 1.0912759  0.9622504       1.1298654\n181      3.483871 1.24482120     1.4465258 1.2622509  0.9631880       1.1509697\n182      3.281250 1.29271770     1.6051831 1.2536238  0.9954534       1.3009767\n183      3.590909 0.95940320     1.0413528 0.9847319  0.9989172       0.7341397\n184      3.500000 0.96416175     1.0486219 1.0860975  1.0116963       1.1832160\n185      4.714286 1.00227010     1.4459976 1.0787198  1.0269106       0.4879500\n186      3.659574 1.21826220     1.3112807 1.2342522  1.0355666       1.1087909\n187      3.241935 1.17516610     1.3203248 1.2208527  1.0366117       1.1878850\n188      2.923077 1.12944280     1.3634421 1.0127394  1.0500305       1.3204506\n189      3.950000 1.26569100     1.5035047 1.2607433  1.0513150       0.9986833\n190      4.266667 0.83037110     1.0467885 0.7006621  1.0701221       0.9802650\n191      4.086957 1.54925720     1.5463843 1.6502485  1.0724727       0.6683115\n192      2.870968 1.07787720     1.2460408 1.0937059  1.0825279       1.1232837\n193      3.761905 0.66619470     0.8025077 0.7066960  1.0876244       0.8499505\n194      2.708333 1.26548390     1.3014763 1.2846643  1.1293194       1.1970677\n195      4.272727 1.64970860     1.7101507 1.6811168  1.2057554       1.1621744\n196      3.833333 0.81067690     1.2431631 0.5149286  1.2060454       1.2673044\n197      2.586207 1.33463480     1.4081416 1.3674647  1.2504032       1.0527936\n198      3.100000 1.30290100     1.4568627 1.3425532  1.2779128       1.2152872\n199      2.957447 1.24308870     1.3771770 1.2936610  1.3114591       1.3180593\n200      3.307692 1.21694387     1.3732131 1.2459458  1.5946339       1.4366985\n201      2.900000 0.18445101     0.0000000 0.3689020  0.5922892       1.3222238\n202      3.333333 1.21797200     1.2776357 1.2718675  0.6642112       0.8164966\n203      3.650000 1.38419940     1.3968564 1.4405203  0.7223151       1.2258188\n204      3.333333 0.63296210     0.7762500 0.6405126  0.7679476       1.3228757\n205      2.846154 1.21818480     1.1821319 1.3008873  0.8006408       0.9870962\n206      2.444444 1.30390310     1.4652846 1.2935233  0.8023658       1.2472191\n207      3.181818 0.15075567     0.3015113 0.0000000  0.8090398       1.4012981\n208      4.043478 0.84238670     1.0228166 0.8086075  0.8236878       1.2239378\n209      3.200000 1.46671000     1.6124516 1.3557637  0.8451543       1.3201731\n210      3.320000 0.43493295     0.5228129 0.4760952  0.8698659       1.2819256\n211      3.142857 1.46547570     1.6704718 1.3540064  0.8701396       1.1952286\n212      3.047619 0.21630646     0.0000000 0.4326129  0.8817078       1.2484601\n213      3.000000 0.59375840     0.7472827 0.6435197  0.8935499       1.3070323\n214      2.909091 1.12815210     1.3003496 1.0954451  0.8944272       1.2210279\n215      2.904762 0.31802452     0.2668803 0.5547002  0.9073929       1.3749459\n216      3.028571 1.42965700     1.5773357 1.4431156  0.9079231       1.3169866\n217      3.228571 1.20101500     1.4242793 1.2209653  0.9102590       1.2622509\n218      3.107143 0.91612536     1.1733914 0.9364012  0.9151166       1.3968029\n219      2.961538 0.80676025     1.2058288 0.6064784  0.9222661       1.1482428\n220      3.642857 0.91504186     0.9164863 1.0408330  0.9371803       0.9440892\n221      3.354839 0.60810500     0.6139169 0.7084447  0.9603898       1.4270806\n222      3.346154 1.22798060     1.4991179 1.1379690  0.9670497       1.1642099\n223      2.796610 1.15508210     1.2557641 1.2337789  0.9723844       1.1563865\n224      3.894737 1.19394220     1.4423487 1.0640912  0.9733285       1.1806886\n225      3.357143 0.91161615     1.1411388 0.9376145  0.9749613       1.2774459\n226      3.896552 1.27813170     1.6822750 1.1838403  0.9775812       1.2913124\n227      2.851064 1.00320000     1.1715584 1.0413763  0.9784634       1.3984265\n228      3.882353 0.55901700     0.6805570 0.5871429  0.9787210       1.1114379\n229      2.722222 1.42218820     1.5039630 1.5321942  0.9952267       1.2274103\n230      3.375000 1.18693470     1.4667249 1.1137256  1.0076629       1.3716451\n231      4.000000 1.39136530     1.5566236 1.4366985  1.0127394       1.2472191\n232      3.611940 1.03199600     1.0770834 1.1120002  1.0139239       1.5270810\n233      3.000000 1.01667380     1.2681432 1.2163602  1.0145145       1.3228757\n234      2.875000 1.03077640     0.9195587 1.3585243  1.0145993       1.1474610\n235      3.363636 0.89157915     0.9021379 1.0455016  1.0192944       1.3471507\n236      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n237      2.812500 1.22304260     1.3647344 1.2500000  1.0246951       1.0468206\n238      3.200000 1.01596430     1.1624764 1.0496223  1.0301293       1.2319282\n239      2.650794 0.97440100     1.0814835 1.0152612  1.0386036       1.2202423\n240      3.645161 1.24819200     1.3233520 1.3578282  1.0427823       1.1704241\n241      2.281250 1.31053040     1.3897667 1.3526408  1.0429293       1.1139692\n242      3.810345 1.23102660     1.4605106 1.1826534  1.0436626       1.4244112\n243      3.350000 1.01653000     1.1212238 1.0910895  1.0442587       1.2258187\n244      3.000000 1.21465170     1.2880570 1.1934163  1.0552897       1.1832160\n245      3.333333 1.35847510     1.4588635 1.3809673  1.0574412       1.2841818\n246      3.000000 0.62568640     0.9492623 0.5345225  1.0716117       1.0377490\n247      3.028571 1.21248590     1.5020311 1.1524100  1.0723805       1.3169866\n248      2.700000 1.22927260     1.2866839 1.2516656  1.0749677       1.2516656\n249      3.413793 0.76263310     1.0618786 0.7738544  1.0806554       0.9826074\n250      4.117647 0.72571800     0.6782330 0.9183318  1.0847427       1.2187264\n251      2.343750 1.33853150     1.4601822 1.3617468  1.1041826       1.1247760\n252      2.333333 1.29379800     1.4996706 1.3082287  1.1051443       1.0846523\n253      2.122807 0.97855616     1.2413160 0.9438567  1.1057107       1.2965638\n254      3.078431 0.84032700     0.8403270 0.7948120  1.1271381       1.2139710\n255      3.341463 1.20162490     1.2326850 1.2892615  1.1348149       1.2963363\n256      3.076923 1.13933180     1.1266014 1.1821319  1.1435437       1.4411534\n257      2.787234 1.20562260     1.2873383 1.2225878  1.1468364       1.2146957\n258      3.303030 1.17944840     1.3342800 1.1256311  1.1489455       1.3574988\n259      2.950000 1.30686450     1.5525870 1.1697953  1.1516578       1.1459310\n260      4.153846 0.31521258     0.2773501 0.4385290  1.1657506       1.2810252\n261      2.285714 1.25680650     1.3535886 1.2408033  1.1694644       1.3385315\n262      3.050000 1.15849270     1.2732057 1.1876558  1.1742859       1.1459310\n263      3.615385 1.20488770     1.3245324 1.2139540  1.1818465       1.1148610\n264      2.978723 1.13765860     1.3147594 1.1938400  1.1865233       1.1700866\n265      2.950000 1.17703720     1.2883571 1.2460823  1.2062056       1.1972190\n266      3.891304 1.15064310     1.3034491 1.1686087  1.2412962       1.0160547\n267      2.350877 1.35781870     1.5355030 1.2970470  1.2746320       1.2318863\n268      4.090909 1.14724730     1.2004901 1.1472473  1.3117119       1.0444659\n269      3.724138 1.34645130     1.5297810 1.3565507  1.3235272       1.2217245\n270      3.692308 1.31558700     1.3301244 1.3445045  1.3634421       1.1094004\n271      3.173913 1.25822410     1.4054784 1.1838403  1.3753638       1.0724727\n272      2.962963 1.30506780     1.3645765 1.3491468  1.4054784       1.2241632\n273      4.181818 1.64869094     1.6124516 1.8090681  1.4206273       0.9816498\n274      2.090909 1.13066760     1.4433757 1.3026779  1.4222262       0.8312094\n275      2.444444 1.36362650     1.4741786 1.4652846  1.4245742       1.3382263\n276      3.000000 1.06904500     1.4864467 1.0997835  1.4375906       1.4142136\n277      4.240000 0.16583124     0.0000000 0.3316625  0.5000000       0.8793937\n278      3.000000 1.02198060     0.8755950 1.3374935  0.5270463       1.3333333\n279      3.250000 0.59646390     0.4385290 0.8697185  0.7250111       1.1381804\n280      2.914894 1.11129090     1.2131716 1.1661270  0.7362680       1.4269120\n281      3.000000 1.45539750     1.6624188 1.3483997  0.9045340       1.4832397\n282      3.588235 1.37466570     1.5550886 1.4061025  0.9217772       0.9393364\n283      2.904762 1.19557330     1.4359334 1.1169687  0.9283883       1.0442587\n284      3.459459 0.99885650     0.9160461 1.1305596  0.9358023       1.1924437\n285      3.961538 0.79534630     0.8083372 0.9721501  0.9603898       0.8708970\n286      3.407407 0.09622505     0.0000000 0.1924501  0.9798541       1.3376000\n287      3.000000 0.73340220     0.6741999 0.8876254  0.9847319       1.6514456\n288      3.850000 1.27009100     1.4238233 1.2581050  1.0110501       1.1220403\n289      2.302326 1.20064120     1.3527498 1.2047948  1.0139335       0.9888638\n290      3.656250 0.70710677     0.6946508 0.8432740  1.0642085       1.0957211\n291      2.607843 1.06251160     1.2539247 1.0757475  1.0772081       1.2973578\n292      2.320000 1.40084460     1.4225526 1.4255729  1.0944631       1.3453624\n293      3.352941 1.23073390     1.3284223 1.2277430  1.1114379       1.2718675\n294      3.366667 1.08860350     1.2452207 1.0618786  1.1121068       1.2726116\n295      3.272727 1.40453820     1.5666989 1.3684763  1.1200649       1.4893562\n296      2.470588 0.91454744     0.9393364 1.1599949  1.1726039       1.2307339\n297      2.733333 1.09883430     1.3442329 1.0482052  1.2169036       1.2339054\n298      3.421053 1.27296710     1.2852322 1.4227760  1.2178386       1.0813300\n299      2.884615 1.02731920     1.3547637 0.9583640  1.2434443       1.4234303\n300      4.068966 1.34365870     1.4418106 1.3993313  1.2614012       1.3074248\n301      2.250000 1.35637750     1.4390636 1.4078071  1.2688132       1.0823902\n302      2.875000 1.14336860     1.3102163 1.0935416  1.3400871       1.3102163\n303      1.900000 1.07496770     1.3165612 0.9660918  1.3703203       1.1005049\n304      2.714286 1.19580300     1.2924124 1.1507283  0.5135526       1.2043875\n305      4.000000 0.28946713     0.2672612 0.3631365  0.5345225       1.1766968\n306      3.416667 1.49443412     1.4298407 1.6329932  0.6992059       1.3540064\n307      2.769231 1.24163870     1.5275252 1.0801234  0.8006408       1.4232502\n308      2.909091 1.15009880     1.2060454 1.2135598  0.8090398       1.1361818\n309      3.055556 0.76706850     0.8148421 0.8938234  0.8098583       1.1697239\n310      3.593750 0.87399140     1.0957211 0.8775883  0.8327955       1.5420844\n311      3.900000 0.78881064     0.6992059 0.6324555  0.8432740       1.2866839\n312      3.894737 0.20118695     0.3585686 0.2182179  0.8728716       1.1969747\n313      2.500000 1.32916010     1.5811388 1.4181365  0.8755950       0.9258201\n314      3.500000 1.11464080     1.6422453 1.0552897  0.9003366       1.1677484\n315      3.666667 1.26585190     1.3557637 1.2909944  0.9102590       1.5886502\n316      3.133333 0.34287268     0.4112002 0.4658123  0.9206548       1.2720778\n317      2.919355 1.06368230     1.1638203 1.2366939  0.9261587       1.1205734\n318      3.348837 1.09459510     1.0766296 1.2070640  0.9339917       1.3252802\n319      2.600000 1.02198060     1.3703203 1.3984118  0.9486833       1.3498971\n320      3.114286 1.38551180     1.5743479 1.3333333  0.9803627       1.2312459\n321      3.357143 0.79432510     0.9611501 0.7988086  1.0000000       1.4468609\n322      3.093750 1.03954090     1.2374369 1.0544644  1.0080323       1.4448881\n323      2.952381 0.91539320     0.8979555 1.0650762  1.0137396       1.2835961\n324      4.000000 1.11987540     1.1411388 1.2043876  1.0271052       1.0377490\n325      3.750000 1.41153260     1.8090681 1.1934162  1.0730867       0.7537783\n326      2.741935 1.17564420     1.2529535 1.2759142  1.0766335       1.2101719\n327      3.863636 0.86366886     0.8329709 1.0494995  1.0901403       1.0371873\n328      3.285714 0.94998010     1.0615526 0.9623598  1.0923286       1.1818737\n329      4.160000 0.63116350     0.8121526 0.5728554  1.0956314       0.8417668\n330      3.636364 1.13618180     1.3618170 1.3684763  1.1037128       1.5015144\n331      3.421053 0.71838840     0.7723284 0.8200699  1.1160708       1.5024347\n332      3.000000 1.41317940     1.5949482 1.4749368  1.1239030       1.1547005\n333      3.092593 1.04803190     1.1538863 1.1036508  1.1498066       1.2017051\n334      3.030303 1.29796560     1.4367224 1.3644976  1.1516090       1.1315048\n335      2.769231 1.34008700     1.3768926 1.3647344  1.1528949       1.4232502\n336      4.000000 1.30558240     1.4832397 1.3483997  1.2210279       0.8944272\n337      3.184211 1.00277390     1.0135446 1.0917505  1.2239198       1.3326516\n338      3.444444 1.15778930     1.3645163 1.1608700  1.2382784       1.0416176\n339      3.200000 0.92646280     1.1005049 0.9189366  0.3162278       1.5491933\n340      4.600000 0.24152295     0.3162278 0.4216370  0.3162278       0.6992059\n341      4.647059 0.41322106     0.5144958 0.3233808  0.5745131       0.6063391\n342      4.200000 0.33747430     0.4216370 0.3162278  0.6666667       0.6324555\n343      3.875000 1.54650140     1.5776213 1.7288403  0.6749486       1.3562027\n344      3.437500 1.44769930     1.5370426 1.4127397  0.7187953       1.0935416\n345      3.187500 0.57373047     0.6191392 0.5773503  0.8164966       0.9810708\n346      4.250000 1.64654520     1.7126977 1.6465452  0.8232726       0.8864053\n347      3.000000 1.48644670     1.5886502 1.4242793  0.9411239       1.4638501\n348      3.034483 0.96042377     1.2614012 0.9001916  0.9813532       1.0850529\n349      2.625000 0.68630000     0.7172800 0.8297000  1.0072200       1.0959400\n350      3.363636 1.30905940     1.4893562 1.2210279  1.0357255       0.6741999\n351      3.636364 0.80750000     0.9625004 0.9714540  1.0568269       1.3988245\n352      2.966102 1.37152520     1.4525821 1.4169220  1.0669052       1.3641481\n353      3.428571 0.80622580     1.0757057 0.7464200  1.0712698       1.0281745\n354      2.825000 1.14243270     1.2315400 1.2074394  1.0734965       1.0594508\n355      2.000000 1.43745890     1.5050420 1.4034589  1.1381804       1.4142136\n356      2.727273 1.67738970     1.7002674 1.7529196  1.1677484       1.4206273\n357      2.823529 1.14777600     1.2363048 1.2610817  1.2119016       1.1407225\n358      2.906977 0.97585590     1.0081791 1.0534049  1.2176242       1.1508579\n359      2.200000 1.02198060     1.1005049 1.1972190  1.2292726       1.1352924\n360      2.562500 1.18241190     1.3593477 1.1726039  1.2740441       1.3149778\n361      2.555556 1.32958880     1.4935760 1.2787993  1.3397283       1.4232502\n362      3.411765 1.54842470     1.5537972 1.6917165  1.4309504       1.0036697\n363      2.533333 1.28822500     1.3345233 1.3557637  1.4474937       1.1254629\n364      2.700000 1.20761480     1.4757296 1.0593499  1.5634719       0.9486833\n365      2.545455 1.37510340     1.5782614 1.3003496  1.6180797       1.6348478\n366      2.272727 0.91701096     0.9244163 1.0357255  1.6292776       1.4206273\n\n\n\n\nCode\nratemyprof<- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\nratemyprof\n\n\n     quality helpfulness  clarity easiness raterInterest\n1   4.636364    4.636364 4.636364 4.818182      3.545455\n2   4.318182    4.545455 4.090909 4.363636      4.000000\n3   4.790698    4.720930 4.860465 4.604651      3.432432\n4   4.250000    4.458333 4.041667 2.791667      3.181818\n5   4.684211    4.684211 4.684211 4.473684      4.214286\n6   4.233333    4.266667 4.200000 4.533333      3.916667\n7   4.382353    4.352941 4.411765 4.117647      3.812500\n8   2.062500    2.062500 2.062500 1.437500      2.937500\n9   2.041667    2.166667 2.000000 1.750000      3.750000\n10  4.111111    4.222222 4.000000 3.666667      4.176471\n11  4.727273    4.909091 4.545455 4.000000      2.900000\n12  3.724242    3.848485 3.606060 4.242424      3.333333\n13  2.804348    2.695652 2.913043 2.217391      3.217391\n14  4.838235    4.823529 4.852941 4.676471      4.718750\n15  4.565217    4.565217 4.565217 2.826087      3.952381\n16  4.944444    4.962963 4.925926 3.703704      3.592593\n17  4.464286    4.714286 4.214286 3.214286      2.357143\n18  4.184211    4.368421 4.000000 3.631579      3.526316\n19  3.909091    4.090909 3.727273 2.272727      4.000000\n20  3.500000    3.285714 3.714286 3.285714      3.230769\n21  3.474576    3.542373 3.406780 2.355932      3.537037\n22  3.696429    3.714286 3.678571 3.642857      3.250000\n23  3.576923    3.461538 3.692308 4.615385      2.923077\n24  1.633333    1.600000 1.666667 2.266667      3.400000\n25  3.531250    3.312500 3.750000 2.562500      3.800000\n26  3.114286    3.057143 3.171429 3.857143      3.000000\n27  4.909091    5.000000 4.818182 4.181818      4.909091\n28  4.239130    4.434783 4.043478 2.826087      3.333333\n29  3.981481    4.037037 3.925926 3.148148      4.153846\n30  4.392857    4.500000 4.285714 3.166667      3.516129\n31  4.526316    4.473684 4.578947 3.131579      4.029412\n32  4.075000    4.150000 4.000000 3.250000      3.631579\n33  2.829546    3.022727 2.636364 2.045454      3.475000\n34  4.552632    4.561404 4.543860 3.614035      3.901961\n35  3.700000    3.600000 3.800000 3.133333      3.933333\n36  1.891304    2.239130 1.543478 3.869565      2.347826\n37  4.277778    4.222222 4.333333 3.888889      3.882353\n38  4.416667    4.611111 4.222222 3.111111      3.500000\n39  2.569620    2.696203 2.443038 2.658228      2.956522\n40  3.472222    3.111111 3.833333 3.388889      4.000000\n41  2.442308    2.692308 2.192308 2.230769      2.692308\n42  3.067308    3.403846 2.692308 3.076923      3.442308\n43  3.600000    3.600000 3.600000 3.200000      4.300000\n44  4.115385    4.230769 4.000000 2.884615      4.320000\n45  2.900000    3.200000 2.600000 3.800000      2.733333\n46  3.388889    3.711111 3.111111 2.844444      3.185185\n47  4.708334    4.750000 4.666666 3.666667      3.416667\n48  3.291667    3.333333 3.250000 3.333333      3.416667\n49  2.250000    2.062500 2.437500 2.812500      3.200000\n50  3.555556    3.722222 3.388889 2.777778      3.290322\n51  3.854167    3.875000 3.833333 3.000000      4.227273\n52  3.913793    3.965517 3.862069 2.965517      4.320000\n53  3.469231    3.353846 3.584615 3.061538      3.280702\n54  3.045455    3.090909 3.000000 3.363636      3.636364\n55  3.464286    3.285714 3.642857 3.357143      2.785714\n56  4.000000    4.000000 4.000000 2.500000      3.875000\n57  3.982759    3.862069 4.103448 3.310345      4.333333\n58  4.518519    4.518519 4.518519 4.074074      4.340909\n59  2.958333    2.833333 3.083333 2.500000      3.500000\n60  2.085965    2.280702 1.894737 2.631579      3.166667\n61  3.538462    3.461538 3.615385 3.230769      3.800000\n62  2.924242    2.696970 3.151515 3.454545      2.696970\n63  2.293478    2.282609 2.304348 2.456522      3.806452\n64  3.684211    3.684211 3.684211 3.157895      2.933333\n65  4.666666    4.888889 4.444445 3.055556      3.937500\n66  4.732143    4.857143 4.607143 4.500000      3.333333\n67  4.880952    4.952381 4.809524 4.809524      3.428571\n68  4.653846    4.615385 4.692308 4.076923      3.550000\n69  3.100000    3.500000 2.700000 3.000000      2.800000\n70  2.107143    1.928571 2.285714 2.000000      3.166667\n71  4.180000    4.320000 4.040000 3.720000      3.840000\n72  3.500000    3.500000 3.500000 2.272727      3.000000\n73  4.625000    4.666667 4.583333 4.527778      3.722222\n74  4.428571    4.642857 4.214286 4.000000      3.214286\n75  2.394737    2.263158 2.526316 1.736842      3.066667\n76  1.674419    1.686047 3.360465 1.558140      2.814815\n77  3.760000    3.840000 3.680000 2.920000      3.631579\n78  4.538462    4.769231 4.307692 3.500000      3.730769\n79  1.875000    2.187500 1.562500 2.687500      2.666667\n80  4.025000    3.950000 4.100000 2.650000      3.812500\n81  3.550000    3.666667 3.433333 1.966667      4.000000\n82  3.166667    3.333333 3.000000 3.916667      3.777778\n83  2.523810    2.428571 2.619048 1.857143      2.450000\n84  3.710526    3.894737 3.526316 2.789474      4.157895\n85  4.458333    4.500000 4.416667 4.166667      3.750000\n86  4.709677    4.806452 4.612903 3.645161      3.870968\n87  4.500000    4.545455 4.454545 3.636364      3.545455\n88  3.461538    3.307692 3.615385 2.923077      3.153846\n89  4.545455    4.545455 4.545455 3.500000      3.818182\n90  3.500000    3.807692 3.192308 3.692308      3.153846\n91  2.200000    2.000000 2.400000 2.400000      2.769231\n92  3.925000    4.000000 3.500000 3.850000      3.250000\n93  3.958333    3.916667 4.000000 2.750000      3.666667\n94  2.782609    3.000000 2.565218 3.768116      2.903226\n95  1.756757    1.810811 1.702703 1.783784      2.545455\n96  4.851351    4.945946 4.756757 3.729730      3.750000\n97  3.218750    3.343750 3.093750 2.156250      2.937500\n98  3.312500    3.416667 3.208333 2.458333      3.125000\n99  3.695122    3.731707 3.658537 3.390244      3.358974\n100 3.794118    3.794118 3.794118 4.088235      2.794118\n101 3.233333    3.266667 3.200000 3.400000      3.400000\n102 3.236842    3.631579 2.842105 3.421053      3.457143\n103 4.700000    4.700000 4.700000 3.700000      3.400000\n104 1.482759    1.517241 1.448276 2.068966      2.400000\n105 3.950000    3.900000 4.000000 3.400000      3.900000\n106 3.537037    3.629630 3.444444 2.814815      3.166667\n107 3.208333    3.222222 3.194444 2.083333      2.647059\n108 2.394737    2.684211 2.105263 1.789474      2.842105\n109 3.500000    3.714286 3.285714 3.500000      3.846154\n110 3.470588    3.529412 3.411765 3.294118      2.875000\n111 4.305556    4.259259 4.351852 3.203704      2.920000\n112 3.095238    3.142857 3.047619 3.857143      2.850000\n113 2.469697    2.454545 2.484848 2.666667      3.363636\n114 2.904255    3.021277 2.787234 3.361702      3.380952\n115 3.189655    3.206897 3.172414 3.672414      3.333333\n116 4.678571    4.714286 4.642857 4.071429      4.071429\n117 3.200000    2.800000 3.600000 2.000000      3.000000\n118 3.833333    3.805556 3.861111 2.638889      3.166667\n119 3.785714    3.500000 4.071429 3.285714      3.714286\n120 3.442308    3.346154 3.500000 4.038462      1.098039\n121 2.589744    2.641026 2.538462 3.923077      3.105263\n122 2.437500    2.375000 2.500000 3.187500      3.062500\n123 2.977612    2.955224 3.000000 2.373134      2.552239\n124 3.224138    3.172414 3.275862 2.965517      2.827586\n125 2.750000    2.961539 2.538461 3.038461      3.277778\n126 3.729167    4.041666 3.416667 3.166667      3.333333\n127 4.625000    4.812500 4.437500 3.062500      4.533333\n128 3.825000    4.000000 3.650000 2.550000      3.235294\n129 3.200000    3.171429 3.228571 3.028571      3.657143\n130 3.347222    3.333333 3.361111 3.500000      3.500000\n131 3.000000    3.074074 2.851852 3.592593      3.148148\n132 4.100000    4.100000 4.100000 2.600000      3.300000\n133 2.792453    2.698113 2.886792 2.735849      2.913043\n134 3.076923    3.076923 3.076923 3.461538      2.615385\n135 4.800000    4.900000 4.700000 4.400000      4.500000\n136 4.571429    4.571429 4.571429 4.238095      4.095238\n137 4.757576    4.818182 4.696970 3.969697      4.000000\n138 3.636364    3.545455 3.727273 3.727273      3.333333\n139 3.583333    3.222222 3.944444 3.833333      3.000000\n140 4.400000    4.366667 4.433333 4.133333      4.000000\n141 4.071429    4.071429 4.142857 3.142857      3.500000\n142 4.333333    4.333333 4.333333 2.933333      3.933333\n143 4.250000    4.220000 4.280000 2.480000      3.312500\n144 4.150000    4.100000 4.200000 2.800000      3.800000\n145 3.366667    3.566667 3.166667 2.366667      3.296296\n146 4.490566    4.283019 4.698113 3.603774      3.428571\n147 4.130435    3.934783 4.326087 3.217391      3.057143\n148 2.700000    2.866667 2.533333 1.633333      3.192308\n149 3.142857    3.119048 3.166667 2.547619      3.428571\n150 2.621622    2.662162 2.581081 2.175676      3.218750\n151 4.372549    4.176471 4.568627 3.588235      4.083334\n152 1.467742    1.483871 1.451613 2.483871      3.225806\n153 3.421053    3.578947 3.263158 4.368421      3.277778\n154 2.419643    2.375000 2.464286 2.142857      3.111111\n155 3.833333    3.805556 3.861111 2.611111      3.606557\n156 4.216418    4.104477 4.328358 2.537314      3.500000\n157 2.494118    2.764706 2.223529 2.082353      3.222222\n158 4.178571    4.142857 4.214286 2.428571      3.785714\n159 2.872340    3.340426 2.404255 3.808511      2.857143\n160 3.086957    3.420290 2.753623 3.579710      3.019608\n161 3.409091    3.454545 3.363636 3.818182      3.090909\n162 2.535714    2.446429 2.625000 2.500000      2.365854\n163 1.604167    1.500000 1.708333 2.083333      2.454545\n164 2.630435    3.173913 2.086957 3.173913      3.650000\n165 2.916667    3.138889 2.694444 3.500000      2.861111\n166 2.733333    2.622222 2.844444 3.177778      3.097561\n167 4.684211    4.894737 4.473684 4.210526      4.368421\n168 4.000000    4.272727 3.727273 3.636364      3.636364\n169 4.600000    4.700000 4.500000 4.500000      3.400000\n170 4.544643    4.517857 4.571429 4.303571      3.285714\n171 4.311765    4.305882 4.317647 4.435294      3.705882\n172 4.937500    4.937500 4.937500 4.000000      2.187500\n173 4.115385    4.179487 4.051282 2.410256      3.828571\n174 2.435484    2.451613 2.419355 1.870968      3.357143\n175 4.574074    4.703704 4.444444 2.888889      3.833333\n176 3.880597    3.746269 4.029851 3.910448      3.566667\n177 3.666667    3.866667 3.466667 3.533333      3.466667\n178 4.195652    4.413043 3.978261 3.869565      3.369565\n179 4.153846    4.384615 3.923077 4.076923      3.692308\n180 2.000000    1.962963 2.037037 3.185185      2.740741\n181 3.742857    3.714286 3.771429 2.885714      3.483871\n182 2.171875    2.437500 1.906250 4.093750      3.281250\n183 4.295455    4.318182 4.272727 3.954545      3.590909\n184 3.686275    3.686275 3.686275 2.764706      3.500000\n185 3.636364    4.090909 3.181818 3.363636      4.714286\n186 4.076923    4.076923 4.076923 3.423077      3.659574\n187 3.435484    3.274194 3.596774 2.322581      3.241935\n188 2.730769    2.230769 3.230769 3.461538      2.923077\n189 3.625000    3.550000 3.700000 3.500000      3.950000\n190 4.562500    4.468750 4.656250 3.875000      4.266667\n191 3.173913    3.130435 3.217391 3.173913      4.086957\n192 3.741935    3.612903 3.870968 3.483871      2.870968\n193 4.535714    4.547619 4.523810 2.500000      3.761905\n194 4.083333    3.958333 4.208333 3.333333      2.708333\n195 2.807692    2.730769 2.884615 3.576923      4.272727\n196 3.791667    3.500000 4.083334 3.000000      3.833333\n197 3.343750    3.218750 3.468750 2.718750      2.586207\n198 2.580000    2.600000 2.560000 2.860000      3.100000\n199 2.728070    2.526316 2.929825 2.684210      2.957447\n200 3.966667    3.800000 4.133333 3.400000      3.307692\n201 4.921875    5.000000 4.843750 3.812500      2.900000\n202 2.529412    2.411765 2.647059 2.235294      3.333333\n203 2.565218    2.565218 2.565218 1.391304      3.650000\n204 4.269231    4.461538 4.076923 2.615385      3.333333\n205 3.730769    3.692308 3.769231 3.846154      2.846154\n206 2.638889    2.833333 2.444444 2.055556      2.444444\n207 4.954545    4.909091 5.000000 4.363636      3.181818\n208 4.480769    4.384615 4.576923 3.038462      4.043478\n209 2.646667    2.800000 2.533333 2.000000      3.200000\n210 4.720000    4.760000 4.680000 2.560000      3.320000\n211 2.619048    2.904762 2.333333 1.571429      3.142857\n212 4.918605    5.000000 4.837209 3.279070      3.047619\n213 4.549020    4.627451 4.470588 4.039216      3.000000\n214 4.045455    4.090909 4.000000 3.000000      2.909091\n215 4.796296    4.925926 4.666667 3.148148      2.904762\n216 3.067568    3.108108 3.027027 2.189189      3.028571\n217 3.385714    3.028571 3.742857 1.628571      3.228571\n218 4.000000    3.655172 4.344828 2.137931      3.107143\n219 1.750000    2.166667 1.333333 2.666667      2.961538\n220 4.321429    4.392857 4.250000 2.714286      3.642857\n221 4.666667    4.757576 4.575758 3.212121      3.354839\n222 2.714286    2.892857 2.535714 2.750000      3.346154\n223 3.338710    3.645161 3.048387 3.193548      2.796610\n224 3.986842    4.026316 3.947368 3.842105      3.894737\n225 3.821429    4.071429 3.571429 3.214286      3.357143\n226 3.482759    3.482759 3.482759 3.206897      3.896552\n227 3.490566    3.924528 3.056604 3.000000      2.851064\n228 4.625000    4.600000 4.650000 3.700000      3.882353\n229 3.452381    3.523810 3.380952 2.238095      2.722222\n230 2.287500    2.450000 2.125000 1.900000      3.375000\n231 3.038462    3.384615 2.692308 2.230769      4.000000\n232 4.365672    4.447761 4.283582 1.820896      3.611940\n233 3.684211    3.947368 3.421053 2.157895      3.000000\n234 3.500000    3.294118 3.705882 2.823529      2.875000\n235 4.159091    4.363636 3.954545 3.090909      3.363636\n236 3.562500    3.437500 3.687500 3.375000      2.812500\n237 3.562500    3.437500 3.687500 3.375000      2.812500\n238 3.960526    4.000000 3.921053 2.578947      3.200000\n239 4.231343    4.164179 4.298507 4.164179      2.650794\n240 2.671429    2.885714 2.457143 1.971429      3.645161\n241 4.015625    3.937500 4.093750 4.406250      2.281250\n242 3.103448    3.275862 2.931034 2.224138      3.810345\n243 4.166667    4.428571 3.904762 3.095238      3.350000\n244 2.791667    2.750000 2.833333 1.750000      3.000000\n245 3.966667    4.088889 3.844444 2.533333      3.333333\n246 4.107143    4.142857 4.142857 3.071429      3.000000\n247 3.329268    3.512195 3.146341 3.000000      3.028571\n248 2.800000    2.900000 2.700000 2.400000      2.700000\n249 4.066667    3.900000 4.233333 2.933333      3.413793\n250 4.620000    4.720000 4.520000 2.480000      4.117647\n251 3.000000    3.081081 2.918919 3.054054      2.343750\n252 2.586957    2.608696 2.565217 2.695652      2.333333\n253 1.915385    2.076923 1.723077 2.107692      2.122807\n254 4.527778    4.537037 4.518519 2.888889      3.078431\n255 3.890244    4.073171 3.707317 2.634146      3.341463\n256 4.384615    4.461538 4.307692 3.846154      3.076923\n257 3.451923    3.596154 3.269231 3.692308      2.787234\n258 3.878788    4.030303 3.727273 3.515152      3.303030\n259 3.950000    3.900000 4.000000 3.200000      2.950000\n260 4.846154    4.923077 4.769231 3.769231      4.153846\n261 2.865385    2.826923 2.903846 2.250000      2.285714\n262 3.000000    3.400000 2.600000 2.300000      3.050000\n263 3.166667    3.333333 3.000000 2.153846      3.615385\n264 3.264151    3.339623 3.188679 3.528302      2.978723\n265 2.928571    3.081633 2.775510 3.591837      2.950000\n266 4.265306    4.265306 4.265306 3.204082      3.891304\n267 2.508772    2.561404 2.473684 3.017544      2.350877\n268 4.235294    4.235294 4.235294 2.705882      4.090909\n269 2.850000    2.933333 2.766667 2.200000      3.724138\n270 3.307692    3.461539 3.153846 2.769231      3.692308\n271 2.620690    2.758621 2.482759 3.034483      3.173913\n272 3.103448    3.172414 3.034483 2.758621      2.962963\n273 3.727273    4.000000 3.454545 3.272727      4.181818\n274 2.375000    2.416667 2.333333 2.750000      2.090909\n275 3.722222    3.944444 3.500000 3.500000      2.444444\n276 2.500000    2.733333 2.266667 2.733333      3.000000\n277 4.940000    5.000000 4.880000 4.800000      4.240000\n278 2.900000    3.100000 2.700000 1.500000      3.000000\n279 4.692308    4.769231 4.615385 4.230769      3.250000\n280 3.436170    3.531915 3.340425 4.744681      2.914894\n281 3.772727    3.818182 3.727273 3.727273      3.000000\n282 2.750000    2.777778 2.722222 2.444444      3.588235\n283 2.619048    3.190476 2.047619 2.523810      2.904762\n284 4.285714    4.452381 4.119048 3.619048      3.459459\n285 4.015152    4.181818 3.848485 1.878788      3.961538\n286 4.981481    5.000000 4.962963 3.962963      3.407407\n287 4.583333    4.500000 4.666667 3.666667      3.000000\n288 2.422222    2.533333 2.311111 2.022222      3.850000\n289 2.336735    2.591837 2.081633 2.183673      2.302326\n290 4.500000    4.555556 4.444444 3.305556      3.656250\n291 3.555556    3.555556 3.555556 2.500000      2.607843\n292 4.064516    4.096774 4.032258 3.258065      2.320000\n293 3.529412    3.470588 3.588235 2.117647      3.352941\n294 4.233333    4.366667 4.100000 2.933333      3.366667\n295 3.454545    3.363636 3.545455 2.636364      3.272727\n296 2.352941    2.411765 2.294118 2.000000      2.470588\n297 2.087500    2.375000 1.800000 2.387500      2.733333\n298 3.653846    4.076923 3.230769 2.794872      3.421053\n299 2.153846    2.346154 1.961538 2.115385      2.884615\n300 2.655172    2.689655 2.620690 2.655172      4.068966\n301 2.608333    2.716667 2.466667 3.483333      2.250000\n302 2.406250    2.375000 2.437500 1.937500      2.875000\n303 2.900000    3.200000 2.600000 3.100000      1.900000\n304 2.392857    2.142857 2.642857 1.428572      2.714286\n305 4.892857    4.928571 4.857143 4.857143      4.000000\n306 2.958333    3.000000 2.916667 3.833333      3.416667\n307 2.000000    2.000000 2.000000 2.846154      2.769231\n308 3.954545    4.363636 3.545455 3.636364      2.909091\n309 4.294872    4.384615 4.205128 3.769231      3.055556\n310 4.453125    4.343750 4.562500 4.625000      3.593750\n311 4.200000    4.600000 4.200000 2.400000      3.900000\n312 4.904762    4.857143 4.952381 4.809524      3.894737\n313 3.100000    3.500000 2.700000 1.900000      2.500000\n314 3.166667    3.166667 3.250000 3.583333      3.500000\n315 3.433333    3.533333 3.333333 3.600000      3.666667\n316 4.811321    4.849057 4.773585 4.132075      3.133333\n317 3.919118    4.250000 3.588235 2.911765      2.919355\n318 4.062500    4.229167 3.895833 1.750000      3.348837\n319 2.600000    3.100000 2.200000 4.300000      2.600000\n320 2.791667    2.750000 2.777778 2.194444      3.114286\n321 4.166667    4.266667 4.066667 3.000000      3.357143\n322 4.250000    4.218750 4.281250 4.125000      3.093750\n323 4.260870    4.478261 4.043478 4.130435      2.952381\n324 4.178571    4.071429 4.285714 3.142857      4.000000\n325 2.916667    3.000000 2.833333 2.333333      3.750000\n326 2.919355    2.645161 3.193548 2.322581      2.741935\n327 4.562500    4.791667 4.333333 3.666667      3.863636\n328 4.393939    4.424242 4.363636 3.454545      3.285714\n329 4.640000    4.560000 4.720000 3.940000      4.160000\n330 2.590909    2.636364 2.545454 2.272727      3.636364\n331 4.394737    4.473684 4.315789 3.368421      3.421053\n332 3.447368    3.105263 3.789474 2.473684      3.000000\n333 3.719298    3.912281 3.526316 2.561404      3.092593\n334 2.720588    2.764706 2.676471 2.647059      3.030303\n335 3.687500    3.812500 3.562500 3.437500      2.769231\n336 3.136364    3.000000 3.272727 2.090909      4.000000\n337 4.288889    4.466667 4.111111 3.155556      3.184211\n338 3.095238    2.809524 3.380952 2.666667      3.444444\n339 4.050000    3.900000 4.200000 4.900000      3.200000\n340 4.850000    4.900000 4.800000 4.900000      4.600000\n341 4.861111    4.833333 4.888889 4.722222      4.647059\n342 4.850000    4.800000 4.900000 4.000000      4.200000\n343 3.350000    3.600000 3.100000 3.700000      3.875000\n344 3.562500    3.687500 3.437500 4.125000      3.437500\n345 4.687500    4.625000 4.750000 3.500000      3.187500\n346 3.600000    3.600000 3.600000 2.700000      4.250000\n347 3.433333    3.666667 3.200000 3.200000      3.000000\n348 4.120690    4.344828 3.896552 4.034483      3.034483\n349 4.083330    4.583330 3.583330 4.166670      2.625000\n350 3.181818    3.272727 3.090909 3.545455      3.363636\n351 4.340909    4.545455 4.090909 4.454545      3.636364\n352 3.491935    3.612903 3.370968 2.467742      2.966102\n353 4.500000    4.428571 4.571429 3.619048      3.428571\n354 3.584906    3.415094 3.754717 3.037736      2.825000\n355 3.708333    3.583333 3.833333 4.250000      2.000000\n356 3.318182    3.090909 3.545455 2.181818      2.727273\n357 3.513158    3.657895 3.368421 3.868421      2.823529\n358 4.228261    4.304348 4.152174 3.369565      2.906977\n359 1.900000    1.900000 1.900000 3.800000      2.200000\n360 1.937500    2.250000 1.625000 2.333333      2.562500\n361 3.462963    3.333333 3.592593 3.111111      2.555556\n362 2.619048    2.714286 2.523810 3.619048      3.411765\n363 2.966667    3.066667 2.866667 2.666667      2.533333\n364 3.250000    3.200000 3.300000 3.000000      2.700000\n365 1.909091    1.909091 1.909091 2.272727      2.545455\n366 1.409091    1.363636 1.454545 2.636364      2.272727\n\n\n\n\nCode\npairs(ratemyprof)\n\n\n\n\n\nQuality, helpfulness, and clarity all have a positive correlation with one another which show that these rating have the most influence on how a professor is rated. The other variables easiness and raterInterest have dispersed data and seem to effect the other variables in a similar way which shows that they don’t have much a sway with rating a professor."
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#question-5",
    "href": "posts/HW3_ShoshanaBuck.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.You can use student.survey in the R console, after loading the package, to see what each variable means.\n\nA\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases.\n\n\nCode\nstudent.survey\n\n\nError in eval(expr, envir, enclos): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#i-y-political-ideology-and-x-religiosity",
    "href": "posts/HW3_ShoshanaBuck.html#i-y-political-ideology-and-x-religiosity",
    "title": "Homework 3",
    "section": "(i) y = political ideology and x = religiosity",
    "text": "(i) y = political ideology and x = religiosity\n\n\nCode\ns<- ggplot(data = student.survey, aes( x= as.numeric(pi), y= as.numeric(re))) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = as.numeric(pi), y = as.numeric(re))): object 'student.survey' not found\n\n\nCode\ns\n\n\nError in eval(expr, envir, enclos): object 's' not found"
  },
  {
    "objectID": "posts/HW3_ShoshanaBuck.html#ii-y-high-school-gpa-and-x-hours-of-tv-watching",
    "href": "posts/HW3_ShoshanaBuck.html#ii-y-high-school-gpa-and-x-hours-of-tv-watching",
    "title": "Homework 3",
    "section": "(ii) y = high school GPA and x = hours of TV watching",
    "text": "(ii) y = high school GPA and x = hours of TV watching\n\n\nCode\np<- ggplot(data = student.survey, aes( x= tv, y= hi)) + geom_smooth()\n\n\nError in ggplot(data = student.survey, aes(x = tv, y = hi)): object 'student.survey' not found\n\n\nCode\np\n\n\nError in eval(expr, envir, enclos): object 'p' not found\n\n\n\nB\nThe first graph that shows a positive correlation with political ideology and religiosity, the stronger political beliefs one has the more religious they are. The second graph shows that the more tv people watch the lower ones high school GPA will be."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html",
    "href": "posts/HW3_Solutions_OmerYalcin.html",
    "title": "Homework 3",
    "section": "",
    "text": "Please check your answers against the solutions."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nLoad the necessary packages.\n\nlibrary(alr4)\nlibrary(smss)\nlibrary(ggplot2)\n\nLoad data:\n\ndata(UN11)\n\n\n1.1.1\nThe predictor is ppgdp, i.e. GDP per capita. The response is fertility, the birth rate per 1000 women.\n\n\n1.1.2.\n\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\nA straight line is not appropriate, because the relationship has an L-shaped structure (or the left half of a U-shape).\n\n\n1.1.2\n\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point()\n\n\n\n\nYes, now a simple linear regression model is more plausible. We can imagine a negative-sloped straight line going through those points."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nThe conversion from USD to British pound will mean the numerical value of the response will be divided by 1.33. To offset that, the slope will also become divided by 1.33.\n\n\n(b)\nCorrelation will not change because it is a standardized measure that is not influenced by the unit of measurement.\nBoth outcomes can easily be shown via simulation."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\ndata(water)\npairs(water)\n\n\n\n\n\nYear appears to be largely unrelated to each of the other variables\nThe three variables starting with “O” seem to be correlated with each other, meaning that all the plot including two of these variables exhibit a dependence between the variables that is stronger than the dependence between the “O” variables and other variables. The three variables starting with “A” also seem to be another correlated group\nBSAAM is more closely related to the “O” variables than the “A” variables"
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-4",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\ndata(Rateprof)\npairs(Rateprof[,c('quality', 'clarity', 'helpfulness',\n                  'easiness', 'raterInterest')])\n\n\n\n\nThe very strong pair-wise correlation among quality, clarity, and helpfulness is very striking. easiness is also correlated fairly highly with the other three. raterInterest is also moderately correlated, but raters almost always say they are at least moderately interested in the subject. Overall, the results might show that people don’t necessarily distinguish all these dimensions very well in their minds—or that professors that do one in one dimension tend to do well on the others too."
  },
  {
    "objectID": "posts/HW3_Solutions_OmerYalcin.html#question-5",
    "href": "posts/HW3_Solutions_OmerYalcin.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\na\nOne way of visually representing the relationship between religiosity and political ideology is as follows (and there are other ways). As we go towards bars to the right (more religiousity), we see lighter colors pop up (more conservatism)\n\ndata(student.survey)\nggplot(data = student.survey, aes(x = re, fill = pi)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThe relationship between high school GPA and hours of watching TV can be shown with a good old scatter plot.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() \n\n\n\n\n\n\nb\nDealing with ordinal variables in linear regression is a difficult problem. We’ll just go ahead and assume that we can just convert them to numeric and use them. This would be done for political ideology and religiosity. High school GPA and hours of TV are already continuous.\n\nm1 <- lm(as.numeric(pi) ~ as.numeric(re), \n         data = student.survey)\nm2 <- lm(hi ~ tv, data = student.survey)\nstargazer(m1, m2, type = 'text', \n          dep.var.labels = c('Pol. Ideology', 'HS GPA'),\n          covariate.labels = c('Religiosity', 'Hours of TV')\n          )\n\nError in stargazer(m1, m2, type = \"text\", dep.var.labels = c(\"Pol. Ideology\", : could not find function \"stargazer\"\n\n\nReligiosity is positively and statistically significantly (at the 0.01 significance level) associated with conservatism.\nHours of TV is negatively and statistically significantly (at the 0.05 significance level) associated with High School GPA. Watching an average of 1 more hour of TV per week is associated with a 0.018 decline in High School GPA."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html",
    "href": "posts/HW3_StephRoberts.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(alr4)\nlibrary(smss)\nlibrary(fastDummies)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-1",
    "href": "posts/HW3_StephRoberts.html#question-1",
    "title": "HW3",
    "section": "Question 1",
    "text": "Question 1\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n1.1.1. Identify the predictor and the response.\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nattach(UN11)\nun <- UN11\nhead(un)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\n\nCode\n#Check correlation\ncor.test(UN11$ppgdp,UN11$fertility)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  UN11$ppgdp and UN11$fertility\nt = -6.877, df = 197, p-value = 7.903e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5456842 -0.3205140\nsample estimates:\n       cor \n-0.4399891 \n\n\nThere is a weak, negative correlation between fertility and ppGDP.\n\n\nCode\n#Check matrix plot\npairs(UN11)\n\n\n\n\n\nThe relationship of per person GDP to fertility looks somewhat curvy, which indicates diminishing returns on fertility from increasing GDP.\n\n\nCode\n#Linear regression\nsummary(lm(fertility ~ ppgdp, data = UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe R squared in our linear model is very low 0.19, suggesting a non-linear relationship.\nLet’s have a closer look of the dependence of fertility on ppGDP\n\n\nCode\n#Plot variables\nplot( x= UN11$ppgdp, y= UN11$fertility)\n\n\n\n\n\nThis graph is a nice visual representation of our negative correlation, because it shows as ppGPD rises, there are fewer and fewer births per 1000 females\nBut the relationship does not appear linear.\n\n\nCode\n#Plot variables with linear regrssion\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAs we could have predicted, a linear explanation does not exist here. There are very large residuals and the predicted value of fertility after about 60,000 ppgdp enter negative values. Since we can’t have negative births and we do have ppgdp values over 60,000, we should explore a better model to explain this relationship.\n\n\nCode\n#Plot regression of variable logarithms\nggplot(UN11, aes(x = log(ppgdp), y = log(fertility))) +\n    geom_point(color=2) + \n    geom_smooth(method = \"lm\") +\n    labs(x=\"ppgdp-Gross National Product Per Person in U.S. dollars\", y=\"fertility-birth rate per 1000 women\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA linear regression for the logarithms of each variable appears to be much more appropriate."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-2",
    "href": "posts/HW3_StephRoberts.html#question-2",
    "title": "HW3",
    "section": "Question 2",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\nHow, if at all, does the slope of the prediction equation change?\n\nIf all explanatory values are converted (multiplied) to another number - British pounds - the slope of the prediction equation will change. Since the rate is x 1.33, multiplying by a positive number should make the slope steeper.\n\nHow, if at all, does the correlation change?\n\nThe correlation would not change if only the units of measurement are different. The values in relation to the predictor variable would remain the same."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-3",
    "href": "posts/HW3_StephRoberts.html#question-3",
    "title": "HW3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots.\n\n\nCode\n#Load data\ndata(\"water\")\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\n\n\nCode\n#Check matrix plot\npairs(water, col = 4,main = \"Water Runoff in Sierras\")\n\n\n\n\n\n\n\nCode\n#Multiple regression\nwat <- (lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water))\nsummary(wat)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nAnalysis: The P-values are not very small across the board. Only OPRC and OPSLAKE have p-values of significance, under 0.05. This shows us that individually, the location precipitation do would not make good explanatory variables.\nHowever, the model as a whole has a p-value of < 2.2e-16, indicating it is statistically significant. Also, while the residuals have a wide range (-12690 to 18542), the 1Q (-4936) and 3Q (4173) are fairly close in absolute value. Therefore, the range may be due to some outliars. The adjusted R-squared (0.9123) tells us that 91% of the variation of runoff can be explained by combined location precipitation.\nWith a strong p-value, decent residuals, and a high adjusted R-squared tells us this model could appropriately be used to predict runoff volume near Bishop, California."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-4",
    "href": "posts/HW3_StephRoberts.html#question-4",
    "title": "HW3",
    "section": "Question 4",
    "text": "Question 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\n\nCode\n#Load data\ndata(\"Rateprof\")\n\n#Select 5 variables in question\nrateprof <- Rateprof %>%\n  select(\"quality\",\"helpfulness\",\"clarity\",\"easiness\",\"raterInterest\")\n\n#Create table of ratings\nkable(head(rateprof), format = \"markdown\", digits = 10, col.names = c('Quality','Helpfulness','Clarity', 'Easiness', 'Rater Interest'), caption = \"**Professor Ratings**\")\n\n\n\nProfessor Ratings\n\n\nQuality\nHelpfulness\nClarity\nEasiness\nRater Interest\n\n\n\n\n4.636364\n4.636364\n4.636364\n4.818182\n3.545455\n\n\n4.318182\n4.545455\n4.090909\n4.363636\n4.000000\n\n\n4.790698\n4.720930\n4.860465\n4.604651\n3.432432\n\n\n4.250000\n4.458333\n4.041667\n2.791667\n3.181818\n\n\n4.684211\n4.684211\n4.684211\n4.473684\n4.214286\n\n\n4.233333\n4.266667\n4.200000\n4.533333\n3.916667\n\n\n\n\n\n\n\nCode\npairs(rateprof, col = 2,main = \"Professor Ratings\")\n\n\n\n\n\nQuality, helpfulness, and clarity all appear to have strong positive linear correlations with one another. There is a moderate positive linear correlation between easiness and clarity, helpfulness, and quality. There is also a moderate positive correlation between rater interest and quality, helpfulness, and clarity. There is a weak positive linear correlation between easiness and rater interest.\n\n\nCode\n#Check the calculated correlations\ncor(rateprof, use = \"complete.obs\",method = c(\"pearson\", \"kendall\", \"spearman\"))\n\n\n                quality helpfulness   clarity  easiness raterInterest\nquality       1.0000000   0.9810314 0.9759608 0.5651154     0.4706688\nhelpfulness   0.9810314   1.0000000 0.9208070 0.5635184     0.4630321\nclarity       0.9759608   0.9208070 1.0000000 0.5358884     0.4611408\neasiness      0.5651154   0.5635184 0.5358884 1.0000000     0.2052237\nraterInterest 0.4706688   0.4630321 0.4611408 0.2052237     1.0000000\n\n\nOur correlation calculations reinforce our interpretation of the relationships based on the scatter plots."
  },
  {
    "objectID": "posts/HW3_StephRoberts.html#question-5",
    "href": "posts/HW3_StephRoberts.html#question-5",
    "title": "HW3",
    "section": "Question 5",
    "text": "Question 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(a)Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases (b) Summarize and interpret results of inferential analyses.\n\n\nCode\ndata(\"student.survey\")\ndim(student.survey)\n\n\n[1] 60 18\n\n\nCode\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n\n\n\nCode\n#Select variable in question\nssurvey <- student.survey %>%\n  select(c(pi, re, hi, tv))%>%\n  rename(political_id = pi, religiosity = re, hs_gpa = hi, tv_hours = tv)\n\n#Check for missing data\nis.na(ssurvey) %>% head()\n\n\n     political_id religiosity hs_gpa tv_hours\n[1,]        FALSE       FALSE  FALSE    FALSE\n[2,]        FALSE       FALSE  FALSE    FALSE\n[3,]        FALSE       FALSE  FALSE    FALSE\n[4,]        FALSE       FALSE  FALSE    FALSE\n[5,]        FALSE       FALSE  FALSE    FALSE\n[6,]        FALSE       FALSE  FALSE    FALSE\n\n\nFortunately, we have no missing data.\n\n\nCode\n#Summarize our df to understand responses and value ranges\nsummary(ssurvey)\n\n\n                political_id       religiosity     hs_gpa         tv_hours     \n very liberal         : 8    never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24    occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6    most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10    every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                      3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                      Max.   :4.000   Max.   :37.000  \n very conservative    : 2                                                      \n\n\n\n\nCode\n#Explore variable relationships\npairs(ssurvey)\n\n\n\n\n\nLet’s take a close look at (i) y = political ideology and x = religiosity.\n\n\nCode\n#Visualize religiosity as an explanatory variable for political ideology\nggplot(ssurvey, aes(x = religiosity, y= political_id,fill= political_id)) + \n  geom_bar(stat = \"identity\")+\n   labs(x=\"Religiosity\", y=\"Political Ideology\")+\n    theme(axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n\n\n\n\n\nThis bar graph shows us what we might expect - that those who attend church every week identify with a very conservative political affiliation. On the contrary, people who never or occasionally go to church trend more liberal.\n\n\nCode\n#Transform categorical data to numeric\nssurvey <- ssurvey %>%\n    mutate(pi_n = dplyr::recode(political_id, \n                         \"very liberal\" = 1,\n                         \"liberal\" = 2, \n                         \"slightly liberal\" = 3, \n                         \"moderate\" = 4,\n                         \"slightly conservative\" = 5,\n                         \"conservative\" = 6, \n                         \"very conservative\" = 7\n                         )) %>%\n    mutate(re_n = dplyr::recode(religiosity, \n                         \"never\" = 0,\n                         \"occasionally\" = 1,\n                         \"most weeks\" = 2,\n                         \"every week\" = 3\n                         )) \nglimpse(ssurvey)\n\n\nRows: 60\nColumns: 6\n$ political_id <ord> conservative, liberal, liberal, moderate, very liberal, l…\n$ religiosity  <ord> most weeks, occasionally, most weeks, occasionally, never…\n$ hs_gpa       <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.…\n$ tv_hours     <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, …\n$ pi_n         <dbl> 6, 2, 2, 4, 1, 2, 2, 2, 1, 3, 5, 2, 1, 4, 1, 2, 3, 2, 2, …\n$ re_n         <dbl> 2, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 3, 3, 0, 0, …\n\n\nCode\n#Visualize linearity\nggplot(data = ssurvey, aes(x = re_n, y = pi_n)) +\n  geom_point() +\n  geom_smooth(method = 'lm')+\n  labs(x=\"Religiosity\", y=\"Political Ideology\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAfter recoding our variable values, political ideology now spans from 1 to 7 with 1 being very liberal and 7 being very conservative. Religiosity spans 0 to 3 with 0 being never attending church and 3 representing attends every week.\n\n\nCode\n#Linear regression summary\n(summary(rel_pol <- lm(formula = pi_n ~ re_n, data = ssurvey)))\n\n\n\nCall:\nlm(formula = pi_n ~ re_n, data = ssurvey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.9012     0.2717   6.997 2.97e-09 ***\nre_n          0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nInterpretation: The residuals of this regression are fairly symmetrical, but they are not very small in relation to our data values. The coefficients tells us that for every 1 unit of increase in political affiliation (ie. 1 category closer to very conservative), there is an estimated 0.9704 increase in religiosity (ie. almost one category closer to every week). The t-values are moderately large, which indicates a relationship exists. With very small p-values, it is unlikely the relationship is due to chance. Therefore, we could reject the null and conclude there is a relationship between religiosity and political affiliation.\nHowever, as the ggplot graph, the high residuals, and the R2 (0.3359) tell us, this model is not a perfect fit for this data. It is likely due to the categorical nature of the explanatory variable that partitions our data into column-like sections in a graph. That is hard to run a straight line through. This model shows about 33% of the variance can be explained by the predictor. There may be other variable that need to be controlled for or added to the analysis to complete a well-fitting model.\nNow, let’s take a close look at (ii) y = high school GPA and x = hours of TV watching.\n\n\nCode\n#Visualize hours per week watching tv as an explanatory variable for high school GPA\nggplot(data = ssurvey, aes(x = tv_hours, y = hs_gpa)) +\n  geom_point() +\n  geom_smooth(method = 'lm')+\n  labs(x=\"TV Hours /Week\", y=\"High School GPA\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nCode\nsummary(lm(hs_gpa ~ tv_hours, data = ssurvey))\n\n\n\nCall:\nlm(formula = hs_gpa ~ tv_hours, data = ssurvey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv_hours    -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nInterpretation: This model shows fairly symmetrical residuals, but some are very large. This can be seen on our plot where lower hours of tv has a wide range of associated GPS values. The p-value of 0.0388 allows us to reject the null hypothesis and conclude there is a relationship between tv hours and gpa. However, with a VERY small R-squared, this model may not be best at predicting gpa SOLELY from hours of tv watched per week. The combination of all our calculations lead me to conclude that higher number of hours watched per week may be related to lower GPA. However, the opposite may not be true - fewer hours of tv is not necessarily associated with higher a higher GPA\n\n\nCode\ncor.test(ssurvey$tv_hours, ssurvey$hs_gpa)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  ssurvey$tv_hours and ssurvey$hs_gpa\nt = -2.1144, df = 58, p-value = 0.03879\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.48826914 -0.01457694\nsample estimates:\n       cor \n-0.2675115 \n\n\nThere is a weak negative correlation between hours of tv watched per week and high school GPA."
  },
  {
    "objectID": "posts/HW3_SteveONeill.html",
    "href": "posts/HW3_SteveONeill.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\ndata(\"UN11\")\ndata(\"water\")\ndata(\"Rateprof\")\ndata(\"student.survey\")\nSome of the questions use data from the alr4 and smss R packages. For the ALR book, the package is. You would need to install those packages in R (no need for an install.packages() call in your .qmd file, though—just use library()) and load the data using the data() function.\nQuestions:"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-1",
    "href": "posts/HW3_SteveONeill.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\nUnited Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n1.1.1. Identify the predictor and the response.\nThe independent (predictor) variable is ppgdp. The response (dependent) variable is fertility.\n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93\n\n\n\n1.1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\nThis data looks curvilinear. A straight-line mean function is not favored.\n\n\nCode\nlibrary(ggplot2)\nggplot(data = UN11, aes(x = ppgdp, y = fertility)) +\n  geom_point()\n\n\n\n\n\n\n1.1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\nYes, in the Log-Log model, linear regression seems more likely to work.\n\n\nCode\nlibrary(ggplot2)\nggplot(data = UN11, aes(x = log(ppgdp), y = log(fertility))) +\n  geom_point() +\n  ggtitle(\"Log-Log\") +\n  geom_smooth(method = 'lm', se = F)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-2",
    "href": "posts/HW3_SteveONeill.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n\n\nHow, if at all, does the slope of the prediction equation change?\n\n\nI assume “responses” mean responses to a survey. I have created fake survey data to understand the question:\n\n\nCode\nsatisfaction <- c(1,2,3,4,5)\nus_income <- c(20010,30450,40000,51000,75000)\nuk_income <- c(us_income / 1.33) \n\nmod_us <- lm(satisfaction ~ us_income)\nsummary(mod_us)\n\n\n\nCall:\nlm(formula = satisfaction ~ us_income)\n\nResiduals:\n       1        2        3        4        5 \n-0.29520 -0.05966  0.24105  0.43559 -0.32178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.700e-01  4.294e-01  -0.396    0.719   \nus_income    7.322e-05  9.092e-06   8.054    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3839 on 3 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9411 \nF-statistic: 64.86 on 1 and 3 DF,  p-value: 0.003999\n\n\nCode\nmod_uk <- lm(satisfaction ~ uk_income)\nsummary(mod_uk)\n\n\n\nCall:\nlm(formula = satisfaction ~ uk_income)\n\nResiduals:\n       1        2        3        4        5 \n-0.29520 -0.05966  0.24105  0.43559 -0.32178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.700e-01  4.294e-01  -0.396    0.719   \nuk_income    9.739e-05  1.209e-05   8.054    0.004 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3839 on 3 degrees of freedom\nMultiple R-squared:  0.9558,    Adjusted R-squared:  0.9411 \nF-statistic: 64.86 on 1 and 3 DF,  p-value: 0.003999\n\n\nIn the UK version, the slope is multiplied by 1.33.\n\n\nHow, if at all, does the correlation change?\n\n\nSince only the units are changing, the correlation does not change. See:\n\n\nCode\ncor(satisfaction,us_income)\n\n\n[1] 0.9776455\n\n\nCode\ncor(satisfaction,uk_income)\n\n\n[1] 0.9776455"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-3",
    "href": "posts/HW3_SteveONeill.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water)\n\n\n\n\n\npair() shows pairwise correlations. It looks like OPBC, OPRC, OPSLAKE, and BSAAM are very similar to each other. Likewise, APSLAK, APSAB, and APMAM are similar to each other. Taken separately, the two groups are not similar to each other. There is no pattern in the year of observation. I would be comfortable making predictions within the groups individually, but not based off the year."
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-4",
    "href": "posts/HW3_SteveONeill.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\nQuality, helpfulness, and clarity are all highly correlated. This tells me that perceptions of quality really hinge on helpfulness and clarity from the professor. However, classes which are high quality, helpful, and clear are not always easy (although there is some kind of positive correlation).\nTo some degree, it does look like each pairwise combination is positively correlated, with the possible exception of raterInterest and easiness.\n\n\nCode\nratings <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(ratings)"
  },
  {
    "objectID": "posts/HW3_SteveONeill.html#question-5",
    "href": "posts/HW3_SteveONeill.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n\n\n\nCode\n#Levels look to be correct already:\n#as.factor(student.survey$pi)\n\nsummary(lm(as.numeric(student.survey$pi) ~ as.numeric(student.survey$re)))\n\n\n\nCall:\nlm(formula = as.numeric(student.survey$pi) ~ as.numeric(student.survey$re))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     0.9308     0.4252   2.189   0.0327 *  \nas.numeric(student.survey$re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nCode\nsummary(lm(as.numeric(student.survey$hi) ~ as.numeric(student.survey$tv)))\n\n\n\nCall:\nlm(formula = as.numeric(student.survey$hi) ~ as.numeric(student.survey$tv))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                    3.441353   0.085345  40.323   <2e-16 ***\nas.numeric(student.survey$tv) -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n(a)Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n\nCode\n#as.numeric(student.survey$pi)\n\n#  (i) y = political ideology and x = religiosity\n\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point()\n\n\n\n\n\nCode\npi_re <- student.survey %>% select(pi, re)\npairs(pi_re)\n\n\n\n\n\nCode\n#Alternate way:\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  ggtitle (\"Religiosity and Political Ideology\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCode\n#(ii) y = high school GPA and x = hours of TV watching\n\nlibrary(ggplot2)\nggplot(data = student.survey, aes(x = hi, y = tv)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = F) +\n  ggtitle (\"High School GPA and TV watched (in hours)\")\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nCode\n#Alternate way:\n\nhi_tv  <- student.survey %>% select(hi, tv)\npairs(hi_tv)\n\n\n\n\n\n\n\nSummarize and interpret results of inferential analyses.\n\n\n\nPolitical Ideology ~ Religiosity\nPolitical ideology is slightly correlated with religiosity with an R-squared of 0.3359. The results are statistically significant. However, I would not say this is quite enough for predictive purposes because of that low R-squared value.\n\n\nHigh School GPA ~ TV Watched\nLooking at the scatterplot makes it appear that this data is all over the place with outliers. It does seem the higher GPA students do watch the least TV, and the linear regression is technically statistically significant with a p-value of 0.0388. However, the R-squared value is a very low 0.07156. I would not make any inferences about GPA’s causal effect on TV."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html",
    "href": "posts/HW3_ToryBartelloni.html",
    "title": "DACSS 603: Homework 3",
    "section": "",
    "text": "United Nations (Data file: UN11in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009…We will study the dependence of fertility on ppgdp.\n\n\nCode\nlibrary(alr4)\ndata(UN11)\n\n\n\n\nIdentify the predictor and the response.\nAnswer: The predictor variable in this scenario is ppgdp and the response or dependent variable will be fertility.\n\n\n\nDraw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\nUN11 %>%\n  ggplot(aes(x=ppgdp, y=fertility)) +\n  geom_point() +\n  theme_bw() +\n  labs(title=\"Scatterplot of Fertility and GDP Per Capita\", \n       subtitle=\"Data from United Nations circa 2009\",\n       x=\"GDP Per Capita USD\",\n       y=\"Children per Woman\")\n\n\n\n\n\nThe data do not appear to have a linear relationship. There is a steep decrease in fertility as GDP per capita increases from closer to 0 to about 15,000 and then a much slower rate of decrease to the upper bounds of the data.\n\n\n\nDraw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change.\n\n\nCode\nUN11 %>%\n  ggplot(aes(x=log(ppgdp), y=log(fertility))) +\n  geom_point() +\n  theme_bw() +\n  labs(title=\"Scatterplot of Fertility and GDP Per Capita\", \n       subtitle=\"Data from United Nations circa 2009\",\n       x=\"Log of GDP Per Capita USD\",\n       y=\"Log of Children per Woman\")\n\n\n\n\n\nThe relationship with the natural log of both variables appears to have a much closer resemblance to a linear relationship. From appearance only I would be confident a simple linear relationship could be applied to this data."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q2a",
    "href": "posts/HW3_ToryBartelloni.html#q2a",
    "title": "DACSS 603: Homework 3",
    "section": "Q2A",
    "text": "Q2A\nHow, if at all, does the slope of the prediction equation change?\nAnswer: The slope would decrease because the units used to measure x would increase, but the amount of change in y would stay the same."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q2b",
    "href": "posts/HW3_ToryBartelloni.html#q2b",
    "title": "DACSS 603: Homework 3",
    "section": "Q2B",
    "text": "Q2B\nHow, if at all, does the correlation change?\nAnswer: The correlation does not change. The old x and new x are perfectly correlated so would share the same exact relationship with y."
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q5a",
    "href": "posts/HW3_ToryBartelloni.html#q5a",
    "title": "DACSS 603: Homework 3",
    "section": "Q5A",
    "text": "Q5A\nGraphically portray how the explanatory variable relates to the outcome variable in each of the two cases\nWe will fist look at the relationship between political ideology and frequency of religious attendance.\n\n\nCode\nstudent.survey.hw %>%\n  ggplot() +\n  geom_point(aes(x=re, y=pi)) +\n  geom_smooth(aes(x=reg2, y=pol2), method=\"lm\") +\n  theme_bw() +\n  labs(title = \"Political Ideology and Religiosity\",\n       subtitle=\"How religious attendance relates to self-identified political ideology\",\n       x=\"Frequency of Religious Attendance\\n(Higher = Attends More Frequently)\",\n       y=\"Political Ideology\\n(Lower = More Liberal)\")\n\n\n\n\n\nNow we will look at the relationship between hours of typical number of hours of TV watched (weekly) and high school GPA.\n\n\nCode\nstudent.survey.hw %>%\n  ggplot(aes(x=tv, y=hi)) +\n  geom_point() +\n  geom_smooth(method=\"lm\") +\n  theme_bw() +\n  labs(title = \"High School GPA and TV Watching Habits\",\n       subtitle=\"How typical hours of TV watching relates to high school GPA\",\n       x=\"Hours of TV Watched\\n(Weekly Average)\",\n       y=\"High School GPA\")"
  },
  {
    "objectID": "posts/HW3_ToryBartelloni.html#q5b",
    "href": "posts/HW3_ToryBartelloni.html#q5b",
    "title": "DACSS 603: Homework 3",
    "section": "Q5B",
    "text": "Q5B\nSummarize and interpret results of inferential analyses.\nFirst, let’s examine the political ideology and reglious attendance model. Below is the summary of the model.\n\n\nCode\nsummary(pol_reg_mod)\n\n\n\nCall:\nlm(formula = pol2 ~ reg2, data = student.survey.hw)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nreg2          0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nThe model is statistically significant, with a < 0.01 p-value for religious attendance. A small amount of the variance in political ideology is explained by the frequency of religious attendance with an R2 of 0.336. The relationship is positive with a one unit increase in religious attendance frequency resulting in an expected increase (toward more conservative) of 0.97 units in political ideology. What we can infer from this model is that people who attend religious services more often are expected to be more conservative in their political ideology. Without more information I would not conclude this is a causal relationship and would want to further examine the relationship, including whehter the relationship could be inferred in the reverse.\nNow let’s examine the high school GPA and TV watching habits model. Below is the summary of the model.\n\n\nCode\nsummary(gpa_tv_mod)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey.hw)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nThe model here is statistically significant with a p = 0.039 so under the 0.05 standard threshold. On the other hand, the model explans very little of the variation in high school GPA with an R2 of 0.072. The relationship is negative with a one hour increase in average weekly TV watching resulting in an expected decrease in high school GPA of 0.02. With such a low R2 I would hesitate to conclude any strong evidence in an explanatory relationship, which observing that they do have a statistically significant correlation."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html",
    "href": "posts/HW3_Yakub Rabiutheen.html",
    "title": "Homework 3",
    "section": "",
    "text": "The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\n\n\nCode\n##load data\ndata(UN11)"
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#question-2.b",
    "href": "posts/HW3_Yakub Rabiutheen.html#question-2.b",
    "title": "Homework 3",
    "section": "Question-2.b",
    "text": "Question-2.b\nHow, if at all, does the correlation change?\n\n\nCode\ncor.test(usdollar,pound)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  usdollar and pound\nt = 189812531, df = 8, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1 \n\n\nCurrency Changes do not affect correlation."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#question-3",
    "href": "posts/HW3_Yakub Rabiutheen.html#question-3",
    "title": "Homework 3",
    "section": "Question-3",
    "text": "Question-3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\n\nCode\npairs(water_supply,main = \"Sierra Southern California Water Supply Runoff\",\n      pch = 21, bg = \"green\")\n\n\nError in pairs(water_supply, main = \"Sierra Southern California Water Supply Runoff\", : object 'water_supply' not found\n\n\n\n\nCode\nlm_water_supply<-lm(BSAAM~APMAM+APSAB+APSLAKE+OPBPC+OPRC+OPSLAKE,data = water)\nsummary(lm_water_supply)\n\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\nThe following variables OPBPC, OPRC, OPSLAKE are correlated with each other."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#q.4",
    "href": "posts/HW3_Yakub Rabiutheen.html#q.4",
    "title": "Homework 3",
    "section": "Q.4",
    "text": "Q.4\n\n\nCode\ndata(\"Rateprof\")\npairs(~Rateprof$quality+Rateprof$helpfulness+Rateprof$clarity+Rateprof$easiness+Rateprof$raterInterest, lwd=2, labels = c(\"QUALITY\", \"HELPFULNESS\", \"CLARITY\", \"EASINESS\", \"Rater INTEREST\"), pch=19, cex = 0.75, col = \"blue\")\n\n\n\n\n\nThe following variables “quality”, “clarity”, and “helpfulnes are correlated with each other."
  },
  {
    "objectID": "posts/HW3_Yakub Rabiutheen.html#q.5",
    "href": "posts/HW3_Yakub Rabiutheen.html#q.5",
    "title": "Homework 3",
    "section": "Q.5",
    "text": "Q.5\n\n\nCode\n##load data\ndata(student.survey)\npi_conv <- as.numeric(student.survey$pi)\nre_conv <- as.numeric(student.survey$re)\n##run regression analysis\nmodel1 <- lm(pi_conv ~ re_conv, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_conv ~ re_conv, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre_conv       0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\nCode\n##run regression analysis\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\nCode\nlibrary(smss)\ndata(\"student.survey\")\nggplot(data=student.survey,aes(x=re,fill=pi))+\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\n\n\n\n\n\nAs shown in the graph,there is a strong correlation association between religiosity and Political Idealogy.\n\n\nCode\ndata(\"student.survey\")\nggplot(data=student.survey,aes(x=hi, y=tv)) +\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")  \n\n\n\n\n\nThere is very little relationship betweeh watching TV and High School GPA.\n\n\nCode\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n\n                     pi                re           hi              tv        \n very liberal         : 8   never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24   occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10   every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                     3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                     Max.   :4.000   Max.   :37.000  \n very conservative    : 2"
  },
  {
    "objectID": "posts/HW4.html",
    "href": "posts/HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "New Home\n\n\nCode\n116.132*(3000) + 17505.416\n\n\n[1] 365901.4\n\n\n\n\nCode\n116.132*(3000) - 40230.867\n\n\n[1] 308165.1\n\n\nD.Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nNew_Price_Model <- lm(Price ~ Size + New + Size*New, data = house.selling.price)\nsummary(New_Price_Model)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nE.Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nThe New Size variable has a high significance and Positive Coefficient.This shows both variables together are is a much better combination.\nF. Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\n104.438*(3000) + 61.916*(3000) - 100755.31\n\n\n[1] 398306.7\n\n\n\n\nCode\n104.438*(3000) - 22227.808\n\n\n[1] 291086.2\n\n\nG.Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\n# New Home\n104.438*(1500) + 61.916*(1500) - 100755.31\n\n\n[1] 148775.7\n\n\n\n\nCode\n# Old Home \n104.438*(1500) - 22227.808\n\n\n[1] 134429.2\n\n\nNewer and Larger homes are the most expensive whereas old smaller houses are least Expensive.\n3H.Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nThe second model I think is better as it has more variability and accounts for the fact that combining Size and New is much better, the signifiance is greatly improved."
  },
  {
    "objectID": "posts/HW4answers-DonnySnyder.html",
    "href": "posts/HW4answers-DonnySnyder.html",
    "title": "Homework 4",
    "section": "",
    "text": "Question 1\n\n\nCode\npredictSell <- -10536 + (53.8 * 1240) + (2.84 * 18000)\nrealSell <- 145000\nresidual <- realSell - predictSell\nratioLottoHome <- 53.8/2.84\n\n\n#Question 1A The predicted selling price is 107,296 dollars, the residual is 37,704 dollars. The predicted selling price undershot the actual selling price.\n#Question 1B With a fixed lot size, the house selling price will increase by 53.8 for each square foot. This is because as the house is getting bigger, the house is selling for more, because the house is more valuable than the empty lot space.\n#Question 1C The lot would have to increase ~18.94 square feet to have the same impact as a one square foot increase size in the home.\n#Question 2\n\n\nCode\ndata <- salary\n\nmodel1 <- lm(salary~sex, data = data)\nsummary(model1)\n\n\n\nCall:\nlm(formula = salary ~ sex, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nCode\nmodel2 <- lm(salary~degree+rank+sex+year+ysdeg, data = data)\nsummary(model2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\nconfint(model2)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nCode\ndata$rank <- factor(data$rank, levels = c(\"Prof\", \"Assoc\", \"Asst\"))\n\nmodel3 <- lm(salary~degree+rank+sex+year+ysdeg, data = data)\nsummary(model3)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\nmodel4 <- lm(salary~degree+sex+year+ysdeg, data = data)\nsummary(model4)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nCode\ndata$dean <- NA\n\nx = 1\nwhile(x < 53){\n  if(data$ysdeg[x] > 15){\n    data$dean[x] = 0\n  }\n  else{\n    data$dean[x] = 1\n  }\n x = x + 1   \n}\n\nmodel5 <- lm(data = data, salary~sex+year+dean+degree)\nsummary(model5)\n\n\n\nCall:\nlm(formula = salary ~ sex + year + dean + degree, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10740.1  -2550.1     -3.3   1942.4  11718.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  22598.7     1792.1  12.610  < 2e-16 ***\nsexFemale     -523.5     1355.1  -0.386 0.701017    \nyear           531.4      130.2   4.082 0.000172 ***\ndean         -4449.8     1347.2  -3.303 0.001834 ** \ndegreePhD    -1186.6     1191.2  -0.996 0.324267    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3958 on 47 degrees of freedom\nMultiple R-squared:  0.5878,    Adjusted R-squared:  0.5527 \nF-statistic: 16.75 on 4 and 47 DF,  p-value: 1.338e-08\n\n\n\n\nQuestion 2A\nAs shown in model1, it seems like the mean salary is the same, as the null hypothesis that there is no difference cannot be rejected with the p-value of 0.0706.\n\n\nQuestion 2B\nThe 95% confidence interval for pay differences between males and females is between -697.8183 and 3030.56452.\n\n\nQuestion 2C\nThere is demonstrated statistically significant evidence that rank and years in current rank show significant results on an increase in salary. Rank as a professor shows the strongest effect, with rank as an associate professor also showing a high effect, as well as years in current rank. Rank has the highest slope for statistically significantly raising salary, particularly among full professors. All other relationships are not statistically significant.\n\n\nQuestion 2D\nChanging the baseline changes the direction of the relationship for associate and assistant professors. It shows that relative to full professors, associate, and assistant professors receive less.\n\n\nQuestion 2E\nExcluding rank makes the degreePhD and years after degree variables more important, as rank was likely previously explaining their variance. Without being able to rely on rank, these serve as proxies for professors receiving more money due to being a higher rank. however, there is also no statistically significant of discrimination based on sex.\n\n\nQuestion 2F\nThere is actually some evidence that the dean has been making less generous offers than previously, per model 5. Multicollinearity can make it harder to make inferences. I avoided it by excluding degree and years after degree variables, as rank explains salary better than those.\n\n\nQuestion 3\n\n\nCode\nsummary(house.selling.price)\n\n\nError in summary(house.selling.price): object 'house.selling.price' not found\n\n\nCode\ndata2 <- house.selling.price\n\n\nError in eval(expr, envir, enclos): object 'house.selling.price' not found\n\n\nCode\nmodel1 <- lm(data = data2, Price~Size+New)\n\n\nError in is.data.frame(data): object 'data2' not found\n\n\nCode\nsummary(model1)\n\n\n\nCall:\nlm(formula = salary ~ sex, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nCode\npredictNew <- (116.132*3000) + 57736.283*1 - 40230.867\n\npredictNotNew <- (116.132*3000) + 57736.283*0 - 40230.867\n\nmodel2 <- lm(data = data2, Price~Size+New+Size*New)\n\n\nError in is.data.frame(data): object 'data2' not found\n\n\nCode\nsummary(model2)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\npredictNew2 <- 104.438*3000 - 78527.502 + 61.916*3000 - 22227.808\n\npredictNotNew2 <- 104.438*3000 - 22227.808\n\npredictNew3 <- 104.438*1500 - 78527.502 + 61.916*1500 - 22227.808\n\npredictNotNew3 <- 104.438*1500 - 22227.808\n\n\n\n\nQuestion 3A\nSize of home and how new the home is both statistically significantly increase the selling prices of homes.\n\n\nQuestion 3B\nNew Home Price = 116.132(size in square feet) + 57736.283(if New) - 40230.867\nNot New Home Price = 116.132(size in square feet) - 40230.867\n\n\nQuestion 3C\nNew Home Prediction = 365901.416 No New Home Prediction = 308165.133\n\n\nQuestion 3D\nIt seems like Size and New have a positive interaction effect. Being a new home and being larger are interrelated. This also removes the statistical significance of being new.\n\n\nQuestion 3E\nNew home Prediction = 104.438(size in square feet) - 78527.502(if new) + 61.916(new times size) - 22227.808\nNot new home Prediction = 104.438(size in square feet) + 22227.808\n\n\nQuestion 3F\nNew Home Prediction = 398306.69\nNo New Home Prediction = 291086.192\n\n\nQuestion 3G\nNew Home Prediction = 148775.69\nNo New Home Prediction = 134429.192\nAs size of home gets smaller, newness tends to matter less towards increasing the predicted selling price, so the disparity between new and not new homes tends to decrease.\n\n\nQuestion 3H\nI think that I prefer the interaction model. It is more responsive to size than just merely adding over 50k for being either new or not new, in a binary. It’s good to have more of a gradient measure like this interaction provides in multiple ways."
  },
  {
    "objectID": "posts/hw4_boonstra.html",
    "href": "posts/hw4_boonstra.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-a",
    "href": "posts/hw4_boonstra.html#part-a",
    "title": "Homework 4",
    "section": "Part A",
    "text": "Part A\n\n\nCode\nq1_a_price <- 145000\nq1_a_predict <- (53.8*1240) + (2.84*18000)\nprint(c(\"Predicted price = \",q1_a_predict))\n\n\n[1] \"Predicted price = \" \"117832\"            \n\n\nCode\nresidual <- q1_a_price - q1_a_predict\nprint(c(\"Residual = \",residual))\n\n\n[1] \"Residual = \" \"27168\"      \n\n\nAccording to the model, predicted selling price was roughly 118000 USD. The residual of roughly 28000 means that the model underpredicted the selling price by roughly 28000 USD."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-b",
    "href": "posts/hw4_boonstra.html#part-b",
    "title": "Homework 4",
    "section": "Part B",
    "text": "Part B\nFor fixed lot size, the house price is expected to increase by 53.8 USD as the square footage of the house itself increases in 1. This is because the coefficient for the home size square footage is 53.8."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-c",
    "href": "posts/hw4_boonstra.html#part-c",
    "title": "Homework 4",
    "section": "Part C",
    "text": "Part C\nLot size would need to increase by 18.943662 to have the same impact as a one-square-foot increase in home size."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-a-1",
    "href": "posts/hw4_boonstra.html#part-a-1",
    "title": "Homework 4",
    "section": "Part A",
    "text": "Part A\n\n\nCode\nsalary_men <- salary %>% \n  filter(sex == \"Male\")\nsalary_women <- salary %>% \n  filter(sex == \"Female\")\n\nt.test(salary_men$salary,salary_women$salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary_men$salary and salary_women$salary\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\nmean of x mean of y \n 24696.79  21357.14 \n\n\nThe findings of a two-sample Welch’s t-test comparing salary by sex are inconclusive; the difference in means is not significant at a 95% confidence level, but is significant at a 90% confidence level, suggesting that further investigation (i.e. multiple regression) could yield significant results."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-b-1",
    "href": "posts/hw4_boonstra.html#part-b-1",
    "title": "Homework 4",
    "section": "Part B",
    "text": "Part B\n\n\nCode\nq2_lm <- lm(\n  salary ~\n    .,\n  data = salary\n)\nsummary(q2_lm)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe 95% confidence interval for the coefficient of sexFemale is the range between -481.0138205 on the low end and 2813.7538205 on the high end. Because this range of values passes from negative to positive (i.e. crosses 0), we say that the result is insignificant at a 95% confidence level."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-c-1",
    "href": "posts/hw4_boonstra.html#part-c-1",
    "title": "Homework 4",
    "section": "Part C",
    "text": "Part C\n1.5746048^{4} – All other things being equal, a professor at this university could be expected to earn $15746.05. This coefficient is significant beyond a 99% confidence level.\n1388.6133186 – A professor with a PhD would be expected to make $1,388.61 more than one with a Master’s degree. However, this coefficient is not significant at a 95% confidence level.\n5292.3607713 – An Associate Professor would be expected to make $5292.36 more than an Assistant Professor. This coefficient is significant at a 95% confidence level.\n1.1118764^{4} – A Full Professor would be expected to make $11118.76 more than an Assistant Professor. This coefficient is significant at a 95% confidence level.\n1166.373101 – A female professor would be expected to make $1166.37 more than a male professor based on this model. However, the coefficient is not significant at a 95% confidence level. The direction of the sign, and the lack of significance, would both help to discredit the notion that female professors earn less than male professors at this university systemically.\n476.3090151 – Each additional year of experience in one’s current rank would be expected to earn a professor an additional $476.31 per year. This coefficient is significant at a 95% confidence level.\n-124.5743208 – A professor would be expected to earn $124.57 less per year based on each year since they earned their highest degree according to this model. However, this coefficient is not significant at a 95% confidence level, which is good, because this effect wouldn’t make much sense when considering the real-world meaning of the coefficient."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-d",
    "href": "posts/hw4_boonstra.html#part-d",
    "title": "Homework 4",
    "section": "Part D",
    "text": "Part D\n\n\nCode\nsalary$rank <- factor(salary$rank, levels = c(\"Prof\",\"Asst\",\"Assoc\"))\n\nsummary(lm(\n  salary ~\n    .,\n  data = salary\n))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nReordering the rank variable to put the “Prof” level first yields the above regression table. As in the first regression, Assistant Professors are here expected to make $11118.76 less per year than Full Professors. Associate Professors are expected to make $5826.40 less per year than Full Professors. Both of these coefficients are significant at a 95% confidence level."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-e",
    "href": "posts/hw4_boonstra.html#part-e",
    "title": "Homework 4",
    "section": "Part E",
    "text": "Part E\n\n\nCode\nsummary(lm(\n  salary ~\n    . - rank,\n  data = salary\n))\n\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWith rank excluded, ysdeg becomes positive and significant. The coefficient for sexFemale is now negative, but is still not significant."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-f",
    "href": "posts/hw4_boonstra.html#part-f",
    "title": "Homework 4",
    "section": "Part F",
    "text": "Part F\n\n\nCode\nsalary_dean <- salary %>% \n  mutate(dean = case_when(\n    ysdeg <= 15 ~ 1,\n    T ~ 0\n  ))\n\nsummary(lm(\n  salary ~\n    . - ysdeg,\n  data = salary_dean\n))\n\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salary_dean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24425.32    1107.52  22.054  < 2e-16 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\ndean          2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nBecause dean is based on ysdeg, and because year and ysdeg measure overlapping lengths of time, I excluded ysdeg for this model. The results are similar to those above, most notably in the lack of significance for the coefficient of sexFemale."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-a-2",
    "href": "posts/hw4_boonstra.html#part-a-2",
    "title": "Homework 4",
    "section": "Part A",
    "text": "Part A\n\n\nCode\nq3_lm <- lm(\n  Price ~ \n    Size + New,\n  data = house.selling.price\n)\nsummary(q3_lm)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nAll coefficients are significant. The intercept (i.e. a theoretical house of no size that is not new) is -$42390.87. Each square foot increases house price by $116.13. A new house would be expected to sell for $57736.28 more than an old house of equal size."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-b-2",
    "href": "posts/hw4_boonstra.html#part-b-2",
    "title": "Homework 4",
    "section": "Part B",
    "text": "Part B\n\\(y\\) is equal to predicted selling price in USD, and \\(x\\) is equal to house size in square feet.\nNew home:\n\\(y = 17505.42 + 116.13x\\)\nOld home:\n\\(y = -40230.87 + 116.132x\\)"
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-c-2",
    "href": "posts/hw4_boonstra.html#part-c-2",
    "title": "Homework 4",
    "section": "Part C",
    "text": "Part C\n\n\nCode\nq3_predict <- data.frame(\n  Size = c(3000,3000),\n  New = c(1,0)\n)\npredict(q3_lm,newdata=q3_predict)\n\n\n       1        2 \n365900.2 308163.9 \n\n\nThe new house would be expected to sell for $365900, while the not-new house would be expected to sell for about $308000."
  },
  {
    "objectID": "posts/hw4_boonstra.html#part-d-1",
    "href": "posts/hw4_boonstra.html#part-d-1",
    "title": "Homework 4",
    "section": "Part D",
    "text": "Part D\n\n\nCode\nq3_lm_interact <- lm(\n  Price ~ \n    Size * New,\n  data = house.selling.price\n)\nsummary(q3_lm_interact)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nThe intercept and New values are no longer significant. Selling price is expected to increase by $104 per square foot for all houses, and an additional $62 per square foot for new houses."
  },
  {
    "objectID": "posts/HW4_CalebHill.html",
    "href": "posts/HW4_CalebHill.html",
    "title": "Homework 4",
    "section": "",
    "text": "First, let’s load the relevant libraries and set all the graph themes to minimal.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\ntheme_minimal()\n\n\n\n\nThe prediction equation for the following three subsections is: ŷ = −10,536 + 53.8x1 + 2.84x.\n\n\nCode\nx <- sum(-10536 + (53.8*1240) + (2.84*18000))\ny <- 145000\n\ny - x\n\n\n[1] 37704\n\n\nThe predicted sale price is $107,296.That is a difference (residual) of $37,704.\n\n\n\n\n\nCode\nx <- sum(-10536 + (53.8*500) + (2.84*1000))\ny <- sum(-10536 + (53.8*501) + (2.84*1000))\ny - x\n\n\n[1] 53.8\n\n\nFor a fixed lot size, the house selling price is predicted to increase $53.80 per each square-foot increase. This is because we are multiplying the size of the home (in square feet) by $53.80.\n\n\n\n\n\nCode\nsum(53.8/2.84)\n\n\n[1] 18.94366\n\n\nLot size would have to increase by almost 19 square feet to have the same impact as a one-square-foot increase in home size."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#a-1",
    "href": "posts/HW4_CalebHill.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nt.test(salary ~ sex, salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nWhile there is a difference in salary by Sex, the p-value does not meet the threshold for statistical significance (0.05). We cannot reject the null hypothesis."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#b-1",
    "href": "posts/HW4_CalebHill.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nsalary$sex <- relevel(salary$sex, ref = 1)\nmodel1 <- lm(salary ~ sex + rank + degree + year + ysdeg, data = salary)\nconfint(model1, level = 0.95)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\nsexFemale    -697.8183  3030.56452\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\ndegreePhD    -663.2482  3440.47485\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% CI for the female sex’s impact on salary is between -697.82 to 3030.56.\n\n\nCode\nsalary$sex <- relevel(salary$sex, ref = 2)\n\nmodel_relevel_sex <- lm(salary ~ sex + rank + degree + year + ysdeg, data = salary)\nconfint(model_relevel_sex, level = 0.95)\n\n\n                 2.5 %      97.5 %\n(Intercept) 15268.0220 18556.81956\nsexMale     -3030.5645   697.81832\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\ndegreePhD    -663.2482  3440.47485\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% CI for the male sex’s impact on salary is between -3030.56 to 697.82."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#c-1",
    "href": "posts/HW4_CalebHill.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nsummary(model1)\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThree variables reach statistical significance (low p-value): rankAssoc, rankProf, and year. SexFemale does not reach statistical significance.\nYear has the lowest estimate and standard error, about 5x to 10x less than the other two variables. The t-value is less than that of rankProf, but it is still the second highest."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#d",
    "href": "posts/HW4_CalebHill.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = 3)\n\nmodel_relevel_rank <- lm(salary ~ rank + degree + sex + year + ysdeg, data = salary)\nsummary(model_relevel_rank)\n\n\n\nCall:\nlm(formula = salary ~ rank + degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  28031.18    1677.06  16.715  < 2e-16 ***\nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nsexMale      -1166.37     925.57  -1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWe excluded Assoc from the rank variable in this relevel and included Asst and Prof. This has shown a positive relationship between rank and salary for Prof, but not for Asst, and the variables are statistically significant at the 0.001 scale for both of them. The standard error is also lower for both compared Assoc, though not by much."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#e",
    "href": "posts/HW4_CalebHill.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n\nCode\nmodel2 <- lm(salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(model2)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15897.03    1259.87  12.618  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexMale      1286.54    1313.09   0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding rank, we now see that ysdeg is the best predictor variable, with the lowest estimate score, standard error, and meets the 0.001 p-value threshold to be statistically significant."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#f",
    "href": "posts/HW4_CalebHill.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary$dean <- ifelse(salary$ysdeg >= '15', \"Old Dean\",\n                  \"New Dean\")\ntable(salary$dean)\n\n\n\nNew Dean Old Dean \n      11       41 \n\n\nCode\nmodel3 <- lm(salary ~ dean, data = salary)\nsummary(model3)\n\n\n\nCall:\nlm(formula = salary ~ dean, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9311.1 -4185.9  -573.6  3931.8 13383.9 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     20580       1727  11.913 3.23e-16 ***\ndeanOld Dean     4082       1945   2.098    0.041 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5729 on 50 degrees of freedom\nMultiple R-squared:  0.08091,   Adjusted R-squared:  0.06253 \nF-statistic: 4.402 on 1 and 50 DF,  p-value: 0.04097\n\n\nIt looks like those hired prior to the new Dean do have a statistically significant impact on salary, though at a minor code of 0.05. What’s interesting though is the positive relationship, which rejects the belief that the new Dean has been making more generous offers to new hires. However, we should add some control variables, making sure to avoid multicollinearity. Three variables that would impact multicollinearity are rank, ysdeg, and year.\n\n\nCode\nmodel4 <- lm(salary ~ dean + degree + sex, data = salary)\nsummary(model4)\n\n\n\nCall:\nlm(formula = salary ~ dean + degree + sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8919.1 -4457.4  -344.1  3386.5 15921.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     18088       2159   8.379 5.92e-11 ***\ndeanOld Dean     4035       1921   2.100   0.0410 *  \ndegreePhD         345       1654   0.209   0.8357    \nsexMale          3296       1768   1.864   0.0685 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5640 on 48 degrees of freedom\nMultiple R-squared:  0.145, Adjusted R-squared:  0.09157 \nF-statistic: 2.714 on 3 and 48 DF,  p-value: 0.05514\n\n\nThe standard error drops, but everything else remains very similar."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#a-2",
    "href": "posts/HW4_CalebHill.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\ndata(house.selling.price)\n\nmodel5 <- lm(Price ~ Size + New, house.selling.price)\nsummary(model5)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nSize is statistically significant (SS) at the 0.001 level, while New is SS at the 0.01 level. Size has a small standard error and larger t-value, while New has a larger estimate, standard error, and lower t-value. We could relevel to see if Old houses had a better impact on Price but that can be observed in other questions.\n\n\nCode\nhouse.selling.price$New <- relevel(factor(house.selling.price$New), ref = 1)\n\nmodel_relevel_new <- lm(Price ~ Size + New, house.selling.price)\nsummary(model_relevel_new)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew1         57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_CalebHill.html#b-2",
    "href": "posts/HW4_CalebHill.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nThe prediction equation for houses is as follows:\nNew: Predicted Price = -40,230 + 116.13(Size) + 57,736(New).\nOld: Predicted Price = -40,230 + 116.13(Size) + -57,736(Old)."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#c-2",
    "href": "posts/HW4_CalebHill.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nsum(-40230 + (116.13*3000) + (57736*1))\n\n\n[1] 365896\n\n\nCode\nsum(-40230 + (116.13*3000) + (-57736*1))\n\n\n[1] 250424\n\n\nFor a new house, the predicted selling price is $365,896. For an old house, it’s $250,424."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#d-1",
    "href": "posts/HW4_CalebHill.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nmodel6 <- lm(Price ~ Taxes + Size + New, house.selling.price)\nsummary(model6)\n\n\n\nCall:\nlm(formula = Price ~ Taxes + Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-165501  -25426    1449   20536  168747 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -21353.776  13311.487  -1.604  0.11196    \nTaxes           37.231      6.735   5.528 2.78e-07 ***\nSize            61.704     12.499   4.937 3.35e-06 ***\nNew1         46373.703  16459.019   2.818  0.00588 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47170 on 96 degrees of freedom\nMultiple R-squared:  0.7896,    Adjusted R-squared:  0.783 \nF-statistic: 120.1 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_CalebHill.html#e-1",
    "href": "posts/HW4_CalebHill.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nThe predicted selling price for the new model for new homes is as follows:\nNew: Predicted Price = -21,353 + 37.23(Taxes) + 61.74(Size) + 46,373.70(New)\nOld: Predicted Price = -21,353 + 37.23(Taxes) + 61.74(Size) + -46,373.70(Old)"
  },
  {
    "objectID": "posts/HW4_CalebHill.html#f-1",
    "href": "posts/HW4_CalebHill.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsum(-21353 + (37.23*1) + (61.74*3000) + (46373.70*1))\n\n\n[1] 210277.9\n\n\nCode\nsum(-21353 + (37.23*1) + (61.74*3000) + (-46373.70*1))\n\n\n[1] 117530.5\n\n\nThe predicted selling price for a new 3000 square foot house is $210,277.90 and for an old house is $117,530.50."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#g",
    "href": "posts/HW4_CalebHill.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n\nCode\nsum(-21353 + (37.23*1) + (61.74*1500) + (46373.70*1))\n\n\n[1] 117667.9\n\n\nCode\nsum(-21353 + (37.23*1) + (61.74*1500) + (-46373.70*1))\n\n\n[1] 24920.53\n\n\nThe predicted selling price for a new 1500 square foot house is $117,667.90 and for an old one is $24,920.53. For each square foot increase, we have a dollar increase of $61.74."
  },
  {
    "objectID": "posts/HW4_CalebHill.html#h",
    "href": "posts/HW4_CalebHill.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nI prefer the model that includes taxes. This reduces both the residuals and the standard error for the original variables, Size and New. With a smaller standard error, we should have a more accurate prediction value when attempting to ascertain what the potential sales price for a house is. There is also a larger adjusted R squared percentage. With all that, I would include Taxes in the model for the best prediction value."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html",
    "href": "posts/HW4_EmmaRasmussen.html",
    "title": "Homework 4",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(alr4)\nlibrary(smss)"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#a.",
    "href": "posts/HW4_EmmaRasmussen.html#a.",
    "title": "Homework 4",
    "section": "1a.",
    "text": "1a.\nPrediction equation: ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n#plugging in size of home and lot size into prediction equation\n-10536 + 53.8*1240 + 2.84*18000\n\n[1] 107296\n\n#Calculating residual, actual-predicted\n145000-107296\n\n[1] 37704\n\n\nPredicted selling price $107,296 Residual: $37,704"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#b.",
    "href": "posts/HW4_EmmaRasmussen.html#b.",
    "title": "Homework 4",
    "section": "1b.",
    "text": "1b.\nAccording to the prediction equations, for a fixed lot size, the price of the house is predicted to increase $53.80 per square foot of house size."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#c.",
    "href": "posts/HW4_EmmaRasmussen.html#c.",
    "title": "Homework 4",
    "section": "1c.",
    "text": "1c.\n\n53.8/2.84\n\n[1] 18.94366\n\n\nFor a fixed home size, lot size would need to increase by 18.94366 feet to have the same impact as a one square foot increase in home size.\n\n#Check: \n-10536 + 53.8*1400 + 2.84*20000\n\n[1] 121584\n\n-10536 + 53.8*1400+ 2.84*20018.94366\n\n[1] 121637.8\n\n121637.8-121584\n\n[1] 53.8"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#a.-1",
    "href": "posts/HW4_EmmaRasmussen.html#a.-1",
    "title": "Homework 4",
    "section": "2a.",
    "text": "2a.\nH0: Salary(male)=Salary(female) Ha:Salary(male) not equal to Salary(female)\n\ndata(salary)\nhead(salary)\n\n   degree rank    sex year ysdeg salary\n1 Masters Prof   Male   25    35  36350\n2 Masters Prof   Male   13    22  35350\n3 Masters Prof   Male   10    23  28200\n4 Masters Prof Female    7    27  26775\n5     PhD Prof   Male   19    30  33696\n6 Masters Prof   Male   16    21  28516\n\nt.test(salary ~ sex, data=salary)\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nAccording to this two sample t.test, there is not evidence for a difference in salary between male and female professors at the 5% significance level. At the 10% significance level, there is a difference."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#b.-1",
    "href": "posts/HW4_EmmaRasmussen.html#b.-1",
    "title": "Homework 4",
    "section": "2b.",
    "text": "2b.\n\n#creating a model\nsummary(lm(salary ~ degree + rank + sex + year + ysdeg, data=salary))\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n#assigning model to an object\nprof_fit_1<- lm(salary ~ degree + rank + sex + year + ysdeg, data=salary)\n\n#Creating a confidence interval for coefficients in the model\nconfint(prof_fit_1)\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% confidence interval for the difference in salary between male and females is -697.82 and 3030.56."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#c.-1",
    "href": "posts/HW4_EmmaRasmussen.html#c.-1",
    "title": "Homework 4",
    "section": "2c.",
    "text": "2c.\nDegree: The p-value for degree is not statistically significant. However, acccoring to this regression equation, for a faculty member with a PhD, their predicted salary is $1,388.61 higher than a faculty member with a masters degree (all other variables held constant).\nRank: The baseline category is Asst professor. For a faculty member of rank Associate, all other variables held constant, their predicted salary is $5,292.36 more than an Asst Professor.\nFor a faculty member of rank Professor, the predicted salary is $11,118.76 more than a faculty of rank Asst Professor (all other variables held constant).\nThese salary differences are statistically significant at the 0.0001 alpha level for both Asst and Professor rank.\nSex: For a faculty member who is female, their predicted salary is $1166.37 more than a faculty member who is male. However, his coefficient is not statistically significant at any alpha level.\nYear: For each year in their current rank, the salary is expected to increase by $478.31. The coeffiticent is significant at the 0.0001 alpha level.\nysdegree: For eah year after completion of their highest degree, salary is expected to decrease by $124.57. However this coefficient is not significant at any alpha level."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#d.",
    "href": "posts/HW4_EmmaRasmussen.html#d.",
    "title": "Homework 4",
    "section": "2d.",
    "text": "2d.\n\nsalary$rank<- relevel(salary$rank, ref = 'Assoc')\nsummary(lm(salary ~ degree + rank + sex + year + ysdeg, data=salary))\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe baseline category is now Assoc. According to these coefficients, faculty of rank asst are expected to make $5292.36 less than Associate professors. Faculty of rank Professor are expected to make $5826.40 more than Associate professors."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#e.",
    "href": "posts/HW4_EmmaRasmussen.html#e.",
    "title": "Homework 4",
    "section": "2e.",
    "text": "2e.\n\nsummary(lm(salary ~ degree + sex + year + ysdeg, data=salary))\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding the rank variable reveals a difference between male and female salaries with females making $1286.54 less than men. However, this difference is not signficant at any standard alpha levels."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#f.",
    "href": "posts/HW4_EmmaRasmussen.html#f.",
    "title": "Homework 4",
    "section": "2f.",
    "text": "2f.\n\n#creating a dummy variable new and old dean\nsalary<-mutate(salary, dean= case_when(ysdeg < 15 ~\"new\",\n                               ysdeg >=15 ~\"old\"))\n\n\nsummary(lm(salary ~ dean + degree + sex + rank +year, data=salary))\n\n\nCall:\nlm(formula = salary ~ dean + degree + sex + rank + year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3588.0 -1532.2  -232.2   565.7  9132.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20468.7      951.7  21.507  < 2e-16 ***\ndeanold      -2421.6     1187.9  -2.038   0.0474 *  \ndegreePhD     1073.5      843.3   1.273   0.2096    \nsexFemale     1046.7      858.0   1.220   0.2289    \nrankAsst     -5012.5     1002.3  -5.001 9.16e-06 ***\nrankProf      6213.3     1045.0   5.946 3.76e-07 ***\nyear           450.7       81.5   5.530 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2360 on 45 degrees of freedom\nMultiple R-squared:  0.8597,    Adjusted R-squared:  0.841 \nF-statistic: 45.95 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nI excluded ysdegree after creating the new variable to prevent multicollinearity. (Because one variable is made from the other.)\nAccording to this equation, faculty hired by the old dean make $2421.60 less than new faculty when we control for other factors. This is significant at the 0.05 alpha level."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#a.-2",
    "href": "posts/HW4_EmmaRasmussen.html#a.-2",
    "title": "Homework 4",
    "section": "3a.",
    "text": "3a.\n\ndata(house.selling.price)\nhouse.selling.price\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\nsummary(lm(Price ~ Size + New, data= house.selling.price))\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nAccording to the coefficient for size, the price of a house is expected to increase by $116.132 for each square foot increase in size. The coefficient is significant at the 0.0001 alpha level, meaning there is a strong correlation between size and price when the age status (new/old) is held fixed.\nAccording to the coefficient for new, a new house is expected to cost $57,736.283 more than an old house. This variable is significant at the 0.001 level, meaning that whether a house is old or new has a strong positive impact on price of the house."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#b.-2",
    "href": "posts/HW4_EmmaRasmussen.html#b.-2",
    "title": "Homework 4",
    "section": "3b.",
    "text": "3b.\nY = -40230.867 + 116.132(X1) + 57736.283 (X2) where X1 represents size and X2 represents new/old.\nFor a new house: Y = -40230.867 + 116.132(size) + 57736.283\nFor an old house: Y = -40230.867 + 116.132(size)"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#c.-2",
    "href": "posts/HW4_EmmaRasmussen.html#c.-2",
    "title": "Homework 4",
    "section": "3c.",
    "text": "3c.\n\n#new:\n-40230.867 + 116.132*3000 +  57736.283\n\n[1] 365901.4\n\n#not new:\n-40230.867 + 116.132*3000 +  57736.283*0\n\n[1] 308165.1\n\n\nNew: $365,901.40 Not new: $308165.10"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#d.-1",
    "href": "posts/HW4_EmmaRasmussen.html#d.-1",
    "title": "Homework 4",
    "section": "3d.",
    "text": "3d.\n\nsummary(lm(Price ~ Size + New + Size*New, data= house.selling.price))\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nY = -2227.808 + 104.438(size) + 61.916(size:new) -78527.502(new) Both size and the interaction term between size and new are statistically significant. The new/coefficient is no longer statistically significant."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#e.-1",
    "href": "posts/HW4_EmmaRasmussen.html#e.-1",
    "title": "Homework 4",
    "section": "3e.",
    "text": "3e.\nFor a new house: Y = -2227.808 + 166.354(size) - 78527.502 Old: Y = -2227.808 + 104.438(size)"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#f.-1",
    "href": "posts/HW4_EmmaRasmussen.html#f.-1",
    "title": "Homework 4",
    "section": "3f.",
    "text": "3f.\n\n#new: \n-2227.808 + 166.354*3000 - 78527.502\n\n[1] 418306.7\n\n#not new:\n-2227.808 + 104.438*3000\n\n[1] 311086.2\n\n\nNew: $418,306.70 Not new: $311,086.20"
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#g.",
    "href": "posts/HW4_EmmaRasmussen.html#g.",
    "title": "Homework 4",
    "section": "3g.",
    "text": "3g.\n\n#new: \n-2227.808 + 166.354*1500 - 78527.502\n\n[1] 168775.7\n\n#not new:\n-2227.808 + 104.438*1500\n\n[1] 154429.2\n\n\nNew: $168,775.70 Not new: $154429.20\nAccording to this equation, houses that are larger are much greater in price, especially when comparing new large houses to small new houses. For larger houses, the difference in cost between new and not new is much larger, compared to smaller houses, where new/not new makes less of a difference in price."
  },
  {
    "objectID": "posts/HW4_EmmaRasmussen.html#h.",
    "href": "posts/HW4_EmmaRasmussen.html#h.",
    "title": "Homework 4",
    "section": "3h.",
    "text": "3h.\nI prefer the second model with the interaction term which provides a clearer picture of how increased square footage makes a larger difference in bigger sized houses. The model with the interaction term also has a larger adjusted R squared.\nHowever, I would be skeptical using this model with small homes: for a home that is 1000 square feet, the predicted price for a new house is greater than for an old house. In other words, I do not feel this model would be good at predicting tiny home prices.\n\n#for a 1000 sq foot home:\n#New:\n-2227.808 + 166.354*1000 - 78527.502\n\n[1] 85598.69\n\n#Not new:\n-2227.808 + 104.438*1000\n\n[1] 102210.2"
  },
  {
    "objectID": "posts/HW4_EthanCampbell.html",
    "href": "posts/HW4_EthanCampbell.html",
    "title": "Homework 4",
    "section": "",
    "text": "Question 1\n\n\n\n\nCode\nlibrary(alr4)\n\n\nWarning: package 'alr4' was built under R version 4.1.3\n\n\nLoading required package: car\n\n\nWarning: package 'car' was built under R version 4.1.3\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nWarning: package 'effects' was built under R version 4.1.3\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.1.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.8     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\nx dplyr::recode() masks car::recode()\nx purrr::some()   masks car::some()\n\n\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is\ny^ = ???10,536 + 53.8x1 + 2.84x2.\n\n\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\n\nCode\nActual_price <- 145000\nPredicted <- -10536 +(53.8*1240) + (2.84*18000)\n\nresidual <- Actual_price - Predicted\n\nQuestion_3 <- 53.8/2.84\n\n\nAnswer:\nThe predicted equation is above and with the given information for x1 and x2 and Y we are able to calculate by inputting the information. The residual is the difference between the actual and predicted here we test for the predicted which is with the information inputted and we test for what the cost should be. Afterward, we subtract that from the actual price and notice a difference of -37704. This means that our equation is underpredicting the value of homes and this could be due to either the lack of data or the lack of certain variables that could sway the outcome.\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\n\nAnswer: If lot size is fixed then we can ignore x2 and focus on x1. x1 is in charge of home size per square foot and here it is at 53.8 dollar increase per square foot. When the prediction was run this was the coefficient thus we can use this to predict other values based on the same information.\n\n\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\nAnswer: For fixed home size to have the same impact as one square foot increase in home size we would need to increase by 18.94. This is calculated by dividing x1 by x2 and this shows the difference in each rate so for them to have the same impact we would need to multiply it by the above number.\n\n\n\nQuestion 2\n\n\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\n\n\n\n\n\nH0A\n\n\n\nMean salary for men and women is the same.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nMean salary for men and women is not the same.\n\n\n\n\nCode\nregression <- lm(salary ~ sex, data = salary)\nsummary(regression)\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nAnswer: Based on this alone we do not have enough information to reject the null hypothesis as the p-value for salary by sex is .0706. This is not statistically significant and does not give me any evidence to reject the null. However, when looking into the coefficient based on sex we notice it is -3340 lower than its sex counterpart. Based on this information we can assume that there is something going on and that women may be getting paid less. More data and more testing would be needed to prove this and more variables would prove essential to proving this hypothesis.\n\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\n\nCode\nregression2 <- lm(salary ~ ., data = salary)\nsummary(regression2)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWhen looking at the data here we notice that p value has grown more becoming harder to reject the null but we also notice a significant shift in the coefficient as it has grown by over 4000 and is now 1166.37. I am assuming this could be related to the other variables shifting this information.\n\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient/slope in relation to the outcome variable and other variables\n\ndegreePHd * p-value = .180 * interpretation = causes an increase in salary of 1388.61 this would make sense as having a higher education is known to lead to an increase in salary. However, it is not significant here which hints to why the number is not as high this could be due to them favoring years of service more than education.\nrankAssoc * p-value = 3.22e-05 * interpretation = This value is very significant and we are able to reject any null and say yes this value has an impact. This values causes salary to increase by 5292.36 which shows the importance of this value.\nrankProf * p-value = 1.62e-10 * interpretation = This value is significant and we can reject any null hypothesis and say yes it has an impact. This one has the largest impact out of all the variables it causes salary to increase by 11118.73 which is a major increase as it it greater than all the other variables combined.\nsexFemale * p-value = .214 * interpretation = This value is not significant and we would fail to reject the null hypothesis. This one has a coefficient of 1166.37 which is how much it would increase salary. This is saying that females have a higher salary by that amount if this was significant and included all other variables.\nyear * p-value = 8.65e-06 *n interpretation = This is a significant value that would allow us to reject the null hypothesis. This means the more you work there the more your salary increases and this is saying that it would increase your salary by 476.31 each year you work there.\nysdeg* p-value = .115 * interpretation = This value is not significant and would fail to reject the null. This value would causes a decrease in salary by 124.57.\n\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref ='Assoc')\n\nregression3 <- lm(salary ~ ., data = salary)\nsummary(regression3)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWhen changing the base value for rank we notice a large shift in the value. Being an asst. reduces the salary by 5292.36. This is noticed in the coefficients and comparing it to the professor’s salary. This has a p value of 3.22e-05 meaning we could reject the null and say it has an impact.\n\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in the promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\n\n\n\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nregression4 <- lm(salary ~ sex + year + degree + ysdeg, data = salary)\nsummary(regression4)\n\n\n\nCall:\nlm(formula = salary ~ sex + year + degree + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen analyzing the changes after removing rank from the equation notice major changes to the p-values of all variables and to their coefficients. We notice that sex female is back to being negative and the p value has increased to .332209. Year has decreased to 351.97 and is significant a 95% only. degreePHd is now hugely negative which was once positive. This is now -3299.35 and is significant at 95%. ysdeg is the only variable that has seen some growth and it is now 339.40 with a p-value of 0.000114. Very interesting to see how much impact the year had on all other variables in this equation.\n\n\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variations in Salary.\n\n\n\nCode\n# we need to make a dummy variable for the dean\nsalary <- salary %>%\n  mutate(new_dean = case_when(\n    ysdeg <= 15 ~ 1,\n    ysdeg > 15 ~ 0\n  ))\n\n# now that we have that we can run the regression using an interaction term here to look into the impact of the dean change\ndean_regresion <- lm(salary ~ year + degree + sex + rank + new_dean + new_dean*year, data = salary)\n\nsummary(dean_regresion)\n\n\n\nCall:\nlm(formula = salary ~ year + degree + sex + rank + new_dean + \n    new_dean * year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3309.8 -1102.5  -265.2   539.2  9339.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   17427.11    1538.08  11.330 1.24e-14 ***\nyear            495.72      97.42   5.088 7.20e-06 ***\ndegreePhD      1161.63     859.23   1.352   0.1833    \nsexFemale      1115.96     862.06   1.295   0.2022    \nrankAsst      -5416.07    1079.72  -5.016 9.14e-06 ***\nrankProf       6196.73    1029.38   6.020 3.16e-07 ***\nnew_dean       3789.08    1867.72   2.029   0.0486 *  \nyear:new_dean  -195.43     183.99  -1.062   0.2940    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2358 on 44 degrees of freedom\nMultiple R-squared:  0.8629,    Adjusted R-squared:  0.8411 \nF-statistic: 39.58 on 7 and 44 DF,  p-value: < 2.2e-16\n\n\nTo avoid some problems with similar information we made an interaction term so we can compare when the new dean came effectively. Now when reading this we notice that at the 95% confidence level the new_Dean which means that being hired by the new dean does result in some higher wages. The new dean variable by itself had an increase of 3789.08 however when we take into consideration the interaction term it brings that number down by 195.43.\n\n\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\n\n\nQuestion 3\n\n\n(Data file: house.selling.price in smss R package)\n\n\n\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of the home (in square feet) and whether the home is new(1 = yes; 0 = no). In particular, for each variable; discuss the statistical significance and interpret the meaning of the coefficient.\n\n\n\nCode\ndata(\"house.selling.price\")\n\nhouse <- house.selling.price\n\nReg1 <- lm(Price ~ Size + New, data = house)\nsummary(Reg1)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nHere there are two variables to look at first is Size, this coefficient is at 116.132 meaning for every sqft that is added the price goes up by this. The other one is new which increases the price of the home by 57736.283 if the home is new. Size is very significant at < 2e-16 while new is significant at 0.00257 both of these variables are significant to predicting house cost. With these two variables alone we see an R squared of .726 which is really good.\n\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\n\nPrediction equation is: Y = -40230.867 +116.132x1 +17505.41x2 This is where x1 = size and x2 = new when it is yes\nPrediction equation is: Y = -40230.867 +116.132x1 +-40230.867x2\nThis is where x1 = size and x2 = new when it is no\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n\nnew <- (116.132*3000) + 17505.41\nold <- (116.132*3000) -40320.867\n\nNew is valued at 365,901.41 while old is valued at 308,075.133\n\n\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\n\nCode\nReg2 <- lm(Price ~ Size + New + New*Size, data = house)\nsummary(Reg2)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + New * Size, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nthe regression results show that with size and the new the cost per sqft is 166.354 which is 61.916 higher compared to the not new version. This is the focus of this regression is to evaluate the interaction term\n\n\n\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\n\n\nCode\n#mod <- -22227.81 + 104.44x1 -78527.50x2 + 61.92(x1*z)\n\n#old1 <- -22227.81 + (104.44*x1)\n#new1 <- -100755.31 + (166.36x1)\n\n\n\n\n\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\n\nCode\nold1 <- -22227.81 + (104.44*3000)\nnew1 <- -100755.31 + (166.36*3000)\n\n\nFor a new home with this interaction term, we see a price of 398324.69 and for an old we see 291092.19 which is a huge difference.\n\n\n\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of the home increases.\n\n\n\nCode\nold2 <- -22227.81 + (104.44*1500)\nnew2 <- -100755.31 + (166.36*1500)\n\n\nHere we see the price of a new home is 148,784.69 at 1500 sqft while an old one is 134,432.19. The difference between these values is much smaller than it was last time and that falls to the slope. There is a difference of 61.916 between the new and old so the larger the home the larger the difference will be between the two.\n\n\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\nWhen analyzing these two regressions I would take the second one with the interaction term as it accounts for the new variable in each sqft compared to the other one that had one static number which does not seem realistic. Another point to note is the difference in the r squared between the two regressions as the one with the interaction term is higher by ~ 2%."
  },
  {
    "objectID": "posts/HW4_KarenKimble.html",
    "href": "posts/HW4_KarenKimble.html",
    "title": "DACSS 603 HW 4",
    "section": "",
    "text": "Code\n# Setup\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/HW4_KarenKimble.html#question-1",
    "href": "posts/HW4_KarenKimble.html#question-1",
    "title": "DACSS 603 HW 4",
    "section": "Question 1",
    "text": "Question 1\n\nPart A\n\n\nCode\n# Predicted\n\ny = -10536 + (53.8 * 1240) + (2.84 * 18000)\n\ny\n\n\n[1] 107296\n\n\nCode\n145000 - y\n\n\n[1] 37704\n\n\nThe predicted selling price is 107,296 dollars but the actual selling price was 145,000 dollars, resulting in a residual of 37,704. This means that the predictor model underestimated the selling price by over 37,000 dollars.\n\n\nPart B\nFor a fixed lot size, the house selling price is predicted to increase by 53.8 for each square foot increase in home size. This is because 53.8 is the coefficient for the square foot variable, meaning that the model estimates this amount of increase for each additional unit of x.\n\n\nPart C\n\n\nCode\n53.8/2.84\n\n\n[1] 18.94366\n\n\nFor a fixed home size, the lot size would need to increase by about 18.94 square feet in order to have an equivalent impact as an additional square foot of home size."
  },
  {
    "objectID": "posts/HW4_KarenKimble.html#question-2",
    "href": "posts/HW4_KarenKimble.html#question-2",
    "title": "DACSS 603 HW 4",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\n\n\nCode\nt.test(salary ~ sex, data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe p-value from the t-test 0.09, greater than a 0.05 significance level. This indicates that there is not statistically significant evidence to reject the hypothesis that the mean salary for men and women are the same.\n\n\nPart B\n\n\nCode\nmodel <- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary)\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\nsexFemale    -697.8183  3030.56452\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe confidence interval for sex means that there is 95% confidence that the true difference in mean salaries for men and women lie between -697.82 and 3,030.56.\n\n\nPart C\n\n\nCode\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe above results show that the p-value for the variable of sex is larger than the significance level of 0.05, meaning there is still no statistically significant evidence to reject the hypothesis that the mean salaries for men and women are the same. When the individual is female, the model predicts the salary increases by 1,166.37.\nFor the degree level, there is also not statistically significant evidence to reject the hypothesis that the mean salaries for those with a master’s degree and a PhD are the same, since the p-value is larger than 0.05. The model predicts that when an individual has a PhD, their predicted salary increases by 1,388.61.\nFor the rank variable, there is statistically significant evidence to reject the hypothesis that the salaries for ranks Associate, Assistant, and Professor are the same. The p-values for both Associate and Professor rankings are extremely small and less than the significance level of 0.05. THe model predicts that faculty with an Associate ranking have a salary increase by 5,292.36, and faculty with a Professor ranking have a salary increase by 111,118.76.\nThe p-value for the variable of the amount of years in the current rank is also extremely small and less than the 0.05 significance level, meaning that there is statistically significant evidence to reject the hypothesis that the amount of years does not affect salary amount. For each additional year spent in the current rank, the model predicts a salary increase of 476.31.\nLastly, the p-value for the amount of years since the highest degree is larger than the signficiance level 0.05. There is no statistically significant evidence to reject the hypothesis that the amount of years since highest degree has no impact on salary amount. The model predicts that for each additional year since the highest degree, the salary decreases by 124.57.\n\n\nPart D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = 'Prof')\nmodel <- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + rank + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\nsexFemale     1166.37     925.57   1.260    0.214    \ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nAfter changing the baseline category, the model shows that for faculty with the ranks of Assistant and Associate, the p-value is still extremely small and shows significant evidence to reject the hypothesis that the salaries for ranks Assistant, Associate, and Professor are the same. The model indicates that for those in the rank Assistant, their predicted salary decreases by 111,118.76. When Assistant is the baseline category, the model predicts a salary decrease if 5,826.40 when the faculty is ranked Associate.\n\n\nPart E\n\n\nCode\nmodel <- lm(salary ~ sex + degree + year + ysdeg, data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\nsexFemale   -1286.54    1313.09  -0.980 0.332209    \ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nAfter excluding rank, all variables except for sex have statistically significant p-values. Though the p-value for the year variable increased, it still reamined under the 0.05 significance level. The variables degree and ysdeg now have p-values less than the significant 0.05 level when they were much higher in the previous model. Removing the rank variable resulted in new coefficients for all variables as well.\n\n\nPart F\n\n\nCode\nsalary$appointed <- ifelse(salary$year > 15, c(\"0\"), c(\"1\"))\nmodel <- lm(salary ~ sex + degree + appointed, data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + appointed, data = salary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11079.0  -4093.5   -333.7   3348.9  16842.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29712.6     2593.2  11.458 2.45e-15 ***\nsexFemale    -2504.7     1793.8  -1.396   0.1691    \ndegreePhD      541.4     1640.5   0.330   0.7428    \nappointed1   -6005.5     2692.8  -2.230   0.0304 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5610 on 48 degrees of freedom\nMultiple R-squared:  0.1541,    Adjusted R-squared:  0.1012 \nF-statistic: 2.915 on 3 and 48 DF,  p-value: 0.04371\n\n\nMulticollinearity would be a concern in this case because multiple variables are related due to the appointment of the new Dean. If the Dean only hired those who recently got their degree, then that means the variables year and years since highest degree are related–only those hired within the past 15 years would also have gotten their degree within 15 years. Thus, I omitted these two variables and created the variable “appointed”, with 1 indicating that the faculty member was appointed by the new dean and 0 indicating that they were not. The results from the model don’t support the hypothesis that the new Dean’s appointees are making higher salaries than those who were are not. The model predicts a salary decrease of 6,005 when the faculty member is appointed by the new Dean. If the people hired by the new Dean were making more money, this predictiin would be an increase."
  },
  {
    "objectID": "posts/HW4_KarenKimble.html#question-3",
    "href": "posts/HW4_KarenKimble.html#question-3",
    "title": "DACSS 603 HW 4",
    "section": "Question 3",
    "text": "Question 3\n\nPart A\n\n\nCode\nmodel <- lm(Price ~ Size + New, data = house.selling.price)\n\n\nError in is.data.frame(data): object 'house.selling.price' not found\n\n\nCode\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + appointed, data = salary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11079.0  -4093.5   -333.7   3348.9  16842.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29712.6     2593.2  11.458 2.45e-15 ***\nsexFemale    -2504.7     1793.8  -1.396   0.1691    \ndegreePhD      541.4     1640.5   0.330   0.7428    \nappointed1   -6005.5     2692.8  -2.230   0.0304 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5610 on 48 degrees of freedom\nMultiple R-squared:  0.1541,    Adjusted R-squared:  0.1012 \nF-statistic: 2.915 on 3 and 48 DF,  p-value: 0.04371\n\n\nFor the variable size, the p-value is extremely small and less than the significance level of 0.05, meaning that there is statistically significant evidence to reject the hypothesis that the size of the house does not affect price. The coefficient of size indicates that for each additional square foot, the model predicts the price to increase by 116.132.\nThe p-value for the variable new is also smaller than the significance level of 0.05, so there is statistically significant evidence to reject the hypothesis that new houses have the same mean price as old houses. The coefficient of new means that for a house that is new, the price is predicted to increase by 57,736.283.\n\n\nPart B\nThe prediction equation is: y = -40,230.867 + 116.132x1 + 57,736.283x2 (with x1 being the square feet of the house and x2 being whether the house is old or new)\nThis means that when the house has 0 square feet and is not new, the price would be predicted to be -40,230.867 (or the y-intercept).\nThe equation for not new homes: y = -40,230.867 + 116.132x1\nThe last part of the prediction equation is omitted since not new homes are equal to 0, cancelling out the last component.\nThe equation for new homes: y = -40,230.867 + 116.132x1 + 57,736.283x2\n\n\nPart C\n\n\nCode\n# New\n\n-40230.867 + (116.132 * 3000) + (57636.283 * 1)\n\n\n[1] 365801.4\n\n\nThe predicted price of a new home with 3,000 square feet is $365,801.40.\n\n\nCode\n# Not new\n\n-40230.867 + (116.132 * 3000)\n\n\n[1] 308165.1\n\n\nThe predicted price of a not new home with 3,000 square feet is $308,165.10.\n\n\nPart D\n\n\nCode\nmodel <- lm(Price ~ Size + New + Size*New, data = house.selling.price)\n\n\nError in is.data.frame(data): object 'house.selling.price' not found\n\n\nCode\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ sex + degree + appointed, data = salary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11079.0  -4093.5   -333.7   3348.9  16842.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29712.6     2593.2  11.458 2.45e-15 ***\nsexFemale    -2504.7     1793.8  -1.396   0.1691    \ndegreePhD      541.4     1640.5   0.330   0.7428    \nappointed1   -6005.5     2692.8  -2.230   0.0304 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5610 on 48 degrees of freedom\nMultiple R-squared:  0.1541,    Adjusted R-squared:  0.1012 \nF-statistic: 2.915 on 3 and 48 DF,  p-value: 0.04371\n\n\nThe coefficients changed for both variables with the new model. The coefficient for size indicates that for each additional square foot, the price of the home is predicted to increase by 104.438. For new, when the house is new, the price of the home is expected to decrease by 78527.502–now it is a negative relationship when in the previous model it was positive. The interaction coefficient is 61.916, meaning that for each additional square foot when the house is new, the price is predicted to increase by 61.916. The size variable’s p-value is still below the 0.05 significance level. However, the p-value for the variable new is above the 0.05 significance level, indicating that there is now no statistically significant evidence to reject the hypothesis that the mean prices for new houses and old houses (regardless of size) are the same. The interaction term’s p-value is smaller than the 0.05 significance level, meaning that there is statistically significant evidence to reject the hypothesis that the price of a new house doesn’t depend on its size and vice versa.\n\n\nPart E\nEquation for a new house: y = -22,2227.808 + 104.438x1 - 78,527.502x2 + 61.916(x1)(x2)\nEquation for a not new house: y = -22,2227.808 + 104.438x1\nAgain, removed the last two terms of this equation since not new is equal to 0 and would therefore cancel out the last two terms.\n\n\nPart F\n\n\nCode\n# New\n\n-22227.808 + (104.438 * 3000) - (78527.502 * 1) + (61.916 * 3000 * 1)\n\n\n[1] 398306.7\n\n\nThe predicted price for a new house with 3,000 square feet with the new equation is $398,306.70.\n\n\nCode\n# Not New\n\n-22227.808 + (104.438 * 3000)\n\n\n[1] 291086.2\n\n\nThe predicted price for a not new house with 3,000 square feet with the new equation is $291,086.19.\n\n\nPart G\n\n\nCode\n# New\n\n-22227.808 + (104.438 * 1500) - (78527.502 * 1) + (61.916 * 1500 * 1)\n\n\n[1] 148775.7\n\n\nThe predicted price for a new house with 1,500 square feet is $148,775.70.\n\n\nCode\n# Not new\n\n-22227.808 + (104.438 * 1500)\n\n\n[1] 134429.2\n\n\nThe predicted price for a not new house with 1,500 square feet is 134,429.20.\n\n\nPart H\nI think the model with the interaction term better represents the relationship of size to the outcome of price, both from the results in the summary and from my own limited knowledge about housing. The regression results from the interaction model showed that the interaction term was statistically significant, indicating strong evidence that the price of a home does depend on size, but whether or not the house is new affects the magnitude of this effect. Also, I think when people buy homes they care both about size and about whether or not the house is new. Additionally, when the interaction model calculated the price of homes for both 3,000 square feet and 1,500 square feet when the house is both new and old, there is a dramatic difference in how much the price increased in the new house with the additional square footage than the not new house."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html",
    "href": "posts/HW4_ManiShankerKamarapu.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#a",
    "href": "posts/HW4_ManiShankerKamarapu.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nPredicted_selling_price <-  -10536 + 53.8 * 1240 + 2.84 * 18000\nPredicted_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nResidual <- Predicted_selling_price - 145000\nResidual\n\n\n[1] -37704\n\n\nFrom the above result, we can say that the house was sold for 37704 dollars greater than predicted."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#b",
    "href": "posts/HW4_ManiShankerKamarapu.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nUsing the prediction equation ŷ = -10536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1 = 1, representing one square-foot of home size, the output would increase by 53.8 * 1 = 53.8."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#c",
    "href": "posts/HW4_ManiShankerKamarapu.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nx2 <- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW4_ManiShankerKamarapu.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"salary\")\nsalary"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in salary of women compared to men falls between -697.8183 dollars and 3030.56452 dollars."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of 0.18, we cannot conclude that degree level has a statistically significant impact on salary.\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary.\nFor the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is 0.214, so this is not a statistically significant relationship.\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than 0.01 so the relationship between year and salary appears to be significant.\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#d",
    "href": "posts/HW4_ManiShankerKamarapu.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\nsummary(lm(salary ~ rank, salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#e",
    "href": "posts/HW4_ManiShankerKamarapu.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#f",
    "href": "posts/HW4_ManiShankerKamarapu.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary <- salary %>%\n  mutate(hired = case_when(ysdeg <= 15 ~ \"1\", ysdeg > 15 ~ \"0\"))\nsummary(lm(salary ~ hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ hired, data = salary)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27469.4      913.4  30.073  < 2e-16 ***\nhired1       -7343.5     1291.8  -5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nCode\nsummary(lm(salary ~ sex + rank + degree + hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29511.3      784.0  37.640  < 2e-16 ***\nsexFemale     -829.2      997.6  -0.831    0.410    \nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\ndegreePhD     1126.2     1018.4   1.106    0.275    \nhired1         319.0     1303.8   0.245    0.808    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both.\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean. When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment by the Dean toward faculty that the Dean specifically hired."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW4_ManiShankerKamarapu.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#a-2",
    "href": "posts/HW4_ManiShankerKamarapu.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(Price ~ Size + New, house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by 116.132 dollars and 57736.283 dollars respectively."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#b-2",
    "href": "posts/HW4_ManiShankerKamarapu.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nnew <- house.selling.price %>% \n  filter(New == 1)\nsummary(lm(Price ~ Size, data = new))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold <- house.selling.price %>% \n  filter(New == 0)\nsummary(lm(Price ~ Size, data = old))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses. Adjusted R-squared for the model is also much higher (0.91 vs. 0.58).\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#c-2",
    "href": "posts/HW4_ManiShankerKamarapu.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nSize <- 3000\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808\nNew_Price\n\n\n[1] 397244.7\n\n\nCode\nOld_Price\n\n\n[1] 289772.2"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#d-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#e-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#f-1",
    "href": "posts/HW4_ManiShankerKamarapu.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nSize <- 3000\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 398324.7\n\n\nCode\nOld_Price\n\n\n[1] 291092.2"
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#g",
    "href": "posts/HW4_ManiShankerKamarapu.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n\nCode\nSize <- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger."
  },
  {
    "objectID": "posts/HW4_ManiShankerKamarapu.html#h",
    "href": "posts/HW4_ManiShankerKamarapu.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nThe prediction model with interaction has a significantly large negative coefficient for the New variable. The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Since the models do have similar adjusted r-squared values, I would prefer the model with interaction because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html",
    "href": "posts/HW4_Niharikapola.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#question-1",
    "href": "posts/HW4_Niharikapola.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#a",
    "href": "posts/HW4_Niharikapola.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nPredicted_selling_price <-  -10536 + 53.8 * 1240 + 2.84 * 18000\nPredicted_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nResidual <- Predicted_selling_price - 145000\nResidual\n\n\n[1] -37704\n\n\nFrom the above result, we can say that the house was sold for 37704 dollars greater than predicted."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#b",
    "href": "posts/HW4_Niharikapola.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nUsing the prediction equation ŷ = -10536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1 = 1, representing one square-foot of home size, the output would increase by 53.8 * 1 = 53.8."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#c",
    "href": "posts/HW4_Niharikapola.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor fixed home size,\n53.8 * 1 = 2.84x2\n\n\nCode\nx2 <- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#question-2",
    "href": "posts/HW4_Niharikapola.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"salary\")\nsalary"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#a-1",
    "href": "posts/HW4_Niharikapola.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#b-1",
    "href": "posts/HW4_Niharikapola.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in salary of women compared to men falls between -697.8183 dollars and 3030.56452 dollars."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#c-1",
    "href": "posts/HW4_Niharikapola.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of 0.18, we cannot conclude that degree level has a statistically significant impact on salary.\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary.\nFor the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is 0.214, so this is not a statistically significant relationship.\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than 0.01 so the relationship between year and salary appears to be significant.\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#d",
    "href": "posts/HW4_Niharikapola.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\nsummary(lm(salary ~ rank, salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#e",
    "href": "posts/HW4_Niharikapola.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#f",
    "href": "posts/HW4_Niharikapola.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary <- salary %>%\n  mutate(hired = case_when(ysdeg <= 15 ~ \"1\", ysdeg > 15 ~ \"0\"))\nsummary(lm(salary ~ hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ hired, data = salary)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27469.4      913.4  30.073  < 2e-16 ***\nhired1       -7343.5     1291.8  -5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nCode\nsummary(lm(salary ~ sex + rank + degree + hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29511.3      784.0  37.640  < 2e-16 ***\nsexFemale     -829.2      997.6  -0.831    0.410    \nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\ndegreePhD     1126.2     1018.4   1.106    0.275    \nhired1         319.0     1303.8   0.245    0.808    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both.\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean. When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment by the Dean toward faculty that the Dean specifically hired."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#question-3",
    "href": "posts/HW4_Niharikapola.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#a-2",
    "href": "posts/HW4_Niharikapola.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(Price ~ Size + New, house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by 116.132 dollars and 57736.283 dollars respectively."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#b-2",
    "href": "posts/HW4_Niharikapola.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nnew <- house.selling.price %>% \n  filter(New == 1)\nsummary(lm(Price ~ Size, data = new))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold <- house.selling.price %>% \n  filter(New == 0)\nsummary(lm(Price ~ Size, data = old))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses. Adjusted R-squared for the model is also much higher (0.91 vs. 0.58).\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#c-2",
    "href": "posts/HW4_Niharikapola.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nSize <- 3000\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808\nNew_Price\n\n\n[1] 397244.7\n\n\nCode\nOld_Price\n\n\n[1] 289772.2"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#d-1",
    "href": "posts/HW4_Niharikapola.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#e-1",
    "href": "posts/HW4_Niharikapola.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#f-1",
    "href": "posts/HW4_Niharikapola.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nSize <- 3000\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 398324.7\n\n\nCode\nOld_Price\n\n\n[1] 291092.2"
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#g",
    "href": "posts/HW4_Niharikapola.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n\nCode\nSize <- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger."
  },
  {
    "objectID": "posts/HW4_Niharikapola.html#h",
    "href": "posts/HW4_Niharikapola.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nThe prediction model with interaction has a significantly large negative coefficient for the New variable. The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Since the models do have similar adjusted r-squared values, I would prefer the model with interaction because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/HW4_PrahithaMovva.html",
    "href": "posts/HW4_PrahithaMovva.html",
    "title": "Homework 4 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW4_PrahithaMovva.html#question-1",
    "href": "posts/HW4_PrahithaMovva.html#question-1",
    "title": "Homework 4 - Prahitha Movva",
    "section": "Question 1",
    "text": "Question 1\n\nA\nGiven prediction equation: y = -10536 + 53.8x1 + 2.84x2 where x1 is the size of the home (sq ft) and x2 is the lot size (sq ft)\n\n\nCode\nlot.size = 18000\nhome.size = 1240\nactual.price = 145000\n\npredicted.price = -10536 + (53.8*home.size) + (2.84*lot.size)\nprint(predicted.price)\n\n\n[1] 107296\n\n\nCode\nresidual = actual.price - predicted.price\nprint(residual)\n\n\n[1] 37704\n\n\nUsing the prediction equation, the value that we get is $107,296 and the residual is $37,704. The residual tells us that the house was under-priced (under-predicted) by $37,704.\n\n\nB\nFor a fixed lot size (say k), our prediction equation becomes y = -10536 + 53.8x1 + 2.84k where x1 (home size) is the only variable. So for each square-foot increase in home size, the house selling price predicted by the model increases by $53.8\n\n\nCode\ny1 = -10536 + (53.8*1) + 2.84\ny2 = -10536 + (53.8*2) + 2.84\n\ny2 - y1\n\n\n[1] 53.8\n\n\n\n\nC\nFor a fixed home size (say k), our prediction equation becomes y = -10536 + 53.8k + 2.84x2 where x2 (lot size) is the only variable. So for each square-foot increase in lot size, the house selling price predicted by the model increases by $2.84. For this to be equal to $53.8 we need to multiply it by 19.94366 - i.e., an increase by 18.94366.\n\n\nCode\n53.8/2.84\n\n\n[1] 18.94366\n\n\n\n\nCode\ny1 = -10536 + (53.8*1) + 2.84\ny2 = -10536 + (53.8*2) + 2.84\nprint(y2 - y1)\n\n\n[1] 53.8\n\n\nCode\ny1 = -10536 + (53.8*1) + 2.84\ny2 = -10536 + (53.8*1) + (2.84*(1 + 18.94366))\nprint(y2 - y1)\n\n\n[1] 53.79999"
  },
  {
    "objectID": "posts/HW4_PrahithaMovva.html#question-2",
    "href": "posts/HW4_PrahithaMovva.html#question-2",
    "title": "Homework 4 - Prahitha Movva",
    "section": "Question 2",
    "text": "Question 2\n\nA\nH0: Mean salary for men and women is the same\nHa: Mean salary for men and women is NOT the same\nWe can test this using a two-sample t-test\n\n\nCode\ndata(salary)\n\n# testing for variance\nvar.test(salary ~ sex, data=salary)\n\n\n\n    F test to compare two variances\n\ndata:  salary by sex\nF = 0.84242, num df = 37, denom df = 13, p-value = 0.6525\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3015275 1.9189474\nsample estimates:\nratio of variances \n         0.8424225 \n\n\nThe p-value of F-test is 0.6525 which is greater than the significance level (alpha = 0.05). So we can say that there is no significant difference between the variances of the two sets of data.\n\n\nCode\nt.test(salary ~ sex, data=salary, var.equal=TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  salary by sex\nt = 1.8474, df = 50, p-value = 0.0706\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -291.257 6970.550\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe p-value of t-test is 0.0706 which is greater than the significance level (alpha = 0.05). So we can say that there is no significant difference in the mean salary between male and female faculty (at a 5% significance level).\n\n\nB\n\n\nCode\nmodel = lm(salary ~ degree + rank + sex + year + ysdeg, data=salary)\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nFrom sexFemale, we can say that the 95% confidence interval for the difference in salary between males and females is [-697.8183, 3030.56452]\n\n\nC\n\n\nCode\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n(a)\nAt a 95% confidence level, only rank and year are (statistically) significant predictors of salary.\n\n\n(b)\nSince we now know that rank and year are the significant variables, we will only consider those for interpretation. Associate Professors (rankAssoc) and Full Professors (rankProf) earn more than Assistant Professors (baseline category) by $5292.36 and $11118.76 respectively. Similarly, professors with more working experience (year) earn more. However, rank has a higher effect on salary than year as the coefficient for rank is much higher.\n\n\n\nD\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref='Assoc')\nmodel.modified = lm(salary ~ degree + rank + sex + year + ysdeg, data=salary)\nsummary(model.modified)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\n5826.40 + 5292.36\n\n\n[1] 11118.76\n\n\nWe see similar results as above. Since the baseline category is now Associate Professor, we see a negative coefficient for Assistant Professor category which says that Associate Professors make $5292.36 more than Assistant Professors. Similarly, Full Professors make $5826.40 more than Associate Professors.\n\n\nE\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nIn addition to the variable year, degreePhD and ysdeg are also significantly contributing to salary. However, the p-value for sexFemale is still much greater than 0.05, meaning - sex is not significant when predicting for salary (at a 95% confidence level). We also observe that the coefficients for degreePhD, sexFemale and ysdeg got reversed (positive to negative and vice-versa). This suggests that professors with an MS are earning approximately $3300 more than the professors with a PhD, male professors are earning around $1286 more than female professors. We also see that the R-squared value has dropped - so this model does a poor job in explaining the variation in comparison with the other model where we included rank.\n\n\nF\n\n\nCode\nsalary <- mutate(salary, dean = case_when(ysdeg < 15 ~ \"New\",\n                                         ysdeg >=15 ~ \"Old\"))\n\n\nSince the hypothesis depends on ysdeg and we created a new variable using it, I’m assuming that removing ysdeg should remove any multicollinearity.\n\n\nCode\nvif(lm(salary ~ dean + degree + sex + rank + year + ysdeg, data=salary))\n\n\n           GVIF Df GVIF^(1/(2*Df))\ndean   3.986217  1        1.996551\ndegree 2.144390  1        1.464373\nsex    1.542571  1        1.242003\nrank   4.347951  2        1.444013\nyear   2.445467  1        1.563799\nysdeg  6.821294  1        2.611761\n\n\nAs a rule of thumb, a vif score over 5 is a problem. So I will be removing ysdeg from the predictors.\n\n\nCode\nsummary(lm(salary ~ dean + degree + sex + rank + year, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ dean + degree + sex + rank + year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3588.0 -1532.2  -232.2   565.7  9132.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20468.7      951.7  21.507  < 2e-16 ***\ndeanOld      -2421.6     1187.9  -2.038   0.0474 *  \ndegreePhD     1073.5      843.3   1.273   0.2096    \nsexFemale     1046.7      858.0   1.220   0.2289    \nrankAsst     -5012.5     1002.3  -5.001 9.16e-06 ***\nrankProf      6213.3     1045.0   5.946 3.76e-07 ***\nyear           450.7       81.5   5.530 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2360 on 45 degrees of freedom\nMultiple R-squared:  0.8597,    Adjusted R-squared:  0.841 \nF-statistic: 45.95 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWe see a negative coefficient for deanOld with a p-value less than 0.05 which suggests that the hypothesis is true. The faculty hired by the new dean make $2421.6 more than the old faculty."
  },
  {
    "objectID": "posts/HW4_PrahithaMovva.html#question-3",
    "href": "posts/HW4_PrahithaMovva.html#question-3",
    "title": "Homework 4 - Prahitha Movva",
    "section": "Question 3",
    "text": "Question 3\n\nA\n\n\nCode\ndata(\"house.selling.price\")\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth size and new are significant at a 95% confidence level. The coefficient for size suggests that for 1 square foot increase in the size of home, the price increases by $116.132 (considering that both are of the same condition - either old/new) and that for new suggests that if the home is new, the price increases by $57736.283 when compared to that of an old home of the same size.\n\n\nB\nUsing the coefficients from above, the prediction equation becomes:\nprice = -40230.867 + (116.132 * home.size) + (57736.283 * home.new)\nSeparate equations for new and not new homes:\nprice.old = -40230.867 + (116.132 * home.size) price.new = -40230.867 + (116.132 * home.size) + 57736.283\n\n\nC\n\n(i)\n\n\nCode\nprice.new = -40230.867 + (116.132 * 3000) + 57736.283\nprice.new\n\n\n[1] 365901.4\n\n\n\n\n(ii)\n\n\nCode\nprice.old = -40230.867 + (116.132 * 3000)\nprice.old\n\n\n[1] 308165.1\n\n\n\n\n\nD\n\n\nCode\nsummary(lm(Price ~ Size * New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\nE\n\n\nCode\ndata.old <- subset(house.selling.price, New == 0)\ndata.new <- subset(house.selling.price, New == 1)\n\n\nggplot() +\n  geom_smooth(data=data.old, aes(x = Size, y = Price), \n              method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_point(data=data.old, aes(x = Size, y = Price), color = \"blue\") +\n  geom_smooth(data=data.new, aes(x = Size, y = Price), \n              method = \"lm\", se = FALSE, color = \"red\") +\n  geom_point(data=data.new, aes(x = Size, y = Price), color = \"red\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe red line and dots represent the new homes whereas the blue line and dots the not new homes.\n\n\nF\nFrom the coefficients that we got in D, the prediction equations become:\nprice = -22227.808 + (104.438 * home.size) + (-78527.502 * home.new) + (61.916 * home.size * home.new)\nprice.old = -22227.808 + (104.438 * home.size)\nprice.new = -22227.808 + (104.438 * home.size) - 78527.502 + (61.916 * home.size)\n\n(i)\n\n\nCode\nprice.new = -22227.808 + (104.438 * 3000) - 78527.502 + (61.916 * 3000)\nprice.new\n\n\n[1] 398306.7\n\n\n\n\n(ii)\n\n\nCode\nprice.old = -22227.808 + (104.438 * 3000)\nprice.old\n\n\n[1] 291086.2\n\n\n\n\n\nG\n\n(i)\n\n\nCode\nprice.new = -22227.808 + (104.438 * 1500) - 78527.502 + (61.916 * 1500)\nprice.new\n\n\n[1] 148775.7\n\n\n\n\n(ii)\n\n\nCode\nprice.old = -22227.808 + (104.438 * 1500)\nprice.old\n\n\n[1] 134429.2\n\n\nThe difference in the price for new and not new homes of the same size seems to be less for smaller homes. This could suggest that size contributes more to price than the condition of the home.\n\n\n\nH\nI would prefer the second model with interaction as both the multiple R-squared and adjusted R-squared for that model are higher than that without interaction."
  },
  {
    "objectID": "posts/HW4_Saaradhaa.html",
    "href": "posts/HW4_Saaradhaa.html",
    "title": "Homework 4",
    "section": "",
    "text": "Qn 1A\n\n# predicted price.\nyhat <- -10536 + 53.8*1240 + 2.84*18000\n\n# residual.\n145000-yhat\n\n[1] 37704\n\n\nThe model under-predicts by $37,704.\n\n\nQn 1B\nIt is predicted to increase by ~$53.80. This is because the effects of x1 and x2 on y are independent of one another, and there is no interaction between them.\n\n\nQn 1C\n\n53.8/2.84\n\n[1] 18.94366\n\n\nIt would need to increase by ~19x.\n\n\nQn 2A\n\n# load dataset.\ndata(salary)\n\n# run model.\nsummary(lm(salary ~ sex, salary))\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe mean salary for men and women does not differ, p = 0.07 (i.e., it is the same).\n\n\nQn 2B\n\n# run model.\nmodel2B <- lm(salary ~ ., salary)\nsummary(model2B)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n# get confidence interval.\nconfint(model2B)\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nSex is still not a significant predictor of salary. The 95% CI for difference in salary between males and females is approx. -698 to 3031.\n\n\nQn 2C\nrank and year are significant predictors of salary, while all others were not.\nBoth rank and year positively predict salary: Associate Professors and full Professors were likely to earn quite a bit more than Assistant Professors, while professors with more years in their current rank also earned more.\nLooking at the magnitude of the coefficients, rank has a greater impact on salary than year does.\n\n\nQn 2D\n\n# change baseline category.\nsalary$rank <- relevel(salary$rank, ref = 'Assoc')\n\n# re-run model.\nmodel2D <- lm(salary ~ ., salary)\nsummary(model2D)\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWe can see that rank now both negatively and positively predicts salary. Specifically, Assistant Professors earn less than Associate Professors do, at the same magnitude that was observed in model1 (approx. ~$5292).\n\n\nQn 2E\n\nmodel2E <- lm(salary ~ . - rank, salary)\nsummary(model2E)\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding rank, both adjusted R2 and the overall F-statistic drop. degree and ysdeg are now significant as well. Curiously, those with a PhD earn less than those with an MS. As years since highest degree increases, so does salary.\n\n\nQn 2F\n\n# create new variable.\nsalary <- mutate(salary, HireTime = case_when(ysdeg <= 15 ~ \"1\", ysdeg > 15 ~ \"0\"))\nsalary$HireTime <- as.numeric(salary$HireTime)\n\n# run correlation matrix to check for multicollinearity. it's important to do so especially because HireTime is derived from ysdeg, so we would expect them to be highly correlated.\nsubset <- salary %>% select(year, ysdeg, HireTime)\nmatrix <- cor(subset, use=\"complete.obs\")\n\nI would remove ysdeg from the regression model since it is highly correlated with HireTime, r = 0.84.\n\nmodel2F <- lm(salary ~ . - ysdeg, salary)\nsummary(model2F)\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18301.04    1301.36  14.063  < 2e-16 ***\ndegreePhD     818.93     797.48   1.027   0.3100    \nrankAsst    -4972.66     997.17  -4.987 9.61e-06 ***\nrankProf     6124.28    1028.58   5.954 3.65e-07 ***\nsexFemale     907.14     840.54   1.079   0.2862    \nyear          434.85      78.89   5.512 1.65e-06 ***\nHireTime     2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nYes, those hired by the new Dean are making more, p = .05.\n\n\nQn 3A\n\n# load dataset.\ndata(house.selling.price)\n\n# run model.\nmodel3A <- lm(Price ~ Size + New, house.selling.price)\nsummary(model3A)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by $116 and $57,736 respectively.\n\n\nQn 3B\n\n# new homes.\nnew <- house.selling.price %>% filter(New == 1)\nmodel3B1 <- lm(Price ~ Size, new)\nsummary(model3B1)\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n# old homes.\nold <- house.selling.price %>% filter(New == 0)\nmodel3B2 <- lm(Price ~ Size, old)\nsummary(model3B2)\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses:\n\nNew: E(Price) = 166*Size - 100,755\n\nAdjusted R2 for the model is also much higher (0.90 vs. 0.57).\n\nOld: E(Price) = 104*Size - 22,228\n\n\n\nQn 3C\n\nNew: E(Price) = 166*3000 - 100,755 = $397,245\nOld: E(Price) = 104*3000 - 22,228 = $289,772\n\n\n\nQn 3D\n\nmodel3D <- lm(Price ~ Size*New, house.selling.price)\nsummary(model3D)\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\ninteract_plot(model3D, pred = Size, modx = New)\n\n\n\n\nSize and New positively interact to affect Price, such that New amplifies the positive relationship between Size and Price.\n\n\nQn 3E\nReferring to model3D above:\n\nNew: -22,228 + 104*Size - 78,528 + 62*Size = 166*Size - 100,756\nOld: 104*Size - 22,228\n\n\n\nQn 3F\n\nNew: 166*3000 -100,756 = $397,244\nOld: 104*3000 - 22,228 = $289,772\n\n\n\nQn 3G\n\nNew: 166*1500 - 100,756 = $148,244\nOld: 104*1500 - 22,228 = $133,772\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger.\n\n\nQn 3H\nQuestions 3F and 3G demonstrate that size affects the relationship between new and price, which reveals some type of dependency between the predictors. Hence, the model with interaction represents the relationships between size, new and price better."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html",
    "href": "posts/HW4_ShoshanaBuck.html",
    "title": "HW4_ShoshanaBuck",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#a",
    "href": "posts/HW4_ShoshanaBuck.html#a",
    "title": "HW4_ShoshanaBuck",
    "section": "A",
    "text": "A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\nPredicted selling price\n\n\nCode\nhome_size<- 1240\nlot_size<- 18000\n\ny= -10536 + (53.8 * home_size) + (2.84 * 18000)\ny\n\n\n[1] 107296\n\n\n\n\nResidual\n\n\nCode\nprice<- 145000\nprice- y\n\n\n[1] 37704\n\n\nThe predicted selling price is $107,296 and the residual is $37,704."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#b",
    "href": "posts/HW4_ShoshanaBuck.html#b",
    "title": "HW4_ShoshanaBuck",
    "section": "B",
    "text": "B\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nThe house selling price is predicted to increase by $53.8 for each square-foot increase in home size. This is due to the fact that x2 which is the lot size is fixed, which means in the prediction equation y = -10356 + 53.8x1 + 2.84x2 x1 would have input values making it increase."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#c",
    "href": "posts/HW4_ShoshanaBuck.html#c",
    "title": "HW4_ShoshanaBuck",
    "section": "C",
    "text": "C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nA fixed home size, 53.8*1 = 2.84\n\n\nCode\nx2<- 53.8/2.84\nx2\n\n\n[1] 18.94366"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#a-1",
    "href": "posts/HW4_ShoshanaBuck.html#a-1",
    "title": "HW4_ShoshanaBuck",
    "section": "A",
    "text": "A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\nsummary(lm(salary~sex,data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nTo test the hypothesis that the mean salary for men and women is the same without regard to other variables but sex I ran a simple regression model. I made Salary the outcome variable and sex as the explanatory variable. The p-value is 0.07 which means we fail to reject the null hypothesis, so we can’t conclude if the mean salary for men and women are the same."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#b-1",
    "href": "posts/HW4_ShoshanaBuck.html#b-1",
    "title": "HW4_ShoshanaBuck",
    "section": "B",
    "text": "B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nmf<- lm(salary~ ., data = salary)\nsummary(mf)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n95 % CI\n\n\nCode\nconfint(mf)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% confidence interval for the difference in salary between male and females is -697.8183 and 3030.56452."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#c-1",
    "href": "posts/HW4_ShoshanaBuck.html#c-1",
    "title": "HW4_ShoshanaBuck",
    "section": "C",
    "text": "C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nDegreeePhD: For a faculty member that has a PhD degree their predicted salary is $1388.61 than other faculty members who don’t have a PhD degree.\nRank: The baseline category is asst prof.For an associate professor their predicted salary is $5,292.36. For a professor their predicted salary is $11,118.76.\nSex: For a faculty member who is female their predicted salary is $1166.37 more than a male.\nYear: Every year a faculty member’s salary is expected to increase by $478.31.\nysdegree: For every year after degree completion they can expect to have their slary decrese by $124.57. ## D Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank<-relevel(salary$rank, ref = 'Assoc')\nsummary(lm( salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe baseline category is now ‘Assoc.’ From the multiple linear regression, faculty of rank are expected to make $5292.36 less than the rank of associate professors."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#e",
    "href": "posts/HW4_ShoshanaBuck.html#e",
    "title": "HW4_ShoshanaBuck",
    "section": "E",
    "text": "E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “a variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nsummary(lm( salary ~ degree + sex + year + ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWith the exclusion of the ‘rank’ coefficient the results vary a little. In this model female would be paid $1286 less than males when rank is excluded. Compared to when rank was included females made $1166 more dollars. However, whether rank is included or not both of the p-values are to high which we result in failing to reject the null hypothesis meaning that there can’t be a conclusion if rank contributes to men getting paid more than females."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#f",
    "href": "posts/HW4_ShoshanaBuck.html#f",
    "title": "HW4_ShoshanaBuck",
    "section": "F",
    "text": "F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nsalary<-mutate(salary, hired= case_when(ysdeg<15 ~ \"new\", ysdeg>=15 ~ \"old\"))\nsummary(lm( salary ~ degree + sex + rank +  hired + year, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + rank + hired + year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3588.0 -1532.2  -232.2   565.7  9132.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  20468.7      951.7  21.507  < 2e-16 ***\ndegreePhD     1073.5      843.3   1.273   0.2096    \nsexFemale     1046.7      858.0   1.220   0.2289    \nrankAsst     -5012.5     1002.3  -5.001 9.16e-06 ***\nrankProf      6213.3     1045.0   5.946 3.76e-07 ***\nhiredold     -2421.6     1187.9  -2.038   0.0474 *  \nyear           450.7       81.5   5.530 1.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2360 on 45 degrees of freedom\nMultiple R-squared:  0.8597,    Adjusted R-squared:  0.841 \nF-statistic: 45.95 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nmulticollinearity can be a concern when creating a new coefficient because the new coefficient was made from the coefficient ‘ysdeg.’ Yes, the findings support the hypothesis that the people hired by the new Dean are making higher than those that were not because old hires are making $2421.6 less than the new hires."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#a-2",
    "href": "posts/HW4_ShoshanaBuck.html#a-2",
    "title": "HW4_ShoshanaBuck",
    "section": "A",
    "text": "A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nFor the coefficient ‘size’, the price of a house is expected to increase by $116.132 in terms of the size of the home increasing by each square foot.\nFor the coefficient ‘new’, a new house is expected to cost $57,7736 more than an old house."
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#b-2",
    "href": "posts/HW4_ShoshanaBuck.html#b-2",
    "title": "HW4_ShoshanaBuck",
    "section": "B",
    "text": "B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nY = -40230.867 + 116.132x1 + 57736.283x2 x1 = size of home (in square feet) x2 = new (1 = yes; 0 = no)\nNew house:-40230.867 + 116.132x1 + 57736.283 Old house: -40230.867 + 116.132x1"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#c-2",
    "href": "posts/HW4_ShoshanaBuck.html#c-2",
    "title": "HW4_ShoshanaBuck",
    "section": "C",
    "text": "C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new\n\nNew\n\n\nCode\nsize<- 3000\n-40230.867 + (116.132 * size) + 57736.283 \n\n\n[1] 365901.4\n\n\n\n\nOld\n\n\nCode\nsize<- 3000\n-40230.867 + (116.132 *size)\n\n\n[1] 308165.1"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#d",
    "href": "posts/HW4_ShoshanaBuck.html#d",
    "title": "HW4_ShoshanaBuck",
    "section": "D",
    "text": "D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nsummary(lm( Price~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#e-1",
    "href": "posts/HW4_ShoshanaBuck.html#e-1",
    "title": "HW4_ShoshanaBuck",
    "section": "E",
    "text": "E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nNew house: -22227.808 + 104.438x1 - 78527.502X2 + 61.96x3 Old house: -22227.808 + 104.438x1"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#f-1",
    "href": "posts/HW4_ShoshanaBuck.html#f-1",
    "title": "HW4_ShoshanaBuck",
    "section": "F",
    "text": "F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\nNew\n\n\nCode\nsize<- 3000\n-22227.808 + (104.438 * 3000) - (78527.502* 3000) + (61.96*3000)\n\n\n[1] -235105540\n\n\n\n\nOld\n\n\nCode\nsize<- 3000\n-22227.808 + (104.438 * 3000)\n\n\n[1] 291086.2"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#g",
    "href": "posts/HW4_ShoshanaBuck.html#g",
    "title": "HW4_ShoshanaBuck",
    "section": "G",
    "text": "G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\nNew\n\n\nCode\nsize<- 1500\n-22227.808 + (104.438 * 1500) - (78527.502* 1500) + (61.96*1500)\n\n\n[1] -117563884\n\n\n\n\nOld\n\n\nCode\nsize<- 1500\n-22227.808 + (104.438 * 1500)\n\n\n[1] 134429.2"
  },
  {
    "objectID": "posts/HW4_ShoshanaBuck.html#h",
    "href": "posts/HW4_ShoshanaBuck.html#h",
    "title": "HW4_ShoshanaBuck",
    "section": "H",
    "text": "H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nI think that the second model with the interaction represents the relationship of size and new to the outcome price. I found that in the first model the outcome price was very high for the new house."
  },
  {
    "objectID": "posts/HW4_Solutions_OmerYalcin.html",
    "href": "posts/HW4_Solutions_OmerYalcin.html",
    "title": "Homework 4",
    "section": "",
    "text": "Please check your answers against the solutions.\nLoad the necessary packages:"
  },
  {
    "objectID": "posts/HW4_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW4_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1\n\nA\nLet’s write a function that expresses the outcome as a function of \\(x_1\\) and \\(x_2\\)\n\ny_hat <- function(x1, x2) {-10536 + 53.8*x1 + 2.84*x2}\n\nThe predicted selling price for a home of 1240 square feet on a lot of 18,000 square feet is:\n\npredicted <- y_hat(x1 = 1240, x2 = 18000)\ncat('$', predicted, sep = '')\n\n$107296\n\n\nThe actual price is $145,000. Thus, the residual is:\n\ncat(145000 - predicted)\n\n37704\n\n\nThe residual is positive. The model underpredicts the selling price for this house, arguably by a lot.\n\n\nB\nFor a fixed lot size, the house selling price is predicted to increase by $53.8, because that’s the coefficient of the size of home variable when lot size is also in the model, thus controlling for (or holding fixed) the latter.\n\n\nC\nOne-square-foot increase in home size is associated with an increase in price of $53.8. One-square-foot increase in lot size is associated with an increase in price of $2.84. To have an impact of a one-square foot increase in home size, which $53.8, lot size would have to increase by 53.8/2.84, which is an increase of about 18.94 square-feet."
  },
  {
    "objectID": "posts/HW4_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW4_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\nLoad the data:\n\ndata(salary)\n\n\nA\nBecause the question says “without regard to any other variable but sex,” we can run a simple linear regression model with sex as the only explanatory variable. This is equivalent to doing a two-sample t-test for salary for the Male and Female groups\n\nsummary(lm(salary ~ sex, data = salary))\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe coefficient -3340 for the sexFemale variable suggests that female faculty makes on average $3340 less than their male colleagues in this college. In other words, the difference in mean is 3340. The variable is statistically significant at the 10% level, but not at the more widely accepted 5% level.\n\n\nB\nFor the model with all predictors, here is what the confidence intervals look like:\n\nlm(salary ~ ., data = salary) |>\n  confint() \n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe 95% confidence interval for the sexFemale variable is (-697, 3031). This suggests an confidence interval between $697 less or $3031 more salary for female faculty relative to male faculty, controlling for other variables.\n\n\nC\n\nsummary(lm(salary ~ ., data = salary))\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\ndegree: The degree variable is not statistically significant at any conventionally accepted level. It is a dummy variable, so the coefficient suggests that PhDs make $1389 more compared to those with Masters controlling for everything else.\nrank The model takes rank as an categorical variable, ignoring its order. The most common practice for ordered categorical variables like rank is to either treat them as just a regular categorical variable or as a numeric variable. The latter is most acceptable when the variable has lots of levels and/or the distance between each level can be reasonably thought of as equal. In this case, because there are only three levels (one more than what a dummy variable has), it makes sense to accept this as a regular categorical variable.\n\nGiven this, the rankAssoc category suggests that Associate Professors make $5292 more than Assistant Professors, the base (reference) level. rankProf suggests full professors make $11118 more than Assistant Professors. Both variables are statistically significant.\nIf we wanted to test for the statistical significance of the rank variable as a whole, rather than for the individual dummy variables, we would need to do a partial F-test to compare the model with all the variables to the one without any rank dummies. The easiest way to do this is:\n\nfit1 <- lm(salary ~ ., data = salary)\nfit2 <- lm(salary ~ . -rank, data = salary)\n# see here: https://www.youtube.com/watch?v=G_obrpV70QQ\nanova(fit1, fit2)\n\nAnalysis of Variance Table\n\nModel 1: salary ~ degree + rank + sex + year + ysdeg\nModel 2: salary ~ (degree + rank + sex + year + ysdeg) - rank\n  Res.Df       RSS Df  Sum of Sq     F    Pr(>F)    \n1     45 258858365                                  \n2     47 658649047 -2 -399790682 34.75 7.485e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe very small p-value (Pr(>F)) suggests that the rank variable is significant as a whole.\n\nsex: As we saw with confidence intervals, this variable is now not statistically significant at conventional levels. The coefficient suggests female faculty make $1166 more after everything is controlled, but interpreting coefficients when the effect is insignificant is not very meaningful.\nyear: This variable is statistically significant. It suggests that every additional in current rank is associated with $476 more salary.\nysdeg: The variable is insignificant. The coefficient would suggest that every additional year that passes since degree is associated with $124 less salary\n\n\n\nD\n\nsalary$rank <- relevel(salary$rank, ref = 'Prof')\nsummary(lm(salary ~ ., data = salary))\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe coefficients are now for the two dummy variables that represent the two categories other than the reference category, which is now Prof. rankAsst being -11118 means Assistant Professors make 11118 less than Full Professors, controlling for other variables. rankAssoc being -5826 means Associate Professors make 5826 less than Full Professors, controlling for other variables. The same information in the previous model is presented in a different way.\n\n\nE\n\nsummary(lm(salary ~ . -rank, data = salary))\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding rank does cause the sign of the sex variable to flip to negative, but it is still not statistically significant.\n\n\nF\nBased on the description, we would need to create a new dummy variable, which I’ll call new_dean, from the ysdeg variable. To avoid multicollinearity, it would be a good idea to not include highly correlated variables. Because we are creating one variable from another, it is likely that they are highly correlated. Let’s see if that’s the case.\n\nsalary$new_dean <- ifelse(salary$ysdeg <= 15, 1, 0)\ncor.test(salary$new_dean, salary$ysdeg)\n\n\n    Pearson's product-moment correlation\n\ndata:  salary$new_dean and salary$ysdeg\nt = -11.101, df = 50, p-value = 4.263e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9074548 -0.7411040\nsample estimates:\n       cor \n-0.8434239 \n\n\nYes, they are very highly (negatively) correlated. So, we’ll exclude ysdeg and only include new_dean in its place, alongside other control variables.\n\nsummary(lm(salary ~ . -ysdeg, data = salary))\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24425.32    1107.52  22.054  < 2e-16 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\nnew_dean      2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nThe model above does suggest that the new dean has been making more generous offers, since the faculty appointed under the new dean make about $2163 more, controlling for other variables. The variable is statistically significant at the 5% level.\nLet’s see what would have happened if we included both variables:\n\nsummary(lm(salary ~ . , data = salary))\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3621.2 -1336.8  -271.6   530.1  9247.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25179.14    1901.59  13.241  < 2e-16 ***\ndegreePhD     1135.00    1031.16   1.101    0.277    \nrankAsst    -11411.45    1362.02  -8.378 1.16e-10 ***\nrankAssoc    -6177.44    1043.04  -5.923 4.39e-07 ***\nsexFemale     1084.09     921.49   1.176    0.246    \nyear           460.35      95.09   4.841 1.63e-05 ***\nysdeg          -47.86      97.71  -0.490    0.627    \nnew_dean      1749.09    1372.83   1.274    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2382 on 44 degrees of freedom\nMultiple R-squared:  0.8602,    Adjusted R-squared:  0.838 \nF-statistic: 38.68 on 7 and 44 DF,  p-value: < 2.2e-16\n\n\nNow, neither variable is significant because of multicollinearity."
  },
  {
    "objectID": "posts/HW4_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW4_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\nLoad the data:\n\ndata(house.selling.price)\n\n\nA\n\nfit <- lm(Price ~ Size + New, data = house.selling.price)\nsummary(fit)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nIn the model, both the Size and the New variables are positively associated with Price. They are both statistically significant at the 5% level. A 1 square foot increase in the size of the house is associated with a $116 increase in house, controlling for whether the house is new. New houses are on average $57736 more expensive than old houses, controlling for size.\n\n\nB\nThe interpretation part is a bit redundant, since it was done in part A.\nThe prediction equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * \\textrm{new}\\]\nFor new homes, the new variable takes the value 1, for old homes, it takes 0.\nSo, for new homes, the equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * 1 = 17505.42 + 116.132 * \\textrm{size} \\]\nFor old homes, the equation is:\n\\[\\mathbf{E}[\\textrm{Price}] = -40230.867 + 116.132 * \\textrm{size} + 57736.283 * 0 = -40230.867 + 116.132 * \\textrm{size}\\]\n\n\nC\nWe can create a data frame and use the predict() function to do the prediction.\n\ndata.frame(Size = c(3000, 3000), New = c(1, 0)) %>%\n  predict(fit, .) \n\n       1        2 \n365900.2 308163.9 \n\n\nA new home of 3000 square feet has a predicted price of 365900.2. An old home of 3000 square feet has a predicted price of 308163.9. (Note that the difference, 57736.3, is the coefficient of New)\n\n\nD\nModel with interaction term:\n\nfit_ia <- lm(Price ~ Size + New + Size * New, data = house.selling.price)\nsummary(fit_ia)\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\nE\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * \\textrm{New} + 61.916 * \\textrm{Size} * \\textrm{New}\\]\nAgain, we follow the same logic from B, replacing New with 1 and 0:\nFor new homes:\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * 1 + 61.916 * \\textrm{Size} * 1 = -100755.3 + 166.354 * size\\]\nFor old homes:\n\\[\\mathbf{E}[\\textrm{Price}] = -22227.808 + 104.438 * \\textrm{size} + -78527.502 * 0 + 61.916 * \\textrm{Size} * 0 = -22227.808 + 104.438 * \\textrm{size}\\]\n\n\nF\nUsing predict() again:\n\ndata.frame(Size = c(3000, 3000), New = c(1, 0)) %>%\n  predict(fit_ia, .) \n\n       1        2 \n398307.5 291087.4 \n\n\nA new home of 3000 square feet has a predicted price of $398307.5. An old home of 3000 square feet has a predicted price of $291087.4. The difference is $107220.1.\n\n\nG\n\ndata.frame(Size = c(1500, 1500), New = c(1, 0)) %>%\n  predict(fit_ia, .) \n\n       1        2 \n148776.1 134429.8 \n\n\nA new home of 1500 square feet has a predicted price of $148776.1. An old home of 1500 square feet has a predicted price of $134429.8. The difference is $14346.3.\nThe difference between new and old home prices is much more when the size of the home is larger. For 3000 sq ft homes, the difference is 107220.1 as opposed to the 14346.1 difference for homes that are 1500 sq ft. This is consistent with the positive coefficient for the interaction term.\n\n\nH\nI prefer the model with the interaction term, because (1) the interaction term is significant, (2) the Adjusted R-squared is higher in the model with interaction (i.e. despite the penalty for the additional term, we have better fit). We could look into cross-validation / PRESS (which also end up showing the interaction model to be superior), but those are not this homework’s topic.\n\nsummary(fit)\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\nsummary(fit_ia)\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_StephRoberts.html",
    "href": "posts/HW4_StephRoberts.html",
    "title": "HW4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW4_StephRoberts.html#question-1",
    "href": "posts/HW4_StephRoberts.html#question-1",
    "title": "HW4",
    "section": "Question 1",
    "text": "Question 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n1A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\nest <- (53.8*1240) + (2.84*18000)-10536\nprint(paste(\"The predicted selling price is\", est))\n\n\n[1] \"The predicted selling price is 107296\"\n\n\n\n\nCode\nres <- 145000 - est\nprint(paste(\"The residual is\", res))\n\n\n[1] \"The residual is 37704\"\n\n\n\n\n1B\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nThe coefficient of the square-footage variable is 53.8, which indicates that is how much the selling price will increase for every increase in square-foot.\n\n\n1C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nCode\nsfcoef <- 53.8\nlotcoef<- 2.84\n\nlotinc<-sfcoef/lotcoef\nprint(paste(\"The lot size would need to increase\", lotinc, \"for every 1 square foot increasenin home size\"))\n\n\n[1] \"The lot size would need to increase 18.943661971831 for every 1 square foot increasenin home size\""
  },
  {
    "objectID": "posts/HW4_StephRoberts.html#question-2",
    "href": "posts/HW4_StephRoberts.html#question-2",
    "title": "HW4",
    "section": "Question 2",
    "text": "Question 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n2A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\ndata(\"salary\")\ndim(salary)\n\n\n[1] 52  6\n\n\nCode\nhead(salary)\n\n\n   degree rank    sex year ysdeg salary\n1 Masters Prof   Male   25    35  36350\n2 Masters Prof   Male   13    22  35350\n3 Masters Prof   Male   10    23  28200\n4 Masters Prof Female    7    27  26775\n5     PhD Prof   Male   19    30  33696\n6 Masters Prof   Male   16    21  28516\n\n\n\n\nCode\nt.test(salary ~ sex, data=salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe null hypothesis would be that male and females have the same mean salary. With a p-value of 0.09 we fail to reject the null hypothesis at the usual significance level of alpha = 0.05. Based on the data, we can conclude there is not enough evidence of a difference between the true average of the two groups.\n\n\n2B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nallvar <- lm(salary ~ ., data = salary)\nsummary(allvar)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nAs discovered earlier, sex does not have a large impact on salary when other variables are involved.\n\n\nCode\nconfint(allvar)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nThe confidence interval for the difference between male and female salary is between -$697.81 and $3,030.57. Because this range includes negative numbers, it shows that females can make more of less than males even when all factors are accounted for.\n\n\n2C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nConsidering an alpha = 0.05, we can interpret the statistical significance of the predictor variables.\nDegree: not significant and has a coefficient of 1388, meaning those with PhDs make an average of $1,388 more. Ranking Associate: IS significant and has a coefficient of 5292.36, meaning a rank change from assistant to Associate impacts the salary an average of $5,292.36. Ranking Professor: IS significant and has a coefficient of 11118.76, meaning one unit of rank from assistant to Professor impacts the salary an average of $11,118.76. Sex: not significant and has a coefficient of 1166.37, meaning a change in the sex of an observation changes the salary an average of $1,166.37, in favor of the females. Year: IS significant and has a coefficient of 476.31, meaning one more year in rank impacts the salary an average of $476.31. Year since degree: not significant and has a coefficient of -124.57, meaning every additional year since degree negatively impacts salary by $124.57.\n\n\n2D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\ntable(salary$rank)\n\n\n\n Asst Assoc  Prof \n   18    14    20 \n\n\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Assoc\")\nmod_rank <- lm(salary ~ ., data = salary)\nsummary(mod_rank)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nChanging the baseline of the rank variable changes the output because out input is asking a different question. Here, we see that a “unit change” is now based on the associate rank as a baseline, instead of assistant, as defaulted before. This tells us that a change in unit from associate to assistant impacts salary by an average of - $5,292.36 and a change to professor impacts salary an average of $5,826.40.\n\n\n2E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nno_rank <- lm(salary ~ degree  + sex + year + ysdeg, data = salary)\nsummary(no_rank)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding rank changes things a bit. Now, having a PhD degree impacts salary -$3,299 compared with masters. Also, removing rank affects the differences in the sex variable. It is still not significant, but with a coefficient of -1286.54, it suggests females make less when rank is controlled for. Years in rank is less significant and years since degree is more significant.\nHowever, with an R-squared of 0.63 compared with the earlier 0.86, this model is a worse fit for the data.\n\n\n2F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary. Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\n#Create new variable\nsalary <- salary %>%\n  mutate(new = case_when(ysdeg> 15 ~ 'no',\n                         ysdeg <= 15 ~ 'yes'))\n\nnew_hire <- lm(salary ~ degree + sex + rank + year + new, data = salary)\nsummary(new_hire)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + rank + year + new, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18301.04    1301.36  14.063  < 2e-16 ***\ndegreePhD     818.93     797.48   1.027   0.3100    \nsexFemale     907.14     840.54   1.079   0.2862    \nrankAsst    -4972.66     997.17  -4.987 9.61e-06 ***\nrankProf     6124.28    1028.58   5.954 3.65e-07 ***\nyear          434.85      78.89   5.512 1.65e-06 ***\nnewyes       2163.46    1072.04   2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nConsidering the concept of multicollinearity, I excluded ysdeg as the “new” variable accounts for the same data. With this modification, our R-squared is still high at 0.86. Rank and years in rank are still significant. Interestingly, “new” is also significant, with a coefficient of 2163.46 suggests being a new hire impacts salary an average of $2,163.46. This supports the hypothesis that those hired by the new dean are getting better offers."
  },
  {
    "objectID": "posts/HW4_StephRoberts.html#question-3",
    "href": "posts/HW4_StephRoberts.html#question-3",
    "title": "HW4",
    "section": "Question 3",
    "text": "Question 3\n(Data file: house.selling.price in smss R package)\n\n3A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\ndata(\"house.selling.price\") \nhouse <- house.selling.price\nhead(house)\n\n\n  case Taxes Beds Baths New  Price Size\n1    1  3104    4     2   0 279900 2048\n2    2  1173    2     1   0 146500  912\n3    3  3076    4     2   0 237700 1654\n4    4  1608    3     2   0 200000 2068\n5    5  1454    3     3   0 159900 1477\n6    6  2997    3     2   1 499900 3153\n\n\n\n\nCode\nsize_new <- lm(Price ~ Size + New, data = house)\nsummary(size_new)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth the size of the house and whether it is new are statistically significant factors in the selling price of these homes. For each square foot increase of house size, the price is impacted by $116.13. A home being new is worth $57,736.28 more on average than homes that are not.\n\n\n3B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nThe prediction equation for the size_new model is y= -40230.867 +116.132x1 +57736.283x2, where x1 = size and x2 = new.\nThe prediction for new homes is y= -40230.867 +116.132x1 +57736.283x2 The prediction for old homes is y= -40230.867 +116.132x1\n\n\n3C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 3000 square feet that is NEW is $\",-40230.867 +116.132*3000 +57736.283))\n\n\n[1] \"The predicted selling price for a home of 3000 square feet that is NEW is $ 365901.416\"\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 3000 square feet that is NOT NEW is $\",-40230.867 +116.132*3000))\n\n\n[1] \"The predicted selling price for a home of 3000 square feet that is NOT NEW is $ 308165.133\"\n\n\n\n\n3D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nia <- lm(Price ~ Size*New, data = house)\nsummary(ia)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nThe R-squared of the interaction model is slightly better than the previous, suggesting it may be a better fit. The interaction between size and “newness” of a house is significant.The effect of size when a house is new increases price by $61.916 per square foot.\n\n\n3E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nThe prediction equation would be y = -22,2227.808 + 104.438x1 - 78,527.502x2 + 61.916x1x2, where x1 = size and x2 = new.\nThe equation for predicting a new house with this model is y = -22,2227.808 + 104.438x1 - 78,527.502 + 61.916x1 The equation for predicting a NOT new house with this model y = -22,2227.808 + 104.438x1\n\n\n3F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 3000 square feet that is NEW is $\", -22227.808 + (104.438 * 3000) - (78527.502 * 1) + (61.916 * 3000 * 1)))\n\n\n[1] \"The predicted selling price for a home of 3000 square feet that is NEW is $ 398306.69\"\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 3000 square feet that is NOT NEW is $\",-22227.808 + (104.438 * 3000)))\n\n\n[1] \"The predicted selling price for a home of 3000 square feet that is NOT NEW is $ 291086.192\"\n\n\n\n\n3G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 1500 square feet that is NEW is $\", -22227.808 + (104.438 * 1500) - (78527.502 * 1) + (61.916 * 1500 * 1)))\n\n\n[1] \"The predicted selling price for a home of 1500 square feet that is NEW is $ 148775.69\"\n\n\nCode\nprint(paste(\"The predicted selling price for a home of 1500 square feet that is NOT NEW is $\",-22227.808 + (104.438 * 1500)))\n\n\n[1] \"The predicted selling price for a home of 1500 square feet that is NOT NEW is $ 134429.192\"\n\n\nThere is less of a difference when size is smaller because there are 2 coefficients that increase price as size increases. So that bigger the house, the more impact “new” has to the price.\n\n\n3H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nI think the model with the interaction represents the relationship of size and new to the price of a house because of a couple of things. First, the fit of the model, seen in the higher R-squared, is slightly favorable than the non-interaction model. Also, logically, it makes sense that a bigger house with more new materials (lumber, counters, lights) would increase the price."
  },
  {
    "objectID": "posts/HW4_SteveONeill.html",
    "href": "posts/HW4_SteveONeill.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/HW4_SteveONeill.html#question-1",
    "href": "posts/HW4_SteveONeill.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2\nA. A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\nhome_size <- 1240\nlot_size <- 18000\n\ny = -10536 + (53.8 * home_size) + (2.84 * lot_size)\ny\n\n\n[1] 107296\n\n\nCode\n145000 - y\n\n\n[1] 37704\n\n\nThe predicted selling price of the home is $107,296. The residual is $37704, which means the model under-predicted.\nB. Holding home size fixed, each one-square-foot increase in lot size brings a 2.84 increase in dollar value.\nC. Holding lot size fixed, each sq. ft. increase in home size results in a $53.8 increase in home price."
  },
  {
    "objectID": "posts/HW4_SteveONeill.html#question-2",
    "href": "posts/HW4_SteveONeill.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nCode\ndata(salary)\nsalary\n\n\n    degree  rank    sex year ysdeg salary\n1  Masters  Prof   Male   25    35  36350\n2  Masters  Prof   Male   13    22  35350\n3  Masters  Prof   Male   10    23  28200\n4  Masters  Prof Female    7    27  26775\n5      PhD  Prof   Male   19    30  33696\n6  Masters  Prof   Male   16    21  28516\n7      PhD  Prof Female    0    32  24900\n8  Masters  Prof   Male   16    18  31909\n9      PhD  Prof   Male   13    30  31850\n10     PhD  Prof   Male   13    31  32850\n11 Masters  Prof   Male   12    22  27025\n12 Masters Assoc   Male   15    19  24750\n13 Masters  Prof   Male    9    17  28200\n14     PhD Assoc   Male    9    27  23712\n15 Masters  Prof   Male    9    24  25748\n16 Masters  Prof   Male    7    15  29342\n17 Masters  Prof   Male   13    20  31114\n18     PhD Assoc   Male   11    14  24742\n19     PhD Assoc   Male   10    15  22906\n20     PhD  Prof   Male    6    21  24450\n21     PhD  Asst   Male   16    23  19175\n22     PhD Assoc   Male    8    31  20525\n23 Masters  Prof   Male    7    13  27959\n24 Masters  Prof Female    8    24  38045\n25 Masters Assoc   Male    9    12  24832\n26 Masters  Prof   Male    5    18  25400\n27 Masters Assoc   Male   11    14  24800\n28 Masters  Prof Female    5    16  25500\n29     PhD Assoc   Male    3     7  26182\n30     PhD Assoc   Male    3    17  23725\n31     PhD  Asst Female   10    15  21600\n32     PhD Assoc   Male   11    31  23300\n33     PhD  Asst   Male    9    14  23713\n34     PhD Assoc Female    4    33  20690\n35     PhD Assoc Female    6    29  22450\n36 Masters Assoc   Male    1     9  20850\n37 Masters  Asst Female    8    14  18304\n38 Masters  Asst   Male    4     4  17095\n39 Masters  Asst   Male    4     5  16700\n40 Masters  Asst   Male    4     4  17600\n41 Masters  Asst   Male    3     4  18075\n42     PhD  Asst   Male    3    11  18000\n43 Masters Assoc   Male    0     7  20999\n44 Masters  Asst Female    3     3  17250\n45 Masters  Asst   Male    2     3  16500\n46 Masters  Asst   Male    2     1  16094\n47 Masters  Asst Female    2     6  16150\n48 Masters  Asst Female    2     2  15350\n49 Masters  Asst   Male    1     1  16244\n50 Masters  Asst Female    1     1  16686\n51 Masters  Asst Female    1     1  15000\n52 Masters  Asst Female    0     2  20300\n\n\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\nsummary(lm(salary$salary ~ salary$sex))\n\n\n\nCall:\nlm(formula = salary$salary ~ salary$sex)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         24697        938  26.330   <2e-16 ***\nsalary$sexFemale    -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe hypothesis that mean salary for men and women is the same without regard to any other variable but sex is confirmed. Normally, I would expect this to be phrased as the null hypothesis. But to answer the specific wording of this question, the hypothesis (of ‘sameness’) is confirmed and the null hypothesis (of difference) is rejected.\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\nmoe = qt(.975, df=45) * 925.57 #aka standard error\nupper<- 1166.37 + moe #1166.37 is the coefficient of sexFemale\nlower<- 1166.37 - moe \nupper\n\n\n[1] 3030.564\n\n\nCode\nlower\n\n\n[1] -697.8237\n\n\nThe confidence interval is (-697.8237, 3030.564)\nHere are the findings from earlier:\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\nThe standout variables are rank and year. Being a full professor has double the effect on salary as an associate, and the p-value is also much smaller. Year has a smaller yet significant effect. Asst is missing because it is the baseline category.\nYears since highest degree earned is not statistically significant, nor seems to be sex (in this dataset, with these observations).\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Assoc\")\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21038.41    1109.12  18.969  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAsst    -5292.36    1145.40  -4.621 3.22e-05 ***\nrankProf     5826.40    1012.93   5.752 7.28e-07 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nWith rankAssoc excluded and rankAsst included, now we also see that being an assistant professor is negatively correlated with salary - this makes sense considering basic intuition.\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nsummary(lm(salary ~ . - rank, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWith rank removed entirely, years since highest degree earned becomes the most significant variable, with PhD and years of service also being significant.\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nsalary$dean_old <- ifelse(salary$year >=15, 1, 0)\nhead(salary[c('dean_old', 'year')], n = 10)\n\n\n   dean_old year\n1         1   25\n2         0   13\n3         0   10\n4         0    7\n5         1   19\n6         1   16\n7         0    0\n8         1   16\n9         0   13\n10        0   13\n\n\nFor the new linear model, I am excluding years of service and only including years since highest degree attained. Since dean_old is derived from year, this avoids multicolinearity:\n\n\nCode\nsummary(lm(salary ~ . -year, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ . - year, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5076.5 -1468.1  -569.6  1382.1  9444.4 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 21735.66    1362.56  15.952  < 2e-16 ***\ndegreePhD     436.19    1217.06   0.358  0.72172    \nrankAsst    -4265.73    1372.22  -3.109  0.00325 ** \nrankProf     6153.34    1225.02   5.023 8.52e-06 ***\nsexFemale    -661.30    1013.29  -0.653  0.51732    \nysdeg          57.20      80.92   0.707  0.48324    \ndean_old     2401.72    1415.86   1.696  0.09674 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2904 on 45 degrees of freedom\nMultiple R-squared:  0.7875,    Adjusted R-squared:  0.7592 \nF-statistic: 27.79 on 6 and 45 DF,  p-value: 1.34e-13\n\n\nAfter including the new “dean” variable and removing year, I don’t find a significant difference in the salaries of professors hiring the two new deans - in other words, the null hypothesis fails to be rejected. It would be significant at a 90% confidence level, but we went in with a 5% significance level\nIt could be argued that year, representing years of service to the university, is colinear with ysdeg, years since highest degree earned. Both of them are a function of time and represent the career-stage of a professor. However, I think professors often come in from different universities so they are not necessarily correlated. I could really see it both ways. If that variable is also removed, the dean_old variable becomes significant at the 5% confidence level.\n\n\nCode\nlibrary(smss)\ndata(\"house.selling.price\")\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\n\n\nCode\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth size and new are statistically significant variables. The size variable has a much lower p-value, meeting 0.1 % significance level.\nAn increase in one sq. ft. of size may increase a house’s value by $116.13 if new is held constant. A house being new can affect the price by $57,736.28 with size being held constant.\nThe prediction equation would be:\nPredicted Price = 116.132(Size) + 57736.283(New) -40230.867\n\n\nCode\nsummary(lm(Price ~ New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-185064  -49042   -9967   22183  448433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   138567       9504  14.580  < 2e-16 ***\nNew           152396      28655   5.318 6.61e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89660 on 98 degrees of freedom\nMultiple R-squared:  0.224, Adjusted R-squared:  0.2161 \nF-statistic: 28.28 on 1 and 98 DF,  p-value: 6.608e-07\n\n\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nEquations for just size and just newness, respectively, are:\nPredicted Price = 126.594(Size) -50926.255\nPredicted Price = 152396(New) + 138567\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nnew <- 1\nnotnew <- 0\nsize <- 3000\n\n(116.132 * size) + (57736.283 * new) -40230.867\n\n\n[1] 365901.4\n\n\nCode\n(116.132 * size) + (57736.283 * notnew) -40230.867\n\n\n[1] 308165.1\n\n\nThe predicted selling price of a new vs not new home of 3000 sq/ft is $365901.4 and $308165.1, respectively.\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nfit <- summary(lm(Price ~ Size + New + Size * New, data = house.selling.price))\nfit\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\n\nCode\nlibrary(ggplot2)\n\nnew_labels <- c(\n                    `0` = \"Not New\",\n                    `1` = \"New\"\n                    )\n\nggplot(house.selling.price, aes(x = Size, y = Price)) + \n  geom_point() +\n  stat_smooth(method = \"lm\", col = \"red\") +\n  facet_grid(house.selling.price$New, labeller = as_labeller(new_labels)) + \n  scale_y_continuous(labels=scales::dollar_format())\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHere is a faceted plot representation of New, re: Size and Price.\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nsize <- 3000\nsize_coef <- 104.438\nnew_coef <- -78527.502\nsizenew_coef <- 61.916\n\n(size_coef * size) + (new_coef * 0) + (sizenew_coef * 0 * 3000) - 22227.808\n\n\n[1] 291086.2\n\n\nCode\n(size_coef * size) + (new_coef * 1) + (sizenew_coef * 1 * 3000) - 22227.808\n\n\n[1] 398306.7\n\n\nThe predicted selling price for homes of 3000 sq/ft are, taking into account condition:\nNot new: $291086.2 New: $398306.7\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\nsize <- 1500\nsize_coef <- 104.438\nnew_coef <- -78527.502\nsizenew_coef <- 61.916\n\n(size_coef * size) + (new_coef * 0) + (sizenew_coef * 0 * size) - 22227.808\n\n\n[1] 134429.2\n\n\nCode\n(size_coef * size) + (new_coef * 1) + (sizenew_coef * 1 * size) - 22227.808\n\n\n[1] 148775.7\n\n\nHere, the costs are $134429.2 and $148775.7, respectively. This is a much smaller difference. It means that as the size of houses increase, newness increases the price of the house more, per dollar.\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nThe model with interaction variables has a higher R-squared, and to me it makes sense that the cost of newness is seen in larger houses. Therefore I think the second model is the better one."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html",
    "href": "posts/HW4_ToryBartelloni.html",
    "title": "DACSS 603: Homework 4",
    "section": "",
    "text": "For recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\nintercept = -10536\nb1 = 53.8\nb2 = 2.84\nselling_price = 145000\n\npred_selling_price <- intercept + b1*1240 + b2*18000\nresidual = selling_price - pred_selling_price\n\nprint(paste(\"Predicted selling price is\", pred_selling_price, ))\n\n\nError in paste(\"Predicted selling price is\", pred_selling_price, ): argument is missing, with no default\n\n\nCode\nprint(paste(\"Residual for the predicted selling price is\", residual))\n\n\n[1] \"Residual for the predicted selling price is 37704\"\n\n\nAnswer: The predicted selling price is significantly lower than the actual selling price for this listing, resulting in a large residual. This means that the house sold for higher than we would expect given our fitted model and either this is an outlier or there may be other factors that drove the selling price up that are not accounted for in the model.\n\n\n\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nAnswer: Selling price is predicted to increase by $53.80 for each additional square foot size of the house. In model terms, this is because our coefficient for the square-foot predictor is 53.8 which is the amount the outcome is expected to change with a one unit increase to the predictor (square-foot house size). In practical terms this is likely because large house sizes are more valuable and desirable for buyers and the market data used to create the model has revealed an expected value for each additional suqare foot.\n\n\n\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\n\n\nCode\nb1 <- 53.8\nb2 <- 2.84\n\nprint(paste(\"The ratio of square-foot house size coefficient to square-foot lot size coefficient is\", b1/b2))\n\n\n[1] \"The ratio of square-foot house size coefficient to square-foot lot size coefficient is 18.943661971831\"\n\n\nAnswer: According to the ratio of house-size to lot-size square-foot coefficients the lot size would need to increase by approximately 18.94 feet to equal the same increase to selling price as an increase of 1 square foot in house size."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#a-1",
    "href": "posts/HW4_ToryBartelloni.html#a-1",
    "title": "DACSS 603: Homework 4",
    "section": "2.A",
    "text": "2.A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\nlibrary(dplyr)\n\nmen <- salary %>% filter(sex==\"Male\") %>% select(salary)\n\nwomen <- salary %>% filter(sex==\"Female\") %>% select(salary)\n\nt.test(men$salary, women$salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  men$salary and women$salary\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\nmean of x mean of y \n 24696.79  21357.14 \n\n\nAnswer: To determine whether male and female salary means are the same I conducted a Welch Two Sample t-Test. At a standard significance level of 0.05 we cannot reject the null hypothesis that the means are the same (p=0.09)."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#b-1",
    "href": "posts/HW4_ToryBartelloni.html#b-1",
    "title": "DACSS 603: Homework 4",
    "section": "2.B",
    "text": "2.B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nsal_model <- lm(salary ~ ., data=salary)\n#summary(sal_model)\n\nprint(paste(\"The 95% confidence interval for differences in salary between men and women is:\"))\n\n\n[1] \"The 95% confidence interval for differences in salary between men and women is:\"\n\n\nCode\nconfint(sal_model,\"sexFemale\")\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\n\nAnswer: Using the linear model for salary we find the 95% confidence interval for differences in mean salary for men and women to be between -698 and 3031 dollars."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#c-1",
    "href": "posts/HW4_ToryBartelloni.html#c-1",
    "title": "DACSS 603: Homework 4",
    "section": "2.C",
    "text": "2.C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables\n\n\nCode\nsummary(sal_model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\n#summary(lm(salary~rank, data=salary))\n\n\nAnswer: To interpret the model we have several variables to understand, most being categorical.\n1: Degree: This variable indicates the highest educational degree the faculty member has earned. There are two possible outcomes: Masters and PhD. The model shows that those with a PhD earn on average $1388 more than those with Master degrees. But the mode also shows us this is likely not a factor in salary with a high p-value of 0.18 so should not consider this a factor.\n2: Rank: This variable indicates what rank of faculty the individual is. There are three possible outcomes: Assistant, Associate, and Professor. The model shows that this variable has statistical significance and an F-test confirms this (p-value < 0.001). What we observe is that higher ranks result in higher salaries, with Associate getting average 5282 more than Assistant and Professor getting 11,118 more than Assistant rank faculty (5836 more than Associate rank faculty).\n3: Sex: This variable indicates the sex of the faculty member. There are two possible outcomes: Male and Female. The model shows that when other possible explanatory variables are held constant the sex of the faculty member would indicate higher salaries for Female faculty, but, consequentially, the model show that sex does not have a significant effect on salary (p-value = 0.21).\n4: Year: This variable indicates the number of years the individual faculty member has been at their current rank. It is an integer variable with a range between 0 and 25. The model shows that with the other explanatory variables held constant each additional year in their current rank is associated with a salary increase of 476 dollars and is significant.\n5: YS Degree: The last variable indicates the number of years since the faculty member earned their most recent degree. This is an integer variable with a range between 1 and 35. The model shows that there is a small negative effect with increased years, but that this variable is not significant at any conventional significance level so should not be viewed as determinate."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#d",
    "href": "posts/HW4_ToryBartelloni.html#d",
    "title": "DACSS 603: Homework 4",
    "section": "2.D",
    "text": "2.D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")   \nsal_model_2 <- lm(salary ~ ., data=salary)\n\nsummary(sal_model_2)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nAnswer: Having changed the baseline category for rank we now show the Assistant and Associate ranks, which are negative (as oppossed to positive in the original model). The coefficients are the same absolute value as before, but now negative because we moved the reference to the highest rank instead of the lowest so the coefficients are showing the changes expected with a demotion in rank as opposed to previously when they were showing expected salary changes with a promotion in rank."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#e",
    "href": "posts/HW4_ToryBartelloni.html#e",
    "title": "DACSS 603: Homework 4",
    "section": "2.E",
    "text": "2.E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nnoRank_model <- lm(salary ~ degree + sex + year + ysdeg, data= salary)\n\n#summary(noRank_model)\n\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nstargazer(sal_model, noRank_model, type=\"text\")\n\n\nError in stargazer(sal_model, noRank_model, type = \"text\"): could not find function \"stargazer\"\n\n\nAnswer: The model without rank provides substantially different results. In this model with see that Degree is now significant when it was not before and actually has a negative coefficient. Sex was observed as not significant again and its coefficient has flipped to negative. Year in current rank is still significant, but with a smaller effect (still positive). And Years since degree earned is now significant when it was not before, its effects has increased substantially, and its effect has flipped to be positive."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#f",
    "href": "posts/HW4_ToryBartelloni.html#f",
    "title": "DACSS 603: Homework 4",
    "section": "2.F",
    "text": "2.F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nsalary$dean <- if_else(salary$ysdeg <= 15, \"New\", \"Previous\")\n\nsummary(lm(salary ~ degree + sex + year + dean, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + dean, data = salary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10740.1  -2550.1     -3.3   1942.4  11718.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   18148.9     1188.2  15.274  < 2e-16 ***\ndegreePhD     -1186.6     1191.2  -0.996 0.324267    \nsexFemale      -523.5     1355.1  -0.386 0.701017    \nyear            531.4      130.2   4.082 0.000172 ***\ndeanPrevious   4449.8     1347.2   3.303 0.001834 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3958 on 47 degrees of freedom\nMultiple R-squared:  0.5878,    Adjusted R-squared:  0.5527 \nF-statistic: 16.75 on 4 and 47 DF,  p-value: 1.338e-08\n\n\nAnswer: In the new model we will fit all variables except rank and years since degree. We exclude rank for the reasons mentioned above (possibly discrimination in promotion). We exclude the years since rank variable because we are testing the effects the new dean may have on salary and we know that these two variables are determinate of each other so there will be multicollinearity.\nRegarding the new hypothesis, we do not find evidence to support it with the new model. In fact, we find evidence to specifically refute it. Under this model of Dean variable is significant and those hired under the previous dean are expected to earn more on average than those hired by the new dean."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#a-2",
    "href": "posts/HW4_ToryBartelloni.html#a-2",
    "title": "DACSS 603: Homework 4",
    "section": "3.A",
    "text": "3.A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\nselling_model <- lm(Price ~ Size + New,data=house.selling.price)\n\nsummary(selling_model)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nAnswer: In this model both explanatory variables are shows to be significant at the 0.05 significance level. Increases in house size are estimated to have an increase in selling price of 116 dollars per square-foot and a new house is expected to sell for 57,736 dollars more than a not-new house."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#b-2",
    "href": "posts/HW4_ToryBartelloni.html#b-2",
    "title": "DACSS 603: Homework 4",
    "section": "3.B",
    "text": "3.B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nAnswer: Below I will show both prediction equations in question.\nFor new houses: Selling Price = -40230.867 116.132 x Size + 57736.283 x New (if new)\nFor not new houses: Selling Price = -40230.867 116.132 x Size + -57736.283 x New (if new)\nFor both equations the selling price is expected to increase by 116.13 dollars for each additional square-foot in size. The difference in equation is whether we use new or not-new houses as our baseline category. In the instance of using new houses the price is expected to be higher when that variable increases (indicating the house is new)."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#c-2",
    "href": "posts/HW4_ToryBartelloni.html#c-2",
    "title": "DACSS 603: Homework 4",
    "section": "3.C",
    "text": "3.C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nnew <- data.frame(Size = c(3000,3000), New = c(0,1))\n\npredict(selling_model, newdata = new)\n\n\n       1        2 \n308163.9 365900.2"
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#d-1",
    "href": "posts/HW4_ToryBartelloni.html#d-1",
    "title": "DACSS 603: Homework 4",
    "section": "3.D",
    "text": "3.D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\n\n\nCode\nint_selling_model <- lm(Price ~ Size + New + Size*New, data=house.selling.price)\n\nsummary(int_selling_model)\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#e-1",
    "href": "posts/HW4_ToryBartelloni.html#e-1",
    "title": "DACSS 603: Homework 4",
    "section": "3.E",
    "text": "3.E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\n\nCode\nlibrary(ggplot2)\n\nhouse.selling.price %>% ggplot() +\n  geom_point(aes(x=Size,y=Price,color=as.factor(New))) +\n  geom_abline(intercept = -22227.808-78527.502, slope = 104.438+61.916, color=\"blue\") +\n  geom_abline(intercept = -22227.808, slope = 104.438, color=\"red\") +\n  scale_color_manual(breaks=c(\"0\",\"1\"), values=c(\"red\",\"blue\"))"
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#f-1",
    "href": "posts/HW4_ToryBartelloni.html#f-1",
    "title": "DACSS 603: Homework 4",
    "section": "3.F",
    "text": "3.F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\npredict(int_selling_model, newdata=new)\n\n\n       1        2 \n291087.4 398307.5"
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#g",
    "href": "posts/HW4_ToryBartelloni.html#g",
    "title": "DACSS 603: Homework 4",
    "section": "3.G",
    "text": "3.G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\nnew2 <- data.frame(Size=c(1500,1500),New=c(0,1))\n\npredict(int_selling_model, newdata=new2)\n\n\n       1        2 \n134429.8 148776.1 \n\n\nAnswer: At the smaller house size we see a smaller price difference between new and not-new house selling prices. From the model perspective this is because the effect of being a new home is negative on its own so we start at lower prices than not-new homes but the interaction term increases the prices quicker when they are new homes than not-new. From a practical perspective this may be because smaller home buyers are purchasing more through their budget than whether the house is new or not. There is a small amount of new home data in the dataset, especially for smaller square-foot new houses, so trust in the lower end of the regression line should be questioned."
  },
  {
    "objectID": "posts/HW4_ToryBartelloni.html#h",
    "href": "posts/HW4_ToryBartelloni.html#h",
    "title": "DACSS 603: Homework 4",
    "section": "3.H",
    "text": "3.H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\n\n\nError in stargazer(new_model, not_new_model, int_selling_model, selling_model, : could not find function \"stargazer\"\n\n\nAnswer: It’s hard for me to say which model I believe represents the true relationship at this point, but I would prefer the model with the interaction term. The model with the interaction term models new homes exceptionally well, while modeling not-new homes well but with some outliers. The two models have similar Adjusted R-squared with the interaction model outperforming slightly, but also increasing the complexity of the model. Complexity not a big concern with a model this simple but still a factor to consider. What makes me question the interaction model the most is the small number of new homes that are included in the dataset, the outlier not-new home data points that could indicate omitted variables or non-representative data, and the range of home sizes overall makes me trust it less at the extremes.\nAll that said, we get better explanatory power from the interaction model."
  },
  {
    "objectID": "posts/HW5answers-DonnySnyder.html",
    "href": "posts/HW5answers-DonnySnyder.html",
    "title": "Homework 5",
    "section": "",
    "text": "Question 1\n\n\nCode\ndata(house.selling.price.2)\nsummary(house.selling.price.2)\n\n\n       P                S              Be              Ba       \n Min.   : 17.50   Min.   :0.40   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 72.90   1st Qu.:1.33   1st Qu.:3.000   1st Qu.:2.000  \n Median : 96.00   Median :1.57   Median :3.000   Median :2.000  \n Mean   : 99.53   Mean   :1.65   Mean   :3.183   Mean   :1.957  \n 3rd Qu.:115.00   3rd Qu.:1.98   3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :309.40   Max.   :3.85   Max.   :5.000   Max.   :3.000  \n      New        \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3011  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\nCode\ndata  <- house.selling.price.2\n\ncor(data)\n\n\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n\n\nCode\nmodel1 <- lm(P ~ S + Ba + New + Be ,data = data)\nbackElim <- lm(P ~ S + Ba + New ,data = data)\nforSel <- lm(P ~ S + New + Ba + Be,data = data)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = P ~ S + Ba + New + Be, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\nBe            -2.766      3.960  -0.698 0.486763    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(backElim)\n\n\n\nCall:\nlm(formula = P ~ S + Ba + New, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\n\nCode\nAIC(model1)\n\n\n[1] 790.6225\n\n\nCode\nAIC(backElim)\n\n\n[1] 789.1366\n\n\nCode\nBIC(model1)\n\n\n[1] 805.8181\n\n\nCode\nBIC(backElim)\n\n\n[1] 801.7996\n\n\nCode\nPRESS(model1)\n\n\nError in PRESS(model1): could not find function \"PRESS\"\n\n\nCode\nPRESS(backElim)\n\n\nError in PRESS(backElim): could not find function \"PRESS\"\n\n\n#Question 1A The Beds variable should be deleted first for backward elimination because it has the highest p-value\n#Question 1B The Size variable should be added first for forward selection, because it has the most explanatory value.\n#Question 1C Beds likely has such a large p-value because its variance that it explains is almost completely absorbed by size, which it is most highly correlated with.\n#Question 1D For R squared, I also prefer the backwards elimination method model, because it reduces the residuals.\nFor adjusted R squared, I prefer the backwards elimination method model without the Bed predictor, because it reduces the residuals.\nFor AIC I would also prefer the backwards elimination model created, because it reduces the AIC score, same with the BIC score.\nThe PRESS metric is the only one where the value is lower for the model with all of the variables, so I would choose that for PRESS.\n#Question 2\n\n\nCode\ndata2 <- trees\n\nmodel2 <- lm(Volume ~ Girth + Height, data = data2)\nsummary(model2)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = data2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\nCode\nplot(model2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2A\nRan model in code above.\n\n\nQuestion 2B\nIt seems like some of the regression assumptions may be violated. There is something of a parabolic relationship in this model, which was left out of the residuals. THe QQ looks normal, but the spread-location plot also shows that the residuals aren’t necessarily spread evenly along the range of values. The residuals vs leverage looks reasonable. It seems that there is not homoscedasticity of errors, which could be an issue for regression.\n\n\nQuestion 3\n\n\nCode\ndata(florida)\ndata3 <- florida\n\nmodel3 <- lm(Buchanan~Bush, data = florida)\nsummary(model3)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\nCode\nplot(model3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel4 <- lm(log(Buchanan)~log(Bush), data = florida)\nsummary(model4)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\n\nCode\nplot(model4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3A\nPalm beach county is an outlier based on the diagnostic plots, given all of the plots, as it’s value is by far the furthest from the fitted line.\n\n\nQuestion 3B\nThe findings change somewhat, but not that much. Palm Beach is still an outlier."
  },
  {
    "objectID": "posts/HW5_CalebHill.html",
    "href": "posts/HW5_CalebHill.html",
    "title": "Homework 5",
    "section": "",
    "text": "First, let’s load the relevant libraries and set all the graph themes to minimal.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\nlibrary(stargazer)\n\n\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\n\n\nVariables Beds would be removed first. It’s P-value does not meet statistically significant threshold.\n\n\n\nFor the opposite reason, Size would be added first.\n\n\n\nBeds is most likely auto-correlated with Size, as bedrooms make up a substantial amount of square footage in a house, driving up the price. However, that is not the only factor if pricing, as is most likely why the number – which can be high – does not necessarily mean we see an improved price. Conversely, this is not the case for bathrooms.\n\n\n\n\n\nCode\ndata(house.selling.price.2)\nmodel_1 <- lm(P ~ S + Ba, data = house.selling.price.2)\nplot(model_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nstargazer(model_1)\n\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n% Date and time: Fri, Dec 09, 2022 - 6:39:51 PM\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{1}{c}{\\textit{Dependent variable:}} \\\\ \n\\cline{2-2} \n\\\\[-1.8ex] & P \\\\ \n\\hline \\\\[-1.8ex] \n S & 63.863$^{***}$ \\\\ \n  & (4.840) \\\\ \n  & \\\\ \n Ba & 22.448$^{***}$ \\\\ \n  & (6.130) \\\\ \n  & \\\\ \n Constant & $-$49.752$^{***}$ \\\\ \n  & (9.183) \\\\ \n  & \\\\ \n\\hline \\\\[-1.8ex] \nObservations & 93 \\\\ \nR$^{2}$ & 0.833 \\\\ \nAdjusted R$^{2}$ & 0.829 \\\\ \nResidual Std. Error & 18.267 (df = 90) \\\\ \nF Statistic & 224.114$^{***}$ (df = 2; 90) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n\\end{tabular} \n\\end{table} \n\n\n\n\nCode\nPRESS <- function(model) {\n  i <- residuals(model)/(1 - lm.influence(model)$hat)\n  sum(i^2)\n}\n\nPRESS(model_1)\n\n\n[1] 34174.5\n\n\n\n\nCode\nbroom::glance(model_1)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.833        0.829  18.3    224. 1.12e-35     2  -401.  809.  819.  30033.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\nNote: observation #5 is an outlier and violates many of these diagnostic plots. If we log transform either P or S, observation #7 does the same. Therefore, we will keep the model as-is and not transform it.\nR2 = 0.83 Adjusted R2 = 0.83 PRESS = 34.174.5 AIC = 809.23 BIC = 819.36\n\n\n\nI chose this model as the other two variables Beds and New either do not meet the P-value threshold or have a low correlation to Price. The other two variables have a high correlation with Price."
  },
  {
    "objectID": "posts/HW5_CalebHill.html#a-1",
    "href": "posts/HW5_CalebHill.html#a-1",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel_q2 <- lm(Volume ~ Girth + Height, data = trees)\n\n\nFitted."
  },
  {
    "objectID": "posts/HW5_CalebHill.html#b-1",
    "href": "posts/HW5_CalebHill.html#b-1",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\nplot(model_q2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth of the Residuals vs. Fitted plots are violated, as they form a U-shape instead of a horizontal line; Cook’s Distance plot is violated as row 31 is outside of the 0.5 dotted line. The Normal Q-Q plot seems fine. The Scale Location plot does not look good, as it has a U-shape as well."
  },
  {
    "objectID": "posts/HW5_CalebHill.html#a-2",
    "href": "posts/HW5_CalebHill.html#a-2",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel_q3a <- lm(Buchanan ~ Bush, data = florida)\nplot(model_q3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYes, Palm Beach County is an outlier. This is especially apparent with Cook’s Distance plot, as it is outside the 1.0 dotted line."
  },
  {
    "objectID": "posts/HW5_CalebHill.html#b-2",
    "href": "posts/HW5_CalebHill.html#b-2",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\nmodel_q3b <- lm(log(Buchanan) ~ log(Bush), data = florida)\nplot(model_q3b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe findings do change, as all observations that violated the tests are now within the lines/meet assumptions."
  },
  {
    "objectID": "posts/HW5_EmmaRasmussen.html",
    "href": "posts/HW5_EmmaRasmussen.html",
    "title": "Homework 5",
    "section": "",
    "text": "data(house.selling.price.2)\nhead(house.selling.price.2)\n\n      P    S Be Ba New\n1  48.5 1.10  3  1   0\n2  55.0 1.01  3  2   0\n3  68.0 1.45  3  2   0\n4 137.0 2.40  3  3   0\n5 309.4 3.30  4  3   1\n6  17.5 0.40  1  1   0\n\nsummary(lm(P~ S + Be + Ba + New, data=house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\n##1a.\nBeds would be deleted first in backwards elimintation because it has the highest p-value.\n##1b.\nFor forward selection, size would be added first because it has the strongest correlation with price according the the correlation matrix.\n##1c.\nThe BEDS variable is likely captured in the model by size and baths, so once all are included it is no longer significant.\n##1d.\nUsing software with these four predictors, find the model that would be selected using each criterion:\n\n#All variables\nsummary(lm(P~ S + Be + Ba + New, data=house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\nfit_a<-lm(P~ S + Be + Ba + New, data=house.selling.price.2)\n\n#All variables, and interaction term\nsummary(lm(P~ S + Be + Ba + New + S*New, data=house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New + S * New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.258  -8.823   0.561  11.581  39.564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -30.214     11.461  -2.636 0.009926 ** \nS             58.874      5.361  10.982  < 2e-16 ***\nBe            -3.884      3.647  -1.065 0.289895    \nBa            19.840      5.192   3.821 0.000249 ***\nNew          -35.838     13.650  -2.625 0.010223 *  \nS:New         31.449      7.560   4.160 7.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.03 on 87 degrees of freedom\nMultiple R-squared:  0.8906,    Adjusted R-squared:  0.8843 \nF-statistic: 141.7 on 5 and 87 DF,  p-value: < 2.2e-16\n\nfit_b<-lm(P~ S + Be + Ba + New + S*New, data=house.selling.price.2)\n\n#Beds removed, with interatction\nsummary(lm(P~ S + Ba + New + S*New, data=house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ S + Ba + New + S * New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.319  -8.329   0.140  11.705  42.302 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -39.088      7.874  -4.964 3.35e-06 ***\nS             55.496      4.325  12.831  < 2e-16 ***\nBa            21.041      5.072   4.149 7.69e-05 ***\nNew          -35.661     13.660  -2.611   0.0106 *  \nS:New         30.855      7.545   4.089 9.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.04 on 88 degrees of freedom\nMultiple R-squared:  0.8892,    Adjusted R-squared:  0.8842 \nF-statistic: 176.5 on 4 and 88 DF,  p-value: < 2.2e-16\n\nfit_c<-lm(P~ S + Ba + New + S*New, data=house.selling.price.2)\n\n#Beds removed, no interaction\nsummary(lm(P~ S + Ba + New, data=house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ S + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\nfit_d<-lm(P~ S + Ba + New, data=house.selling.price.2)\n\n\nstargazer(fit_a, fit_b, fit_c, fit_d, type= \"text\", style=\"apsr\")\n\nError in stargazer(fit_a, fit_b, fit_c, fit_d, type = \"text\", style = \"apsr\"): could not find function \"stargazer\"\n\n\n###R2\nThe model with the highest R-squared is fit_b (all variables, and the interaction term between size and new)\nlm(P~ S + Be + Ba + New + S*New)\n###Adjusted R2\nThe model with the highest adjusted R-squared is also fit_b (but not by much, essentially the same R-squared as the same model without the BEDS variable, so I am skeptical this variable is useful)\nlm(P~ S + Be + Ba + New + S*New)\n###PRESS\n\nPRESS(fit_a)\n\nError in PRESS(fit_a): could not find function \"PRESS\"\n\nPRESS(fit_b)\n\nError in PRESS(fit_b): could not find function \"PRESS\"\n\nPRESS(fit_c)\n\nError in PRESS(fit_c): could not find function \"PRESS\"\n\nPRESS(fit_d)\n\nError in PRESS(fit_d): could not find function \"PRESS\"\n\n\nThe model with the lowest PRESS statistic is fit_c (with the interaction term but no BEDS variable) lm(P~ S + Ba + New + S*New)\n###AIC\nBecause all models have 93 observations, we can compare AIC and BIC. The model that minimizes AIC is fit_c but fit_b is not much worse.\nlm(P~ S + Ba + New + S*New)\n\nAIC(fit_a)\n\n[1] 790.6225\n\nAIC(fit_b)\n\n[1] 775.7515\n\nAIC(fit_c)\n\n[1] 774.9558\n\nAIC(fit_d)\n\n[1] 789.1366\n\n\n###BIC\n\nBIC(fit_a)\n\n[1] 805.8181\n\nBIC(fit_b)\n\n[1] 793.4797\n\nBIC(fit_c)\n\n[1] 790.1514\n\nBIC(fit_d)\n\n[1] 801.7996\n\n\nThe model that minimizes BIC is fit_c. lm(P~ S + Ba + New + S*New)\n##1e.\nI prefer fit_c lm(P~ S + Ba + New + S*New) because it performs the best in terms of PRESS, AIC, and BIC, and about the same as the best one for adjusted R-squared. Because we have multiple variables included in the model I prefer to use adjusted R-squared over R-squared.\n##2a.\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular,\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\ndata(trees)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\nsummary(lm(Volume ~ Height + Girth, data=trees))\n\n\nCall:\nlm(formula = Volume ~ Height + Girth, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \nGirth         4.7082     0.2643  17.816  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\nfit_trees<-lm(Volume ~ Height + Girth, data=trees)\n\n##2b.\n\npar(mfrow= c(2,3)); plot(fit_trees, which=1:6)\n\n\n\n\nConstant variance (of residuals) appears to be violated from these plots (Residuals vs Fitted is funnel shaped and Scale Location is not linear)\n##3a.\n\ndata(florida)\nhead(florida)\n\n           Gore   Bush Buchanan\nALACHUA   47300  34062      262\nBAKER      2392   5610       73\nBAY       18850  38637      248\nBRADFORD   3072   5413       65\nBREVARD   97318 115185      570\nBROWARD  386518 177279      789\n\n\n\nsummary(lm(Buchanan~ Bush, data=florida))\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\nfit_florida<- (lm(Buchanan~ Bush, data=florida))\npar(mfrow= c(2,3)); plot(fit_florida, which=1:6)\n\n\n\n\nPalm Beach (and Dade) County appears to be an outlier. Cooks distance is greater than 1, and it has high leverage. ##3b.\n\nsummary(lm(log(Buchanan)~ log(Bush), data=florida))\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\nfit_florida2<- (lm(log(Buchanan)~ log(Bush), data=florida))\npar(mfrow= c(2,3)); plot(fit_florida2, which=1:6)\n\n\n\n\nPalm beach could still be an potential outlier according to these plots, but the magnitude is less strong than in the previous plots."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html",
    "href": "posts/HW5_EthanCampbell.html",
    "title": "Homework 5",
    "section": "",
    "text": "Homework 5 DACSS 603, Fall 2022 Due: December 9, 2022, 11.59pm\nPlease consult the relevant tutorials if you’re having trouble with coding the answers. Please write up your solutions as a .qmd (Quarto) document and publish in the Course Blog.\nSome of the questions use data from the alr4 and smss R packages. You would need to install those packages in R (no need for an install.packages() call in your .qmd file, though—just use library()) and load the data using the data() function."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html#section",
    "href": "posts/HW5_EthanCampbell.html#section",
    "title": "Homework 5",
    "section": "1.1",
    "text": "1.1\nFor backward elimination, which variable would be deleted first? Why?\nFor backward elimination we remove the one with the largest p values which would be beds as its p value is .487."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html#section-1",
    "href": "posts/HW5_EthanCampbell.html#section-1",
    "title": "Homework 5",
    "section": "1.2",
    "text": "1.2\nFor forward selection, which variable would be added first? Why?\nFor forward selection we add the ones with the lowest p-value which would be size or new. However, while they have the same p-value size has a higher correlation based on the tabes above I would add size first then add new followed by bath."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html#section-2",
    "href": "posts/HW5_EthanCampbell.html#section-2",
    "title": "Homework 5",
    "section": "1.3",
    "text": "1.3\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nI believe this is not showing a sub .05 p-value because either the model is too complex and considering the other variables it throughs off the bed variable. This is also a fairly small sample size of ~90 so we can also say maybe there is not enough infomration to accurately describe this effect."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html#section-3",
    "href": "posts/HW5_EthanCampbell.html#section-3",
    "title": "Homework 5",
    "section": "1.4",
    "text": "1.4\nUsing software with these four predictors, find the model that would be selected using each criterion:\nR2 - Multiple linear regression model using lm function followed by summary. This will show the r-squared in the summary so it is really easy to find this information which is nice. Adjusted R2 - this will be the same as R2. This will show the adjusted r-squared for the regression in the summary so it is really easy to find this infomration. PRESS - utilizing the MPV package we can use the function press which simiply needs the linear model you used. This is much shorter than writing it all out and much easier. AIC - for this we can use the AIC function on the model that we have created before. BIC - for this we can use the BIC function on the model that we have created before.\n\n\nCode\nlibrary(smss)\nlibrary(MPV)\n\n\nLoading required package: lattice\n\n\nLoading required package: KernSmooth\n\n\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\n\nCode\ndata(\"house.selling.price.2\")\nmod1 <- lm(P ~ S + Be + Ba + New, data = house.selling.price.2)\nsummary(mod1)\n\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\nCode\nAIC(mod1)\n\n\n[1] 790.6225\n\n\nCode\nBIC(mod1)\n\n\n[1] 805.8181\n\n\nCode\nPRESS(mod1)\n\n\n[1] 28390.22\n\n\n\nExplain\nExplain which model you prefer and why.\nI usually go off the R2 for simple models as there is not too much to take into consideration but as it gets more complex I believe there are certain factors that need to be taken into consideration. When the equation gets more complex I would prefer using the AIC as it takes into consideration the other models this means that they are relative. This means that when one model gets to complex it can get penalized and not reflect a better score so this helps prevent over-complication of models."
  },
  {
    "objectID": "posts/HW5_EthanCampbell.html#section-4",
    "href": "posts/HW5_EthanCampbell.html#section-4",
    "title": "Homework 5",
    "section": "2.1",
    "text": "2.1\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\nAfter running the regression modeling and plotting to look at the data we can see there is definatly something wrong with this infomation and it seems to break the regression assumptions. It appears that linearity is a problem as the residuals vs fitted plot does not show a linear line and is very curved. This is due to the lack of constant variance resulting in the line being curved. The scale-location plot also looks like it has been violated as it is also not linear and shows a very strong dip then positive line.\n\n\nCode\ndata(\"trees\")\n\ntree <- lm(Volume ~ Girth + Height, data = trees)\n\nsummary(tree)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\nCode\nplot(tree)"
  },
  {
    "objectID": "posts/HW5_KarenKimble.html",
    "href": "posts/HW5_KarenKimble.html",
    "title": "DACSS 603 HW 5",
    "section": "",
    "text": "Code\n# Setup\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(plyr)\n\n\n------------------------------------------------------------------------------\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n------------------------------------------------------------------------------\n\nAttaching package: 'plyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/HW5_KarenKimble.html#question-1",
    "href": "posts/HW5_KarenKimble.html#question-1",
    "title": "DACSS 603 HW 5",
    "section": "Question 1",
    "text": "Question 1\n\nPart A\nThe first variable to be deleted would be beds because it has the largest p-value, and backward elimination begins with deleting the variable with the largest p-value.\n\n\nPart B\nThe first variable added in forward selection would be size because it has the smallest p-value.\n\n\nPart C\nBeds has such a large p-value despite its correlation with price because it also has strong correlations with other variables. This may cause multicollinearity.\n\n\nPart D\n\n\nCode\n# Model 1\n\nmodel <- lm(P ~ . , data = house.selling.price.2)\n\n\nError in is.data.frame(data): object 'house.selling.price.2' not found\n\n\nCode\nmodel1 <- step(model)\n\n\nError in terms(object): object 'model' not found\n\n\nCode\nsummary(model1)\n\n\nError in summary(model1): object 'model1' not found\n\n\n\n\nCode\n# Model 2\n\nmodel <- lm(P ~ . , data = house.selling.price.2)\n\n\nError in is.data.frame(data): object 'house.selling.price.2' not found\n\n\nCode\nmodel2 <- step(model, direction = c(\"forward\"))\n\n\nError in terms(object): object 'model' not found\n\n\nCode\nsummary(model2)\n\n\nError in summary(model2): object 'model2' not found\n\n\nBased on R-squared, we would want to pick Model 2 since it has a higher R-squared. This model includes all four predictor variables and was picked using forward selection. However, if judging by the Adjusted R-squared criteria, we would want to pick Model 1 since that has a higher Adjusted R-squared. Model 1 was chosen using backward eliminiation and does not include the Bed variable.\n\n\nCode\n# Calculating PRESS\n\nPRESS <- function(linear.model) {\n\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n\n  PRESS <- sum(pr^2)\n  \n  return(PRESS)\n}\n\nPRESS(model1)\n\n\nError in residuals(linear.model): object 'model1' not found\n\n\nCode\nPRESS(model2)\n\n\nError in residuals(linear.model): object 'model2' not found\n\n\nFor the PRESS criteria, the model we would want to pick is Model 1, which was found using backward elimination and only includes New, Bath, and Size as predictor variables.\n\n\nCode\nAIC(model1)\n\n\nError in AIC(model1): object 'model1' not found\n\n\nCode\nAIC(model2)\n\n\nError in AIC(model2): object 'model2' not found\n\n\nJudging by the AIC, Model 1 is a better fit because it has a lower AIC.\n\n\nCode\nBIC(model1)\n\n\nError in BIC(model1): object 'model1' not found\n\n\nCode\nBIC(model2)\n\n\nError in BIC(model2): object 'model2' not found\n\n\nModel 1 is also a better fit from the BIC criteria because it also has a lower BIC. Overall, judging by all five criteria, Model 1 would be the best fit since it has better results in four out of the five.\n\n\nPart E\nAs stated before, Model 1 has better results in four out of the five criteria (Adjusted R-Squared, PRESS, AIC, and BIC). Thus, the model I would prefer overall is Model 1, which omitted the Bed variable. The Bed variable also had an extremely high p-value compared to the other variables, so it would make sense to construct a model without it."
  },
  {
    "objectID": "posts/HW5_KarenKimble.html#question-2",
    "href": "posts/HW5_KarenKimble.html#question-2",
    "title": "DACSS 603 HW 5",
    "section": "Question 2",
    "text": "Question 2\n\nPart A\n\n\nCode\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\n\n\nPart B\n\n\nCode\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are some regression assumptions that are violated because of the data present in the plots. In the Residuals vs. Fitted plot, the line is not linear, indicating that the variances of the error terms are not equal and there may not be a linear relationship. The Scale-Location plot also shows a non-linear line, indicating that the assumption of constant variance is violated. The other two plots, Normal Q-Q and Residuals vs. Leverage, are normal."
  },
  {
    "objectID": "posts/HW5_KarenKimble.html#question-3",
    "href": "posts/HW5_KarenKimble.html#question-3",
    "title": "DACSS 603 HW 5",
    "section": "Question 3",
    "text": "Question 3\n\nPart A\n\n\nCode\nmodel <- lm(Buchanan ~ Bush, data = florida)\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalm Beach is an outlier based on the diagnostic plots for the model because, while all the other data points are fairly close together, the Palm Beach data point is extremely far in each plot. Additionally, in the Residuals vs. Leverage plot, the Palm Beach point is outside of Cook’s distance, meaning it is an outlier with extreme influence on the data.\n\n\nPart B\n\n\nCode\nmodel <- lm(log(Buchanan) ~ log(Bush), data = florida)\nplot(model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe findings do change somewhat because in the new model using logs, the Palm Beach data point is now inside Cook’s distance, meaning it has less influence over the data and is less of an outlier. The distance between Palm Beach and the other data points has been reduced, but it still seems to remain somewhat of an outlier but much less than in the previous model."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html",
    "href": "posts/HW5_ManiShankerKamarapu.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(MPV)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW5_ManiShankerKamarapu.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(house.selling.price.2)\nhouse.selling.price.2"
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#a",
    "href": "posts/HW5_ManiShankerKamarapu.html#a",
    "title": "Homework 5",
    "section": "A",
    "text": "A\nFor backward elimination, you fit a model using all possible explanatory values to predict the output. Then one by one, you delete the least significant explanatory variable in the model, which would have the largest p-value. In this example, we would delete Beds first, which has a p-value of 0.487."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#b",
    "href": "posts/HW5_ManiShankerKamarapu.html#b",
    "title": "Homework 5",
    "section": "B",
    "text": "B\nWith forward selection, you begin with no explanatory variables, then add one variable at a time to the model. The variable you add should be the most significant one, based on it having the lowest P-value of the group of possible explanatory variables. In this example, the first variable to add to the model is Size, given its extremely small p-value < 2e-16."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#c",
    "href": "posts/HW5_ManiShankerKamarapu.html#c",
    "title": "Homework 5",
    "section": "C",
    "text": "C\nWhile the variable Beds does have a strong correlation with price, when adding additional variables using a regression model, the relationship significantly diminishes, thus the other variables may act as a control on the bed variable."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#d",
    "href": "posts/HW5_ManiShankerKamarapu.html#d",
    "title": "Homework 5",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(P ~ S, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ S, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.407 -10.656   2.126  11.412  85.091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -25.194      6.688  -3.767 0.000293 ***\nS             75.607      3.865  19.561  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.47 on 91 degrees of freedom\nMultiple R-squared:  0.8079,    Adjusted R-squared:  0.8058 \nF-statistic: 382.6 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ S+New, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ ., data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ . -Be -Ba, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ . - Be - Ba, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n\n\n\na. R^2\nAs expected, the model with the most explanatory variables has the highest R-squared value at 0.8689. Therefore, if you were to select a model solely based on maximizing the R-squared value, it would be: ŷ = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New).\n\n\nb. Adjusted R^2\nHowever, if you were to select a model based on adjusted R-squared, the best model for predicting selling price would exclude Beds and use Size, Baths, and New as explanatory variables. The adjusted R-squared value see a slight increase when Beds is removed (from 0.8629 to 0.8637). The model would be: ŷ = -47.99 + 62.26(Size) + 20.07(Baths) + 18.37(New).\n\n\nc. PRESS\n\n\nCode\nPRESS(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 28390.22\n\n\nCode\nPRESS(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 27860.05\n\n\nWhen considering PRESS, a smaller PRESS value indicates a better predictive model. Comparing the PRESS value of the model with all variables and the model excluding Bed, the PRESS values would lead us to select the model with Size, Baths, and New as variables for predicting selling price.\n\n\nd. AIC\n\n\nCode\nAIC(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 790.6225\n\n\nCode\nAIC(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 789.1366\n\n\nWhen considering the AIC for both models, the value is slightly lower for the model that excludes Bed as a variable. Therefore, the AIC would lead us to use the model with Size, Baths, and New as explanatory variables to predicting selling price.\n\n\ne. BIC\n\n\nCode\nBIC(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 805.8181\n\n\nCode\nBIC(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 801.7996\n\n\nLastly, like AIC, the BIC value is lower for the model that excludes Bed as a variable. Once again, we’d select the model that uses Size, Baths, and New as explanatory variables to predict selling price."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#e",
    "href": "posts/HW5_ManiShankerKamarapu.html#e",
    "title": "Homework 5",
    "section": "E",
    "text": "E\nGiven the results from the various criteria above, the model I would prefer to use to predict selling price is that which excludes Bed and includes Size, Bath, and New as variables: ŷ = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New). This is because each of the criterion indicate this model as slightly stronger in its predictive power than the model that includes all variables except R-squared, which cannot be used alone to determine model strength."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW5_ManiShankerKamarapu.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"trees\")\ntrees"
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW5_ManiShankerKamarapu.html#a-1",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW5_ManiShankerKamarapu.html#b-1",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the residuals vs. fitted values plot, the central points appear to roughly bounce randomly above and below 0, but the lowest and highest point appear to be very influential residuals. The red line should be flat along 0 horizontally, but it is U-shaped. This curvature may suggest a violation in the linearity assumption. With the normal Q-Q plot, it’s difficult to confidently say that the assumption of normality appears to be violated. The points generally run along the trend-line, but they do deviate above the line for the higher points. It’s a noteworthy deviation, but it’s difficult to make a certain decision based on the plot. In the scale-location plot, the line is not horizontal, thus suggesting a violation in the assumption of constant variance. Cook’s distance suggests that the 31st observation is above the threshold, meaning it is too influential as one observation."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW5_ManiShankerKamarapu.html#question-3",
    "title": "Homework 5",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"florida\")\nflorida"
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#a-2",
    "href": "posts/HW5_ManiShankerKamarapu.html#a-2",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel <- lm(formula = Buchanan ~ Bush, data = florida)\nsummary(model)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the diagnostic plots, Palm Beach County is an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is very large. When referring to the summary of the simple regression model, the third quartile for residuals is 12.26, yet the max is 2610.19. This is a significant jump and indicative of the value being an outlier. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows two points that may be of concern as outliers if you follow the metric of observations scoring over 1, which are DADE and Palm Beach at about 2. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model."
  },
  {
    "objectID": "posts/HW5_ManiShankerKamarapu.html#b-2",
    "href": "posts/HW5_ManiShankerKamarapu.html#b-2",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(formula = log(Buchanan) ~ log(Bush), data = florida)\nsummary(model)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the diagnostic plots, Palm Beach County is still an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is still very large. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows that may be of concern as outlier if you follow the metric of observations scoring over 0.2, which is Palm Beach at about 0.3. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model."
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html",
    "href": "posts/HW5_PrahithaMovva.html",
    "title": "Homework 5 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html#question-1",
    "href": "posts/HW5_PrahithaMovva.html#question-1",
    "title": "Homework 5 - Prahitha Movva",
    "section": "Question 1",
    "text": "Question 1\n\nA\nWith backward elimination, the variable ‘Beds’ would be deleted first as it has the largest p-value (0.487) and is therefore the weakest indicator of house price.\n\n\nB\nWith forward selection, the variable ‘Size’ would be added first as it has the least p-value (0) and is therefore the strongest indicator of house price.\n\n\nC\nBeds has a substantial correlation with not only Price but also Size (which is a statistically significant predictor of house price). This suggests that the variables Beds and Size maybe multicollinear owing to the large p-value for the variables Beds.\n\n\nD\n\n\nCode\ndata(house.selling.price.2)\nm1 <- lm(P ~ S, data = house.selling.price.2)\nm2 <- lm(P ~ S + New, data = house.selling.price.2)\nm3 <- lm(P ~ . -Be, data = house.selling.price.2)\nm4 <- lm(P ~ ., data = house.selling.price.2)\n\n\n\na. R^2\n\n\nCode\nsummary(m1)$r.squared\n\n\n[1] 0.807866\n\n\nCode\nsummary(m2)$r.squared\n\n\n[1] 0.8483699\n\n\nCode\nsummary(m3)$r.squared\n\n\n[1] 0.8681361\n\n\nCode\nsummary(m4)$r.squared\n\n\n[1] 0.868863\n\n\nThe model using all the variables (m4), since it has the highest R^2\n\n\nb. Adjusted R^2\n\n\nCode\nsummary(m1)$adj.r.squared\n\n\n[1] 0.8057546\n\n\nCode\nsummary(m2)$adj.r.squared\n\n\n[1] 0.8450003\n\n\nCode\nsummary(m3)$adj.r.squared\n\n\n[1] 0.8636912\n\n\nCode\nsummary(m4)$adj.r.squared\n\n\n[1] 0.8629022\n\n\nThe model using all the variables except Beds (m3), since it has the highest adjusted R^2\n\n\nc. PRESS\nUsed the code from this discussion on Stack Exchange: https://stats.stackexchange.com/questions/248603/how-can-one-compute-the-press-diagnostic\n\n\nCode\nPRESS <- function(linear.model) {\n    pr <- residuals(linear.model)/(1 - lm.influence(linear.model)$hat)\n    sum(pr^2)\n}\n\nPRESS(m1)\n\n\n[1] 38203.29\n\n\nCode\nPRESS(m2)\n\n\n[1] 31066\n\n\nCode\nPRESS(m3)\n\n\n[1] 27860.05\n\n\nCode\nPRESS(m4)\n\n\n[1] 28390.22\n\n\nThe model using all the variables except Beds (m3), since it has the least error\n\n\nd. AIC\n\n\nCode\nAIC(m1)\n\n\n[1] 820.1439\n\n\nCode\nAIC(m2)\n\n\n[1] 800.1262\n\n\nCode\nAIC(m3)\n\n\n[1] 789.1366\n\n\nCode\nAIC(m4)\n\n\n[1] 790.6225\n\n\nThe model using all the variables except Beds (m3), since it has the least AIC, meaning it’s a better fit\n\n\nd. BIC\n\n\nCode\nBIC(m1)\n\n\n[1] 827.7417\n\n\nCode\nBIC(m2)\n\n\n[1] 810.2566\n\n\nCode\nBIC(m3)\n\n\n[1] 801.7996\n\n\nCode\nBIC(m4)\n\n\n[1] 805.8181\n\n\nThe model using all the variables except Beds (m3), since it has the least BIC, meaning it’s a better fit\n\n\n\nE\nI would prefer the model using all the variables except Beds (m3) because the other criterion estimate the fit better than R^2 (we know that using R^2, the value increases with the number of variables even though they are not necessarily good predictors)."
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html#question-2",
    "href": "posts/HW5_PrahithaMovva.html#question-2",
    "title": "Homework 5 - Prahitha Movva",
    "section": "Question 2",
    "text": "Question 2\n\nA\n\n\nCode\ndata(\"trees\")\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\n\n\nB\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nResiduals vs Fitted: We can see that it’s not a horizontal line (has distinct U-shaped pattern) which is a good indication for a non-linear relationship. So we can say that the ‘Linearity’ assumption is violated.\nNormal Q-Q: Most of the residuals seem to be following the straight dashed line, which means that the data is normally distributed.\nScale-Location: A horizontal line with equally spread points is a good indication of homoscedasticity but this is not the case with our data, so we can say that the ‘Homoscedasticity’ assumption is violated.\nResiduals vs Leverage: For it to have a linear relationship (between the predictors and the outcome variables), the red line must be approximately horizontal to zero. Also, the observations 31, 3 and 18 are outside of the dotted line which means they influence the results of the regression (outliers)."
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html#question-3",
    "href": "posts/HW5_PrahithaMovva.html#question-3",
    "title": "Homework 5 - Prahitha Movva",
    "section": "Question 3",
    "text": "Question 3"
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html#a-2",
    "href": "posts/HW5_PrahithaMovva.html#a-2",
    "title": "Homework 5 - Prahitha Movva",
    "section": "A",
    "text": "A\n\n\nCode\ndata(florida)\nmodel <- lm(Buchanan ~ Bush, data = florida)\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nYes, Palm Beach County is outside the dotted line in all the plots so I think it is an outlier."
  },
  {
    "objectID": "posts/HW5_PrahithaMovva.html#b-2",
    "href": "posts/HW5_PrahithaMovva.html#b-2",
    "title": "Homework 5 - Prahitha Movva",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nNo, Palm Beach County is still an outlier but the Scale-Location and the Residuals vs Leverage plots look better (horizontal line) than in the earlier model."
  },
  {
    "objectID": "posts/HW5_Saaradhaa.html",
    "href": "posts/HW5_Saaradhaa.html",
    "title": "Homework 5",
    "section": "",
    "text": "Qn 1A\nBeds, as it has the largest p-value.\n\n\nQn 1B\nSize, as it would produce the smallest p-value.\n\n\nQn 1C\nBeds is highly correlated with Size, so multicollinearity might be a possible reason for this.\n\n\nQn 1D\nThe model with all predictors has the highest R2.\n\ndata(house.selling.price.2)\nsummary(lm(P ~ ., house.selling.price.2))\n\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\nThe model below without Beds has the highest adjusted R2 and lowest PRESS/BIC/AIC.\n\nmodel <- lm(P ~ .-Be, house.selling.price.2)\n\nPRESS <- function(model) {\n  pr <- residuals(model)/(1-lm.influence(model)$hat)\n  PRESS <- sum(pr^2)\n  return(PRESS)\n}\n\nPRESS(model)\n\n[1] 27860.05\n\nBIC(model)\n\n[1] 801.7996\n\nAIC(model)\n\n[1] 789.1366\n\n\n\n\nQn 1E\nI prefer the model without Beds because it doesn’t seem to be adding great value to the regression model, and the other 4 measures (adjusted R2, PRESS, AIC and BIC) are better ways of assessing a multiple regression model than just R2.\n\n\nQn 2A\n\ndata(trees)\nmodel2 <- lm(Volume ~ poly(Girth, 2) + Height, trees)\nsummary(model2)\n\n\nCall:\nlm(formula = Volume ~ poly(Girth, 2) + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2928 -1.6693 -0.1018  1.7851  4.3489 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      1.56553    6.72218   0.233 0.817603    \npoly(Girth, 2)1 80.25223    3.07346  26.111  < 2e-16 ***\npoly(Girth, 2)2 15.39923    2.63157   5.852 3.13e-06 ***\nHeight           0.37639    0.08823   4.266 0.000218 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.625 on 27 degrees of freedom\nMultiple R-squared:  0.9771,    Adjusted R-squared:  0.9745 \nF-statistic: 383.2 on 3 and 27 DF,  p-value: < 2.2e-16\n\n\n\n\nQn 2B\n\npar(mfrow = c(2,3)); plot(model2, which = 1:6)\n\n\n\n\nScale-Location plot indicates heteroskedasticity. Cook’s distance, residuals vs. leverage and Cook’s distance vs. leverage plots indicate 3 outliers.\n\n\nQn 3A\n\ndata(florida)\nmodel3a <- lm(Buchanan ~ Bush, florida)\npar(mfrow = c(2,3)); plot(model3a, which = 1:6)\n\n\n\n\nIt seems to stick out in all 6 plots, so I would say it’s an outlier.\n\n\nQn 3B\n\nmodel3b <- lm(log(Buchanan) ~ log(Bush), florida)\npar(mfrow = c(2,3)); plot(model3b, which = 1:6)\n\n\n\n\nPalm Beach still seems to be an outlier, but the diagnostic plots look more normal than in the previous model."
  },
  {
    "objectID": "posts/HW5_ShoshanaBuck.html",
    "href": "posts/HW5_ShoshanaBuck.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW5_ShoshanaBuck.html#question-1",
    "href": "posts/HW5_ShoshanaBuck.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(house.selling.price)\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\n\n1a.\nFor backward elimination, which variable would be deleted first? Why?\n‘Beds’ would be the variable that would be deleted first because it has the highest p-value.\n\n\n1b.\nFor forward selection, which variable would be added first? Why?\n‘New’ and ‘Size’ are two variables that would be added first because it has the smallest p-value. However, looking at the correlation matrix ‘Size’ has a higher correlation (0.89) with ‘Price’ than ‘New’ (0.357), so ‘Size’ would be added first.\n\n\n1c.\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nI think that ‘Beds’ has a large p-value because there are too many variables and or redundant interaction terms. I think also the fact that there are only 100 observations does not get an large enough sample which causes for such a high p-value in ‘Beds.’\n\n\n1d.\nUsing software with these four predictors, find the model that would be selected using each criterion:\n\n\nCode\nhsp<-lm(Price~ .-Taxes - case, data = house.selling.price)\nhsp2<- lm(Price~ .- Taxes - case- Beds, data = house.selling.price)\nhsp3<- lm (Price~ .- Taxes - case- Beds - Baths, data = house.selling.price)\nhsp4<- lm (Price~ .- Taxes - case- Beds-New, data = house.selling.price)\n\n\n\n\nR2 & Adjusted R2\n\n\nCode\nsummary(hsp)\n\n\n\nCall:\nlm(formula = Price ~ . - Taxes - case, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-215747  -30833   -5574   18800  164471 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -28849.22   27261.16  -1.058  0.29262    \nBeds         -8202.38   10449.84  -0.785  0.43445    \nBaths         5273.78   13080.17   0.403  0.68772    \nNew          54562.38   19214.89   2.840  0.00553 ** \nSize           118.12      12.32   9.585 1.27e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 54250 on 95 degrees of freedom\nMultiple R-squared:  0.7245,    Adjusted R-squared:  0.713 \nF-statistic: 62.47 on 4 and 95 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(hsp2)\n\n\n\nCall:\nlm(formula = Price ~ . - Taxes - case - Beds, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-204134  -34520   -5483   18325  161674 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -43433.23   19908.84  -2.182  0.03158 *  \nBaths         3057.13   12746.12   0.240  0.81096    \nNew          57745.94   18744.37   3.081  0.00269 ** \nSize           114.42      11.36  10.070  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 54140 on 96 degrees of freedom\nMultiple R-squared:  0.7228,    Adjusted R-squared:  0.7141 \nF-statistic: 83.42 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(hsp3)\n\n\n\nCall:\nlm(formula = Price ~ . - Taxes - case - Beds - Baths, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nNew          57736.283  18653.041   3.095  0.00257 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(hsp4)\n\n\n\nCall:\nlm(formula = Price ~ . - Taxes - case - Beds - New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-235844  -27889   -2667   20749  154091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -54042.0    20449.0  -2.643  0.00959 ** \nBaths         2972.8    13292.2   0.224  0.82350    \nSize           124.9       11.3  11.053  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56460 on 97 degrees of freedom\nMultiple R-squared:  0.6954,    Adjusted R-squared:  0.6891 \nF-statistic: 110.7 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\n\n\nPRESS\n\n\nCode\n#hsp\nres<- resid(hsp)\np<- resid(hsp)/(1-lm.influence(hsp)$hat)\nPRESS<- sum(p^2)\nPRESS\n\n\n[1] 366008520409\n\n\nCode\n#hsp2\nres2<- resid(hsp2)\np2<- resid(hsp2)/(1-lm.influence(hsp2)$hat)\nPRESS2<- sum(p2^2)\nPRESS2\n\n\n[1] 3.43386e+11\n\n\nCode\n#hsp3\nres3<- resid(hsp3)\np3<- resid(hsp3)/(1-lm.influence(hsp3)$hat)\nPRESS3<- sum(p3^2)\nPRESS3\n\n\n[1] 3.33901e+11\n\n\nCode\n#hsp4\nres4<- resid(hsp4)\np4<- resid(hsp4)/(1-lm.influence(hsp4)$hat)\nPRESS4<- sum(p4^2)\nPRESS4\n\n\n[1] 361316756861\n\n\n\n\nAIC\n\n\nCode\nAIC(hsp)\n\n\n[1] 2470.942\n\n\nCode\nAIC(hsp2)\n\n\n[1] 2469.588\n\n\nCode\nAIC(hsp3)\n\n\n[1] 2467.648\n\n\nCode\nAIC(hsp4)\n\n\n[1] 2477.016\n\n\n\n\nBIC\n\n\nCode\nBIC(hsp)\n\n\n[1] 2486.573\n\n\nCode\nBIC(hsp2)\n\n\n[1] 2482.614\n\n\nCode\nBIC(hsp3)\n\n\n[1] 2478.069\n\n\nCode\nBIC(hsp4)\n\n\n[1] 2487.437\n\n\n\n\n1e.\nExplain which model you prefer and why.\nI prefer using the AIC or BIC model because they penalize the addition of variables and the smaller the output value is the better the model is."
  },
  {
    "objectID": "posts/HW5_ShoshanaBuck.html#question-2",
    "href": "posts/HW5_ShoshanaBuck.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(trees)\ntrees\n\n\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n11  11.3     79   24.2\n12  11.4     76   21.0\n13  11.4     76   21.4\n14  11.7     69   21.3\n15  12.0     75   19.1\n16  12.9     74   22.2\n17  12.9     85   33.8\n18  13.3     86   27.4\n19  13.7     71   25.7\n20  13.8     64   24.9\n21  14.0     78   34.5\n22  14.2     80   31.7\n23  14.5     74   36.3\n24  16.0     72   38.3\n25  16.3     77   42.6\n26  17.3     81   55.4\n27  17.5     82   55.7\n28  17.9     80   58.3\n29  18.0     80   51.5\n30  18.0     80   51.0\n31  20.6     87   77.0\n\n\n\n2a.\nfit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables.\n\n\nCode\nt<- lm(Volume ~ Girth + Height, data = trees)\nsummary(t)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\n\n\n2b.\nRun regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\nCode\npar(mfrow = c (2,3)); plot(t,which = 1:6)\n\n\n\n\n\nAfter running a diagnostic plot on the models I think that the Residuals vs Fitted and Scale- Location have been violated in the regression assumptions.\nResiduals vs Fitted graph the residuals should be more uniform and in an arch that is surrounding the red line.\nScale-Location residuals should be more uniform and surrounding the red line.\n\n\nQuestion 3\n\n\nCode\ndata(florida)\nflorida\n\n\n               Gore   Bush Buchanan\nALACHUA       47300  34062      262\nBAKER          2392   5610       73\nBAY           18850  38637      248\nBRADFORD       3072   5413       65\nBREVARD       97318 115185      570\nBROWARD      386518 177279      789\nCALHOUN        2155   2873       90\nCHARLOTTE     29641  35419      182\nCITRUS        25501  29744      270\nCLAY          14630  41745      186\nCOLLIER       29905  60426      122\nCOLUMBIA       7047  10964       89\nDADE         328702 289456      561\nDE SOTO        3322   4256       36\nDIXIE          1825   2698       29\nDUVAL        107680 152082      650\nESCAMBIA      40958  73029      504\nFLAGLER       13891  12608       83\nFRANKLIN       2042   2448       33\nGADSDEN        9565   4750       39\nGILCHRIST      1910   3300       29\nGLADES         1420   1840        9\nGULF           2389   3546       71\nHAMILTON       1718   2153       24\nHARDEE         2341   3764       30\nHENDRY         3239   4743       22\nHERNANDO      32644  30646      242\nHIGHLANDS     14152  20196       99\nHILLSBOROUGH 166581 176967      836\nHOLMES         2154   4985       76\nINDIAN RIVER  19769  28627      105\nJACKSON        6868   9138      102\nJEFFERSON      3038   2481       29\nLAFAYETTE       788   1669       10\nLAKE          36555  49963      289\nLEE           73560 106141      305\nLEON          61425  39053      282\nLEVY           5403   6860       67\nLIBERTY        1011   1316       39\nMADISON        3011   3038       29\nMANATEE       49169  57948      272\nMARION        44648  55135      563\nMARTIN        26619  33864      108\nMONROE        16483  16059       47\nNASSAU         6952  16404       90\nOKALOOSA      16924  52043      267\nOKEECHOBEE     4588   5058       43\nORANGE       140115 134476      446\nOSCEOLA       28177  26216      145\nPALM BEACH   268945 152846     3407\nPASCO         69550  68581      570\nPINELLAS     199660 184312     1010\nPOLK          74977  90101      538\nPUTNAM        12091  13439      147\nST. JOHNS     19482  39497      229\nST. LUCIE     41559  34705      124\nSANTA ROSA    12795  36248      311\nSARASOTA      72854  83100      305\nSEMINOLE      58888  75293      194\nSUMTER         9634  12126      114\nSUWANNEE       4084   8014      108\nTAYLOR         2647   4051       27\nUNION          1399   2326       26\nVOLUSIA       97063  82214      396\nWAKULLA        3835   4511       46\nWALTON         5637  12176      120\nWASHINGTON     2796   4983       88\n\n\n\n\n3a.\nRun a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\nCode\nflo<-lm(Buchanan ~ Bush, data = florida)\nsummary(flo)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\n\n\nDiagnostic plot\n\n\nCode\npar(mfrow = c (2,3)); plot(flo,which = 1:6)\n\n\n\n\n\nBased on the diagnostic plots ‘Palm Beach’ is an outlier. Most of the residuals are grouped together don’t violate the the regression assumptions. However, in all of the plots the ‘Palm Beach’ residual violates all of the regression assumptions. For example, in the Residuals vs Fitted plot the data is uniform and linear. The ‘Palm Beach’ residual is no where near the rest of the residuals.\n\n\n3b.\nTake the log of both variables (Bush vote and Buchanan Vote) and repeat the analysis in (a). Does your findings change?\n\n\nCode\nlog_flo<- lm(log(Buchanan) ~ log(Bush), data = florida)\nsummary(log_flo)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\npar(mfrow = c (2,3)); plot(log_flo,which = 1:6)\n\n\n\n\n\nYes, the findings did change after I logged ‘Buchanan’ and ‘Bush.’ After running a diagnostic plot the Residuals vs Fitted residuals are more uniform and evenly dispersed across the line. The scale-location has a similar pattern as the Residuals vs Fitted, there is more uniformity and even dispersement of residuals. Furthermore, the Normal Q-Q residuals seem to be better fit to the line because there are linear and uniform."
  },
  {
    "objectID": "posts/HW5_Solutions_OmerYalcin.html",
    "href": "posts/HW5_Solutions_OmerYalcin.html",
    "title": "Homework 5",
    "section": "",
    "text": "Please check your answers against the solutions.\nLoad the necessary packages:"
  },
  {
    "objectID": "posts/HW5_Solutions_OmerYalcin.html#question-1",
    "href": "posts/HW5_Solutions_OmerYalcin.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\n\ndata(\"house.selling.price.2\")\n# object name too long, rename\nhouse <- house.selling.price.2\nrm(house.selling.price.2)\n\n# also rename the variables to make them more intuitive\n\nnames(house) <- c('Price', 'Size', 'Beds', 'Baths', 'New')\n\n\nA\nThe variable \\(\\textrm{Beds}\\) would be deleted first in backward elimination, because its p-value 0.487 is the largest of all the variables.\n\n\nB\n\n# Write a function that, for any given variable name, regresses that variable on Price (P) and gives the p-value:\n\nget_p_value <- function(variable) {\n  variable %>%\n    paste0(\"Price ~ \", .) %>%\n    as.formula() %>%\n    lm(data = house) %>%\n    summary() %>%\n    use_series('coefficients') %>%\n    extract(variable,'Pr(>|t|)') %>%\n    return()\n}\n\n# apply the function to all potential explanatory variables\nsapply(names(house)[-1], get_p_value)\n\n        Size         Beds        Baths          New \n2.346660e-34 4.758795e-10 9.839165e-16 4.514570e-04 \n\n\nSize has the lowest p-value when regressed against Price, so it should be the first variable to be added in forward selection.\nThe correlation matrix given in the question gives us the same information: out of all the candidates, Size has the highest correlation with Price: 0.899. Because all pair-wise correlations calculated in the correlation matrix are for samples of same size, the one with highest magnitude will also have the smallest p-value.\n\n\nC\nOnce other variables are controlled for, most of the explanation done by Beds is instead done by other variables. For example, houses with more beds are also larger in size. When Beds is the only explanatory variable, the model attributes the effect of the Size variable to Beds. However, once Size is also in the model and thus controlled for, the p-value of Beds gets larger as it doesn’t really have much explanatory power when it comes to variation in Price that remains unexplained after accounting for Size.\n\n\nD\nThe question seems to suggest we should compare all possible models, which is possible but probably not a very good idea. When we do backward elimination, we get the model that only excludes ‘Beds.’ So, let’s compare the model that has Beds to the one that doesn’t instead of comparing everything.\n\nfit_full <- lm(Price ~ ., data = house)\nfit_nobeds <- lm(Price ~ . -Beds, data = house)\n\nNow, let’s write functions to gather each metric that we want to use:\n\nrsq <- function(fit) summary(fit)$r.squared\nadj_rsq <- function(fit) summary(fit)$adj.r.squared\nPRESS <- function(fit) {\n    pr <- residuals(fit)/(1 - lm.influence(fit)$hat)\n    sum(pr^2)\n}\n# the functions for AIC and BIC are already AIC() and BIC()\n\nApply the two functions to the two objects:\n\nmodels <- list(fit_full, fit_nobeds)\ndata.frame(models = c('fit_full', 'fit_nobeds'),\n           rsq = sapply(models, rsq),\n           adj.rsq = sapply(models, adj_rsq),\n           PRESS = sapply(models, PRESS),\n           AIC = sapply(models, AIC),\n           BIC = sapply(models, BIC)\n) %>% \n  print()\n\n      models       rsq   adj.rsq    PRESS      AIC      BIC\n1   fit_full 0.8688630 0.8629022 28390.22 790.6225 805.8181\n2 fit_nobeds 0.8681361 0.8636912 27860.05 789.1366 801.7996\n\n\nNote that, for R-squares and Adjusted R-squared, larger is “better” while for PRESS, AIC, and BIC, lower is “better.” However, because R-squared always increases with the addition of new variables, it’s not very important for our purposes here. With other metrics, the model without Beds is the better model since it has higher adjusted R-squared, lower PRESS, AIC, and BIC.\n\n\nE\nThus I select the model without the Beds variable."
  },
  {
    "objectID": "posts/HW5_Solutions_OmerYalcin.html#question-2",
    "href": "posts/HW5_Solutions_OmerYalcin.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\n\ndata(trees)\n\n\nA\n\ntree_model <- lm(Volume ~ Girth + Height, data = trees)\nsummary(tree_model)\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\n\n\nB\n\npar(mfrow = c(2,3))\nplot(tree_model, which = 1:6)\n\n\n\n\nThe most obvious violation, based on the first plot, is the violation of the linearity assumption. The red line in the fitted values vs residuals plot should be somewhat straight, but it is actually U-shaped. The main reason for this is that volume is related to \\(\\textrm{diameter}^2\\), but we use the diameter itself in the model (Girth). Let’s see if using a quadratic term fixes the issue.\n\ntree_model_quad <- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees)\nplot(tree_model_quad, which = 1)\n\n\n\n\nThe line is much more straight now."
  },
  {
    "objectID": "posts/HW5_Solutions_OmerYalcin.html#question-3",
    "href": "posts/HW5_Solutions_OmerYalcin.html#question-3",
    "title": "Homework 5",
    "section": "Question 3",
    "text": "Question 3\n\nA\n\ndata(\"florida\")\nflorida_model <- lm(Buchanan ~ Bush, data = florida)\npar(mfrow = c(2,3))\nplot(florida_model, which = 1:6)\n\n\n\n\nIn this model, Palm Beach County is a massive outlier.\n\n\nB\nIf we fit a model where each variable is logged, Palm Beach County would be somewhat less of an outlier but would still stand out. Let’s see:\n\ndata(\"florida\")\nflorida_model_logged <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2,3))\nplot(florida_model_logged, which = 1:6)"
  },
  {
    "objectID": "posts/HW5_SteveONeill.html",
    "href": "posts/HW5_SteveONeill.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndata(house.selling.price.2)\nFor the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.\nIn this data, the variables are meant as:\nP: selling price\nBe: number of bedrooms\nBa: number of bathrooms\nNew: whether new (1 = yes, 0 = no)\nHere is my impression of the correlation matrix:\nAnd regression output:"
  },
  {
    "objectID": "posts/HW5_SteveONeill.html#automated-variable-selection",
    "href": "posts/HW5_SteveONeill.html#automated-variable-selection",
    "title": "Homework 5",
    "section": "Automated variable selection",
    "text": "Automated variable selection\nFor backward elimination, which variable would be deleted first? Why?\nIf I was doing backward elimination, I would pick a significance level (let’s say alpha = .05) and, at each stage, delete the variable with the largest p-value. I would stop when all variables are significant.\nIn this example, I would delete the Be (bedroom) variable.\n\n\nCode\nsummary(lm(P ~ . - Be, data=house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\n\nFor forward selection, which variable would be added first? Why?\nLike backward elimination, I would also predetermine a significance level (say, 5%). But here I would begin with no explanatory variable.\nThe Size variable would be added first in forward selection.\n\n\nCode\nintercept_only <- lm(P ~ 1, data=house.selling.price.2)\n\nstep(intercept_only, direction = \"forward\", scope=~ S + Be + Ba + New)\n\n\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq   RSS    AIC\n+ New   1    7274.7 27234 534.20\n+ Ba    1    4475.6 30033 543.30\n<none>              34508 554.22\n+ Be    1      40.4 34468 556.11\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq   RSS    AIC\n+ Ba    1    3550.1 23684 523.21\n+ Be    1     588.8 26645 534.17\n<none>              27234 534.20\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1    130.55 23553 524.70\n\n\n\nCall:\nlm(formula = P ~ S + New + Ba, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S          New           Ba  \n     -47.99        62.26        18.37        20.07  \n\n\nWhy do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\nAs pointed out, Be does have a substantial correlation with P at .59. However, the large P-values in multiple regression indicate that while holding other variables fixed, it does not ‘explain’ the response variable of P, price.\nUsing software with these four predictors, find the model that would be selected using each criterion:\nI’m not sure if I exactly get the question, but I will arbitrarily compare some models that I have made from combinations of the predictors.\n\n\nCode\nmod1 <- lm(P ~ S, data=house.selling.price.2)\nmod2 <- lm(P ~ S + New, data=house.selling.price.2)\nmod3 <- lm(P ~ S + New + Ba, data=house.selling.price.2)\nmod4 <- lm(P ~ .,data=house.selling.price.2)\n\n#A few with interaction variables\n\nmod5 <- lm(P ~ S + New + S*New, data=house.selling.price.2)\nmod6 <- lm(P ~ S + New + Ba + S*New, data=house.selling.price.2)\nmod7 <- lm(P ~ . + S * New, data=house.selling.price.2)\n\n\n###R2\n\n\nCode\nsummary(mod1)$r.squared\n\n\n[1] 0.807866\n\n\nCode\nsummary(mod2)$r.squared\n\n\n[1] 0.8483699\n\n\nCode\nsummary(mod3)$r.squared\n\n\n[1] 0.8681361\n\n\nCode\nsummary(mod4)$r.squared\n\n\n[1] 0.868863\n\n\nCode\nsummary(mod5)$r.squared\n\n\n[1] 0.8675196\n\n\nCode\nsummary(mod6)$r.squared\n\n\n[1] 0.8891938\n\n\nCode\nsummary(mod7)$r.squared\n\n\n[1] 0.8906193\n\n\nUsing ‘highest R-squared’ as our criteria, P ~ . + S * New is the winner. That one includes all the predictor variables in the equation, with size and newness as interaction variables.\n###Adjusted R2\n\n\nCode\nsummary(mod1)$adj.r.squared\n\n\n[1] 0.8057546\n\n\nCode\nsummary(mod2)$adj.r.squared\n\n\n[1] 0.8450003\n\n\nCode\nsummary(mod3)$adj.r.squared\n\n\n[1] 0.8636912\n\n\nCode\nsummary(mod4)$adj.r.squared\n\n\n[1] 0.8629022\n\n\nCode\nsummary(mod5)$adj.r.squared\n\n\n[1] 0.8630539\n\n\nCode\nsummary(mod6)$adj.r.squared\n\n\n[1] 0.8841571\n\n\nCode\nsummary(mod7)$adj.r.squared\n\n\n[1] 0.8843331\n\n\nAdjusted R-squared penalizes for adding more explanatory variables to the regression. However, it is still a virtual tie between P ~ S + New + Ba + S*New and P ~ . + S * New. Still, the latter wins.\n###PRESS\nThis elegant function is found on Github and requires no additional libraries.\n\n\nCode\nPRESS <- function(linear.model) {\n  #' calculate the predictive residuals\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n  #' calculate the PRESS\n  PRESS <- sum(pr^2)\n  \n  return(PRESS)\n}\n\n\nAccording to a comparison of the PRESS statistics, the best model is P ~ S + New + Ba + S*New with a PRESS of 27501.78\n\n\nCode\nPRESS(mod1)\n\n\n[1] 38203.29\n\n\nCode\nPRESS(mod2)\n\n\n[1] 31066\n\n\nCode\nPRESS(mod3)\n\n\n[1] 27860.05\n\n\nCode\nPRESS(mod4)\n\n\n[1] 28390.22\n\n\nCode\nPRESS(mod5)\n\n\n[1] 31899.8\n\n\nCode\nPRESS(mod6)\n\n\n[1] 27501.78\n\n\nCode\nPRESS(mod7)\n\n\n[1] 27665.14\n\n\n###AIC\nAccording to AIC, the best (lowest) score is 774.9558, associated with the model P ~ S + New + Ba + S*New\n\n\nCode\nAIC(mod1)\n\n\n[1] 820.1439\n\n\nCode\nAIC(mod2)\n\n\n[1] 800.1262\n\n\nCode\nAIC(mod3)\n\n\n[1] 789.1366\n\n\nCode\nAIC(mod4)\n\n\n[1] 790.6225\n\n\nCode\nAIC(mod5)\n\n\n[1] 789.5704\n\n\nCode\nAIC(mod6)\n\n\n[1] 774.9558\n\n\nCode\nAIC(mod7)\n\n\n[1] 775.7515\n\n\n###BIC\nAccording to BIC, the best model is the same - P ~ S + New + Ba + S*New, with a statistic of 790.1514.\n\n\nCode\nBIC(mod1)\n\n\n[1] 827.7417\n\n\nCode\nBIC(mod2)\n\n\n[1] 810.2566\n\n\nCode\nBIC(mod3)\n\n\n[1] 801.7996\n\n\nCode\nBIC(mod4)\n\n\n[1] 805.8181\n\n\nCode\nBIC(mod5)\n\n\n[1] 802.2334\n\n\nCode\nBIC(mod6)\n\n\n[1] 790.1514\n\n\nCode\nBIC(mod7)\n\n\n[1] 793.4797\n\n\nExplain which model you prefer and why.\nI prefer the model favored by AIC and BIC, P ~ S + New + Ba + S*New. Intuitively, it makes sense that bedrooms are not a significant driver of a house’s price, although bathrooms are. And the size of a house is more consequential on the outcome of price when the building is newer.\n\n\nCode\npar(mfrow = c(2,3)); plot(mod2, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(mod5, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(mod6, which = 1:6)\n\n\n\n\n\nCode\npar(mfrow = c(2,3)); plot(mod7, which = 1:6)"
  },
  {
    "objectID": "posts/HW_1_603.html",
    "href": "posts/HW_1_603.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_603.html#question-1",
    "href": "posts/HW_1_603.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nLung_data<- read_excel(\"C:/Users/manik/Desktop/LungCapData.xls\")\nLung_data\n\n\n\n\n  \n\n\n\nGiven data consists of 725 rows and 6 columns\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nLung_data %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram() +\n  geom_density(color = \"Red\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBased on above histogram , we can say the distribution is very close to the normal distribution\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nLung_data %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nMean_smokers <- Lung_data %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smokers\n\n\n\n\n  \n\n\n\nThe mean of the lung capacity who smokes is greater than the people who doesnt smoke which doesnt make any sense in practical\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nLung_data <- mutate(Lung_data, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nLung_data %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + coord_flip()\n\n\n\n\n\nCode\n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n$y\n[1] \"Lung Capacity\"\n\n$x\n[1] \"Frequency\"\n\n$title\n[1] \"Relationship of LungCap and Smoke based on age categories\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\nLung_data %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke.\nCalculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nCovariance_LA <- cov(Lung_data$LungCap, Lung_data$Age)\nCorrelation_LA <- cor(Lung_data$LungCap, Lung_data$Age)\nCovariance_LA\n\n\n[1] 8.738289\n\n\nCode\nCorrelation_LA\n\n\n[1] 0.8196749\n\n\nFrom the above result we can say that both covariance and correlation is positive and which indicates direct relationship that means Lungcapacity increases as age increases"
  },
  {
    "objectID": "posts/HW_1_603.html#question-2",
    "href": "posts/HW_1_603.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nCode\nIP\n\n\n\n\n  \n\n\n\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nIP %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\np_2 <- IP %>%\n  filter(Prior_convitions < 2)\nsum(p_2$Probability)\n\n\n[1] 0.6938272\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\np <- IP %>%\n  filter(Prior_convitions <= 2)\nsum(p$Probability)\n\n\n[1] 0.891358\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nP_3 <- IP %>%\n  filter(Prior_convitions > 2)\nsum(P_3$Probability)\n\n\n[1] 0.108642\n\n\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nIP <- mutate(IP, Wm = Prior_convitions*Probability)\nexpe<- sum(IP$Wm)\nexpe\n\n\n[1] 1.28642\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar_ <-sum(((IP$Prior_convitions-expe)^2)*IP$Probability)\nvar_\n\n\n[1] 0.8562353\n\n\nstandard deviation:\n\n\nCode\nsqrt(var_)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW_1_QH.html",
    "href": "posts/HW_1_QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a",
    "href": "posts/HW_1_QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/HW_1_QH.html#d",
    "href": "posts/HW_1_QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/HW_1_QH.html#e",
    "href": "posts/HW_1_QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/HW_1_QH.html#f",
    "href": "posts/HW_1_QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a-1",
    "href": "posts/HW_1_QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/HW_1_QH.html#b",
    "href": "posts/HW_1_QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/HW_1_QH.html#c",
    "href": "posts/HW_1_QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/HW_1_QH.html#d-1",
    "href": "posts/HW_1_QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/HW_1_QH.html#e-1",
    "href": "posts/HW_1_QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/HW_1_QH.html#f-1",
    "href": "posts/HW_1_QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/HW_2_QH.html",
    "href": "posts/HW_2_QH.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\nI want to create a data set with the information provided in the HW."
  },
  {
    "objectID": "posts/HW_2_QH.html#section",
    "href": "posts/HW_2_QH.html#section",
    "title": "Homework 2",
    "section": "1",
    "text": "1\nI did it manually below because using the t.test() function would give me the 95% CI. The CI for 90% is 15.34 and 21.65.\n\n\nCode\nmn <- mean(df$mean_wait)\n\nstandard_dev <- sd(df$mean_wait)\n\nsize <- length(df$mean_wait)\n\nstd_er <- standard_dev / sqrt(size)\n\nconfidence <- 0.9\ntail_area <- (1-confidence)/2\n\nt_score <- qt(p = 1 - tail_area, df = size-1)\n\nCI <- c(mn - t_score * std_er,\n        mn + t_score * std_er)\n\nprint(CI)\n\n\n[1] 15.34312 21.65688"
  },
  {
    "objectID": "posts/HW_2_QH.html#section-1",
    "href": "posts/HW_2_QH.html#section-1",
    "title": "Homework 2",
    "section": "2",
    "text": "2\nI found p by dividing the number of people that said college is essential for success with the total number of participants. Then I found the margin of error to calculate the low and high of the confidence interval for 95%. I believe this formula is accurate. https://www.geeksforgeeks.org/how-to-calculate-point-estimates-in-r/\nThe mean would fall between 503 and 630 with a confidence interval of 95%.\n\n\nCode\np <- 567/1031\n\nmargin <- qt(0.975, df = 1031 - 1) * sqrt(1031)\n\nlow <- 567 - margin\n\nhigh <- 567 + margin\n\nCI <- c(low, high)\n\nprint(CI)\n\n\n[1] 503.9931 630.0069"
  },
  {
    "objectID": "posts/HW_2_QH.html#section-2",
    "href": "posts/HW_2_QH.html#section-2",
    "title": "Homework 2",
    "section": "3",
    "text": "3\nThe sample size should be about 278.\n\n\nCode\n#1.96 * 42.5/sqrt(n) = 5\n\n(1.96*42.5/5)^2\n\n\n[1] 277.5556"
  },
  {
    "objectID": "posts/HW_2_QH.html#a",
    "href": "posts/HW_2_QH.html#a",
    "title": "Homework 2",
    "section": "4a",
    "text": "4a\n\n\nCode\nmen_mean <- 500\n\nsample_mean <- 410\n\nsd <- 90\n\n\nI create a data frame with the parameters of the question in order to perform a t test to compare the means.\n\n\nCode\n#pnorm(9, mean = 410, sd = 90)\n\ndf <- rnorm(9, mean = 410, sd = 90)\n\nt.test(df, mu = 500, alternative = \"two.sided\")\n\n\n\n    One Sample t-test\n\ndata:  df\nt = -3.0972, df = 8, p-value = 0.01473\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 340.4972 476.6449\nsample estimates:\nmean of x \n 408.5711"
  },
  {
    "objectID": "posts/HW_2_QH.html#b",
    "href": "posts/HW_2_QH.html#b",
    "title": "Homework 2",
    "section": "4b",
    "text": "4b\nThe p value is 0.002."
  },
  {
    "objectID": "posts/HW_2_QH.html#c",
    "href": "posts/HW_2_QH.html#c",
    "title": "Homework 2",
    "section": "4c",
    "text": "4c"
  },
  {
    "objectID": "posts/HW_2_QH.html#a-1",
    "href": "posts/HW_2_QH.html#a-1",
    "title": "Homework 2",
    "section": "5a",
    "text": "5a\nFor Jones. Plug t value into t value formula\n\n\nCode\n2 * (1 - pt(1.95, df = 999))\n\n\n[1] 0.05145555\n\n\nFor Smith.\n\n\nCode\n2 * (1 - pt(1.97, df = 999))\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW_2_QH.html#b-1",
    "href": "posts/HW_2_QH.html#b-1",
    "title": "Homework 2",
    "section": "5b",
    "text": "5b\nFor Jones, we can retain the null hypothesis since the level for Jones is greater than 0.05. For Smith we must reject the null hypothesis since his significance level is below 0.05."
  },
  {
    "objectID": "posts/HW_2_QH.html#c-1",
    "href": "posts/HW_2_QH.html#c-1",
    "title": "Homework 2",
    "section": "5c",
    "text": "5c\nIf a result from a test is less than 0.05 we must reject the null hypothesis, but if it’s greater than 0.05, we must fail to reject. There is no accepting a null hypothesis, we must either reject or fail to reject."
  },
  {
    "objectID": "posts/HW_2_QH.html#section-3",
    "href": "posts/HW_2_QH.html#section-3",
    "title": "Homework 2",
    "section": "6",
    "text": "6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\nt.test(gas_taxes, mu = 45, alternative = \"less\", conf.level = 0.95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nSince the sample mean is lower than the 45 and the p value is lower than 0.05, it is statistically significant. This means the null hypothesis should be rejected."
  },
  {
    "objectID": "posts/HW_3_603.html",
    "href": "posts/HW_3_603.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_3_603.html#question-1",
    "href": "posts/HW_3_603.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/HW_3_603.html#a",
    "href": "posts/HW_3_603.html#a",
    "title": "Homework 3",
    "section": "A",
    "text": "A\nThe Predicted variable here is ppgdp."
  },
  {
    "objectID": "posts/HW_3_603.html#b",
    "href": "posts/HW_3_603.html#b",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe graph show negative realtionship between ppgdp and fertility and here straight line mean function does not seem an appropriate measure for a summary of this graph."
  },
  {
    "objectID": "posts/HW_3_603.html#c",
    "href": "posts/HW_3_603.html#c",
    "title": "Homework 3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/HW_3_603.html#question-2",
    "href": "posts/HW_3_603.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW_3_603.html#a-1",
    "href": "posts/HW_3_603.html#a-1",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11"
  },
  {
    "objectID": "posts/HW_3_603.html#question-3",
    "href": "posts/HW_3_603.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/HW_3_603.html#question-4",
    "href": "posts/HW_3_603.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/HW_3_603.html#question-5",
    "href": "posts/HW_3_603.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/HW_3_603.html#a-2",
    "href": "posts/HW_3_603.html#a-2",
    "title": "Homework 3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/HW_3_603.html#b-2",
    "href": "posts/HW_3_603.html#b-2",
    "title": "Homework 3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/HW_3_QH.html",
    "href": "posts/HW_3_QH.html",
    "title": "Homework 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\nlibrary(fastDummies)\nlibrary(GGally)\n\n\nError in library(GGally): there is no package called 'GGally'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_3_QH.html#section",
    "href": "posts/HW_3_QH.html#section",
    "title": "Homework 3",
    "section": "1.1.1",
    "text": "1.1.1\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/HW_3_QH.html#section-1",
    "href": "posts/HW_3_QH.html#section-1",
    "title": "Homework 3",
    "section": "1.1.2",
    "text": "1.1.2\nA straight line doesn’t totally fit for this graph.\n\n\nCode\nggplot(UN11, mapping = aes(x = UN11$ppgdp, y = UN11$fertility))+\n  geom_point()+\n  geom_smooth(method = lm, se = F)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/HW_3_QH.html#section-2",
    "href": "posts/HW_3_QH.html#section-2",
    "title": "Homework 3",
    "section": "1.1.3",
    "text": "1.1.3\nI do not think this is practical way to view the data. While it has a similar trend line to the previous graph, I still do not think viewing the data with logs is an accurate way to represent the relationship between the two variables.\nAlthough I suppose it would be practical on second thought because it’s clearly showing what the above graph is trying to display. This is a more drastic negative correlation.\n\n\nCode\nggplot(UN11, mapping = aes(x = log(UN11$ppgdp), y = log(UN11$fertility)))+\n  geom_point()+\n  geom_smooth(method = lm, se = F)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/HW_3_QH.html#a",
    "href": "posts/HW_3_QH.html#a",
    "title": "Homework 3",
    "section": "2a",
    "text": "2a\nThis question is a bit challenge but I think the slope would decrease because the exchange rate decreases with conversion."
  },
  {
    "objectID": "posts/HW_3_QH.html#b",
    "href": "posts/HW_3_QH.html#b",
    "title": "Homework 3",
    "section": "2b",
    "text": "2b\nThe correlation wouldn’t change because the strength of the relationship isn’t affected."
  },
  {
    "objectID": "posts/HW_3_QH.html#section-3",
    "href": "posts/HW_3_QH.html#section-3",
    "title": "Homework 3",
    "section": "3",
    "text": "3\n\n\nCode\nsummary(pairs(water[2:8]))\n\n\n\n\n\nLength  Class   Mode \n     0   NULL   NULL \n\n\nBSAAM to OPBPC seem to be positively correlated with each other. This leads me to think that OPBPC, OPRC and OPSLAKE have significant runoff because the high level of precipitation is positively correlated with the volume of runoff measured by BSAAM."
  },
  {
    "objectID": "posts/HW_3_QH.html#section-4",
    "href": "posts/HW_3_QH.html#section-4",
    "title": "Homework 3",
    "section": "4",
    "text": "4\n\n\nCode\npairs(Rateprof[8:12])\n\n\n\n\n\nAccording to the scatter plot matrix, we can observe a few relationships of the various entries. Helpfulness and quality have a strong positive correlation. Clarity and quality and helpfulness and clarity also have a very strong positive correlation, all of which is expected. It’s when easiness comes it there is less positive correlation. I would imagine easiness and clarity would have a stronger correlation but here there appears to be less of a positive relationship.\nraterInterest is the variable with the least correlation with any of the other variables."
  },
  {
    "objectID": "posts/HW_3_QH.html#section-5",
    "href": "posts/HW_3_QH.html#section-5",
    "title": "Homework 3",
    "section": "5",
    "text": "5\nCreating dummy variables. So now there are many different columns as dummy variables but now it’s impossible to run a regression analysis.\n\n\nCode\nstudent.survey <-dummy_cols(student.survey, select_columns = \"pi\")\n\n\nError in dummy_cols(student.survey, select_columns = \"pi\"): object 'student.survey' not found\n\n\nCode\nstudent.survey <-dummy_cols(student.survey, select_columns = \"re\")\n\n\nError in dummy_cols(student.survey, select_columns = \"re\"): object 'student.survey' not found\n\n\nCode\n#ifelse(student.survey$pi == 'conservative', )\n\n#conservative <- ifelse(student.survey$pi == 'conservative', 1,0)\n#sl_conser <- ifelse(student.survey$pi == 'slightly conservative', 2,0)\n\n\n\n\nCode\nnum_pi <- model.matrix(~pi, data = student.survey)\n\n\nError in terms.formula(object, data = data): object 'student.survey' not found\n\n\nCode\nnum_re <- model.matrix(~re, data = student.survey)\n\n\nError in terms.formula(object, data = data): object 'student.survey' not found\n\n\n\n\nCode\nsummary(lm(num_pi ~ num_re))\n\n\nError in eval(predvars, data, env): object 'num_pi' not found\n\n\n\n\nCode\nsummary(lm(student.survey$hi ~ student.survey$tv))\n\n\nError in eval(predvars, data, env): object 'student.survey' not found"
  },
  {
    "objectID": "posts/HW_3_QH.html#a-1",
    "href": "posts/HW_3_QH.html#a-1",
    "title": "Homework 3",
    "section": "5a",
    "text": "5a\n\n\nCode\nggplot(student.survey, aes(re, ..count..)) + geom_bar(aes(fill = pi), position = \"dodge\")+\n  labs(title = \"Relationship between political ideology and religiosity\", x = \"Religiosity\", fill = \"Political Ideology\")\n\n\nError in ggplot(student.survey, aes(re, ..count..)): object 'student.survey' not found\n\n\nStudents who occasionally go to church tend to be very liberal, while students who lean conservative go to church more frequently. Overall, students who lean closer to the liberal ideology go to church less, if at all, and students who move right on the political spectrum tend to attend church more frequently.\n\n\nCode\nplot(student.survey$hi, student.survey$tv)\n\n\nError in plot(student.survey$hi, student.survey$tv): object 'student.survey' not found\n\n\nAccording to the linear model and by visualizing the trend, it appears there is not much of a correlation between hours of TV watched and high school GPA."
  },
  {
    "objectID": "posts/HW_4.html",
    "href": "posts/HW_4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_4.html#question-one",
    "href": "posts/HW_4.html#question-one",
    "title": "Homework 4",
    "section": "Question One",
    "text": "Question One"
  },
  {
    "objectID": "posts/HW_4.html#a",
    "href": "posts/HW_4.html#a",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nPredicted_selling_price <-  -10536 + 53.8 * 1240 + 2.84 * 18000\nPredicted_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nResidual <- Predicted_selling_price - 145000\nResidual\n\n\n[1] -37704\n\n\nFrom the above result, we can say that the house was sold for 37704 dollars greater than predicted."
  },
  {
    "objectID": "posts/HW_4.html#b",
    "href": "posts/HW_4.html#b",
    "title": "Homework 4",
    "section": "B",
    "text": "B\nUsing the prediction equation ŷ = -10536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1 = 1, representing one square-foot of home size, the output would increase by 53.8 * 1 = 53.8."
  },
  {
    "objectID": "posts/HW_4.html#c",
    "href": "posts/HW_4.html#c",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nx2 <- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price."
  },
  {
    "objectID": "posts/HW_4.html#question-2",
    "href": "posts/HW_4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"salary\")\nsalary"
  },
  {
    "objectID": "posts/HW_4.html#a-1",
    "href": "posts/HW_4.html#a-1",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women."
  },
  {
    "objectID": "posts/HW_4.html#b-1",
    "href": "posts/HW_4.html#b-1",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in salary of women compared to men falls between -697.8183 dollars and 3030.56452 dollars."
  },
  {
    "objectID": "posts/HW_4.html#c-1",
    "href": "posts/HW_4.html#c-1",
    "title": "Homework 4",
    "section": "C",
    "text": "C\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of 0.18, we cannot conclude that degree level has a statistically significant impact on salary.\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary.\nFor the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is 0.214, so this is not a statistically significant relationship.\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than 0.01 so the relationship between year and salary appears to be significant.\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant."
  },
  {
    "objectID": "posts/HW_4.html#d",
    "href": "posts/HW_4.html#d",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\nsummary(lm(salary ~ rank, salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary."
  },
  {
    "objectID": "posts/HW_4.html#e",
    "href": "posts/HW_4.html#e",
    "title": "Homework 4",
    "section": "E",
    "text": "E\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex."
  },
  {
    "objectID": "posts/HW_4.html#f",
    "href": "posts/HW_4.html#f",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary <- salary %>%\n  mutate(hired = case_when(ysdeg <= 15 ~ \"1\", ysdeg > 15 ~ \"0\"))\nsummary(lm(salary ~ hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ hired, data = salary)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27469.4      913.4  30.073  < 2e-16 ***\nhired1       -7343.5     1291.8  -5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nCode\nsummary(lm(salary ~ sex + rank + degree + hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29511.3      784.0  37.640  < 2e-16 ***\nsexFemale     -829.2      997.6  -0.831    0.410    \nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\ndegreePhD     1126.2     1018.4   1.106    0.275    \nhired1         319.0     1303.8   0.245    0.808    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both.\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean. When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment by the Dean toward faculty that the Dean specifically hired."
  },
  {
    "objectID": "posts/HW_4.html#question-3",
    "href": "posts/HW_4.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price"
  },
  {
    "objectID": "posts/HW_4.html#a-2",
    "href": "posts/HW_4.html#a-2",
    "title": "Homework 4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(Price ~ Size + New, house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by 116.132 dollars and 57736.283 dollars respectively."
  },
  {
    "objectID": "posts/HW_4.html#b-2",
    "href": "posts/HW_4.html#b-2",
    "title": "Homework 4",
    "section": "B",
    "text": "B\n\n\nCode\nnew <- house.selling.price %>% \n  filter(New == 1)\nsummary(lm(Price ~ Size, data = new))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold <- house.selling.price %>% \n  filter(New == 0)\nsummary(lm(Price ~ Size, data = old))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses. Adjusted R-squared for the model is also much higher (0.91 vs. 0.58).\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808"
  },
  {
    "objectID": "posts/HW_4.html#c-2",
    "href": "posts/HW_4.html#c-2",
    "title": "Homework 4",
    "section": "C",
    "text": "C\n\n\nCode\nSize <- 3000\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808\nNew_Price\n\n\n[1] 397244.7\n\n\nCode\nOld_Price\n\n\n[1] 289772.2"
  },
  {
    "objectID": "posts/HW_4.html#d-1",
    "href": "posts/HW_4.html#d-1",
    "title": "Homework 4",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW_4.html#e-1",
    "href": "posts/HW_4.html#e-1",
    "title": "Homework 4",
    "section": "E",
    "text": "E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size"
  },
  {
    "objectID": "posts/HW_4.html#f-1",
    "href": "posts/HW_4.html#f-1",
    "title": "Homework 4",
    "section": "F",
    "text": "F\n\n\nCode\nSize <- 3000\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 398324.7\n\n\nCode\nOld_Price\n\n\n[1] 291092.2"
  },
  {
    "objectID": "posts/HW_4.html#g",
    "href": "posts/HW_4.html#g",
    "title": "Homework 4",
    "section": "G",
    "text": "G\n\n\nCode\nSize <- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger."
  },
  {
    "objectID": "posts/HW_4.html#h",
    "href": "posts/HW_4.html#h",
    "title": "Homework 4",
    "section": "H",
    "text": "H\nThe prediction model with interaction has a significantly large negative coefficient for the New variable. The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169."
  },
  {
    "objectID": "posts/HW_4_QH.html",
    "href": "posts/HW_4_QH.html",
    "title": "Homework 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_4_QH.html#a",
    "href": "posts/HW_4_QH.html#a",
    "title": "Homework 4",
    "section": "1a",
    "text": "1a\nBased off the equation given in the question, the predicted value is $107,296 for a home with the parameters given.\n\n\nCode\nval1 <- -10536 + (53.8*1240) + (2.84*18000)\n\nval1\n\n\n[1] 107296\n\n\nThe residual is 37,704.\n\n\nCode\n145000 - 107296\n\n\n[1] 37704\n\n\nIt’s hard to analyze a single residual, but it seems like the number is off by more than it should. There isn’t enough data to make a concrete conclusion."
  },
  {
    "objectID": "posts/HW_4_QH.html#b",
    "href": "posts/HW_4_QH.html#b",
    "title": "Homework 4",
    "section": "1b",
    "text": "1b\nThe house size is predicted to increase by $53.80 for each square foot increase of the house because of the 53.8 variable."
  },
  {
    "objectID": "posts/HW_4_QH.html#c",
    "href": "posts/HW_4_QH.html#c",
    "title": "Homework 4",
    "section": "1c",
    "text": "1c\nI’m going to use the equation from before, but increase the square foot by 1.\n\n\nCode\nval <- -10536 + (53.8*1241) + (2.84*18000)\n\nval\n\n\n[1] 107349.8\n\n\n\n\nCode\nval-val1\n\n\n[1] 53.8\n\n\nLot size would need to increase by 53.8."
  },
  {
    "objectID": "posts/HW_4_QH.html#a-1",
    "href": "posts/HW_4_QH.html#a-1",
    "title": "Homework 4",
    "section": "2a",
    "text": "2a\n\n\nCode\ndata(\"salary\")\n\n\nIt appears the mean salary for men is about $3,000 more than the mean salary of women, indicating sex does play a factor in salary. The mean salary is not the same for both men and women. Even with a boxplot, it’s easy to observe the difference in salary across both genders.\n\n\nCode\nt.test(salary$salary ~ salary$sex, mu = 0, alternative = \"two.sided\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary$salary by salary$sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\n\n\nCode\nsalary %>% \n  ggplot(aes(x = sex, y = salary))+\n  geom_boxplot()"
  },
  {
    "objectID": "posts/HW_4_QH.html#b-1",
    "href": "posts/HW_4_QH.html#b-1",
    "title": "Homework 4",
    "section": "2b",
    "text": "2b\nIn the summary, since there is no code next to the sexFemale variable, we can see that it has not met a significant threshold. This shows we did obtain a 95% confidence interval.\n\n\nCode\nfit <- summary(lm(salary ~ ., data = salary))\n\nfit\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW_4_QH.html#c-1",
    "href": "posts/HW_4_QH.html#c-1",
    "title": "Homework 4",
    "section": "2c",
    "text": "2c\nDegree does not appear to be significant when it comes to the outcome of salary. The R squared is actually a negative number so it really is not an appropriate measure of salary.\n\n\nCode\nm1 <- summary(lm(salary ~ degree, data = salary))\nm1\n\n\n\nCall:\nlm(formula = salary ~ degree, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8500.4 -5253.6  -640.2  3758.1 14544.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  23500.4     1022.4  22.985   <2e-16 ***\ndegreePhD      858.9     1737.8   0.494    0.623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5962 on 50 degrees of freedom\nMultiple R-squared:  0.004862,  Adjusted R-squared:  -0.01504 \nF-statistic: 0.2443 on 1 and 50 DF,  p-value: 0.6233\n\n\nRank actually does have significance in the outcome of salary. Both Assoc and Prof have significant p values and the R squared is about 0.74 which indicates some level of significance. In this model, I can assume that if you are a professor, you can observe an increase in $11,890.\n\n\nCode\nm2 <- summary(lm(salary ~ rank, data = salary))\nm2\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  17768.7      705.5   25.19  < 2e-16 ***\nrankAssoc     5407.3     1066.6    5.07 6.09e-06 ***\nrankProf     11890.3      972.4   12.23  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAccording to this model, sex is not relevant when it comes to the outcome variable, containing a low R squared and an insignificant p value.\n\n\nCode\nm3 <- summary(lm(salary ~ sex, data = salary))\nm3\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe years someone has been in their position does have a significant impact on their salary, which makes sense because, usually, the longer someone is in a job, the more opportunities they will have for a promotion.\n\n\nCode\nm4 <- summary(lm(salary ~ year, data = salary))\nm4\n\n\n\nCall:\nlm(formula = salary ~ year, data = salary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11035.9  -3172.4   -561.7   3185.8  13856.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18166.1     1003.7  18.100  < 2e-16 ***\nyear           752.8      108.4   6.944 7.34e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4264 on 50 degrees of freedom\nMultiple R-squared:  0.4909,    Adjusted R-squared:  0.4808 \nF-statistic: 48.22 on 1 and 50 DF,  p-value: 7.341e-09\n\n\nI think years since highest degree earned falls under the same logic as the years column since, presummably, many of the years spent in the job were years also in the ysdeg column. I expect that with every 1 year increase in years after highest degree, there will be a $390 increase in salary.\n\n\nCode\nm5 <- summary(lm(salary ~ ysdeg, data = salary))\nm5\n\n\n\nCall:\nlm(formula = salary ~ ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9703.5 -2319.5  -437.1  2631.8 11167.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17502.26    1149.70  15.223  < 2e-16 ***\nysdeg         390.65      60.41   6.466  4.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4410 on 50 degrees of freedom\nMultiple R-squared:  0.4554,    Adjusted R-squared:  0.4445 \nF-statistic: 41.82 on 1 and 50 DF,  p-value: 4.102e-08\n\n\nI may want to use the par function but I cannot find the proper way to type it out. Will revisit."
  },
  {
    "objectID": "posts/HW_4_QH.html#d",
    "href": "posts/HW_4_QH.html#d",
    "title": "Homework 4",
    "section": "2d",
    "text": "2d\n\n\nCode\n#rank <- relevel(rank)\n\nrelevel_mod <- lm(salary ~ rank, data = salary)\n\nsummary(relevel_mod)\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  17768.7      705.5   25.19  < 2e-16 ***\nrankAssoc     5407.3     1066.6    5.07 6.09e-06 ***\nrankProf     11890.3      972.4   12.23  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15"
  },
  {
    "objectID": "posts/HW_4_QH.html#e",
    "href": "posts/HW_4_QH.html#e",
    "title": "Homework 4",
    "section": "2e",
    "text": "2e\nIf you compare this model with models that contain the rank variable, you’ll see that rank actually does play a significant role in salary. Rank always has a p value worth noting as seen with the significance codes. The adjusted R squared is also much lower in this model than any model containing rank as an explanatory variable.\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09"
  },
  {
    "objectID": "posts/HW_4_QH.html#f",
    "href": "posts/HW_4_QH.html#f",
    "title": "Homework 4",
    "section": "2f",
    "text": "2f\nI’m trying to make a new column for someone hired within the 15 year period of the new Dean. Now the new column should have a 1 if the person was hired with 15 years or less from earning their highest degree and people will have a -1 if its more. The 1 will indicate if they have been hired by the new dean and we may see an increase in salary for those people.\n\n\nCode\nsalary <- salary %>% \n  mutate(hired_by_new_dean = case_when(\n    ysdeg <= 15 ~ 1,\n    T ~ -1\n  ))\n\n\n\n\nCode\nsummary(lm(salary ~ ysdeg*hired_by_new_dean, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ysdeg * hired_by_new_dean, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8251.2 -2619.1  -317.1  1428.3 10592.7 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              21206.7     1966.3  10.785 2.01e-14 ***\nysdeg                      312.5      106.0   2.947  0.00494 ** \nhired_by_new_dean        -5618.2     1966.3  -2.857  0.00630 ** \nysdeg:hired_by_new_dean    286.3      106.0   2.701  0.00954 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4150 on 48 degrees of freedom\nMultiple R-squared:  0.5372,    Adjusted R-squared:  0.5082 \nF-statistic: 18.57 on 3 and 48 DF,  p-value: 3.909e-08\n\n\n\n\nCode\nsummary(lm(salary ~ ysdeg, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9703.5 -2319.5  -437.1  2631.8 11167.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17502.26    1149.70  15.223  < 2e-16 ***\nysdeg         390.65      60.41   6.466  4.1e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4410 on 50 degrees of freedom\nMultiple R-squared:  0.4554,    Adjusted R-squared:  0.4445 \nF-statistic: 41.82 on 1 and 50 DF,  p-value: 4.102e-08\n\n\nWith an interaction between ysdeg and the new column I created to separate people hired by the new dean, I can see the first model is slightly a better fit with an R squared of 0.5 and all significantly low p values. We can conclude that if you were hired by the new dean, you are more likely to have a higher salary. I avoided multicollinearity by putting an interaction symbol for ysdeg and hired_by_new_dean. I’d be worried about ysdeg and hired_by_new interacting with each other to cause multicollinearity."
  },
  {
    "objectID": "posts/HW_4_QH.html#a-2",
    "href": "posts/HW_4_QH.html#a-2",
    "title": "Homework 4",
    "section": "3a",
    "text": "3a\n\n\nCode\ndata(\"house.selling.price\")\n\n\nBoth size and whether the house is new or not are statistically significant as per the significance codes related to the p values. The adjusted R squared is also pretty high which indicates this model is a good fit. Going by the significance codes, Size is more important when it comes to the model compared to the New variable.\n\n\nCode\nmodel <-lm(Price ~ Size + New, data = house.selling.price)\nsummary(model)\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW_4_QH.html#b-2",
    "href": "posts/HW_4_QH.html#b-2",
    "title": "Homework 4",
    "section": "3b",
    "text": "3b\nAccording to my prediction model, the mean for both actual price and predicted price are exactly the same, which I find odd because would there not be some variation? I used the predict function on the model then added them together so the data was easier to work with. I suppose I could have created a test set and a training set, but I think that was outside the scope of the homework.\n\n\nCode\npred <- predict(model)\npred\n\n\n        1         2         3         4         5         6         7         8 \n197606.63  65681.14 151850.78 199929.26 131295.49 383668.32 117127.44 258478.46 \n        9        10        11        12        13        14        15        16 \n423134.17  94481.78 101449.67 156031.52 120030.73 144418.36  81707.30  49190.46 \n       17        18        19        20        21        22        23        24 \n123514.67  82868.62 110740.20 133966.52  54997.04 476109.06  95643.09 133966.52 \n       25        26        27        28        29        30        31        32 \n283776.27  79384.67 104933.62 164160.73 139773.10  89836.51 192032.31 116546.78 \n       33        34        35        36        37        38        39        40 \n231187.54 251259.42 378674.66 164160.73 158354.15  88675.20 117708.09 104933.62 \n       41        42        43        44        45        46        47        48 \n104933.62 131643.89 136289.15 194354.94  77062.04 150224.94  68932.83 143257.04 \n       49        50        51        52        53        54        55        56 \n 59642.30 124675.99 107256.25  73578.09 226871.79 125837.31 120030.73 103772.30 \n       57        58        59        60        61        62        63        64 \n 89836.51  89836.51 180419.15 241968.90  85191.25 116546.78 159515.47 430102.07 \n       65        66        67        68        69        70        71        72 \n133966.52 259504.76 263704.39 336286.63 136289.15 108417.57 147902.31 136289.15 \n       73        74        75        76        77        78        79        80 \n195516.26 121192.04 178096.52 295505.56 115385.46  68932.83  27125.45 123514.67 \n       81        82        83        84        85        86        87        88 \n 93320.46 120030.73 188218.85 202154.64 156863.32 182741.78 209452.05 215258.63 \n       89        90        91        92        93        94        95        96 \n 59642.30 102610.99  92159.14 325254.13  82868.62 165322.05 175773.89  82868.62 \n       97        98        99       100 \n160676.78 118869.41 140934.41 115385.46 \n\n\nCode\nnew.house.selling.price <- cbind(house.selling.price, pred)\n\n\n\n\nCode\nnew.house.selling.price %>% \n  group_by(New) %>% \n  summarize(Price_actual = mean(Price), Price_prediction = mean(pred))\n\n\n# A tibble: 2 × 3\n    New Price_actual Price_prediction\n  <int>        <dbl>            <dbl>\n1     0      138567.          138567.\n2     1      290964.          290964."
  },
  {
    "objectID": "posts/HW_4_QH.html#c-2",
    "href": "posts/HW_4_QH.html#c-2",
    "title": "Homework 4",
    "section": "3c",
    "text": "3c\n\n\nCode\nvariable1 <- data.frame(Size = 3000, New = 1)\n\npredict(model, newdata = variable1)\n\n\n       1 \n365900.2 \n\n\n\n\nCode\nvariable2 <- data.frame(Size = 3000, New = 0)\n\npredict(model, newdata = variable2)\n\n\n       1 \n308163.9"
  },
  {
    "objectID": "posts/HW_4_QH.html#d-1",
    "href": "posts/HW_4_QH.html#d-1",
    "title": "Homework 4",
    "section": "3d",
    "text": "3d\nThe interaction between New and Size actually is more significant than New solely alone. Overall, the model is a pretty good fit since its R squared is 0.73.\n\n\nCode\nmod <- lm(Price ~ Size*New, data = house.selling.price)\n\nsummary(mod)\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW_4_QH.html#e-1",
    "href": "posts/HW_4_QH.html#e-1",
    "title": "Homework 4",
    "section": "3e",
    "text": "3e\nNew home price = -22228 + 104 + -78527 + 62 Old home price = -22228 + 104\nI included the last two numbers in the first equation because those are the interaction terms."
  },
  {
    "objectID": "posts/HW_4_QH.html#f-1",
    "href": "posts/HW_4_QH.html#f-1",
    "title": "Homework 4",
    "section": "3f",
    "text": "3f\nRepeat question"
  },
  {
    "objectID": "posts/HW_4_QH.html#g",
    "href": "posts/HW_4_QH.html#g",
    "title": "Homework 4",
    "section": "3g",
    "text": "3g\nThe whether the home is new or not impacts the price of the house in a pretty significant way. About a $60,000 difference in price is not something to dismiss. With the same lot size, a home that is new will fetch significantly more money than the same home, but old.\n\n\nCode\nvar3 <- data.frame(Size = 1500, New = 0)\n\nvar4 <- data.frame(Size = 1500, New = 1)\n\n\nHouse is new.\n\n\nCode\npredict(model, newdata = var4)\n\n\n       1 \n191702.8 \n\n\nHouse is old.\n\n\nCode\npredict(model, newdata = var3)\n\n\n       1 \n133966.5"
  },
  {
    "objectID": "posts/HW_4_QH.html#h",
    "href": "posts/HW_4_QH.html#h",
    "title": "Homework 4",
    "section": "3h",
    "text": "3h\nI prefer the model with interaction between size and new because its adjusted R squared is slightly higher than that of the model without interaction. Be it only 0.02 points, it still shows a better fit compared to the other model. The p value for the interaction between new and old is also labeled as significant in the interaction model. They are so similar, but again I would choose the interaction model because the adjusted R squared is slightly higher."
  },
  {
    "objectID": "posts/HW_5.html",
    "href": "posts/HW_5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.5.0 \nv readr   2.1.3      v forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(MPV)\n\n\nWarning: package 'MPV' was built under R version 4.1.3\n\n\nLoading required package: lattice\nLoading required package: KernSmooth\nKernSmooth 2.23 loaded\nCopyright M. P. Wand 1997-2009\n\n\nCode\nlibrary(alr4)\n\n\nWarning: package 'alr4' was built under R version 4.1.3\n\n\nLoading required package: car\n\n\nWarning: package 'car' was built under R version 4.1.3\n\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.1.3\n\n\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\n\n\nWarning: package 'effects' was built under R version 4.1.3\n\n\nUse the command\n    lattice::trellis.par.set(effectsTheme())\n  to customize lattice options for effects plots.\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss)\n\n\nWarning: package 'smss' was built under R version 4.1.3\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_5.html#question-1",
    "href": "posts/HW_5.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(house.selling.price.2)\nhouse.selling.price.2"
  },
  {
    "objectID": "posts/HW_5.html#a",
    "href": "posts/HW_5.html#a",
    "title": "Homework 5",
    "section": "A",
    "text": "A\nFor backward elimination, you fit a model using all possible explanatory values to predict the output. Then one by one, you delete the least significant explanatory variable in the model, which would have the largest p-value. In this example, we would delete Beds first, which has a p-value of 0.487."
  },
  {
    "objectID": "posts/HW_5.html#b",
    "href": "posts/HW_5.html#b",
    "title": "Homework 5",
    "section": "B",
    "text": "B\nWith forward selection, you begin with no explanatory variables, then add one variable at a time to the model. The variable you add should be the most significant one, based on it having the lowest P-value of the group of possible explanatory variables. In this example, the first variable to add to the model is Size, given its extremely small p-value < 2e-16."
  },
  {
    "objectID": "posts/HW_5.html#c",
    "href": "posts/HW_5.html#c",
    "title": "Homework 5",
    "section": "C",
    "text": "C\nWhile the variable Beds does have a strong correlation with price, when adding additional variables using a regression model, the relationship significantly diminishes, thus the other variables may act as a control on the bed variable."
  },
  {
    "objectID": "posts/HW_5.html#d",
    "href": "posts/HW_5.html#d",
    "title": "Homework 5",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(P ~ S, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ S, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.407 -10.656   2.126  11.412  85.091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -25.194      6.688  -3.767 0.000293 ***\nS             75.607      3.865  19.561  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.47 on 91 degrees of freedom\nMultiple R-squared:  0.8079,    Adjusted R-squared:  0.8058 \nF-statistic: 382.6 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ S+New, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ S + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ ., data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,    Adjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(P ~ . -Be -Ba, data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ . - Be - Ba, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.207  -9.763  -0.091   9.984  76.405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\nS             72.575      3.508  20.690  < 2e-16 ***\nNew           19.587      3.995   4.903 4.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.4 on 90 degrees of freedom\nMultiple R-squared:  0.8484,    Adjusted R-squared:  0.845 \nF-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n\n\n\na. R^2\nAs expected, the model with the most explanatory variables has the highest R-squared value at 0.8689. Therefore, if you were to select a model solely based on maximizing the R-squared value, it would be: ŷ = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New).\n\n\nb. Adjusted R^2\nHowever, if you were to select a model based on adjusted R-squared, the best model for predicting selling price would exclude Beds and use Size, Baths, and New as explanatory variables. The adjusted R-squared value see a slight increase when Beds is removed (from 0.8629 to 0.8637). The model would be: ŷ = -47.99 + 62.26(Size) + 20.07(Baths) + 18.37(New).\n\n\nc. PRESS\n\n\nCode\nPRESS(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 28390.22\n\n\nCode\nPRESS(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 27860.05\n\n\nWhen considering PRESS, a smaller PRESS value indicates a better predictive model. Comparing the PRESS value of the model with all variables and the model excluding Bed, the PRESS values would lead us to select the model with Size, Baths, and New as variables for predicting selling price.\n\n\nd. AIC\n\n\nCode\nAIC(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 790.6225\n\n\nCode\nAIC(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 789.1366\n\n\nWhen considering the AIC for both models, the value is slightly lower for the model that excludes Bed as a variable. Therefore, the AIC would lead us to use the model with Size, Baths, and New as explanatory variables to predicting selling price.\n\n\ne. BIC\n\n\nCode\nBIC(lm(P ~ ., data = house.selling.price.2))\n\n\n[1] 805.8181\n\n\nCode\nBIC(lm(P ~ . -Be, data = house.selling.price.2))\n\n\n[1] 801.7996\n\n\nLastly, like AIC, the BIC value is lower for the model that excludes Bed as a variable. Once again, we’d select the model that uses Size, Baths, and New as explanatory variables to predict selling price."
  },
  {
    "objectID": "posts/HW_5.html#e",
    "href": "posts/HW_5.html#e",
    "title": "Homework 5",
    "section": "E",
    "text": "E\nGiven the results from the various criteria above, the model I would prefer to use to predict selling price is that which excludes Bed and includes Size, Bath, and New as variables: ŷ = -41.79 + 64.76(Size) - 2.77(Beds) + 19.2(Baths) + 18.98(New). This is because each of the criterion indicate this model as slightly stronger in its predictive power than the model that includes all variables except R-squared, which cannot be used alone to determine model strength."
  },
  {
    "objectID": "posts/HW_5.html#question-2",
    "href": "posts/HW_5.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"trees\")\ntrees"
  },
  {
    "objectID": "posts/HW_5.html#a-1",
    "href": "posts/HW_5.html#a-1",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/HW_5.html#b-1",
    "href": "posts/HW_5.html#b-1",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the residuals vs. fitted values plot, the central points appear to roughly bounce randomly above and below 0, but the lowest and highest point appear to be very influential residuals. The red line should be flat along 0 horizontally, but it is U-shaped. This curvature may suggest a violation in the linearity assumption. With the normal Q-Q plot, it’s difficult to confidently say that the assumption of normality appears to be violated. The points generally run along the trend-line, but they do deviate above the line for the higher points. It’s a noteworthy deviation, but it’s difficult to make a certain decision based on the plot. In the scale-location plot, the line is not horizontal, thus suggesting a violation in the assumption of constant variance. Cook’s distance suggests that the 31st observation is above the threshold, meaning it is too influential as one observation."
  },
  {
    "objectID": "posts/HW_5.html#question-3",
    "href": "posts/HW_5.html#question-3",
    "title": "Homework 5",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"florida\")\nflorida"
  },
  {
    "objectID": "posts/HW_5.html#a-2",
    "href": "posts/HW_5.html#a-2",
    "title": "Homework 5",
    "section": "A",
    "text": "A\n\n\nCode\nmodel <- lm(formula = Buchanan ~ Bush, data = florida)\nsummary(model)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the diagnostic plots, Palm Beach County is an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is very large. When referring to the summary of the simple regression model, the third quartile for residuals is 12.26, yet the max is 2610.19. This is a significant jump and indicative of the value being an outlier. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows two points that may be of concern as outliers if you follow the metric of observations scoring over 1, which are DADE and Palm Beach at about 2. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model."
  },
  {
    "objectID": "posts/HW_5.html#b-2",
    "href": "posts/HW_5.html#b-2",
    "title": "Homework 5",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(formula = log(Buchanan) ~ log(Bush), data = florida)\nsummary(model)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n\n\n\n\n\nBased on the diagnostic plots, Palm Beach County is still an outlier. First, when looking at the residuals vs fitted plot, the Palm Beach County residual is still very large. The normal Q-Q plot also indicates that the residuals for the model are generally normal except for the Palm Beach County residual, as it greatly deviates from the line in the plot. The Cook’s distance plot shows that may be of concern as outlier if you follow the metric of observations scoring over 0.2, which is Palm Beach at about 0.3. The residuals and leverages plot shows the Palm Beach County standardized residual value beyond the dashed line indicating Cook’s distance. This also suggests that the observation is an outlier and the observation has the potential to influence the regression model."
  },
  {
    "objectID": "posts/HW_5_QH.html",
    "href": "posts/HW_5_QH.html",
    "title": "Homework 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_5_QH.html#a",
    "href": "posts/HW_5_QH.html#a",
    "title": "Homework 5",
    "section": "1a",
    "text": "1a\nI would delete Beds because it is the least significant variable to the regression equation. That would get deleted first because under backwards elimination, the variable least significant(the smallest P value) to the regression equation gets deleted first, and so on.\n\n\nCode\n#summary(lm(P ~ ., data = house.selling.price.2))"
  },
  {
    "objectID": "posts/HW_5_QH.html#b",
    "href": "posts/HW_5_QH.html#b",
    "title": "Homework 5",
    "section": "1b",
    "text": "1b\nSize would be added first because it has the highest t value at 11.5. Forward selection starts with no variables, then adds each one by how large their significance, or how large their t value is to the regression model."
  },
  {
    "objectID": "posts/HW_5_QH.html#c",
    "href": "posts/HW_5_QH.html#c",
    "title": "Homework 5",
    "section": "1c",
    "text": "1c\nCorrelation does not always equal causation. Beds does not have to be a significant variable for it to be correlated with Price. Also, when a sample size is small, it can account for the high p values in the regression model."
  },
  {
    "objectID": "posts/HW_5_QH.html#d",
    "href": "posts/HW_5_QH.html#d",
    "title": "Homework 5",
    "section": "1d",
    "text": "1d\n\n\nCode\nm1 <- lm(P ~., data = house.selling.price.2)\n\n\nError in is.data.frame(data): object 'house.selling.price.2' not found\n\n\n\n\nCode\nAIC(m1)\n\n\nError in AIC(m1): object 'm1' not found\n\n\nCode\nBIC(m1)\n\n\nError in BIC(m1): object 'm1' not found\n\n\nCode\nPRESS <- function(m1) {\n  pr <- residuals(m1)/(1-lm.influence(m1)$hat)\n  \n  PRESS <- sum(pr^2)\n  \n  return(PRESS)\n}\n\nPRESS(m1)\n\n\nError in residuals(m1): object 'm1' not found\n\n\nI needed assistance figuring out the PRESS statistic so I searched for functions to build online."
  },
  {
    "objectID": "posts/HW_5_QH.html#e",
    "href": "posts/HW_5_QH.html#e",
    "title": "Homework 5",
    "section": "1e",
    "text": "1e\nWith PRESS, you want a smaller number, which indicates better prediction of the model. Since the number appears to be ~28,000 (if I did the calculations correctly) I do not want to use PRESS. Instead, I would prefer the AIC model since it is the lowest score, indicating the best fit."
  },
  {
    "objectID": "posts/HW_5_QH.html#a-1",
    "href": "posts/HW_5_QH.html#a-1",
    "title": "Homework 5",
    "section": "2a",
    "text": "2a\n\n\nCode\ndata(trees)\n\nmodel <- lm(Volume ~ Girth + Height, data = trees)\n\n#summary(model)"
  },
  {
    "objectID": "posts/HW_5_QH.html#b-1",
    "href": "posts/HW_5_QH.html#b-1",
    "title": "Homework 5",
    "section": "2b",
    "text": "2b\n\n\nCode\npar(mfrow = c(2,3)); plot(model, which = 1:6)\n\n\n\n\n\nLinearity is violated because the residuals plot exhibits a curved line, not a horizontal line across zero. Assumption of equal variance is also violated because the plots are not too scattered. The line should be horizontal, but in this diagnostic plot, it is quite jagged. The QQ plot looks pretty linear so that one is fine the way it is suggesting the data follows a normal distribution. There are also no extreme values in the residuals vs leverage plot so there is nothing problematic there."
  },
  {
    "objectID": "posts/HW_5_QH.html#a-2",
    "href": "posts/HW_5_QH.html#a-2",
    "title": "Homework 5",
    "section": "3a",
    "text": "3a\n\n\nCode\ndata(\"florida\")\n\n\nfit <- lm(Buchanan ~ Bush, data = florida)\n\n\nI ran the linear regression and assigned it to the fit variable.\n\n\nCode\npar(mfrow = c(2,3))\nplot(fit, which = 1:6)\n\n\n\n\n\nIt’s clear there is an outlier in each of the diagnostic plots. In each of the diagnostic plots, Palm Beach is a definite outlier among the other data points. Palm Beach County has almost 3,500 votes, the next highest in another county being 750. The residuals plot is normal with a horizontal line, but the residuals are not spread out which leads me to believe there is some violation. Palm Beach and Dade county both lay outside of Cook’s distance, meaning they are highly influential to the regression results."
  },
  {
    "objectID": "posts/HW_5_QH.html#b-2",
    "href": "posts/HW_5_QH.html#b-2",
    "title": "Homework 5",
    "section": "3b",
    "text": "3b\n\n\nCode\nfit2 <- lm(log(florida$Buchanan) ~ log(florida$Bush))\n\n\npar(mfrow = c(2,3)); plot(fit2, which = 1:6)\n\n\n\n\n\nFrom logging the variables, the QQ plot becomes linear and normalizes both residuals and scale-location plots. Residuals should “bounce around” the line at zero, which they do not do previously to logging the variables. When the variables are logged, it corrects this, making the data linear and demonstrates the relationship between the explanatory and outcome variables are linear.The normal QQ is also corrected showing the residuals are normally distributed with very little deviation. The scale location is how we check for homoskedasticity. When the variables are logged, the graph is perfect with a horizontal line and plots scattered a bit. It is the opposite of what we see in the graph when the variables are not logged. The residuals vs leverage extreme values are now inside of Cook’s distance so they will not impact the regression results. Logging the variables normalized all the results and seemed to resolve issues with the spread of the data. We should still note Palm Beach as the outlier."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2.html",
    "href": "posts/KalimahMuhammad_finalpart2.html",
    "title": "Final Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nperformance<- read_csv(\"_data/CompleteDataAndBiases.csv\")\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2.html#background",
    "href": "posts/KalimahMuhammad_finalpart2.html#background",
    "title": "Final Part 2",
    "section": "Background",
    "text": "Background\nResearchers have examined the differences impacting student achievement among genders for decades. Early meta-analyses conducted by Hyde (1990) revealed no significant difference between the groups’ cognitive abilities; yet intelligence and self-perception of ability, or self-esteem, were determined as the strongest predictors of academic success (Spinath et al., 2010, Booth and Gerard, 2011). Some research explains differences in achievement partially by stereotype threat (ST). Stereotype threat, originally used to research the vulnerability of racial stereotypes on aptitude tests, is defined as “being at risk of confirming, as self-characteristic, a negative stereotype about one’s group” (Steele and Aronson, 1995). Conventional research has focused on gender differences to stereotype threat (Moè & Putwain, 2020), and two prevailing theories have emerged.\nThe first theory suggests that ST affects male and female students differently. Here male students achieve better outcomes when stereotype threat is present than stereotype lift, and the reverse is true for female students who are adversely impacted by stereotype threat and positively impacted by stereotype lift (Johnson et al., 2012). Other studies debunk this theory suggesting no evidence of ST as a phenomenon in female students (Warne, 2022), particularly in the domain of mathematical achievement (Ganley et al., 2013). A second theory exists that the effects of ST may not be fully realized outcomes immediately but affect achievement over time as chronic ST impacts working memory and intellectual helplessness in girls’ math scores (Bedyńska, Krejtz, and Sedek, 2019) and male language arts scores (Bedyńska, Krejtz, and Sedek, 2020)."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2.html#hypotheses",
    "href": "posts/KalimahMuhammad_finalpart2.html#hypotheses",
    "title": "Final Part 2",
    "section": "Hypotheses",
    "text": "Hypotheses\nThis study tests both theories as an assumption in predicting student achievement based on actual student performance data and predictions based on the participants’ exposure to three types of stereotypes activation:\n-No stereotypes,\n-Case-based stereotypes where students were shown three student profiles in which one female student had a high grade and two male students had low grades, and\n-Statistical stereotypes where students were shown statistics that boys performed less well in school than girls.\nBased on this experiment and prior research, I will test the following hypotheses: 1. Does exposure to negative stereotypes about male achievement and positive stereotypes of female achievement result in higher predicted achievement among both male and female students?\n2. Is there a statistically significant difference between male and female achievement based on the type of stereotype activation within each group?\n\n\nCode\nperformance<- arrange(performance,(index)) #arrange table by index in ascending order"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2.html#data",
    "href": "posts/KalimahMuhammad_finalpart2.html#data",
    "title": "Final Part 2",
    "section": "Data",
    "text": "Data\nData for this project was collected from Kaggle (Performance vs. Predicted Performance, 2022) and is a collection of existing student performance data from a study by Cortez and Silva (2008) on predicting secondary school student performance and the collector’s addition of stereotype activation for machine learning. The data included information on actual student demographics such as gender, parents’ highest level of education, as well as time studying vs. free time, number of absences, and grade on a final exam, among other variables.\nA few important variables to define include:\n-Index - number of students included in the original and predicted data (N=856 of the original 991 students)\n-Participant ID - number associated with participants making predictions (N=107)\n-Sex - student’s sex (binary: ‘F’ - female or ‘M’ - male)\n-Studytime - weekly study time is categorized as 1= less than 2 hours, 2= 2-5 hours and 3= 5+ hours\n-Freetime - free time after school group as 1 = low, 2= medium, 3 = high\n-Goout - how often a student goes out with friends where 1 is very low and 4 is very often\n-Absences - number of school absences 1 - 7 where 7 represents any absences equal to or above 7\n-Walc - weekend alcohol consumption\n-Parents_edu - the higher of original variables mother’s edu and father’s edu, where 4 = the highest level of education\n-G3 - final grade (numeric: from 0 to 20)\n-Reason - The reason for why a student chose to go to the school in question. The levels are close to home, school’s reputation, school’s curricular and other\n-PredictedGrade - the grade participants predicted based on actual data and their exposed level of StereotypeActivation\n-StereotypeActivation - see three levels of stereotype activation above\n-Pass - A binary variable showing whether G3 is a passing grade (i.e. >=10) or not\n-PassFailStrategy - A binary variable showing whether the PredictedGrade is a passing grade (i.e. >=10) or not\nBelow is a summary of the variables.\n\n\nCode\nsummary(performance)\n\n\n     index       ParticipantID     name               sex           \n Min.   :  1.0   Min.   :  1   Length:856         Length:856        \n 1st Qu.:241.8   1st Qu.: 27   Class :character   Class :character  \n Median :487.0   Median : 54   Mode  :character   Mode  :character  \n Mean   :486.9   Mean   : 54                                        \n 3rd Qu.:727.2   3rd Qu.: 81                                        \n Max.   :990.0   Max.   :107                                        \n   studytime        freetime       romantic              Walc      \n Min.   :1.000   Min.   :1.000   Length:856         Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   Class :character   1st Qu.:1.000  \n Median :2.000   Median :2.000   Mode  :character   Median :2.000  \n Mean   :1.887   Mean   :2.183                      Mean   :2.284  \n 3rd Qu.:2.000   3rd Qu.:3.000                      3rd Qu.:3.000  \n Max.   :3.000   Max.   :3.000                      Max.   :4.000  \n     goout        Parents_edu       absences        reason         \n Min.   :1.000   Min.   :1.000   Min.   :0.000   Length:856        \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:0.000   Class :character  \n Median :3.000   Median :3.000   Median :2.000   Mode  :character  \n Mean   :3.022   Mean   :2.854   Mean   :2.794                     \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000                     \n Max.   :4.000   Max.   :4.000   Max.   :7.000                     \n       G3           Pass         PredictedGrade  PredictedRank \n Min.   : 1.00   Mode :logical   Min.   : 0.00   Min.   :1.00  \n 1st Qu.:10.00   FALSE:149       1st Qu.:10.00   1st Qu.:2.75  \n Median :12.00   TRUE :707       Median :14.00   Median :4.50  \n Mean   :11.97                   Mean   :13.14   Mean   :4.50  \n 3rd Qu.:14.00                   3rd Qu.:16.00   3rd Qu.:6.25  \n Max.   :20.00                   Max.   :20.00   Max.   :8.00  \n StereotypeActivation Predicted_Pass_PassFailStrategy\n Length:856           Mode :logical                  \n Class :character     FALSE:170                      \n Mode  :character     TRUE :686                      \n                                                     \n                                                     \n                                                     \n Predicted_Pass_RankingStrategy\n Mode :logical                 \n FALSE:284                     \n TRUE :572                     \n                               \n                               \n                               \n\n\nThe next two graphs compare the actual final grades vs. the predicted grade for both genders.\n\n\nCode\nggplot(performance, aes(x=G3, fill=sex))+ geom_histogram(position = \"dodge\")+labs(x=\"Students Actual Final Grade (G3)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(performance, aes(x=PredictedGrade, fill=sex))+ geom_histogram(position = \"dodge\")+labs(x=\"Students Predicted Final Grade (PredictedGrade)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nIn the first graph, there is a fairly normal distribution of actual grades for both male and female students. Male students are more concentrated near the median value of 10 while female students are skewed slightly higher near the 11 value. However, in the second graph of predicted grades for both groups of students, predicted values are skewed higher than actual. Grade predictions for female students are significantly higher than actual as well as predictions for male students.\n\n\nCode\nby_sex<-performance%>% group_by(sex)\nby_sex%>% summarise(mean(studytime))\n\n\n# A tibble: 2 × 2\n  sex   `mean(studytime)`\n  <chr>             <dbl>\n1 F                  2.07\n2 M                  1.71\n\n\nCode\nby_sex%>% summarise(mean(freetime))\n\n\n# A tibble: 2 × 2\n  sex   `mean(freetime)`\n  <chr>            <dbl>\n1 F                 2.07\n2 M                 2.30\n\n\nCode\nby_sex%>% summarise(mean(goout))\n\n\n# A tibble: 2 × 2\n  sex   `mean(goout)`\n  <chr>         <dbl>\n1 F              2.99\n2 M              3.05\n\n\nCode\nby_sex%>% summarise(mean(absences))\n\n\n# A tibble: 2 × 2\n  sex   `mean(absences)`\n  <chr>            <dbl>\n1 F                 2.70\n2 M                 2.89\n\n\nCode\nby_sex%>% summarise(mean(G3))\n\n\n# A tibble: 2 × 2\n  sex   `mean(G3)`\n  <chr>      <dbl>\n1 F           12.1\n2 M           11.8\n\n\nCode\nby_sex%>% summarise(mean(PredictedGrade))\n\n\n# A tibble: 2 × 2\n  sex   `mean(PredictedGrade)`\n  <chr>                  <dbl>\n1 F                       14.0\n2 M                       12.3\n\n\nThe above tables summarize differences in the mean values for several variables of interest by gender. Here we see, female students score slightly higher in study time (2.07 vs. 1.7) and while lower in free time (2.07 vs, 2.3), goout (3 vs. 3.05), absences (2.7 vs. 2.9). These would suggest female students may have a slightly higher grade based on good practices which is proven by the actual grades in G3 (12.15 vs. 11.80). However, the larger discrepancies in predicted rages (14.0 for females vs. 12.28 for males) may be attributed to the stereotype activation.\nAdditionally we can perform a t-test on both male and female actual and predicted score.\n\n\nCode\n#create two tables for female students and male students\nf_performance<-performance%>%\nfilter(sex==\"F\") \n\nm_performance<-performance%>%\nfilter(sex==\"M\") \n\nt.test(f_performance$G3)\n\n\n\n    One Sample t-test\n\ndata:  f_performance$G3\nt = 87.488, df = 427, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.87658 12.42249\nsample estimates:\nmean of x \n 12.14953 \n\n\nCode\nt.test(m_performance$G3)\n\n\n\n    One Sample t-test\n\ndata:  m_performance$G3\nt = 83.481, df = 427, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.51670 12.07209\nsample estimates:\nmean of x \n 11.79439 \n\n\nCode\nt.test(f_performance$PredictedGrade)\n\n\n\n    One Sample t-test\n\ndata:  f_performance$PredictedGrade\nt = 77.636, df = 427, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 13.66150 14.37121\nsample estimates:\nmean of x \n 14.01636 \n\n\nCode\nt.test(m_performance$PredictedGrade)\n\n\n\n    One Sample t-test\n\ndata:  m_performance$PredictedGrade\nt = 61.908, df = 427, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.88369 12.66304\nsample estimates:\nmean of x \n 12.27336 \n\n\nBased on the above tests, we can say with a 95% confidence that the following groups have the following means:\nActual female grade - 12.14953\nPredicted female grade - 14.01636\nActual male grade - 11.79439\nPredicted male grade - 12.27336\nThe graph below calculates the mean predicted grade for both groups based on the type of stereotype activation.\n\n\nCode\nperformance%>%\ngroup_by(sex, StereotypeActivation)%>%\nsummarise(mean(PredictedGrade))\n\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 6 × 3\n# Groups:   sex [2]\n  sex   StereotypeActivation `mean(PredictedGrade)`\n  <chr> <chr>                                 <dbl>\n1 F     CaseBased                              13.4\n2 F     None                                   14.1\n3 F     Statistics                             14.5\n4 M     CaseBased                              12.0\n5 M     None                                   12.7\n6 M     Statistics                             12.2\n\n\nIn the cross tabulation above, participants predicted the grade of female students 1 - 2.35 points higher than the average actual female performance (12.15). For male students, predictions were closer to the average actual performance (11.80) ranging at a difference of 0.16 - 0.88 points. With no stereotype activation, males students garnered their highest predicted average at 12.68 suggesting introducing stereotypes negatively affected predictions in male students although still higher than actual performance. For female students, introducing statistics of lower male performance garnered the highest grade prediction for female students. For both genders, case based stereotype activation which included one example of a high female score and two examples of lower male scores produced the lowest predicted grades.\nThis may suggest that other variables in the study having an influential role in predicting scores outside of stereotype activation."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2.html#methodology-and-results",
    "href": "posts/KalimahMuhammad_finalpart2.html#methodology-and-results",
    "title": "Final Part 2",
    "section": "Methodology and Results",
    "text": "Methodology and Results\nTo test my hypotheses, I will examine if the predicted grade was higher or lower than the actual grade among the genders controlling for various types of stereotype activation. Here the response variable is the predicted grade, and the explanatory variables are the types of gender as controlled by the exposure of the types of stereotype activation.\n\nDoes exposure to negative stereotypes about male achievement and positive stereotypes of female achievement result in higher predicted achievement among both male and female students?\nBased on the cross tabulation above, exposure to negative statistics of male performance, resulted in the highest prediction for female grades even more than no stereotype exposure. However, those same statistics resulted in lower prediction of male grades than no stereotype exposure.\nIs there a statistically significant difference between male and female achievement based on the type of stereotype activation within each group?\n\n\n\nCode\nsummary(lm(PredictedGrade ~ StereotypeActivation, data=f_performance))\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation, data = f_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.404  -2.500   0.500   2.939   6.596 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     13.4044     0.3186  42.068   <2e-16 ***\nStereotypeActivationNone         0.6562     0.4540   1.445   0.1491    \nStereotypeActivationStatistics   1.0956     0.4334   2.528   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.716 on 425 degrees of freedom\nMultiple R-squared:  0.01488,   Adjusted R-squared:  0.01024 \nF-statistic: 3.209 on 2 and 425 DF,  p-value: 0.04138\n\n\nCode\nsummary(lm(PredictedGrade ~ StereotypeActivation, data=m_performance))\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation, data = m_performance)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2000 -3.2000  0.0368  3.0368  8.0368 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     11.9632     0.3516  34.021   <2e-16 ***\nStereotypeActivationNone         0.7186     0.5010   1.434    0.152    \nStereotypeActivationStatistics   0.2368     0.4783   0.495    0.621    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.101 on 425 degrees of freedom\nMultiple R-squared:  0.005007,  Adjusted R-squared:  0.0003245 \nF-statistic: 1.069 on 2 and 425 DF,  p-value: 0.3442\n\n\nHere, there appears that activating stereotype statistics resulted in a statistically significant difference in prediction of female grades. For predictions in male students, case based stereotypes resulted in the lowest predictions.\n\n\nCode\nperformance_fit<-lm(PredictedGrade ~ StereotypeActivation*sex, data=performance)\nperformance_fit\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation * sex, data = performance)\n\nCoefficients:\n                        (Intercept)             StereotypeActivationNone  \n                           13.40441                              0.65619  \n     StereotypeActivationStatistics                                 sexM  \n                            1.09559                             -1.44118  \n      StereotypeActivationNone:sexM  StereotypeActivationStatistics:sexM  \n                            0.06239                             -0.85882  \n\n\nCode\nsummary(performance_fit) #fit based on gender, predicted grade, and stereotype activation\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation * sex, data = performance)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.4044  -2.9632   0.4091   3.0368   8.0368 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         13.40441    0.33554  39.948  < 2e-16 ***\nStereotypeActivationNone             0.65619    0.47811   1.372  0.17028    \nStereotypeActivationStatistics       1.09559    0.45639   2.401  0.01658 *  \nsexM                                -1.44118    0.47453  -3.037  0.00246 ** \nStereotypeActivationNone:sexM        0.06239    0.67615   0.092  0.92651    \nStereotypeActivationStatistics:sexM -0.85882    0.64543  -1.331  0.18367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.913 on 850 degrees of freedom\nMultiple R-squared:  0.05618,   Adjusted R-squared:  0.05063 \nF-statistic: 10.12 on 5 and 850 DF,  p-value: 1.989e-09\n\n\nWhen adding gender as an interacting variables, r-squared and the adjusted r-squared increased and we find a statistically significant result for male grades predicted after exposure to case based stereotypes.\n\nSources\nBedyńska, S., Krejtz, I. & Sedek, G. Chronic stereotype threat and mathematical achievement in age cohorts of secondary school girls: mediational role of working memory, and intellectual helplessness. Soc Psychol Educ 22, 321–335 (2019). https://doi.org/10.1007/s11218-019-09478-6\nBedyńska, S., Krejtz, I., Rycielski, P. et al. Stereotype threat as an antecedent to domain identification and achievement in language arts in boys: a cross-sectional study. Soc Psychol Educ 23, 755–771 (2020). https://doi.org/10.1007/s11218-020-09557-z\nBooth MZ, Gerard JM. Self-esteem and academic achievement: a comparative study of adolescent students in England and the United States. Compare. 2011 Sep;41(5):629-648. doi: 10.1080/03057925.2011.566688\nCortez, P. and Silva, A. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7\nGanley, C. M., Mingle, L. A., Ryan, A. M., Ryan, K., Vasilyeva, M., & Perry, M. (2013). An examination of stereotype threat effects on girls’ mathematics performance. Developmental Psychology, 49(10), 1886–1897. https://doi.org/10.1037/a0031412\nHyde, Janet Shibley. “Meta-analysis and the psychology of gender differences.” Signs: Journal of Women in Culture and Society 16.1 (1990): 55-73.\nJohnson, H., Barnard-Brak, L., Saxon, T., & Johnson, M.K. (2012) An Experimental Study of the Effects of Stereotype Threat and Stereotype Lift on Men and Women’s Performance in Mathematics, The Journal of Experimental Education, 80:2, 137-149, DOI: 10.1080/00220973.2011.567312\nMoè, A., & Putwain, D. W. (2020). An evaluative message fosters mathematics performance in male students but decreases intrinsic motivation in female students. Educational Psychology, 1–20. https://doi.org/10.1080/01443410.2020.1730767\nSpinath, B., Harald Freudenthaler, H., & Neubauer, A. C. (2010). Domain-specific school achievement in boys and girls as predicted by intelligence, personality and motivation. Personality and Individual Differences, 48(4), 481-486. https://doi.org/10.1016/j.paid.2009.11.028\nSteele, Claude M., and Joshua Aronson. “Stereotype threat and the intellectual test performance of African Americans.” Journal of personality and social psychology 69.5 (1995): 797.\nWarne, R. T. (2022). No Strong Evidence of Stereotype Threat in Females: A Reassessment of the Meta-Analysis. Journal of Advanced Academics, 33(2), 171–186. https://doi.org/10.1177/1932202X211061517\nData source: Anonymous. (2022). Performance vs. Predicted Performance [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4282405"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html",
    "title": "Final Pt.2: Updates",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nperformance<- read_csv(\"_data/CompleteDataAndBiases.csv\")\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#background",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#background",
    "title": "Final Pt.2: Updates",
    "section": "Background",
    "text": "Background\nResearchers have examined the differences impacting student achievement among genders for decades. Early meta-analyses conducted by Hyde (1990) revealed no significant difference between the groups’ cognitive abilities; yet intelligence and self-perception of ability, or self-esteem, were determined as the strongest predictors of academic success (Spinath et al., 2010, Booth and Gerard, 2011). Some research explains differences in achievement partially by stereotype threat (ST). Stereotype threat, originally used to research the vulnerability of racial stereotypes on aptitude tests, is defined as “being at risk of confirming, as self-characteristic, a negative stereotype about one’s group” (Steele and Aronson, 1995). Conventional research has focused on gender differences to stereotype threat (Moè & Putwain, 2020), and two prevailing theories have emerged.\nThe first theory suggests that ST affects male and female students differently. Here male students achieve better outcomes when stereotype threat is present than stereotype lift, and the reverse is true for female students who are adversely impacted by stereotype threat and positively impacted by stereotype lift (Johnson et al., 2012). Other studies debunk this theory suggesting no evidence of ST as a phenomenon in female students (Warne, 2022), particularly in the domain of mathematical achievement (Ganley et al., 2013). A second theory exists that the effects of ST may not be fully realized as outcomes immediately but affect achievement over time as chronic ST impacts working memory and intellectual helplessness in girls’ math scores (Bedyńska, Krejtz, and Sedek, 2019) and male language arts scores (Bedyńska, Krejtz, and Sedek, 2020)."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#research-questions",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#research-questions",
    "title": "Final Pt.2: Updates",
    "section": "Research Questions",
    "text": "Research Questions\nThis study tests both theories. First, it questions if there are differences in predicted student achievement based on gender when exposed to negative stereotypes about males and positive stereotypes about females. Second, it evaluates the effect of the perceived prevalence of stereotypes as shown by the participants’ exposure to three types of stereotype activation in predicting grades. The types of stereotype activation include:\n\nNone: Participants were not exposed to stereotypes during this experiment.\nCase-based: Participants were shown three student profiles in which one female student had a high grade, and two male students had low grades.\nStatistics: Participants were shown statistics suggesting boys performed less well in school than girls.\n\nThis study tests the following hypotheses:\nH1. Does exposure to negative stereotypes about male achievement (ST) and positive stereotypes of female achievement (SL) result in higher predicted achievement among both male and female students?\nH2. Was there a statistically significant difference between predicted male and female achievement based on the type of stereotype activation?"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#data",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#data",
    "title": "Final Pt.2: Updates",
    "section": "Data",
    "text": "Data\nData for this project was collected from Kaggle (Performance vs. Predicted Performance, 2022) and is a collection of existing student performance data from a study by Cortez and Silva (2008) on predicting secondary school student performance and the collector’s addition of stereotype activation for machine learning. The data included information on actual student demographics such as gender, parents’ highest level of education, as well as time studying vs. free time, number of absences, and grade on a final exam, among other variables.\nA few important variables to define include:\n-Index - number of students included in the original and predicted data (N=856 of the original 991 students)\n-Participant ID - number associated with participants making predictions (N=107)\n-Sex - student’s sex (binary: ‘F’ - female or ‘M’ - male)\n-Studytime - weekly study time is categorized as 1= less than 2 hours, 2= 2-5 hours and 3= 5+ hours\n-Freetime - free time after school group as 1 = low, 2= medium, 3 = high\n-Goout - how often a student goes out with friends where 1 is very low and 4 is very often\n-Absences - number of school absences 1 - 7 where 7 represents any absences equal to or above 7\n-Walc - weekend alcohol consumption\n-Parents_edu - the higher of original variables mother’s edu and father’s edu, where 4 = the highest level of education\n-G3 - final grade (numeric: from 0 to 20)\n-Reason - The reason for why a student chose to go to the school in question. The levels are close to home, school’s reputation, school’s curricular and other\n-PredictedGrade - the grade participants predicted based on actual data and their exposed level of StereotypeActivation\n-StereotypeActivation - see three levels of stereotype activation above\n-Pass - A binary variable showing whether G3 is a passing grade (i.e. >=10) or not\n-PassFailStrategy - A binary variable showing whether the PredictedGrade is a passing grade (i.e. >=10) or not\nAn additional variable I added to the data set, STPresent, identifies whether stereotype activation was present (True or False) for participants when predicting student scores.\nBelow is a summary of the variables.\n\n\nCode\n#view top 5 observations\nhead(performance, 10)\n\n\n# A tibble: 10 × 19\n   index Parti…¹ name  sex   study…² freet…³ roman…⁴  Walc goout Paren…⁵ absen…⁶\n   <dbl>   <dbl> <chr> <chr>   <dbl>   <dbl> <chr>   <dbl> <dbl>   <dbl>   <dbl>\n 1   132       1 Anna  F           1       2 no          1     2       4       0\n 2   724       1 Mich… M           1       1 no          4     4       4       1\n 3   637       1 David M           1       2 no          4     2       2       0\n 4   884       1 Brian M           1       1 no          4     4       3       7\n 5   194       1 Jenny F           2       2 no          1     4       2       0\n 6   388       1 Oliv… M           2       2 no          1     1       4       1\n 7    65       1 Lisa  F           2       3 no          2     3       4       1\n 8   303       1 Sarah F           3       3 yes         1     3       4       6\n 9   312       2 Oliv… M           3       2 yes         2     2       4       7\n10   305       2 Lisa  F           3       3 no          2     3       3       7\n# … with 8 more variables: reason <chr>, G3 <dbl>, Pass <lgl>,\n#   PredictedGrade <dbl>, PredictedRank <dbl>, StereotypeActivation <chr>,\n#   Predicted_Pass_PassFailStrategy <lgl>,\n#   Predicted_Pass_RankingStrategy <lgl>, and abbreviated variable names\n#   ¹​ParticipantID, ²​studytime, ³​freetime, ⁴​romantic, ⁵​Parents_edu, ⁶​absences\n\n\nCode\n#Add column that distinguishes if stereotype activation is present\nperformance <-performance%>%\n  mutate(STPresent = case_when(StereotypeActivation == \"CaseBased\"| StereotypeActivation == \"Statistics\" ~ TRUE, StereotypeActivation==\"None\" ~ FALSE))\n\nsummary(performance)\n\n\n     index       ParticipantID     name               sex           \n Min.   :  1.0   Min.   :  1   Length:856         Length:856        \n 1st Qu.:241.8   1st Qu.: 27   Class :character   Class :character  \n Median :487.0   Median : 54   Mode  :character   Mode  :character  \n Mean   :486.9   Mean   : 54                                        \n 3rd Qu.:727.2   3rd Qu.: 81                                        \n Max.   :990.0   Max.   :107                                        \n   studytime        freetime       romantic              Walc      \n Min.   :1.000   Min.   :1.000   Length:856         Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:2.000   Class :character   1st Qu.:1.000  \n Median :2.000   Median :2.000   Mode  :character   Median :2.000  \n Mean   :1.887   Mean   :2.183                      Mean   :2.284  \n 3rd Qu.:2.000   3rd Qu.:3.000                      3rd Qu.:3.000  \n Max.   :3.000   Max.   :3.000                      Max.   :4.000  \n     goout        Parents_edu       absences        reason         \n Min.   :1.000   Min.   :1.000   Min.   :0.000   Length:856        \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:0.000   Class :character  \n Median :3.000   Median :3.000   Median :2.000   Mode  :character  \n Mean   :3.022   Mean   :2.854   Mean   :2.794                     \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000                     \n Max.   :4.000   Max.   :4.000   Max.   :7.000                     \n       G3           Pass         PredictedGrade  PredictedRank \n Min.   : 1.00   Mode :logical   Min.   : 0.00   Min.   :1.00  \n 1st Qu.:10.00   FALSE:149       1st Qu.:10.00   1st Qu.:2.75  \n Median :12.00   TRUE :707       Median :14.00   Median :4.50  \n Mean   :11.97                   Mean   :13.14   Mean   :4.50  \n 3rd Qu.:14.00                   3rd Qu.:16.00   3rd Qu.:6.25  \n Max.   :20.00                   Max.   :20.00   Max.   :8.00  \n StereotypeActivation Predicted_Pass_PassFailStrategy\n Length:856           Mode :logical                  \n Class :character     FALSE:170                      \n Mode  :character     TRUE :686                      \n                                                     \n                                                     \n                                                     \n Predicted_Pass_RankingStrategy STPresent      \n Mode :logical                  Mode :logical  \n FALSE:284                      FALSE:264      \n TRUE :572                      TRUE :592      \n                                               \n                                               \n                                               \n\n\nCode\n#cross-tabulation of stereotype activation by gender \nxtabs(~sex + STPresent, performance)\n\n\n   STPresent\nsex FALSE TRUE\n  F   132  296\n  M   132  296\n\n\nThere is an even distribution of male and female students (428 each), with 69% of participants exposed to some type of stereotype activation during grade prediction. The actual and predicted rate of passing are similar, 707 students passed versus the predicted 686, approximately 83% to 80% respectively. This may suggest other variables as strong indicators in passing. There is, however, a notable difference in the average actual scores (11.97) and predicted scores (13.14). This is further emphasized by the median actual scores (12) and predicted median (14). This early finding suggest that although the rate of passing was slightly lower, the overall predicted scores are skewed higher than actual.\nThe next two graphs compare the actual final grade vs. the predicted grade for both genders.\n\n\nCode\nggplot(performance, aes(x=G3, fill=sex))+ geom_histogram(position = \"dodge\")+labs(x=\"Students Actual Final Grade (G3)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(performance, aes(x=PredictedGrade, fill=sex))+ geom_histogram(position = \"dodge\")+labs(x=\"Students Predicted Final Grade (PredictedGrade)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nIn the first graph, there is a fairly normal distribution of actual grades for both male and female students. Male students are more concentrated near the median value of 10 while female students are skewed slightly higher near the 11 value. However, in the second graph of predicted grades for both groups, female students are skewed significantly higher near 15 points while predictions for male students spreads near 12 points. This study will investigate if stereotype activation is a possible explanation for this variance.\n\n\nCode\n#boxplot of predicted grades by gender and if stereotype activation is present\nggplot(performance, aes(x=STPresent, y=PredictedGrade, fill=sex))+\n  geom_boxplot()+\n  theme(legend.position=\"none\") +\n  ggtitle(\"Predicted Grades by Gender and Presence of Stereotype Activation\")\n\n\n\n\n\nThe chart above shows more variability in predicted grades for female students without any stereotype activation; yet once present, the concentration of scores skews higher. For male students, predicted scores increased in the presence of stereotype activation accounting for more low scores. Note, male scores were overall predicted lower than females independent of stereotype activation. Is there another variable contributing to this difference?"
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#hypothesis-testing",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#hypothesis-testing",
    "title": "Final Pt.2: Updates",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nH1. Does exposure to negative stereotypes about male achievement (ST) and positive stereotypes of female achievement (SL) result in differences in predicted achievement among both male and female students?\nHere the dependent variable is the predicted grade and independent variable is presence of stereotype threat. The Null hypothesis states stereotype activation does not have a statistically significant effect on predicted grades for male and female students and the alternative hypothesis is negative stereotypes about male achievement and positive stereotypes of female achievement result in changes in predicted grades among both male and female students.\nI conducted a two directional, two sample t-test to investigate the differences in the mean predicted score for students with it labeled “TRUE” and without stereotype activation labeled “FALSE”.\n\n\nCode\nt.test(PredictedGrade ~ STPresent, data= performance, alternative = c(\"two.sided\"), var.equal = FALSE, conf.level = 0.95)\n\n\n\n    Welch Two Sample t-test\n\ndata:  PredictedGrade by STPresent\nt = 1.1314, df = 539.07, p-value = 0.2584\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -0.2409671  0.8955535\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           13.37121            13.04392 \n\n\nAlthough the mean predicted grades for those not exposed to ST are higher (13.37) compared to those where ST was present (13.04), the resulting p-value of 0.26, renders this test not significant in the predicted grades based on if stereotype activation was present. Thus, the null was retained.\nThe next test isolates predicted grades for male students only.\n\n\nCode\n#create two tables for female students and male students\n\nm_performance<-performance%>%\n  filter(sex==\"M\")\n\n#t.test for difference in mean of predicted grades for males with and without ST present\n#null hypothesis - mean of predicted grades is equals the mean of actual grades\nt.test(PredictedGrade ~ STPresent, data= m_performance, alternative = c(\"two.sided\"), conf.level = 0.95)\n\n\n\n    Welch Two Sample t-test\n\ndata:  PredictedGrade by STPresent\nt = 1.4747, df = 297.81, p-value = 0.1414\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -0.1975596  1.3787636\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           12.68182            12.09122 \n\n\nThe following test isolates predicted grades for female students only.\n\n\nCode\n#create two tables for female students and male students\n\nm_performance<-performance%>%\n  filter(sex==\"F\")\n\n#t.test for difference in mean of predicted grades for males with and without ST present\n#null hypothesis - mean of predicted grades is equals the mean of actual grades\nt.test(PredictedGrade ~ STPresent, data= f_performance, alternative = c(\"two.sided\"), conf.level = 0.95)\n\n\nError in eval(m$data, parent.frame()): object 'f_performance' not found\n\n\nBoth tests have a p-value higher than 0.05 denoting that neither gender experienced a statistically significant difference in predicted average grades based on exposure to stereotype activation or not. Further rejecting hypothesis one.\n\n\nH2. Was there a statistically significant difference between predicted male and female achievement based on the type of stereotype activation?\nThe graph below calculates the mean predicted grade for both groups based on the type of stereotype activation.\n\n\nCode\nperformance%>%\ngroup_by(sex, StereotypeActivation)%>%\nsummarise(mean(PredictedGrade), mean(G3), mean(PredictedGrade)-mean(G3))\n\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 6 × 5\n# Groups:   sex [2]\n  sex   StereotypeActivation `mean(PredictedGrade)` `mean(G3)` mean(PredictedG…¹\n  <chr> <chr>                                 <dbl>      <dbl>             <dbl>\n1 F     CaseBased                              13.4       12.2             1.22 \n2 F     None                                   14.1       12.3             1.80 \n3 F     Statistics                             14.5       12.0             2.47 \n4 M     CaseBased                              12.0       11.6             0.353\n5 M     None                                   12.7       11.8             0.932\n6 M     Statistics                             12.2       12.0             0.212\n# … with abbreviated variable name ¹​`mean(PredictedGrade) - mean(G3)`\n\n\nIn the summary above, participants ranked the average predicted scores higher overall than the actual scores irrespective of gender or stereotype activation. The difference in predicted versus actual score was greatest among female students. Here participants exposed to the stereotype statistics predicted the widest difference compared to actual scores (2.47) and the highest overall scores (14.5).\nFor male students, predictions were closer to the average actual performance (11.80) ranging at a difference of 0.16 - 0.88 points. With no stereotype activation, males students garnered their highest predicted average at 12.68 suggesting introducing stereotypes negatively affected predictions in male students although still higher than actual performance. For female students, introducing statistics of lower male performance than females garnered the highest grade prediction for female students. For both genders, case based stereotype activation which included one example of a high female score and two examples of lower male scores produced the lowest predicted grades.\n\n\nCode\npairwise.t.test(x= performance$PredictedGrade, g=performance$StereotypeActivation, p.adjust.method = \"none\")\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  performance$PredictedGrade and performance$StereotypeActivation \n\n           CaseBased None \nNone       0.047     -    \nStatistics 0.044     0.949\n\nP value adjustment method: none \n\n\nThis may suggest that other variables in the study have an influential role in predicting scores outside of stereotype activation."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#model-comparison",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#model-comparison",
    "title": "Final Pt.2: Updates",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nModel 1 - Predicted Grades and Presence of Stereotype Activation\nIn the first model, I will use PredictedGrade as the outcome or dependent variable and the STPresent variable as the explanatory or independent variable adding in student sex as an interaction term.\n\n\nCode\nmodel1<- (lm(PredictedGrade ~ STPresent * sex, data=performance))\nsummary(model1)\n\n\n\nCall:\nlm(formula = PredictedGrade ~ STPresent * sex, data = performance)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.9966  -3.0126   0.0034   3.0034   7.9088 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        14.06061    0.34139  41.186   <2e-16 ***\nSTPresentTRUE      -0.06398    0.41052  -0.156   0.8762    \nsexM               -1.37879    0.48280  -2.856   0.0044 ** \nSTPresentTRUE:sexM -0.52662    0.58056  -0.907   0.3646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.922 on 852 degrees of freedom\nMultiple R-squared:  0.04948,   Adjusted R-squared:  0.04613 \nF-statistic: 14.78 on 3 and 852 DF,  p-value: 2.165e-09\n\n\nHere we find in the absence of stereotype activation, gender is a statistically significant predictor of grades as represented by p-values under 0.05 and higher p-values when ST is present. However, the overall model of presence of stereotype activation and gender are not significant in predicting grades with a p-value of 2.17. This model confirms earlier hypothesis testing results that mean predicted grades based on ST is not as significant but gender may play a role.\n\n\nModel 2 and 3 - Predicted Grades by Stereotype Activation and Gender\nThe next two models address the second hypothesis and distinguishes the stereotype activation by type, adding the sex variable as both a second explanatory variable (model 2) and an interaction term (model 3).\n\n\nCode\n#model2 uses \"sex\" as explanatory variable\nmodel2<-(lm(PredictedGrade ~ StereotypeActivation + sex, data=performance))\nsummary(model2)\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation + sex, data = performance)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5553  -2.8123   0.4447   3.1877   8.1877 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                     13.5553     0.2725  49.752  < 2e-16 ***\nStereotypeActivationNone         0.6874     0.3382   2.033   0.0424 *  \nStereotypeActivationStatistics   0.6662     0.3228   2.064   0.0394 *  \nsexM                            -1.7430     0.2676  -6.514 1.25e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.914 on 852 degrees of freedom\nMultiple R-squared:  0.0533,    Adjusted R-squared:  0.04996 \nF-statistic: 15.99 on 3 and 852 DF,  p-value: 4.046e-10\n\n\nCode\n#model3 uses \"sex\" as interaction terms\nmodel3<- lm(PredictedGrade ~ StereotypeActivation * sex, data=performance)\nsummary(model3)\n\n\n\nCall:\nlm(formula = PredictedGrade ~ StereotypeActivation * sex, data = performance)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.4044  -2.9632   0.4091   3.0368   8.0368 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                         13.40441    0.33554  39.948  < 2e-16 ***\nStereotypeActivationNone             0.65619    0.47811   1.372  0.17028    \nStereotypeActivationStatistics       1.09559    0.45639   2.401  0.01658 *  \nsexM                                -1.44118    0.47453  -3.037  0.00246 ** \nStereotypeActivationNone:sexM        0.06239    0.67615   0.092  0.92651    \nStereotypeActivationStatistics:sexM -0.85882    0.64543  -1.331  0.18367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.913 on 850 degrees of freedom\nMultiple R-squared:  0.05618,   Adjusted R-squared:  0.05063 \nF-statistic: 10.12 on 5 and 850 DF,  p-value: 1.989e-09\n\n\nCode\n#ANOVA of both models\nanova(model2, model3)\n\n\nAnalysis of Variance Table\n\nModel 1: PredictedGrade ~ StereotypeActivation + sex\nModel 2: PredictedGrade ~ StereotypeActivation * sex\n  Res.Df   RSS Df Sum of Sq      F Pr(>F)\n1    852 13055                           \n2    850 13015  2    39.769 1.2986 0.2734\n\n\nIn both models, there is a significance to introducing case based ST for both genders, negatively for male students and positively for female students. The use of statistics garnered less significance for both genders, however still significant for predicting mean grades for female students. Once again the highest predicted scores for males were under conditions when ST was not present.\nComparing the two models, results are similar and there is minimal increase in the R-squared (0.053 to 0.056) and adjusted R-squared (0.050 to 0.051) for using sex as an explanatory variable vs. an interaction term. Residuals are also lower in model 3 than for model 2. The analysis of variance for the two models also confirms this result with lower RSS for model 3, minimizing residuals in the model. Thus model 3 is a better fit than model two.\n\n\nModel 4 - Predicted Grades by Other Potential Explanatory Variables\nUntil now, I investigated stereotype activation as the primary explanatory variable in predicting grades for both genders. However, in both my hypothesis testing and previous models, this variable was not significant in comparing scores with or without the presence of stereotype activation. This next model explores which of the other variables could explain this phenomena including gender as an interaction term.\nVariables selected for exploration include:\n-Studytime - weekly study time is categorized as 1= less than 2 hours, 2= 2-5 hours and 3= 5+ hours\n-Freetime - free time after school group as 1 = low, 2= medium, 3 = high\n-Goout - how often a student goes out with friends where 1 is very low and 4 is very often\n-Absences - number of school absences 1 - 7 where 7 represents any absences equal to or above 7\n-Walc - weekend alcohol consumption\n-Parents_edu - the higher of original variables mother’s edu and father’s edu, where 4 = the highest level of education\n\n\nCode\nperformance%>%\ngroup_by(sex)%>%\nsummarise(mean(studytime), mean(freetime), mean(goout), mean(absences),mean(G3), mean(PredictedGrade))\n\n\n# A tibble: 2 × 7\n  sex   `mean(studytime)` `mean(freetime)` `mean(goout)` mean(…¹ mean(…² mean(…³\n  <chr>             <dbl>            <dbl>         <dbl>   <dbl>   <dbl>   <dbl>\n1 F                  2.07             2.07          2.99    2.70    12.1    14.0\n2 M                  1.71             2.30          3.05    2.89    11.8    12.3\n# … with abbreviated variable names ¹​`mean(absences)`, ²​`mean(G3)`,\n#   ³​`mean(PredictedGrade)`\n\n\nThe above tables summarize differences in the mean values for several variables of interest by gender. Here we see, female students score slightly higher in study time (2.07 vs. 1.7) and while lower in free time (2.07 vs, 2.3), goout (3 vs. 3.05), absences (2.7 vs. 2.9). These would suggest female students may have a higher predicted grade based on good practices which is proven by the actual grades in G3 (12.15 vs. 11.80). However, there is larger discrepancies in predicted rages (14.0 for females vs. 12.28 for males).\n\n\nCode\n#model other variables as possible explanatory variables \nmodel4 <-lm(PredictedGrade ~ (studytime + freetime + Walc + goout + Parents_edu + absences) * sex, data=performance)\nsummary(model4)\n\n\n\nCall:\nlm(formula = PredictedGrade ~ (studytime + freetime + Walc + \n    goout + Parents_edu + absences) * sex, data = performance)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.1916  -2.2127   0.2298   2.2908   9.5489 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      12.20702    0.90896  13.430  < 2e-16 ***\nstudytime         1.70283    0.23838   7.143 1.97e-12 ***\nfreetime          0.32324    0.22396   1.443 0.149303    \nWalc             -0.57563    0.16305  -3.530 0.000437 ***\ngoout            -0.43723    0.18408  -2.375 0.017763 *  \nParents_edu       0.40536    0.15127   2.680 0.007512 ** \nabsences         -0.38691    0.05807  -6.663 4.83e-11 ***\nsexM             -1.02714    1.30371  -0.788 0.431000    \nstudytime:sexM   -0.25917    0.33611  -0.771 0.440867    \nfreetime:sexM    -0.33562    0.30899  -1.086 0.277716    \nWalc:sexM        -0.13419    0.22607  -0.594 0.552946    \ngoout:sexM        0.36529    0.27901   1.309 0.190810    \nParents_edu:sexM  0.29052    0.22058   1.317 0.188168    \nabsences:sexM    -0.07768    0.08359  -0.929 0.352963    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.279 on 842 degrees of freedom\nMultiple R-squared:  0.3435,    Adjusted R-squared:  0.3334 \nF-statistic:  33.9 on 13 and 842 DF,  p-value: < 2.2e-16\n\n\nCode\nanova(model3, model4)\n\n\nAnalysis of Variance Table\n\nModel 1: PredictedGrade ~ StereotypeActivation * sex\nModel 2: PredictedGrade ~ (studytime + freetime + Walc + goout + Parents_edu + \n    absences) * sex\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n1    850 13015.3                                  \n2    842  9052.6  8    3962.7 46.073 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#create matrix of model plots\npar(mfrow = c(2,3)); plot(model4, which = 1:6)\n\n\n\n\n\nIn this final model, we find similar residuals to model 3 but a large increase in the R-squared (0.057 to 0.34), adjusted r-squared values (0.051 to 0.33) and F-statistic (10.12 on 5 to 33.9 on 13). Based on the p-values, we also note study time, weekly alcohol consumption, parents education, and number of absences as most significant in predicting mean grades. Female students scored slighly more favorably in each of these areas than male students accounting a slight increase in actual grades as well. Thus perhaps participants over-estimated these values in predicting grades.\nComparing model 4 to model 3 using analysis of variance, there is a significant reduction in RSS and increase in p-value significance suggesting the introduction of these behavioral variables as a better fit for predicting grades than the introduction of stereotype activation. Note that in diagnosis the model 4 there are issues in normality (see the Cook’s distance and Residuals vs. Leverage plots) as well as heteroskedasticity (see the Scale-Location plot). Since further evaluation of these variables are outside of the scope of this project, I will select model 3 for diagnostics."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#diagnostics",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#diagnostics",
    "title": "Final Pt.2: Updates",
    "section": "Diagnostics",
    "text": "Diagnostics\nFor this section, I will evaluate a few assumptions with model 3 which aimed to predict grades by the type of stereotype activation and the sex variable as an interaction term.\n\n\nCode\n#create matrix of model plots\npar(mfrow = c(2,3)); plot(model3, which = 1:6)\n\n\n\n\n\nNormality of Errors - In the Normal Q-Q plot, most observations fall close to the line assuming normality. Using the Cook’s Distance and Residuals vs. Leverage, we can assume residuals are mostly normal as well with the exception of observation #62 whose leverage makes it an influential observation.\nLinearity - Viewing the Residuals vs. Fitted plot, we see a relatively straight and horizontal line suggesting the average residual for the fitted values are relatively similar. Note, there are three observations, #62, #348, and #742 that are identified as outliers.\nEqual Variance of Errors - Although the Residuals vs. Fitted plot show a near equal variance of the error terms based on its straight and horizontal line, the Scale-Location plot shows a decreasing rather than flat trend as the fitted values increase."
  },
  {
    "objectID": "posts/KalimahMuhammad_finalpart2_v2.html#summary",
    "href": "posts/KalimahMuhammad_finalpart2_v2.html#summary",
    "title": "Final Pt.2: Updates",
    "section": "Summary",
    "text": "Summary\nIn summary, stereotype activation has some influence in predicting higher scores for certain groups under certain conditions but was not a statistically significant predictor in grades for either male or female students based on this experiment. In this study, stereotypes favored female students over male students. Where this stereotype was present for both case based and statistic stereotypes, there was an increase in female predicted scores versus when ST was absent. Predictions for male grades also benefited from a higher predicted score when ST was absent. This does suggest there is some bias introduced with stereotype activation but not enough to be statistically significant in predicting grades.\nA more robust model for predicting grades (model 4) incorporated student behavioral factors such as study time, weekly alcohol consumption, and number of school absences as well demographic factors such as parents education. Future research should further investigate these variables as well as test the inverse of the stereotype which would favor male students. Overall, participants generally predicted grades higher for both genders compared to actual but the average predicted scores were not statistically significant based on stereotype activation.\n\nSources\nBedyńska, S., Krejtz, I. & Sedek, G. Chronic stereotype threat and mathematical achievement in age cohorts of secondary school girls: mediational role of working memory, and intellectual helplessness. Soc Psychol Educ 22, 321–335 (2019). https://doi.org/10.1007/s11218-019-09478-6\nBedyńska, S., Krejtz, I., Rycielski, P. et al. Stereotype threat as an antecedent to domain identification and achievement in language arts in boys: a cross-sectional study. Soc Psychol Educ 23, 755–771 (2020). https://doi.org/10.1007/s11218-020-09557-z\nBooth MZ, Gerard JM. Self-esteem and academic achievement: a comparative study of adolescent students in England and the United States. Compare. 2011 Sep;41(5):629-648. doi: 10.1080/03057925.2011.566688\nCortez, P. and Silva, A. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7\nGanley, C. M., Mingle, L. A., Ryan, A. M., Ryan, K., Vasilyeva, M., & Perry, M. (2013). An examination of stereotype threat effects on girls’ mathematics performance. Developmental Psychology, 49(10), 1886–1897. https://doi.org/10.1037/a0031412\nHyde, Janet Shibley. “Meta-analysis and the psychology of gender differences.” Signs: Journal of Women in Culture and Society 16.1 (1990): 55-73.\nJohnson, H., Barnard-Brak, L., Saxon, T., & Johnson, M.K. (2012) An Experimental Study of the Effects of Stereotype Threat and Stereotype Lift on Men and Women’s Performance in Mathematics, The Journal of Experimental Education, 80:2, 137-149, DOI: 10.1080/00220973.2011.567312\nMoè, A., & Putwain, D. W. (2020). An evaluative message fosters mathematics performance in male students but decreases intrinsic motivation in female students. Educational Psychology, 1–20. https://doi.org/10.1080/01443410.2020.1730767\nSpinath, B., Harald Freudenthaler, H., & Neubauer, A. C. (2010). Domain-specific school achievement in boys and girls as predicted by intelligence, personality and motivation. Personality and Individual Differences, 48(4), 481-486. https://doi.org/10.1016/j.paid.2009.11.028\nSteele, Claude M., and Joshua Aronson. “Stereotype threat and the intellectual test performance of African Americans.” Journal of personality and social psychology 69.5 (1995): 797.\nWarne, R. T. (2022). No Strong Evidence of Stereotype Threat in Females: A Reassessment of the Meta-Analysis. Journal of Advanced Academics, 33(2), 171–186. https://doi.org/10.1177/1932202X211061517\nData source: Anonymous. (2022). Performance vs. Predicted Performance[Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/4282405"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html",
    "href": "posts/KalimahMuhammad_hw1.html",
    "title": "Resubmission: Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot2)\nlungcap<- read_excel(\"_data/LungCapData.xls\")\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "href": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "title": "Resubmission: Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n\n\n1b. Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Gender `mean(LungCap)`\n  <chr>            <dbl>\n1 female            7.41\n2 male              8.31\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Lung capacity for those age 13 and under\nlungcap %>%\nfilter(Age <= 13)%>%\ngroup_by(Smoke)%>%\nsummarise(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\nCode\n#Lung capacity for those between the age of 14 to 15\nlungcap%>%\nfilter(Age== 14 | Age ==15)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               9.14\n2 yes              8.39\n\n\nCode\n#Lung capacity for those between the age of 16 to 17\nlungcap%>%\nfilter(Age==16 |Age==16)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no              10.1 \n2 yes              8.90\n\n\nCode\n#Lung capacity for those 18 and older\nlungcap%>%\nfilter(Age>=18)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               11.1\n2 yes              10.5\n\n\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nWith the exception of those age 13 years old and under, all non-smokers had a greater lung capacity than smokers. For those over the age of 18, the difference of the average in lung capacity for non-smokers to smokers was 0.55. For 16-17 year olds, the difference was the greatest at 1.16. The difference for 14-15 year olds was 0.74 and for those 13 years old and under, the difference was -0.843.\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\nHere the average lung capacity for non-smokers is higher than for smokers. This differs from the results of question 1c. Overall, we see lung capacity increase with age irrespective of smoking so this may contribute to the change in results.\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "href": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "title": "Resubmission: Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\npriors<- c(0, 1, 2, 3, 4)\nfrequency<- c(128, 434, 160, 64, 24)\nprison <-data.frame(priors,frequency)\nView(prison)\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20%\n2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69%\n2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89%\n2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11%\n2e. What is the expected value for the number of prior convictions?\n\n\nCode\n((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/sum(frequency)\n\n\n[1] 1.28642\n\n\n2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison$priors)*((810-1)/810)#calculate population variance\n\n\n[1] 2.496914\n\n\nCode\nsd(prison$priors)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html",
    "href": "posts/KalimahMuhammad_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html#questions",
    "href": "posts/KalimahMuhammad_hw2.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\n1.Cardiac Care Network - Wait Times for Cardiac Surgeries\nPrompt: The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\n\nBypass Surgery Confidence Interval\n\n\nCode\n#calculate confidence interval for bypass surgery\nmean<- 19 #mean wait time\nsd<-10 #standard deviation\nn <-539 #sample size\nbypass_se <- (sd/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score<- qt(p=1-tail_area, df=n-1) #determine t-score\nbypass_CI <- c(mean - t_score* bypass_se,\n               mean + t_score* bypass_se) #calculate confidence interval\nprint(bypass_CI)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval (CI) for the average wait time for bypass surgery is between 18.29 and 19.71 days.\n\n\nAngiography Confidence Interval\n\n\nCode\n#Calculate cofidence interval for angiography\n#mean= 18, sd=9, n=847\nmean_ag<- 18 #mean wait time\nsd_ag<-9 #standard deviation\nn_ag <-847 #sample size\nag_se <- (sd_ag/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score_ag<- qt(p=1-tail_area, df=n-1) #determine t-score\nag_CI <- c(mean_ag - t_score_ag* ag_se,\n               mean_ag + t_score_ag* ag_se) #calculate confidence interval\nprint(ag_CI)\n\n\n[1] 17.36126 18.63874\n\n\nMeanwhile, the CI for the angiography mean wait time is between 17.36 and 18.63 days.\n\n\nIs the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n19.70971-18.29029 #difference in bypass surgery CI range\n\n\n[1] 1.41942\n\n\nCode\n18.63874-17.36126 #difference in angiography CI range\n\n\n[1] 1.27748\n\n\nThe range in the confidence interval for angiography is 1.28 narrower than the bypass surgery, 1.42.\n\n\n\n\n2. National Center for Public Policy - Is college essential for success?\nPrompt: A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#proportion of US adults who believe college is essential for success\nprop.test(567,1031,conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate of the proportion of all adult Americans who believe that a college education is essential for success is 0.55. The confidence interval set at 95% ranges between 0.52 and 0.58.\n\n\n\n3. Student Sample Size\nPrompt: Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within 5 dollars of the true population mean (i.e. they want the confidence interval to have a length of 10 dollars or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between 30 dollars and 200 dollars. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculate the sample size\npop_sd<-(200-30)/4\ncritical_value<-1.96 #based off signigicance level of 5\nsample_size<- ((pop_sd*critical_value)/5)^2 \nprint(sample_size)\n\n\n[1] 277.5556\n\n\nThe sample size should be 278 students to estimate the mean cost of textbooks per semester.\n\n\n\n4. Income for Union Workers\nPrompt: According to a union agreement, the mean income for all senior-level workers in a large service company equals 500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nsam_mean<-410\nmu<-500\nsam_sd<-90\nn<-9\n\nt_score<- (sam_mean-mu)/(sam_sd/(sqrt(n)))\nprint(t_score)\n\n\n[1] -3\n\n\nCode\nupper_tail<- pt(t_score, df=n-1, lower.tail = FALSE)\nprint(upper_tail)\n\n\n[1] 0.9914642\n\n\nCode\nlower_tail<- pt(t_score, df=n-1, lower.tail = TRUE)\nprint(lower_tail)\n\n\n[1] 0.008535841\n\n\nCode\np_value<- upper_tail + lower_tail\nprint(p_value)\n\n\n[1] 1\n\n\n\n\n\n5. Jones and Smith\nPrompt: Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 with se = 10.0.\n\n\nCode\nmu<- 500 #hypothesized population mean\nj_mean<-519.5 #Jones's mean\ns_mean<-519.7 #Smith's mean\nn=1000 #sample size\nse<-10 #standard error\n\n\nShow that t = 1.95 and P-value = 0.051 for Jones.\n\n\nCode\n#calculate the t-score for Jones\nj_tscore<-(j_mean - mu)/se\nprint(j_tscore)\n\n\n[1] 1.95\n\n\nCode\n#calculate p-value for Jones\nj_pvalue<- pt(j_tscore, df=n-1, lower.tail = FALSE) *2\nprint(j_pvalue)\n\n\n[1] 0.05145555\n\n\nShow that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#calculate the t-score for Smith\ns_tscore<-(s_mean - mu)/se\nprint(s_tscore)\n\n\n[1] 1.97\n\n\nCode\n#calculate p-value for Smith\ns_pvalue<- pt(s_tscore, df=n-1, lower.tail = FALSE) *2\nprint(s_pvalue)\n\n\n[1] 0.04911426\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIn Smith’s test, the p-value of .049, less than 0.05, coupled with the significance level of 0.05 indicate a statistically significant result to reject the null hypothesis. However, the results from the Jones’s test with a p-value of 0.052, greater than 0.05, indicates the results were not statistically significant and the null was retained.\nTheses results can be misleading as the significance level impacts how the p-values are referenced when under 0.05. P-values over 0.05 will typically retain the null, however p-values under 0.05 are influenced by the significance level to determine whether results are statistically significant to reject the null hypothesis.\n\n\n\n6. US Gas Tax\nPrompt:Are the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#t-test of average gas taxes from sample cities\nt.test(gas_taxes, alternative = c(\"less\"), mu=45, conf.level = 0.95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes, using the t-test to compare the sample mean (40.86) to the hypothesized population mean (45) at the confidence level of 95% resulted in a favorable conclusion that the sample average was less than 45 cents."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html",
    "href": "posts/KalimahMuhammad_hw3.html",
    "title": "Homework #3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-1-united-nations",
    "href": "posts/KalimahMuhammad_hw3.html#question-1-united-nations",
    "title": "Homework #3",
    "section": "Question 1: United Nations",
    "text": "Question 1: United Nations\n\n\nCode\ndata(UN11) #load United Nations data\n\n\n\n1.1.1\nThe predictor variable is ppgdp (the gross national product per person in USD) and the response or outcome variable in fertility (the birth rate per 1000 females).\n\n\n1.1.2\n\n\nCode\nggplot(UN11, aes(x=ppgdp, y=fertility))+ geom_point()\n\n\n\n\n\nFertility has the most variability at zero gross national product per person (ppgdp) where countries range from birthrate of 1 to 7 per 1000 females. There is a sharp decline thereafter where the fertility rate is consistently 3 or under and hovers between slightly above 2 and 1 for countries with a ppgdp above 30,000.\n\n\nCode\nplot(x=UN11$ppgdp, y=UN11$fertility)\n\n\n\n\n\nBased on the plot above, there is little difference between the two plots.\n\n\n1.1.3.\n\n\nCode\nplot(x= log(UN11$ppgdp), y=UN11$fertility)\n\n\n\n\n\nUsing natural logarithms, the model seems more plausible as the data becomes normalized."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-2-annual-income",
    "href": "posts/KalimahMuhammad_hw3.html#question-2-annual-income",
    "title": "Homework #3",
    "section": "Question 2: Annual Income",
    "text": "Question 2: Annual Income\n\n2a.\nWhen the British pound is used instead of the dollar, the steep of the slop minimizes.\n\n\n2b.\nTHe correlation does not change when factoring in pound in lieu of the dollar."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-3-water-runoff-in-the-sierras",
    "href": "posts/KalimahMuhammad_hw3.html#question-3-water-runoff-in-the-sierras",
    "title": "Homework #3",
    "section": "Question 3: Water runoff in the Sierras",
    "text": "Question 3: Water runoff in the Sierras\nUsing the pairwise scatterplot, we find some pairs are better represented by a straight line than others. This seems to be common among mountain ranges with similar starting initials (i.e. of the ranges starting with “A” or those starting with “O”).In each case, those pairs have a clear positive slope. Examining the percipitation by year and mountain range shows a wide range of variability with a singular outlier for each mountain range.\n\n\nCode\ndata(water) #load water data\npairs(water) #plot pairs\n\n\n\n\n\n\n\nCode\nfit_water <- lm(Year~APMAM+ APSAB+ APSLAKE+ OPBPC+ OPRC+ OPSLAKE+ BSAAM, data=water) #create linear regression model based on year and mountain range\nsummary(fit_water) #summarize model\n\n\n\nCall:\nlm(formula = Year ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE + BSAAM, data = water)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.135  -7.762  -1.500   8.473  27.092 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.962e+03  7.979e+00 245.831   <2e-16 ***\nAPMAM       -1.256e+00  1.158e+00  -1.084    0.286    \nAPSAB       -7.327e-01  2.494e+00  -0.294    0.771    \nAPSLAKE      2.351e+00  2.276e+00   1.033    0.309    \nOPBPC       -7.900e-02  7.543e-01  -0.105    0.917    \nOPRC        -1.975e+00  1.170e+00  -1.687    0.100    \nOPSLAKE      5.219e-01  1.369e+00   0.381    0.705    \nBSAAM        3.353e-04  2.722e-04   1.232    0.226    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.34 on 35 degrees of freedom\nMultiple R-squared:  0.1949,    Adjusted R-squared:  0.03388 \nF-statistic:  1.21 on 7 and 35 DF,  p-value: 0.3231"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-4-professor-ratings",
    "href": "posts/KalimahMuhammad_hw3.html#question-4-professor-ratings",
    "title": "Homework #3",
    "section": "Question 4: Professor ratings",
    "text": "Question 4: Professor ratings\n\n\nCode\ndata(\"Rateprof\") #load data\nRateprof%>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)%>%\npairs()\n\n\n\n\n\nBased on the scatterplot, quality appears to be positive related to helpfulness and clarity. There’s more variablity in results for easiness and aterInterest."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw3.html#question-5-student-survey",
    "href": "posts/KalimahMuhammad_hw3.html#question-5-student-survey",
    "title": "Homework #3",
    "section": "Question 5: Student Survey",
    "text": "Question 5: Student Survey\n\n\nCode\ndata(\"student.survey\")\nfit_smss<- lm(factor(pi) ~ re, data = student.survey)\n\n\nWarning in model.response(mf, \"numeric\"): using type = \"numeric\" with a factor\nresponse will be ignored\n\n\nWarning in Ops.ordered(y, z$residuals): '-' is not meaningful for ordered\nfactors\n\n\nCode\nplot(x=student.survey$re, y=student.survey$pi)\n\n\n\n\n\n\n\nCode\nfit_smss2<- lm(tv ~ hi, data = student.survey)\nsummary(fit_smss2)\n\n\n\nCall:\nlm(formula = tv ~ hi, data = student.survey)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.600 -3.790 -1.167  2.408 27.746 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   20.200      6.175   3.271   0.0018 **\nhi            -3.909      1.849  -2.114   0.0388 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.528 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nCode\nplot(x=student.survey$hi, y=student.survey$tv)\n\n\n\n\n\nThe summary above shows there is a statistically signifcate relationship between high school GPA and the number of TV hours watched per week."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw4.html",
    "href": "posts/KalimahMuhammad_hw4.html",
    "title": "Homework #4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw4.html#question-1",
    "href": "posts/KalimahMuhammad_hw4.html#question-1",
    "title": "Homework #4",
    "section": "Question 1",
    "text": "Question 1\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n1.A\nA particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\n\nCode\ntest <- data.frame(sq=c(1240), lot=c(18000), price=c(145000))\ntest_fit <- lm(price~sq+lot, data=test)\npredict(test_fit)\n\n\n     1 \n145000 \n\n\nCode\nsummary(test_fit)\n\n\n\nCall:\nlm(formula = price ~ sq + lot, data = test)\n\nResiduals:\nALL 1 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (2 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   145000        NaN     NaN      NaN\nsq                NA         NA      NA       NA\nlot               NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n\n\n\n\n1.B\nFor fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why? The selling price would increase by 2,374.01 for each sqaure foot.\n\n\n1.C\nAccording to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size? 2.84."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw4.html#question-2",
    "href": "posts/KalimahMuhammad_hw4.html#question-2",
    "title": "Homework #4",
    "section": "Question 2",
    "text": "Question 2\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\nCode\ndata(salary) #load salary data\n\n\n\n2.A\nTest the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\n\nCode\nt.test(salary ~ sex, data=salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nBased on the above t.test, the average male salary is higher than the average female salary, 24,696.79 to 21,357.14 respectively.\n\n\n2.B\nRun a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\n\nCode\nsalaryfit<-lm(salary ~ degree + rank + sex + year + ysdeg, data=salary)\nsalaryfit\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nCoefficients:\n(Intercept)    degreePhD    rankAssoc     rankProf    sexFemale         year  \n    15746.0       1388.6       5292.4      11118.8       1166.4        476.3  \n      ysdeg  \n     -124.6  \n\n\nCode\nsummary(salaryfit)\n\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nCode\npairs(salary)\n\n\n\n\n\n\n\n2.C\nInterpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\n-MS degree: baseline estimate\n-PHD degree: increase in estimate\n-Rank Asst.: significantly lowers estimate, statistically significant\n-Rank Assoc.: significantly lowers estimate, statistically significant\n-Rank Prof.: baseline estimate, statistically significant\n-Female: increase in estimate\n-Year: statistically significant\n-Year since degree: minimally lowers estimate\nFrom the pairs graph, we also see an association between rank, year, years since degree, and salary.\n\n\n2.D\nChange the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\n\nCode\nsalary$rank <-relevel(salary$rank, ref=\"Prof\")\nlm(salary ~ rank, data=salary)\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nCoefficients:\n(Intercept)     rankAsst    rankAssoc  \n      29659       -11890        -6483  \n\n\nCode\nsummary(lm(salary ~ rank, data=salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nHere, we can see higher salary associated with higher ranking (Professor compared to Assistant and Associate Professor) based on the estimate and coefficient values.\n\n\n2.E\nFinkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\n\nCode\nsalaryfit2<-lm(salary ~ degree + sex + year + ysdeg, data=salary)\nsalaryfit2\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nCoefficients:\n(Intercept)    degreePhD    sexFemale         year        ysdeg  \n    17183.6      -3299.3      -1286.5        352.0        339.4  \n\n\nCode\nsummary(salaryfit2)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nExcluding rank, variables for degree, year, and years since degree become more significant.\n\n\n2.F\nEveryone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\n\nCode\nsalary2 <- salary%>%\n  mutate(new_dean= year<'16') #create column determining if new dean present\n\nt.test(salary ~ new_dean, data=salary2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by new_dean\nt = -0.55774, df = 39.995, p-value = 0.5801\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -4337.222  2461.145\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           23454.91            24392.95 \n\n\nCode\nsummary(lm(salary ~ new_dean, data=salary2))\n\n\n\nCall:\nlm(formula = salary ~ new_dean, data = salary2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9392.9 -5208.2   264.1  3441.8 14590.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     23455       1037  22.613   <2e-16 ***\nnew_deanTRUE      938       1716   0.547    0.587    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5958 on 50 degrees of freedom\nMultiple R-squared:  0.005941,  Adjusted R-squared:  -0.01394 \nF-statistic: 0.2988 on 1 and 50 DF,  p-value: 0.587\n\n\nBased on the two-sample t-test and regression model, faculty hired by the new dean 15 years ago have a slightly higher salary than those hired prior. This may be attributed to the rank or experience of those hired under the new dean’s tenure so we will perform another model using the interacting term rank and degree.\n\n\nCode\nsummary(lm(salary ~ degree + rank *new_dean, data=salary2))\n\n\n\nCall:\nlm(formula = salary ~ degree + rank * new_dean, data = salary2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5886.9 -1584.4  -499.6  1538.8  8831.1 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             29213.89     867.27  33.685  < 2e-16 ***\ndegreePhD                1054.74    1020.26   1.034    0.307    \nrankAsst               -11764.52    1208.79  -9.732 1.21e-12 ***\nrankAssoc               -6958.52    1610.43  -4.321 8.47e-05 ***\nnew_deanTRUE              518.22    1468.73   0.353    0.726    \nrankAsst:new_deanTRUE    -212.54    2189.70  -0.097    0.923    \nrankAssoc:new_deanTRUE    -33.19    2301.23  -0.014    0.989    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3075 on 45 degrees of freedom\nMultiple R-squared:  0.7617,    Adjusted R-squared:  0.7299 \nF-statistic: 23.97 on 6 and 45 DF,  p-value: 1.663e-12\n\n\nOnce controlled for the dean status, faculty hired as Assistant and Associate Professors during the prior dean’s tenure were estimated considerably less.\n\n\n3.A\nUsing the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\n\n\nCode\ndata(house.selling.price) #load data\nsummary(lm(Price ~ Size + New, data=house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nNewness appears as the most statistically significant variable in determining price followed by size.\n\n\n3.B\nReport and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\n\n\nCode\nsummary(lm(Price ~ New*Size, data=house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ New * Size, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nNew         -78527.502  51007.642  -1.540  0.12697    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew:Size        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nOlder homes are estimated less than newer homes but the size of a newer home is the most statistically significant.\n\n\n3.C\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nold <- data.frame(Size=c(3000), New = 0)\npredict((lm(Price~Size+New, data=house.selling.price)), old)\n\n\n       1 \n308163.9 \n\n\nCode\nnew <- data.frame(Size=c(3000), New = 1)\npredict((lm(Price~Size+New, data=house.selling.price)), new)\n\n\n       1 \n365900.2 \n\n\n\n\n3.D\nFit another model, this time with an interaction term allowing interaction between size and new, and report the regression results.\n\n\nCode\nold <- data.frame(Size=c(3000), New = 0)\npredict((lm(Price~Size*New, data=house.selling.price)), old)\n\n\n       1 \n291087.4 \n\n\nCode\nnew <- data.frame(Size=c(3000), New = 1)\npredict((lm(Price~Size*New, data=house.selling.price)), new)\n\n\n       1 \n398307.5 \n\n\n\n\n3.E\nReport the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\nWhen the interacting term is added both the price of the older home decreases and the price for the newer home increases. This creates a greater range in variability compared to the first model.\n\n\n3.F\nFind the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\n\nCode\nold <- data.frame(Size=c(3000), New = 0)\npredict((lm(Price~Size+New, data=house.selling.price)), old)\n\n\n       1 \n308163.9 \n\n\nCode\nnew <- data.frame(Size=c(3000), New = 1)\npredict((lm(Price~Size+New, data=house.selling.price)), new)\n\n\n       1 \n365900.2 \n\n\nFor an older home, the price for a 3000sq home is predicted at 308,163.90. For a new home, the predicted price would be 365,900.20.\n\n\n3.G\nFind the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\n\nCode\nold2 <- data.frame(Size=c(1500), New = 0)\npredict((lm(Price~ Size + New, data=house.selling.price)), old2)\n\n\n       1 \n133966.5 \n\n\nCode\nnew2 <- data.frame(Size=c(1500), New = 1)\npredict((lm(Price~ Size + New, data=house.selling.price)), new2)\n\n\n       1 \n191702.8 \n\n\nFor an older home, the price for a 1500sq home is predicted at 133,966.50. For a new home, the predicted price would be 191,702.80.\nAs the size of the home doubles, the predicted price of both the older and newer home increased by the same value of 174,197.40.\n\n\n3.H\nDo you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another? The model that adds whether the home is new rather than multiply by it, is a better model more reflective of the actual rates."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw5.html",
    "href": "posts/KalimahMuhammad_hw5.html",
    "title": "Homework #5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw5.html#question-1",
    "href": "posts/KalimahMuhammad_hw5.html#question-1",
    "title": "Homework #5",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(\"house.selling.price.2\") #load housing data\nsummary(lm(P ~ ., data = house.selling.price.2))\n\n\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\n\n1.A\nFor backward elimination, the first variable that should be deleted is Beds as it has the largest p-value in the regression model at 0.486763.\n\n\n1.B\nFor forward selection, the first variable that should be added is New since it has the lowest p-value at 4.3e-06.\n\n\n1.C\nThe BEDS variable could have a large p-value and high correlation because it no longer becomes significant compared to the other variables are added.\n\n\n1.D\n\n\nCode\n#model 1\nhouse_model1 <-lm(P ~ New, data = house.selling.price.2)\nsummary(house_model1)\n\n\n\nCall:\nlm(formula = P ~ New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.749 -21.249  -7.449  17.251 190.751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   89.249      5.148  17.336  < 2e-16 ***\nNew           34.158      9.383   3.641 0.000451 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.51 on 91 degrees of freedom\nMultiple R-squared:  0.1271,    Adjusted R-squared:  0.1175 \nF-statistic: 13.25 on 1 and 91 DF,  p-value: 0.0004515\n\n\nCode\n#model 2\nhouse_model2 <-lm(P ~ S, data = house.selling.price.2)\nsummary(house_model2)\n\n\n\nCall:\nlm(formula = P ~ S, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.407 -10.656   2.126  11.412  85.091 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -25.194      6.688  -3.767 0.000293 ***\nS             75.607      3.865  19.561  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.47 on 91 degrees of freedom\nMultiple R-squared:  0.8079,    Adjusted R-squared:  0.8058 \nF-statistic: 382.6 on 1 and 91 DF,  p-value: < 2.2e-16\n\n\nCode\n#model 3\nhouse_model3 <-lm(P ~ Ba, data = house.selling.price.2)\nsummary(house_model3)\n\n\n\nCall:\nlm(formula = P ~ Ba, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.903 -21.003  -2.703  12.197 130.571 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -49.248     15.644  -3.148  0.00222 ** \nBa            76.026      7.822   9.720 9.84e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.12 on 91 degrees of freedom\nMultiple R-squared:  0.5094,    Adjusted R-squared:  0.504 \nF-statistic: 94.47 on 1 and 91 DF,  p-value: 9.839e-16\n\n\nCode\n#model 4\nhouse_model4 <-lm(P ~ Be, data = house.selling.price.2)\nsummary(house_model4)\n\n\n\nCall:\nlm(formula = P ~ Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-72.079 -19.679  -3.779  14.352 174.752 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -37.23      19.95  -1.866   0.0653 .  \nBe             42.97       6.16   6.976 4.76e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.86 on 91 degrees of freedom\nMultiple R-squared:  0.3484,    Adjusted R-squared:  0.3413 \nF-statistic: 48.66 on 1 and 91 DF,  p-value: 4.759e-10\n\n\nCode\n#summary of AIC comparisons\nhouse.selling.price.2%>%\nsummarise(AIC(house_model1), AIC(house_model2), AIC(house_model3), AIC(house_model4))\n\n\n  AIC(house_model1) AIC(house_model2) AIC(house_model3) AIC(house_model4)\n1           960.908          820.1439          907.3327          933.7168\n\n\nCode\n#summary of BIC comparisons        \nhouse.selling.price.2%>%\nsummarise(BIC(house_model1), BIC(house_model2), BIC(house_model3), BIC(house_model4))\n\n\n  BIC(house_model1) BIC(house_model2) BIC(house_model3) BIC(house_model4)\n1          968.5058          827.7417          914.9305          941.3146\n\n\nUsing software with these four predictors, find the model that would be selected using each criterion: a. R2 - Model2 with the Size variable would be the best with the highest R-squared value of 0.8079. b. Adjusted R2 - Model2 would be the best fit with the highest value at 0.8058. c. PRESS - Model2 is the best fit. d. AIC - Model2 with the Size would be the best with an AIC of 820.1439. e. BIC - Model2 with the Size would be the best with an BIC of 827.74.\n\n\n1.E\nThe model using the Size variable resulted in the best fit model."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw5.html#question-2",
    "href": "posts/KalimahMuhammad_hw5.html#question-2",
    "title": "Homework #5",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"trees\")\n\n\n\n2.A\n\n\nCode\ntree_model1 <-lm(Volume ~ Girth + Height, trees)\nsummary(tree_model1)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\nCode\ntree_model2 <- lm(Volume ~ Girth * Height, trees)\nsummary(tree_model2)\n\n\n\nCall:\nlm(formula = Volume ~ Girth * Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5821 -1.0673  0.3026  1.5641  4.6649 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  69.39632   23.83575   2.911  0.00713 ** \nGirth        -5.85585    1.92134  -3.048  0.00511 ** \nHeight       -1.29708    0.30984  -4.186  0.00027 ***\nGirth:Height  0.13465    0.02438   5.524 7.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.709 on 27 degrees of freedom\nMultiple R-squared:  0.9756,    Adjusted R-squared:  0.9728 \nF-statistic: 359.3 on 3 and 27 DF,  p-value: < 2.2e-16\n\n\nThe best fit model to predict tree volume includes girth plus height as an interaction term. This is displayed in tree_model2.\n\n\nCode\n#create matrix of model plots\npar(mfrow = c(2,3)); plot(tree_model2, which = 1:6)\n\n\n\n\n\n\n\n2.B\nA few assumptions that may be violated include:\n- Potential heteroskedasticity as suggested in the curved red line in the Scale-Location plot suggesting standardized residuals may be changing as a product of fitting.\n- Observation #18 has significant leverage compared to the observation’s (see Cook’s distance) and may indicate a larger potential influence on the model."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw5.html#question-3",
    "href": "posts/KalimahMuhammad_hw5.html#question-3",
    "title": "Homework #5",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"florida\") #load data\n\n\n\n3.A\nRun a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots.\n\n\nCode\nfla_model <- (lm(Buchanan ~ Bush, data=florida))\nsummary(fla_model)\n\n\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,    Adjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n\n\nCode\npar(mfrow = c(2,3)); plot(fla_model, which = 1:6)\n\n\n\n\n\nPalm Beach County is an outlier. This may be due to this county having the most Buchanan votes (3404).\n\n\n3.B\n\n\nCode\nfla_model2 <- (lm(log(Buchanan) ~ log(Bush), data=florida))\nsummary(fla_model2)\n\n\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,    Adjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n\n\nCode\npar(mfrow = c(2,3)); plot(fla_model2, which = 1:6)\n\n\n\n\n\nPalm Beach County, remains as the most prominent outlier. However, the second model highlights a second tier of outliers including Glades and Liberty."
  },
  {
    "objectID": "posts/KarenDetter_FinalPt1.html",
    "href": "posts/KarenDetter_FinalPt1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Background / Research Question\nWhat predicts support for government regulation of ‘Big Tech’?\nIn 2001, Google piloted a program to boost profits, which were sinking as the “dot-com bubble” burst, by collecting data generated from users’ search queries and using it to sell precisely targeted advertising. The company’s ad revenues grew so quickly that they expanded their data collection tools with tracking “cookies” and predictive algorithms. Other technology firms took notice of Google’s soaring profits, and the sale of passively-collected data from people’s online activities soon became the predominant business model of the internet economy (Zuboff, 2015).\nAs the data-collection practices of ‘Big Tech’ firms, including Google, Amazon, Facebook (Meta), Apple, and Microsoft, have gradually been exposed, the public is now aware that the ‘free’ platforms that have become essential to daily life are actually harvesting personal information as payment. Despite consumers being essentially extorted into accepting this arrangement, regulatory intervention of ‘surveillance capitalism’ has remained limited.\nOver the two decades since passive data collection began commercializing the internet, survey research has shown the American public’s increasing concern about the dominance Big Tech has been allowed to exert. A 2019 study conducted by Pew Research Center found that 81% of Democrats and 70% of Republicans think there should be more government regulation of corporate data-use practices (Pew Research Center, 2019). It is very unusual to find majorities of both Republicans and Democrats agreeing on any policy position, since party affiliation is known to be a main predictor of any political stance, especially in the current polarized climate. The natural question that arises, then, is what other factors predict support for increased regulation of data-collection practices?\n\n\nHypothesis\nAlthough few studies have directly examined the mechanisms behind public support for regulation of passive data collection, a good amount of research has been done on factors influencing individual adoption of privacy protection measures (Barth et al., 2019; Boerman et al., 2021; Turow et al., 2015). It seems a reasonable extrapolation that these factors would similarly influence support for additional data privacy regulation, leading to these hypotheses:\n\nA higher level of awareness of data collection issues predicts support for increased ‘Big Tech’ regulation.\nGreater understanding of how companies use passively collected data predicts support for increased regulation.\nThe feeling of having no personal control over online tracking ‘digital resignation’ predicts support for increased regulation.\nCertain demographic traits (age group, education level, and political ideology) have some kind of effect on attitudes toward ‘Big Tech’ regulation.\n\nSince there are currently dozens of data privacy bills pending in Congress, pinpointing the forces driving support for this type of legislation can help with both shaping the regulatory framework needed and appealing for broader support from voters.\n\n\nDescriptive Statistics\nPew Research Center’s American Trends Panel (Wave 49) data set can provide insight into which of these factors are predictive of support for greater regulation of technology company data practices. In June 2019, an online survey covering a wide variety of topics was conducted and 4,272 separate observations for 144 variables were collected from adults age 18 and over. The margin of error (at the 95% confidence level) is given as +/- 1.87 percentage points.\nThe data set was compiled in SPSS and all pertinent variables are categorical.\n\n\nCode\n#read in data from SPSS file\nwav49 <- read_sav(\"_data/ATPW49.sav\")\nwav49\n\n\n# A tibble: 4,272 × 144\n     QKEY DEVICE_TYPE_…¹ LANG_…² FORM_…³ SOCME…⁴ SOCME…⁵ SOCME…⁶ SOCME…⁷ SNSUS…⁸\n    <dbl> <dbl+lbl>      <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l>\n 1 100260 2 [Tablet]     9 [Eng… 2 [For… 2 [No,… 2 [No,… 2 [No,… 2 [No,… 0 [Doe…\n 2 100588 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 2 [No,… 1 [Soc…\n 3 100637 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 4 101224 1 [Mobile pho… 9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 5 101322 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 6 101437 3 [Desktop]    9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 7 101472 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 1 [Yes… 2 [No,… 1 [Soc…\n 8 101493 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 1 [Yes… 1 [Soc…\n 9 102198 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 2 [No,… 1 [Yes… 1 [Soc…\n10 103094 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Soc…\n# … with 4,262 more rows, 135 more variables: ELECTFTGSNSINT_W49 <dbl+lbl>,\n#   TALKDISASNSINT_W49 <dbl+lbl>, TALKCMNSNSINT_W49 <dbl+lbl>,\n#   SECUR1_W49 <dbl+lbl>, PRIVACYNEWS1_W49 <dbl+lbl>,\n#   HOMEASSIST1_W49 <dbl+lbl>, HOMEASSIST2_W49 <dbl+lbl>,\n#   HOMEASSIST3_W49 <dbl+lbl>, HOMEASSIST4_W49 <dbl+lbl>,\n#   HOMEASSIST5a_W49 <dbl+lbl>, HOMEASSIST5b_W49 <dbl+lbl>,\n#   HOMEIOT_W49 <dbl+lbl>, FITTRACK_W49 <dbl+lbl>, LOYALTY_W49 <dbl+lbl>, …\n\n\nSince there are so many variables in the data set, selecting the variables of interest into a new data frame will make it easier to manage:\n\n\nCode\nsel_vars <- c('PRIVACYNEWS1_W49', 'TRACKCO1a_W49', 'CONTROLCO_W49', 'UNDERSTANDCO_W49', 'ANONYMOUS1CO_W49', 'PP4_W49', 'PRIVACYREG_W49', 'GOVREGV1_W49', 'PROFILE4_W49', 'F_AGECAT', 'F_EDUCCAT', 'F_PARTYSUM_FINAL', 'F_IDEO')\nwav49_selected <- wav49[sel_vars]\nwav49_selected\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS1_…¹ TRACKC…² CONTRO…³ UNDERS…⁴ ANONYM…⁵ PP4_W49  PRIVA…⁶ GOVREG…⁷\n   <dbl+lbl>       <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+l> <dbl+lb>\n 1 4 [Not at all … NA       NA       NA       NA       NA       3 [Ver… NA      \n 2 3 [Not too clo…  3 [Som…  2 [Som…  3 [Ver…  1 [Yes…  3 [Ver… 3 [Ver…  1 [Mor…\n 3 3 [Not too clo…  3 [Som…  3 [Ver…  3 [Ver…  1 [Yes…  2 [Som… 3 [Ver…  1 [Mor…\n 4 4 [Not at all … NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 5 4 [Not at all …  1 [All…  4 [No …  4 [Not…  2 [No,… NA       4 [Not…  1 [Mor…\n 6 2 [Somewhat cl… NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 7 2 [Somewhat cl…  2 [Mos…  3 [Ver…  3 [Ver…  2 [No,…  2 [Som… 2 [Som…  3 [Abo…\n 8 1 [Very closel…  1 [All…  4 [No …  4 [Not…  2 [No,… NA       3 [Ver…  3 [Abo…\n 9 3 [Not too clo…  1 [All…  3 [Ver…  2 [Som…  2 [No,…  3 [Ver… 3 [Ver…  1 [Mor…\n10 3 [Not too clo…  3 [Som…  2 [Som…  1 [A g…  1 [Yes…  2 [Som… 2 [Som…  2 [Les…\n# … with 4,262 more rows, 5 more variables: PROFILE4_W49 <dbl+lbl>,\n#   F_AGECAT <dbl+lbl>, F_EDUCCAT <dbl+lbl>, F_PARTYSUM_FINAL <dbl+lbl>,\n#   F_IDEO <dbl+lbl>, and abbreviated variable names ¹​PRIVACYNEWS1_W49,\n#   ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49, ⁵​ANONYMOUS1CO_W49,\n#   ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49\n\n\nThe variable labels contain the survey questions asked:\n\n\nCode\n#summary of $variable names and their [labels]\nvar_label(wav49_selected)\n\n\n$PRIVACYNEWS1_W49\n[1] \"PRIVACYNEWS1. How closely, if at all, do you follow news about privacy issues?\"\n\n$TRACKCO1a_W49\n[1] \"TRACKCO1a. As far as you know, how much of what you do ONLINE or on your cellphone is being tracked by advertisers, technology firms or other companies?\"\n\n$CONTROLCO_W49\n[1] \"CONTROLCO. How much control do you think you have over the data that companies collect about you?\"\n\n$UNDERSTANDCO_W49\n[1] \"UNDERSTANDCO. How much do you feel you understand what companies are doing with the data they collect about you?\"\n\n$ANONYMOUS1CO_W49\n[1] \"ANONYMOUS1CO. Do you think it is possible to go about daily life today without having companies collect data about you?\"\n\n$PP4_W49\n[1] \"PP4. How much do you typically understand the privacy policies you read?\"\n\n$PRIVACYREG_W49\n[1] \"PRIVACYREG. How much do you feel you understand the laws and regulations that are currently in place to protect your data privacy?\"\n\n$GOVREGV1_W49\n[1] \"GOVREGV1. How much government regulation of what companies can do with their customers’ personal information do you think there should be?\"\n\n$PROFILE4_W49\n[1] \"PROFILE4. How much, if at all, do you understand what data about you is being used to create these advertisements?\"\n\n$F_AGECAT\n[1] \"Age category\"\n\n$F_EDUCCAT\n[1] \"Education level category\"\n\n$F_PARTYSUM_FINAL\n[1] \"Party summary\"\n\n$F_IDEO\n[1] \"Ideology\"\n\n\nBecause the data set is made up of categorical variables, transformation is required before computing any statistics:\n\n\nCode\n#convert all variables to factors\nwav49_factored <- wav49_selected %>%\n  mutate_all(as_factor)\n#convert user-defined missing values to regular missing values\nzap_missing(wav49_factored)\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS…¹ TRACK…² CONTR…³ UNDER…⁴ ANONY…⁵ PP4_W49 PRIVA…⁶ GOVRE…⁷ PROFI…⁸\n   <fct>         <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>  \n 1 Not at all c… <NA>    <NA>    <NA>    <NA>    <NA>    Very l… <NA>    <NA>   \n 2 Not too clos… Some o… Some c… Very l… Yes, i… Very l… Very l… More r… <NA>   \n 3 Not too clos… Some o… Very l… Very l… Yes, i… Some    Very l… More r… Somewh…\n 4 Not at all c… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    <NA>   \n 5 Not at all c… All or… No con… Nothing No, it… <NA>    Not at… More r… Not to…\n 6 Somewhat clo… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    Not to…\n 7 Somewhat clo… Most o… Very l… Very l… No, it… Some    Some    About … Somewh…\n 8 Very closely  All or… No con… Nothing No, it… <NA>    Very l… About … Somewh…\n 9 Not too clos… All or… Very l… Some    No, it… Very l… Very l… More r… Somewh…\n10 Not too clos… Some o… Some c… A grea… Yes, i… Some    Some    Less r… Somewh…\n# … with 4,262 more rows, 4 more variables: F_AGECAT <fct>, F_EDUCCAT <fct>,\n#   F_PARTYSUM_FINAL <fct>, F_IDEO <fct>, and abbreviated variable names\n#   ¹​PRIVACYNEWS1_W49, ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49,\n#   ⁵​ANONYMOUS1CO_W49, ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49, ⁸​PROFILE4_W49\n\n\nAfter the variables are converted to meaningful factors, a summary of response frequencies can be generated:\n\n\nCode\nsummary(wav49_factored)\n\n\n           PRIVACYNEWS1_W49                 TRACKCO1a_W49 \n Very closely      : 461    All or almost all of it: 881  \n Somewhat closely  :2046    Most of it             : 703  \n Not too closely   :1397    Some of it             : 381  \n Not at all closely: 359    Very little of it      :  88  \n Refused           :   9    None of it             :  76  \n                            Refused                :  11  \n                            NA's                   :2132  \n                 CONTROLCO_W49      UNDERSTANDCO_W49\n A great deal of control:  68   A great deal: 132   \n Some control           : 313   Some        : 716   \n Very little control    :1134   Very little :1040   \n No control             : 621   Nothing     : 242   \n Refused                :   4   Refused     :  10   \n NA's                   :2132   NA's        :2132   \n                                                    \n               ANONYMOUS1CO_W49         PP4_W49          PRIVACYREG_W49\n Yes, it is possible   : 772    A great deal: 328   A great deal: 136  \n No, it is not possible:1357    Some        :1405   Some        :1380  \n Refused               :  11    Very little : 751   Very little :2153  \n NA's                  :2132    Not at all  :  82   Not at all  : 593  \n                                Refused     :   5   Refused     :  10  \n                                NA's        :1701                      \n                                                                       \n                GOVREGV1_W49        PROFILE4_W49    F_AGECAT   \n More regulation      :1631   A great deal: 384   18-29 : 671  \n Less regulation      : 145   Somewhat    :1410   30-49 :1314  \n About the same amount: 331   Not too much: 900   50-64 :1308  \n Refused              :  33   Not at all  : 113   65+   : 977  \n NA's                 :2132   Refused     :   9   DK/REF:   2  \n                              NA's        :1456                \n                                                               \n                 F_EDUCCAT              F_PARTYSUM_FINAL\n College graduate+    :1600   Rep/Lean Rep      :1823   \n Some College         :1182   Dem/Lean Dem      :2296   \n H.S. graduate or less:1483   DK/Refused/No lean: 153   \n Don't know/Refused   :   7                             \n                                                        \n                                                        \n                                                        \n               F_IDEO    \n Very conservative: 353  \n Conservative     : 977  \n Moderate         :1615  \n Liberal          : 828  \n Very liberal     : 386  \n Refused          : 113  \n                         \n\n\n*High NA value indicates that the question was not presented to all respondents\nThe data set is now primed for examining correlations and testing hypotheses.\n\n\nReferences\nBarth, S., de Jong, M. D. T., Junger, M., Hartel, P. H. & Roppelt, J. C. (2019). Putting the privacy paradox to the test: Online privacy and security behaviors among users with technical knowledge, privacy awareness, and financial resources. Telematics and Informatics, 41, 55–69. doi:10.1016/j.tele.2019.03.003\nBoerman, S. C., Kruikemeier, S., & Zuiderveen Borgesius, F. J. (2021). Exploring Motivations for Online Privacy Protection Behavior: Insights From Panel Data. Communication Research, 48(7), 953–977. https://doi.org/10.1177/0093650218800915\nPew Research Center. (2019). Americans and privacy: Concerned, confused and feeling lack of control over their personal information. https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-concerned-confused-and- feeling-lack-of-control-over-their-personal-information/\nPew Research Center. (2020). Wave 49 American trends panel [Data set]. https://www.pewresearch.org/internet/dataset/american-trends-panel-wave-49/\nTurow, J., Hennessy, M. & Draper, N. (2015). The tradeoff fallacy – How marketers are misrepresenting American consumers and opening them up to exploitation. Annenberg School for Communication.\nZuboff, S. (2015). Big other: Surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1), 75–89. doi:10.1057/jit.2015.5"
  },
  {
    "objectID": "posts/KarenDetter_FinalPt2.html",
    "href": "posts/KarenDetter_FinalPt2.html",
    "title": "Final Project - Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(labelled)\nlibrary(crosstable)\nlibrary(MASS)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_FinalPt2.html#what-predicts-opinions-on-government-regulation-of-big-tech",
    "href": "posts/KarenDetter_FinalPt2.html#what-predicts-opinions-on-government-regulation-of-big-tech",
    "title": "Final Project - Part 2",
    "section": "What predicts opinions on government regulation of ‘Big Tech’?",
    "text": "What predicts opinions on government regulation of ‘Big Tech’?\nIn 2001, Google piloted a program to boost profits, which were sinking as the “dot-com bubble” burst, by collecting data generated from users’ search queries and using it to sell precisely targeted advertising. The company’s ad revenues grew so quickly that they expanded their data collection tools with tracking “cookies” and predictive algorithms. Other technology firms took notice of Google’s soaring profits, and the sale of passively-collected data from people’s online activities soon became the predominant business model of the internet economy (Zuboff, 2015).\nAs the data-collection practices of “Big Tech” firms, including Google, Amazon, Facebook (Meta), Apple, and Microsoft, have gradually been exposed, the public is now aware that the “free” platforms that have become essential to daily life are actually harvesting personal information as payment. Despite consumers being essentially extorted into accepting this arrangement, regulatory intervention into “surveillance capitalism” has remained limited.\nOver the two decades since passive data collection began commercializing the internet, survey research has shown the American public’s increasing concern over the dominance Big Tech has been allowed to exert. A 2019 study conducted by Pew Research Center found that 81% of Democrats and 70% of Republicans think there should be more government regulation of corporate data-use practices (Pew Research Center, 2019). It is very unusual to find majorities of both Republicans and Democrats agreeing on any policy position, since party affiliation is known to be a main predictor of any political stance, especially in the current polarized climate. The natural question that arises, then, is what other factors might predict support for increased regulation of data-collection practices?"
  },
  {
    "objectID": "posts/KarenDetter_HW2.html",
    "href": "posts/KarenDetter_HW2.html",
    "title": "HW 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.1",
    "href": "posts/KarenDetter_HW2.html#q.1",
    "title": "HW 2",
    "section": "Q.1",
    "text": "Q.1\n- Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures:\nBypass -\n\n\nCode\n#assign values\nBs_sd <- 10\nBs_size <- 539\nBs_mean <- 19\n#calculate standard error\nBstandard_error <- Bs_sd / sqrt(Bs_size)\nBstandard_error\n\n\n[1] 0.4307305\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nBtail_area <- (1-confidence_level)/2\nBtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nBt_score <- qt(p = 1-Btail_area, df = Bs_size-1)\nBt_score\n\n\n[1] 1.647691\n\n\n\n\nCode\n#calculate confidence interval\nBCI <- c(Bs_mean - Bt_score * Bstandard_error, Bs_mean + Bt_score * Bstandard_error)\nprint(BCI)\n\n\n[1] 18.29029 19.70971\n\n\nAngiography-\n\n\nCode\n#assign values\nAs_sd <- 9\nAs_size <- 847\nAs_mean <- 18\n#calculate standard error\nAstandard_error <- As_sd / sqrt(As_size)\nAstandard_error\n\n\n[1] 0.3092437\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nAtail_area <- (1-confidence_level)/2\nAtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nAt_score <- qt(p = 1-Atail_area, df = As_size-1)\nAt_score\n\n\n[1] 1.646657\n\n\n\n\nCode\n#calculate confidence interval\nACI <- c(As_mean - At_score * Astandard_error, As_mean + At_score * Astandard_error)\nprint(ACI)\n\n\n[1] 17.49078 18.50922\n\n\n- Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#calculate differences in upper and lower bounds of both confidence intervals\n(Bs_mean + Bt_score * Bstandard_error) - (Bs_mean - Bt_score * Bstandard_error)\n\n\n[1] 1.419421\n\n\nCode\n(As_mean + At_score * Astandard_error) - (As_mean - At_score * Astandard_error)\n\n\n[1] 1.018436\n\n\nAngiography has a narrower confidence interval."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.2",
    "href": "posts/KarenDetter_HW2.html#q.2",
    "title": "HW 2",
    "section": "Q.2",
    "text": "Q.2\n- Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n#assign values\nk <- 567\nn <- 1031\n#calculate sample proportion\np <- k/n\np\n\n\n[1] 0.5499515\n\n\n- Construct and interpret a 95% confidence interval for p\n\n\nCode\n#calculate margin of error\nmargin <- qnorm(0.975) * sqrt(p*(1-p)/n)\n#calculate lower and upper bounds of confidence interval\nlow <- p - margin\nhigh <- p + margin\nprint(low)\n\n\n[1] 0.5195839\n\n\nCode\nprint(high)\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the population proportion is [.52, .58]. Since 95% of confidence intervals calculated from point estimates of population proportions would contain the true mean population proportion, we can be reasonably confident that the true mean proportion of adult Americans who believe a college education is essential for success lies somewhere between 52 and 58%."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.3",
    "href": "posts/KarenDetter_HW2.html#q.3",
    "title": "HW 2",
    "section": "Q.3",
    "text": "Q.3\n- Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#assign values\nz_score <- qnorm(.975) #assuming normal distribution and 95% confidence level\nmargin_error <- 5 #half of confidence interval\n#calculate population standard deviation (one quarter of the range)\npop_sd <- (200-30) / 4\n\n\n\n\nCode\n#calculate sampling size of population mean\nsamp_size <- z_score^2 * pop_sd^2 / margin_error^2\nsamp_size\n\n\n[1] 277.5454\n\n\nThe sample size should be 278."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.4",
    "href": "posts/KarenDetter_HW2.html#q.4",
    "title": "HW 2",
    "section": "Q.4",
    "text": "Q.4\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nassumptions: random sampling, normally distributed data, adequate sample size; hypotheses: \\(H_{0}\\) : \\(\\bar{y}\\) = \\(\\mu\\) ; \\(H_{\\alpha}\\) : \\(\\bar{y}\\) \\(\\neq\\) \\(\\mu\\) ; test statistic: t-statistic\n\n\nCode\n#calculate t-statistic\nt_stat <- (410 - 500) / (90 / (sqrt(9)))\n#calculate two-tailed p-value\np_val <- 2 * (pt(q = t_stat, df=8))\np_val\n\n\n[1] 0.01707168\n\n\nAssuming \\(\\alpha\\) = .05, we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\).\nB. Report the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) < 500. Interpret.\n\n\nCode\n#calculate lower-tail p-value\np_low <- pt(t_stat, df = 8, lower.tail = TRUE)\np_low\n\n\n[1] 0.008535841\n\n\nThis p-value is significantly lower than the .05 significance level, which means that we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\) : \\(\\mu\\) < 500.\nC. Report and interpret the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) > 500.\n\n\nCode\n#calculate lower-tail p-value\np_high <- pt(t_stat, df = 8, lower.tail = FALSE)\np_high\n\n\n[1] 0.9914642\n\n\n\n\nCode\n#double-check p-values\ncheck <- p_high + p_low\ncheck\n\n\n[1] 1\n\n\nThis p-value is significantly higher than the .05 significance level, so in this case we fail to reject \\(H_{0}\\) in favor of \\(H_{\\alpha}\\) : \\(\\mu\\) > 500."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.5",
    "href": "posts/KarenDetter_HW2.html#q.5",
    "title": "HW 2",
    "section": "Q.5",
    "text": "Q.5\nA. Show that t = 1.95 and P-value = 0.051 for Jones Show that t = 1.97 and P-value = 0.049 for Smith\n\n\nCode\n#calculate t-statistics\nJones_t <- (519.5 - 500) / 10\nJones_t\n\n\n[1] 1.95\n\n\nCode\nSmith_t <- (519.7 - 500) / 10\nSmith_t\n\n\n[1] 1.97\n\n\n\n\nCode\n#calculate p-values\nJones_p <- 2 * (pt(q = Jones_t, df=999, lower.tail = FALSE))\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p <- 2 * (pt(q = Smith_t, df=999, lower.tail = FALSE))\nSmith_p\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAt this significance level, Smith’s study would be considered significant and allow for rejection of the null hypothesis. Jones’ study, however, would fail to reject the null.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05”, or as “reject H0” versus “Do not reject H0”, without reporting the actual P-value.\nThis example shows the importance of being specific and thorough in reporting the “significance” of study findings. Both Smith and Jones produced results very near the cutoff point for statistical significance, so it would be critical to know both the actual p-value AND the exact standard, \\(\\leq\\) or <, being used to interpret the results in order to assess the actual impact of the findings. Reporting only “reject” or “do not reject” the null hypothesis would also not provide the information needed to make a judgment of the meaning of the findings, as it would not provide any evidence in support of the claim."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.6",
    "href": "posts/KarenDetter_HW2.html#q.6",
    "title": "HW 2",
    "section": "Q.6",
    "text": "Q.6\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\n#assign values\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n#run one sample t-test\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nAt the 95% confidence level, the p-value of \\(H_{\\alpha}\\) : \\(\\mu\\) < 45 is .04, indicating that we can reject \\(H_{0}\\). Additionally, 45 is above the upper bound of the confidence interval, which also supports the alternative hypothesis."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html",
    "href": "posts/KarenDetter_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.1",
    "href": "posts/KarenDetter_HW3.html#q.1",
    "title": "HW3",
    "section": "Q.1",
    "text": "Q.1\n\n\nCode\n##load data\ndata(UN11)\n\n\n\n1.1.1\nIn this model, the predictor variable is ‘ppgdp’ ($ gross national product per person) and the response variable is ‘fertility’.\n\n\n1.1.2\n\n\nCode\n##draw scatterplot\nplot(x = UN11$ppgdp, y = UN11$fertility)\n\n\n\n\n\nThe relationship between fertility and ppgdp is not exactly linear because increasing gross national product only decreases birth rate until it nears about the 10,000 point; after that, the effect seems to disappear.\n\n\n1.1.3\n\n\nCode\n##draw scatterplot with logs of both variables\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility))\n\n\n\n\n\nThe log transformation scatterplot shows a relationship that looks much closer to that of the linear regression model."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.2",
    "href": "posts/KarenDetter_HW3.html#q.2",
    "title": "HW3",
    "section": "Q.2",
    "text": "Q.2\n\n(a)\nConverting the currency from American dollars to British pounds causes the mean of the x-axis (explanatory variable) to increase while the mean of the y-axis (response variable) remains the same. As a result, the prediction equation line becomes less steep, as each value of x is increased for the identical corresponding y-value.\n\n\n(b)\nThe currency conversion would not change the correlation, as the relative values of the variables remain unchanged."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.3",
    "href": "posts/KarenDetter_HW3.html#q.3",
    "title": "HW3",
    "section": "Q.3",
    "text": "Q.3\n\n\nCode\n##load data\ndata(water)\n##draw scatterplot matrix\npairs(water)\n\n\n\n\n\nThese scatterplots show that when precipitation at OPBPC, OPRC, and OPSLAKE increases, the runoff volume at BSAAM goes up. Precipitation at the other three locations does not seem to have a strong linear relationship with stream runoff volume.\nAlso, precipitation rates at the first three sites seem to be somewhat intercorrelated, as do the rates at the last three sites, indicating that the sites in each set may be closer to each other or share similar geographic features."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.4",
    "href": "posts/KarenDetter_HW3.html#q.4",
    "title": "HW3",
    "section": "Q.4",
    "text": "Q.4\n\n\nCode\n##load data\ndata(\"Rateprof\")\n##draw scatterplot matrix of selected variables\npairs(~Rateprof$quality+Rateprof$helpfulness+Rateprof$clarity+Rateprof$easiness+Rateprof$raterInterest, lwd=2, labels = c(\"QUALITY\", \"HELPFULNESS\", \"CLARITY\", \"EASINESS\", \"Rater INTEREST\"), pch=19, cex = 0.75, col = \"blue\")\n\n\n\n\n\nSurprisingly, it doesn’t seem that reviewers’ ratings of their interest in the subject or the easiness of the course correlate with ratings of the professor’s quality, helpfulness, or clarity. Ratings for those three traits, however, all seem to have linear relationships with each other."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.5",
    "href": "posts/KarenDetter_HW3.html#q.5",
    "title": "HW3",
    "section": "Q.5",
    "text": "Q.5\n\n\nCode\n##load data\ndata(student.survey)\n\n\n\n(i)\n\n\nCode\n##convert factor variables to numeric\npi_conv <- as.numeric(student.survey$pi)\nre_conv <- as.numeric(student.survey$re)\n##run regression analysis\nmodel1 <- lm(pi_conv ~ re_conv, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_conv ~ re_conv, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre_conv       0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\n(ii)\n\n\nCode\n##run regression analysis\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\n(a) & (b)\n\n\nCode\n##visualize relationships in the two models with scatterplots\n##include regression lines of coefficients\n##use jitter plots due to small sample size of 60\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_jitter(color = \"blue\") +\n    geom_abline(intercept = .9308, slope = .9704) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe first regression model shows evidence of a strong, statistically significant effect of religiousness on political ideology, with the p-value of .00000122 being well below the significance threshold of .05. As the level of religiousness increases, political ideology becomes more conservative, with religiousness explaining 34% of the variance in ideology. Because of the small number of observations (n=60), scatterplot points do not appear tightly aligned to the regression line, but there is a clear upward-moving trend.\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_jitter(color = \"blue\") +\n  geom_abline(intercept = 3.441353, slope = -0.018305) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe effect of hours of tv watched on grade point average is not very strong, with the p-value of .0388 being just below the significance threshold. The relationship between the variables is inverse - mean gpa decreases by .02 for every increase in hours of tv watched. Hours of tv watched per week explain 7% of the variance in grade point averages. The scatterplot, again affected by small sample size, does show a slight trend of gpa decreasing as tv level increases."
  },
  {
    "objectID": "posts/KarenDetter_HW4.html",
    "href": "posts/KarenDetter_HW4.html",
    "title": "HW 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW4.html#question-1",
    "href": "posts/KarenDetter_HW4.html#question-1",
    "title": "HW 4",
    "section": "Question 1",
    "text": "Question 1\n\nA.\n\\(\\hat{y}\\) (predicted selling price) =\n\n\nCode\nyhat <- (-10536 + (53.8*1240) + (2.84*18000))\nyhat\n\n\n[1] 107296\n\n\nresidual = observed - predicted\n\n\nCode\nres <- 145000 - 107296\nres\n\n\n[1] 37704\n\n\nThe home in question sold for $37,704 more than the equation predicted, indicating that other variables that were not included in the equation have an impact on selling price.\n\n\nB.\nFor fixed lot size, the house selling price is predicted to increase 53.8 for each square foot in home size. When lot size is fixed, that variable is disregarded, leaving \\(\\hat{y}\\) = 53.8\\(x_{1}\\), which means that for each unit of x, the predicted value of y will increase by 53.8.\n\n\nC.\n\n\nCode\nincr <- 53.8 / 2.84\nincr\n\n\n[1] 18.94366\n\n\nLot size (\\(x_{2}\\)) would need to increase by 18.94 units to have the same impact as a one unit increase in home size (\\(x_{1}\\))."
  },
  {
    "objectID": "posts/KarenDetter_HW4.html#question-2",
    "href": "posts/KarenDetter_HW4.html#question-2",
    "title": "HW 4",
    "section": "Question 2",
    "text": "Question 2\n\nA.\n\n\nCode\n#test hypothesis with two sample t-test\n\ndata(\"salary\")\nt.test(salary ~ sex, data = salary)\n\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nAlthough the sample estimate means show a difference in salary between men and women, the null hypothesis that there is no difference between the two groups cannot be rejected on the basis of this test alone, due to the p-value of .09 being higher than the threshhold of .05.\n\n\nB.\n\n\nCode\n#run a multiple linear regression with all variables explaining salary\n\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\n#obtain a 95% confidence interval for difference in salary by sex\n\nconfint(model, 'sexFemale')\n\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\n\nBecause the confidence interval includes 0, it was correct to reject the null hypothesis.\n\n\nC.\nIn this model, the intercept shows a base expected salary of 15746 for all observations in the data set, without consideration of any other variables. Rank and years in current rank show statistically significant effects on salary.\nGaining a level of degree (from Masters to PhD) is associated with a salary increase of 1389, but not within the statistically significant threshhold.\nMoving from rankAsst to rankAssoc corresponds to an increase in salary of 5292, and moving from rankAsst to rankProf yields a salary increase of 11119.\nEach unit of years in current rank corresponds to a salary increase of 476, while each unit of years since highest degree is associated with a decrease in salary of 125, although this association is not statistically significant.\nBeing female is associated with an increase in salary of 1166, but the relationship is not at the level of statistical significance.\n\n\nD.\n\n\nCode\n#change baseline for rank category\nsalary$rank <- relevel(salary$rank, ref = 'Prof')\n\nsummary(lm(salary ~ ., data = salary))\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\ndegreePhD     1388.61    1018.75   1.363    0.180    \nrankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\nrankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\nsexFemale     1166.37     925.57   1.260    0.214    \nyear           476.31      94.91   5.018 8.65e-06 ***\nysdeg         -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nHaving the rank of Associate correlates to a salary of 5826 less than the salary correlated to the rank of Professor, and the rank of Assistant correlates to a salary of 11119 less than that of the Professor rank.\n\n\nE.\n\n\nCode\n#refit model excluding the rank variable\nmodel_alt <- lm(salary ~ degree + sex + year + ysdeg, data = salary)\nsummary(model_alt)\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nIn this model, being female is associated with a salary decrease of 1287, but the effect is, again, well outside the acceptable range of statistical significance.\n\n\nF.\n\n\nCode\n#create new variable for hiring dean\nhiring_dean <- salary %>%\n              mutate(dean = \n                       case_when(`ysdeg` > 15 ~ 'prev',\n                                 `ysdeg` <= 15 ~ 'new'))\n\n#fit new model to test hypothesis while avoiding multicollinearity\ndean_model <- lm(salary ~ . - ysdeg, data = hiring_dean)\nsummary(dean_model)\n\n\n\nCall:\nlm(formula = salary ~ . - ysdeg, data = hiring_dean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3403.3 -1387.0  -167.0   528.2  9233.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26588.79    1168.06  22.763  < 2e-16 ***\ndegreePhD      818.93     797.48   1.027   0.3100    \nrankAsst    -11096.95    1191.00  -9.317 4.54e-12 ***\nrankAssoc    -6124.28    1028.58  -5.954 3.65e-07 ***\nsexFemale      907.14     840.54   1.079   0.2862    \nyear           434.85      78.89   5.512 1.65e-06 ***\ndeanprev     -2163.46    1072.04  -2.018   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2362 on 45 degrees of freedom\nMultiple R-squared:  0.8594,    Adjusted R-squared:  0.8407 \nF-statistic: 45.86 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nBecause the variable ysdeg would, by nature, be highly correlated to the variable dean, ysdeg was omitted in the model examining the effect of dean on salary.\nThe resulting model shows a statistically significant (p = .05) effect of hiring dean on salary, with hiring by the previous dean correlating to a decrease in salary of 2163."
  },
  {
    "objectID": "posts/KarenDetter_HW4.html#question-3",
    "href": "posts/KarenDetter_HW4.html#question-3",
    "title": "HW 4",
    "section": "Question 3",
    "text": "Question 3\n\nA.\n\n\nCode\ndata(\"house.selling.price\")\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nSize and whether a house is new each have a statistically significant effect on house price, both with p-values well below the significance threshhold of .05.\nEach unit of size is associated with an increase in price of 116, and new houses are associated with a price increase of 57736.\n\n\nB.\nprediction equation:\n\\(\\hat{y}\\) = -40231 + 116\\(x_{1}\\) + 57736\\(x_{2}\\)\nwhere y = house selling price, \\(x_{1}\\) = house size (in sq ft), \\(x_{2}\\) = house is new\nalternative prediction equation:\n\\(\\hat{y}\\) = -40231 + 116\\(x_{1}\\) + 0\\(x_{2}\\)\nwhere y = house selling price, \\(x_{1}\\) = house size (in sq ft), \\(x_{2}\\) = house is not new\n\n\nC.\n(i)\n3000 sq ft, new house:\n\\(\\hat{y}\\) = -40231 + 116(3000) + 57736 = -40231 + 348000 + 57736 = -40231 + 405736 = $365,505\n(ii)\n3000 sq ft, not new house:\n\\(\\hat{y}\\) = -40231 + 116(3000) + 0 = -40231 + 348000 = $307,769\n\n\nD.\n\n\nCode\n#fit model with an interaction term between variables\n\nsummary(lm(Price ~ Size + New + Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\nE.\nThe prediction equations generated from this model are:\n(i) New\nPrice = -22228 + 104(Size) - 78528 + 62(Size)\n(ii) Not New\nPrice = -22228 + 104(Size)\nTherefore, new houses are associated with an additional price increase of 62 per unit of size increase.\n\n\nF.\nPredicted Prices:\n(i) 3000 sq ft New House\nPrice = -22228 + 104(3000) - 78528 + 62(3000) = $397,244\n(ii) 3000 sq ft Not New House\nPrice = -22228 + 104(3000) = $289,772\n\n\nG.\nPredicted Prices:\n(i) 1500 sq ft New House\nPrice = -22228 + 104(1500) - 78528 + 62(1500) = $148,244\n(ii) 1500 sq ft Not New House\nPrice = -22228 + 104(1500) = $133,772\nThe ratio between the predicted selling prices of the new and not new, 1500 sq ft house is 1.108. The ratio between the predicted selling prices of the new and not new, 3000 sq ft house is 1.371.\nAs size increases, the price difference between new and not new houses also increases, indicating that there is an interaction between Size and New.\n\n\nH.\nThe model with the interaction term between Size and New seems to better represent the relationship between these variables and Price. Also, the Adjusted \\(R^{2}\\) for this model is a bit higher, indicating that it explains a higher portion of the variance than the original model."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html",
    "href": "posts/KarenDetter_HW5.html",
    "title": "HW 5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#a.",
    "href": "posts/KarenDetter_HW5.html#a.",
    "title": "HW 5",
    "section": "A.",
    "text": "A.\nUsing backward elimination would result in Beds being removed, because it has the highest p-value of all the variables."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#b.",
    "href": "posts/KarenDetter_HW5.html#b.",
    "title": "HW 5",
    "section": "B.",
    "text": "B.\nUnder the forward selection method, Size would be added first, because it has the largest t-value, indicating the greatest improvement to the null model."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#c.",
    "href": "posts/KarenDetter_HW5.html#c.",
    "title": "HW 5",
    "section": "C.",
    "text": "C.\nThe fact that Beds has a substantial correlation with Price, but a large p-value in the regression model is an indication of small sample size, since p-value is a function of both correlation coefficient and sample size."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#d.",
    "href": "posts/KarenDetter_HW5.html#d.",
    "title": "HW 5",
    "section": "D.",
    "text": "D.\n\n\nCode\n#test regression models\nlibrary(smss)\ndata(house.selling.price.2)\nfull <- lm(P ~ ., data = house.selling.price.2)\nforw1 <- lm(P ~ S, data = house.selling.price.2)\nforw2 <- lm(P ~ S + New, data = house.selling.price.2)\nforw3 <- lm(P ~ S + New + Ba, data = house.selling.price.2)\n\n\n\na.\nI used the forward selection method to fit models, adding variables, one at a time, based on t-values (highest to lowest).\n\\(R^{2}\\) is highest for the full model with all variables.\n\n\nb.\nAdjusted \\(R^{2}\\) is highest for the model of Price as a function of Size, Baths, and New.\n\n\nc.\n\n\nCode\n#calculate PRESS statistics\nPRESS <- function(linear.model) {\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat) \n  PRESS <- sum(pr^2) \n  return(PRESS)\n}\nPRESS(full)\n\n\n[1] 28390.22\n\n\nCode\nPRESS(forw1)\n\n\n[1] 38203.29\n\n\nCode\nPRESS(forw2)\n\n\n[1] 31066\n\n\nCode\nPRESS(forw3)\n\n\n[1] 27860.05\n\n\nThe model with Price as a function of Size, Baths, and New has the lowest PRESS calculation.\n\n\nd.\n\n\nCode\n#calculate AIC values\nAIC(full, k=2)\n\n\n[1] 790.6225\n\n\nCode\nAIC(forw1, k=2)\n\n\n[1] 820.1439\n\n\nCode\nAIC(forw2, k=2)\n\n\n[1] 800.1262\n\n\nCode\nAIC(forw3, k=2)\n\n\n[1] 789.1366\n\n\nThe model with Price as a function of Size, Baths, and New has the lowest AIC calculation.\n\n\ne.\n\n\nCode\n#calculate BIC values\nBIC(full)\n\n\n[1] 805.8181\n\n\nCode\nBIC(forw1)\n\n\n[1] 827.7417\n\n\nCode\nBIC(forw2)\n\n\n[1] 810.2566\n\n\nCode\nBIC(forw3)\n\n\n[1] 801.7996\n\n\nThe model with Price as a function of Size, Baths, and New has the lowest BIC calculation."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#e.-1",
    "href": "posts/KarenDetter_HW5.html#e.-1",
    "title": "HW 5",
    "section": "E.",
    "text": "E.\nSince the model with Price as a function of Size, Baths, and New has the highest Adjusted \\(R^{2}\\), and the lowest PRESS, AIC, and BIC calculations, I would choose it as the best one."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#a.-2",
    "href": "posts/KarenDetter_HW5.html#a.-2",
    "title": "HW 5",
    "section": "A.",
    "text": "A.\n\n\nCode\n#fit multiple regression model\ntrees_full <- lm(Volume ~ Girth + Height, data = trees)\nsummary(trees_full)\n\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#b.-2",
    "href": "posts/KarenDetter_HW5.html#b.-2",
    "title": "HW 5",
    "section": "B.",
    "text": "B.\n\n\nCode\n#run diagnostic plots\npar(mfrow = c(2,3)); plot(trees_full, which = 1:6)\n\n\n\n\n\nFrom the appearance of the Residuals vs Fitted plot, it seems the assumption of a linear relationship is violated, because the residuals form a pattern of groups, instead of being randomly distributed around the 0 line.\nIt also appears that the assumption of constant variance is violated, as the Scale-Location plot shows heteroskedasticity in the residuals because the baseline shows magnitude changes."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#a",
    "href": "posts/KarenDetter_HW5.html#a",
    "title": "HW 5",
    "section": "a)",
    "text": "a)\n\n\nCode\nlibrary(alr4)\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata(florida)\nvote <- lm(Buchanan ~ Bush, data = florida)\n#produce diagnostic plots\npar(mfrow = c(2,3)); plot(vote, which = 1:6)\n\n\n\n\n\nPalm Beach County is a definite outlier in this model, as it does not follow the patterns produced by the other county data in any of the diagnostic plots, and its residual values are much higher."
  },
  {
    "objectID": "posts/KarenDetter_HW5.html#b",
    "href": "posts/KarenDetter_HW5.html#b",
    "title": "HW 5",
    "section": "b)",
    "text": "b)\n\n\nCode\n#log both variables in model\nvote_log <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2,3)); plot(vote_log, which = 1:6)\n\n\n\n\n\nPalm Beach County still appears to be at the high end of the data, but it no longer appears to be an actual outlier, because it is now much closer in line with the patterns and values of the other counties."
  },
  {
    "objectID": "posts/KenDocekal_finalproject1.html",
    "href": "posts/KenDocekal_finalproject1.html",
    "title": "Final Project 1",
    "section": "",
    "text": "Research Question\nHow much does state policy intervention impact future social and economic value preferences in residents?\nWhile political values often explicitly inform social and economic policy actions taken by governments, policy actions themselves can also affect the development of the values of both program recipients and the greater public. Low-income recipients are assumed to benefit from, and therefore favor, state intervention and redistributive policies while upper income groups are assumed to be against but this is not always true, especially at the program level (Bueno et al.). Authors like Holland note that “the poor only have an economic interest in supporting social expenditures in contexts where they expect policies to redistribute resources or risks in their favor”.\nThis study seeks to better understand the relationship between policy action and value formation at the sub-national level by looking at the effect of US state policy interventions on residents’ subsequent policy preferences. By looking at how differences in US states’ social and economic policy intervention from 1936 to 2000 we can see how these factors may shape the subsequent policy values of residents. The dataset “Correlates of State Policy” includes variables which also allow us to better understand the role of differences in policy design and implementation by controlling for variables that may moderate impact, such as the length of policy implementation (Soss) and differences in economic interest (Ansell).\nSources:\nAnsell, Ben. 2014. “The Political Economy of Ownership.” American Political Science Review 108(02):383{402.\nBoehmke, Frederick J., and Paul Skinner. 2012. “State Policy Innovativeness Revisited.” State Politics and Policy Quarterly, 12(3):303-29.\nBueno, Natalia and Nunes, Felipe and Zucco, Cesar, Making the bourgeoisie? Values, voice, and state-provided homeownership (January 7, 2022). SSRN.\nCaughey, Devin, and Christopher Warshaw. 2015. “The Dynamics of State Policy Liberalism, 1936–2014.” American Journal of Political Science, September. doi: 10.1111/ajps.12219.\nHolland, Alisha C. 2018. “Diminished Expectations: Redistributive preferences in truncated welfare states.” World Politics 70(4):555{594\nJacoby, William G., and Saundra K. Schneider. 2008. “A New Measure of Policy Spending Priorities in the American States.”\nJordan, Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing, MI: Institute for Public Policy and Social Research (IPPSR).\nRigby, Elizabeth and Gerald C. Wright. 2013. “Political Parties and Representation of the Poor in the American States.” American Journal of Political Science 57(3): 552-565.\nSoss, Joe. 1999. “Lessons of Welfare: Policy Design, Political Learning, and Political Action.” The American Political Science Review 93(2):363{380.\n\n\nHypothesis\nIncreased state intervention increases US state residents’ preference for future interventions in social and economic policy.\nThis study proposes to build on Bueno et al.’s exploration of the effects of state-provided home ownership on political values and policy preferences by exploring that relationship at the level of US states. Additionally, instead of focusing on a single social program, we will examine the cumulative effects of multiple policy interventions across 65 years in 50 US states. This will provide insights into the effect of public policy on value differences at the sub-national level and on different subgroups including program non-participants. We will be able to see how this relationship may vary according to state and population characteristics despite differences in policy design and implementation.\n\n\nDescriptive Statistics\nThis dataset is from the Correlates of State Policy Project by the Institute for Public Policy and Social Research at Michigan State University. The full dataset, which contains 928 variables and covers data from 1900 to 2016, draws from multiple sources including government agencies and peer-reviewed articles listed in the Sources section. Due to limited data coverage across all years however, this study will focus on the period from 1935 to 2000. We will examining the following 25 variables (listed with description and years available):\nIndependent-\nYear 1935 - 2000\nState 1935 - 2000\nEcondev - Did State adopt Strategic Planning for Economic Development? 1981 – 1992\nPldvpag - Did State adopt Planning/Development Agency? 1935 – 1978\nUrbrenen - Did State adopt Urban Renewal ? 1941 – 1952\nPollib_median - State Policy Liberalism Score – Median 1936 – 2014\nPolicypriorityscore - State Policy Priority Score - collective goods (e.g., education and highways) v particularized benefits (e.g., health care and welfare) 1982-2005\nPoptotal - Population Total 1900 – 2008\nPopfemale - Female Population 1994 – 2010\nNonwhite - Proportion of the population that is nonwhite 1974 - 2011\nSoc_capital_ma - Hawes et al. Weighted Moving Average Measure of Social Capital 1984 - 2011\nEvangelical_pop - Evangelical Population 1975 - 2013\nNewimmig - New Immigrant Green Card Holders 1988 – 2011\nPopdensity - Population Density 1975 – 1999\nGsp_q - Gross State Product Combined in Millions of 2016 Dollars 1963 – 2010\nGini_coef - Gini Coefficient 1917 - 2013\nHsdiploma - High School Diploma 1975 – 2006\nEducspend - State Education Spending 1975 – 2001\nNofelons - Number of Felons Ineligible to Vote 1980 – 2010\nCo2emissions - Total CO2 emissions from fossil-fuels (metric tons) 1960 – 2001\nIdeo - State Ideology Score 1976 – 2011\nDependent-\nVst_ec - Mean Economic Liberalism- All Voters 2000\nVst_soc - Mean Social Liberalism- All Voters 2000\nVavgec_low - Mean Economic Liberalism Score for Low Income Voting Citizens 2000\nVavgsoc_low - Mean Social Liberalism Score for Low Income Voting Citizens 2000\nReading in dataset\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\n\nstatedata <- read.csv(\"_data/correlatesofstatepolicyprojectv1_10.csv\")\n\n\nSpecifying variables\n\n\nCode\nstatedata1 = subset(statedata, select = c(policypriorityscore, econdev, pldvpag, urbrenen, year, state, poptotal, popfemale, nonwhite, soc_capital_ma, evangelical_pop, newimmig, popdensity, gsp_q, gini_coef, hsdiploma, educspend, nofelons, co2emissions, ideo, pollib_median,vst_ec, vst_soc, vavgec_low, vavgsoc_low))\n\n\nSpecifying date range\n\n\nCode\nsd <- subset(statedata1, year>1934 & year<2001, na.rm = TRUE ) \n\n\nDescriptive statistics\n\n\nCode\nstr(sd)\n\n\n'data.frame':   3366 obs. of  25 variables:\n $ policypriorityscore: num  NA NA NA NA NA NA NA NA NA NA ...\n $ econdev            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pldvpag            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urbrenen           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ year               : int  1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 ...\n $ state              : chr  \"Alaska\" \"Alaska\" \"Alaska\" \"Alaska\" ...\n $ poptotal           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popfemale          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nonwhite           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ soc_capital_ma     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ evangelical_pop    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ newimmig           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popdensity         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ gsp_q              : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gini_coef          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ hsdiploma          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ educspend          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nofelons           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ co2emissions       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ideo               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ pollib_median      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_ec             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_soc            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgec_low         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgsoc_low        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nCode\nglimpse(sd)\n\n\nRows: 3,366\nColumns: 25\n$ policypriorityscore <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ econdev             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pldvpag             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ urbrenen            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ year                <int> 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 19…\n$ state               <chr> \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", …\n$ poptotal            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popfemale           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nonwhite            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ soc_capital_ma      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ evangelical_pop     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ newimmig            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popdensity          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gsp_q               <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gini_coef           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hsdiploma           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ educspend           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nofelons            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ co2emissions        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ideo                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pollib_median       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_ec              <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_soc             <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgec_low          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgsoc_low         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nsummary(sd)\n\n\n policypriorityscore    econdev           pldvpag          urbrenen     \n Min.   :-0.2296     Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.0372     1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median : 0.0144     Median :0.00000   Median :1.0000   Median :1.0000  \n Mean   : 0.0093     Mean   :0.09364   Mean   :0.7703   Mean   :0.5327  \n 3rd Qu.: 0.0638     3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   : 0.1987     Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :2416        NA's   :66        NA's   :66       NA's   :66      \n      year         state              poptotal          popfemale       \n Min.   :1935   Length:3366        Min.   :  100000   Min.   :  236763  \n 1st Qu.:1951   Class :character   1st Qu.:  960954   1st Qu.:  645293  \n Median :1968   Mode  :character   Median : 2600000   Median : 1900000  \n Mean   :1968                      Mean   : 3892303   Mean   : 2692111  \n 3rd Qu.:1984                      3rd Qu.: 4700000   3rd Qu.: 3100000  \n Max.   :2000                      Max.   :34000000   Max.   :17000000  \n                                   NA's   :30         NA's   :3009      \n    nonwhite      soc_capital_ma    evangelical_pop    newimmig     \n Min.   :0.0048   Min.   :-2.9133   Min.   : 1.10   Min.   :   159  \n 1st Qu.:0.0785   1st Qu.:-0.4193   1st Qu.: 9.60   1st Qu.:  1518  \n Median :0.1360   Median : 0.2357   Median :14.10   Median :  3973  \n Mean   :0.1752   Mean   : 0.3108   Mean   :18.83   Mean   : 18447  \n 3rd Qu.:0.2586   3rd Qu.: 1.0615   3rd Qu.:26.00   3rd Qu.: 11424  \n Max.   :0.7130   Max.   : 3.0868   Max.   :74.00   Max.   :732735  \n NA's   :2016     NA's   :2550      NA's   :2066    NA's   :2703    \n   popdensity            gsp_q           gini_coef        hsdiploma    \n Min.   :   0.6496   Min.   :    993   Min.   :0.3215   Min.   : 0.00  \n 1st Qu.:  31.2611   1st Qu.:  12325   1st Qu.:0.4324   1st Qu.:73.90  \n Median :  85.3188   Median :  31568   Median :0.4667   Median :76.80  \n Mean   : 163.7982   Mean   :  74118   Mean   :0.4766   Mean   :75.94  \n 3rd Qu.: 165.7868   3rd Qu.:  83769   3rd Qu.:0.5147   3rd Qu.:80.80  \n Max.   :1082.7000   Max.   :1300000   Max.   :0.7172   Max.   :91.80  \n NA's   :2116        NA's   :1428      NA's   :48       NA's   :2054   \n   educspend          nofelons       co2emissions         ideo        \n Min.   :    0.0   Min.   :     0   Min.   :  4.00   Min.   :-0.5806  \n 1st Qu.:  816.2   1st Qu.:  4668   1st Qu.: 24.00   1st Qu.:-0.2157  \n Median : 1809.5   Median : 15733   Median : 60.00   Median :-0.1392  \n Mean   : 3421.9   Mean   : 34844   Mean   : 88.28   Mean   :-0.1364  \n 3rd Qu.: 4058.0   3rd Qu.: 41280   3rd Qu.:107.50   3rd Qu.:-0.0625  \n Max.   :35482.0   Max.   :499362   Max.   :669.00   Max.   : 0.4545  \n NA's   :2054      NA's   :2805     NA's   :1275     NA's   :2129     \n pollib_median          vst_ec          vst_soc         vavgec_low    \n Min.   :-2.32065   Min.   :-0.367   Min.   :-0.379   Min.   :-0.387  \n 1st Qu.:-0.66509   1st Qu.:-0.171   1st Qu.:-0.163   1st Qu.: 0.021  \n Median :-0.07600   Median :-0.094   Median :-0.001   Median : 0.108  \n Mean   :-0.01096   Mean   :-0.094   Mean   :-0.024   Mean   : 0.079  \n 3rd Qu.: 0.68865   3rd Qu.:-0.047   3rd Qu.: 0.106   3rd Qu.: 0.174  \n Max.   : 2.57199   Max.   : 0.147   Max.   : 0.357   Max.   : 0.290  \n NA's   :114        NA's   :3319     NA's   :3319     NA's   :3319    \n  vavgsoc_low    \n Min.   :-0.466  \n 1st Qu.:-0.200  \n Median :-0.093  \n Mean   :-0.073  \n 3rd Qu.: 0.052  \n Max.   : 0.377  \n NA's   :3319"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html",
    "href": "posts/KenDocekal_finalproject2.html",
    "title": "Final Project Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)\n#Expanded Data Description\n###Variables and missing data\nSpecifying variables for relevance and data availability\nWe transform co2 emission variable from raw tons to ton per state resident\nCreating data subset with new variables, removing pre-transformation and aggregate population measures\nDue to additional missing observations we will set the new data range as 1980 to 2000.\nNAs are reduced as listwise removal of observations that are missing completely at random analysis will be unbiased\nExcluding District of Columbia, Alaska, Hawaii to further reduce NAs\n###Visual description\nMost explanatory variables like econdev, pldvpag, and urbrenen are dummy variables.\nThe exception is policypriorityscore which is normally distributed.\nControl variables nonwhite, soc_capital_ma, evangelical_pop, pc_inc_ann, gini_coef, hsdiploma, and co2 are normally distributed\nResponse variables are also linear but st_ec and st_soc are only avaliable for the year 2000\nThe relationship between response st_ec and select explanatory and control variables in scatterplots:\nWith log(co2):\nSimilar relationship with boxplots:"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section",
    "href": "posts/KenDocekal_finalproject2.html#section",
    "title": "Final Project Part 2",
    "section": "1",
    "text": "1\nCross-validation with training and test set\n\n\nCode\nset.seed(1)\nsd2 %>% nrow() %>% multiply_by(0.7) %>% round() -> training_set_size\ntrain_indices <- sample(1:nrow(sd2), training_set_size)\ntrain <- sd2[train_indices,]\ntest <- sd2[-train_indices,]\n\n\n\n\nCode\nnrow(train)\n\n\n[1] 706\n\n\n\n\nCode\nnrow(test)\n\n\n[1] 302\n\n\nBase model with all variables\n\n\nCode\nbase <- lm(st_ec ~ . , data = train)\n\n\n\n\nCode\nplot(base)\n\n\nWarning: not plotting observations with leverage one:\n  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nError in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...): y is empty or has only NAs\n\n\n\n\n\nAttempt to remove state\n\n\nCode\nno_region <- lm(st_ec ~ . - state, data = train)\n\n\nWith interaction term\n\n\nCode\nia <- lm(st_ec ~ . + pc_inc_ann*hsdiploma, data = train)\n\n\n\n\nCode\nplot(ia)\n\n\nWarning: not plotting observations with leverage one:\n  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nError in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...): y is empty or has only NAs\n\n\n\n\n\n\n\nCode\nmean((predict(base, test) - test$st_ec)^2) -> MSE_base\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(no_region, test) - test$st_ec)^2) -> MSE_no_region\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(ia, test) - test$st_ec)^2) -> MSE_ia\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming\n\n\n\n\nCode\nmean((predict(ia, test) - test$st_ec)^2) -> MSE_ia\n\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor state has new levels Alabama, Arizona, Delaware, Georgia, Idaho, Iowa, Michigan, Nebraska, Pennsylvania, Utah, Wisconsin, Wyoming"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section-1",
    "href": "posts/KenDocekal_finalproject2.html#section-1",
    "title": "Final Project Part 2",
    "section": "2",
    "text": "2\nBackward elimination\n\n\nCode\nlm(st_ec ~ ., data = sd2) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ ., data = sd2)\n\nResiduals:\nALL 47 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (14 not defined because of singularities)\n                     Estimate Std. Error t value Pr(>|t|)\n(Intercept)         -0.038403        NaN     NaN      NaN\nstateArizona        -0.012308        NaN     NaN      NaN\nstateArkansas        0.021562        NaN     NaN      NaN\nstateCalifornia      0.031151        NaN     NaN      NaN\nstateColorado       -0.080098        NaN     NaN      NaN\nstateConnecticut     0.131448        NaN     NaN      NaN\nstateDelaware        0.014173        NaN     NaN      NaN\nstateFlorida         0.049612        NaN     NaN      NaN\nstateGeorgia        -0.067636        NaN     NaN      NaN\nstateIdaho          -0.148045        NaN     NaN      NaN\nstateIllinois        0.062778        NaN     NaN      NaN\nstateIndiana        -0.036880        NaN     NaN      NaN\nstateIowa           -0.132971        NaN     NaN      NaN\nstateKansas         -0.071066        NaN     NaN      NaN\nstateKentucky        0.054573        NaN     NaN      NaN\nstateLouisiana       0.021910        NaN     NaN      NaN\nstateMaine           0.147176        NaN     NaN      NaN\nstateMaryland        0.145870        NaN     NaN      NaN\nstateMassachusetts   0.221486        NaN     NaN      NaN\nstateMichigan        0.037647        NaN     NaN      NaN\nstateMinnesota      -0.053475        NaN     NaN      NaN\nstateMississippi     0.021773        NaN     NaN      NaN\nstateMissouri       -0.080424        NaN     NaN      NaN\nstateMontana         0.044836        NaN     NaN      NaN\nstateNevada         -0.018206        NaN     NaN      NaN\nstateNew Hampshire   0.103719        NaN     NaN      NaN\nstateNew Jersey      0.151180        NaN     NaN      NaN\nstateNew Mexico      0.021713        NaN     NaN      NaN\nstateNew York        0.230023        NaN     NaN      NaN\nstateNorth Carolina  0.057222        NaN     NaN      NaN\nstateNorth Dakota   -0.119446        NaN     NaN      NaN\nstateOhio            0.019121        NaN     NaN      NaN\nstateOklahoma       -0.043207        NaN     NaN      NaN\nstateOregon          0.056329        NaN     NaN      NaN\nstatePennsylvania    0.126404        NaN     NaN      NaN\nstateRhode Island    0.187720        NaN     NaN      NaN\nstateSouth Carolina -0.001439        NaN     NaN      NaN\nstateSouth Dakota   -0.176105        NaN     NaN      NaN\nstateTennessee       0.032912        NaN     NaN      NaN\nstateTexas          -0.067259        NaN     NaN      NaN\nstateUtah           -0.090002        NaN     NaN      NaN\nstateVermont         0.208597        NaN     NaN      NaN\nstateVirginia        0.062810        NaN     NaN      NaN\nstateWashington     -0.006842        NaN     NaN      NaN\nstateWest Virginia   0.128481        NaN     NaN      NaN\nstateWisconsin       0.005363        NaN     NaN      NaN\nstateWyoming        -0.127431        NaN     NaN      NaN\nyear                       NA         NA      NA       NA\npolicypriorityscore        NA         NA      NA       NA\necondev                    NA         NA      NA       NA\npldvpag                    NA         NA      NA       NA\nurbrenen                   NA         NA      NA       NA\nnonwhite                   NA         NA      NA       NA\nsoc_capital_ma             NA         NA      NA       NA\nevangelical_pop            NA         NA      NA       NA\npc_inc_ann                 NA         NA      NA       NA\ngini_coef                  NA         NA      NA       NA\nhsdiploma                  NA         NA      NA       NA\nco2                        NA         NA      NA       NA\nideo                       NA         NA      NA       NA\nst_soc                     NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 46 and 0 DF,  p-value: NA\n\n\n\n\nCode\nsd3 = subset(sd2, select = c(year, policypriorityscore, econdev, pldvpag, urbrenen, nonwhite, soc_capital_ma, evangelical_pop, pc_inc_ann, gini_coef, hsdiploma, co2, ideo,st_ec, st_soc))\n\n\n\n\nCode\nlm(st_ec ~ ., data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ ., data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.100478 -0.032490 -0.005094  0.039044  0.098876 \n\nCoefficients: (1 not defined because of singularities)\n                      Estimate Std. Error t value Pr(>|t|)   \n(Intercept)          1.220e+00  3.456e-01   3.531  0.00125 **\nyear                        NA         NA      NA       NA   \npolicypriorityscore -4.394e-01  1.866e-01  -2.355  0.02462 * \necondev             -1.698e-02  1.804e-02  -0.941  0.35331   \npldvpag              3.069e-02  3.116e-02   0.985  0.33179   \nurbrenen             1.308e-02  2.683e-02   0.488  0.62906   \nnonwhite            -2.035e-01  1.042e-01  -1.953  0.05929 . \nsoc_capital_ma      -3.491e-02  2.629e-02  -1.328  0.19324   \nevangelical_pop     -7.226e-04  9.797e-04  -0.738  0.46601   \npc_inc_ann          -2.421e-06  4.374e-06  -0.554  0.58365   \ngini_coef           -4.285e-01  4.835e-01  -0.886  0.38192   \nhsdiploma           -1.004e-02  3.590e-03  -2.798  0.00851 **\nco2                 -1.521e+02  7.283e+02  -0.209  0.83581   \nideo                 1.505e-01  1.641e-01   0.917  0.36572   \nst_soc               3.714e-01  1.279e-01   2.905  0.00651 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05653 on 33 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7675,    Adjusted R-squared:  0.6759 \nF-statistic: 8.378 on 13 and 33 DF,  p-value: 4.133e-07\n\n\nInclusion of policypriorityscore, nonwhite, hsdiploma, and st_soc only has the highest adjusted R squared\n\n\nCode\nlm(st_ec ~ .-econdev -pldvpag -urbrenen -soc_capital_ma -evangelical_pop -pc_inc_ann -gini_coef -co2 -ideo , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ . - econdev - pldvpag - urbrenen - soc_capital_ma - \n    evangelical_pop - pc_inc_ann - gini_coef - co2 - ideo, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.108606 -0.039738 -0.002334  0.031586  0.101660 \n\nCoefficients: (1 not defined because of singularities)\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          1.174528   0.267140   4.397 7.35e-05 ***\nyear                       NA         NA      NA       NA    \npolicypriorityscore -0.585220   0.136907  -4.275 0.000108 ***\nnonwhite            -0.206434   0.076446  -2.700 0.009941 ** \nhsdiploma           -0.013410   0.003006  -4.461 6.00e-05 ***\nst_soc               0.360374   0.054152   6.655 4.54e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05607 on 42 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7089,    Adjusted R-squared:  0.6811 \nF-statistic: 25.57 on 4 and 42 DF,  p-value: 8.847e-11"
  },
  {
    "objectID": "posts/KenDocekal_finalproject2.html#section-2",
    "href": "posts/KenDocekal_finalproject2.html#section-2",
    "title": "Final Project Part 2",
    "section": "3",
    "text": "3\nForward selection\n\n\nCode\nlm(st_ec ~ policypriorityscore , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.162156 -0.054892  0.003181  0.042129  0.235594 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.02663    0.01155  -2.305   0.0258 *  \npolicypriorityscore -0.92890    0.17092  -5.435 2.14e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.078 on 45 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.3963,    Adjusted R-squared:  0.3829 \nF-statistic: 29.54 on 1 and 45 DF,  p-value: 2.136e-06\n\n\n\n\nCode\nlm(st_ec ~ policypriorityscore + econdev, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + econdev, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.167145 -0.058159  0.008671  0.040615  0.230549 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.02175    0.01595  -1.364    0.180    \npolicypriorityscore -0.92496    0.17268  -5.357 2.95e-06 ***\necondev             -0.01033    0.02304  -0.448    0.656    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0787 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.399, Adjusted R-squared:  0.3717 \nF-statistic: 14.61 on 2 and 44 DF,  p-value: 1.364e-05\n\n\n\n\nCode\nlm(st_ec ~ policypriorityscore + pldvpag, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + pldvpag, data = sd3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13585 -0.05871  0.00821  0.03793  0.22754 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -0.07464    0.03463  -2.155   0.0367 *  \npolicypriorityscore -0.88617    0.17126  -5.175 5.41e-06 ***\npldvpag              0.05428    0.03698   1.468   0.1492    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07702 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.4245,    Adjusted R-squared:  0.3983 \nF-statistic: 16.23 on 2 and 44 DF,  p-value: 5.267e-06\n\n\nEvangelical_pop is does not increase p-value\n\n\nCode\nlm(st_ec ~ policypriorityscore + evangelical_pop, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + evangelical_pop, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.175633 -0.037558  0.000244  0.049656  0.188991 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          0.0217794  0.0177642   1.226   0.2267    \npolicypriorityscore -0.7956940  0.1591509  -5.000 9.65e-06 ***\nevangelical_pop     -0.0025410  0.0007552  -3.365   0.0016 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07035 on 44 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.5198,    Adjusted R-squared:  0.498 \nF-statistic: 23.82 on 2 and 44 DF,  p-value: 9.786e-08\n\n\nAll other additions increase p-value\n\n\nCode\nlm(st_ec ~ policypriorityscore + evangelical_pop + pc_inc_ann, data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + evangelical_pop + \n    pc_inc_ann, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.173379 -0.041717 -0.000328  0.051878  0.191366 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         -7.023e-03  9.369e-02  -0.075   0.9406    \npolicypriorityscore -7.852e-01  1.643e-01  -4.780 2.07e-05 ***\nevangelical_pop     -2.388e-03  9.052e-04  -2.639   0.0115 *  \npc_inc_ann           9.061e-07  2.893e-06   0.313   0.7556    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07108 on 43 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.5209,    Adjusted R-squared:  0.4875 \nF-statistic: 15.59 on 3 and 43 DF,  p-value: 5.269e-07\n\n\n##4\nLog-linear transformation\n\n\nCode\nlm(log(st_ec) ~ .-econdev -pldvpag -urbrenen -soc_capital_ma -evangelical_pop -pc_inc_ann -gini_coef -co2 -ideo , data = sd3) |> summary()\n\n\nWarning in log(st_ec): NaNs produced\n\n\n\nCall:\nlm(formula = log(st_ec) ~ . - econdev - pldvpag - urbrenen - \n    soc_capital_ma - evangelical_pop - pc_inc_ann - gini_coef - \n    co2 - ideo, data = sd3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9592 -0.4953 -0.1379  0.5761  1.1909 \n\nCoefficients: (1 not defined because of singularities)\n                    Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         10.00268    6.53823   1.530  0.15001   \nyear                      NA         NA      NA       NA   \npolicypriorityscore -4.83181    3.11076  -1.553  0.14436   \nnonwhite            -2.51193    1.63967  -1.532  0.14950   \nhsdiploma           -0.15322    0.07597  -2.017  0.06484 . \nst_soc               4.59439    1.49963   3.064  0.00906 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7851 on 13 degrees of freedom\n  (990 observations deleted due to missingness)\nMultiple R-squared:  0.5959,    Adjusted R-squared:  0.4715 \nF-statistic: 4.792 on 4 and 13 DF,  p-value: 0.0135\n\n\n##5\nLinear-log\n\n\nCode\nlm(st_ec ~ log(policypriorityscore) + nonwhite + hsdiploma + st_soc , data = sd3) |> summary()\n\n\nWarning in log(policypriorityscore): NaNs produced\n\n\n\nCall:\nlm(formula = st_ec ~ log(policypriorityscore) + nonwhite + hsdiploma + \n    st_soc, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.115600 -0.050721 -0.000798  0.040976  0.097815 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)   \n(Intercept)               0.990308   0.600184   1.650  0.11729   \nlog(policypriorityscore) -0.006349   0.016818  -0.377  0.71048   \nnonwhite                 -0.182903   0.165846  -1.103  0.28546   \nhsdiploma                -0.011932   0.006463  -1.846  0.08233 . \nst_soc                    0.387463   0.100425   3.858  0.00126 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06913 on 17 degrees of freedom\n  (986 observations deleted due to missingness)\nMultiple R-squared:  0.4971,    Adjusted R-squared:  0.3788 \nF-statistic: 4.201 on 4 and 17 DF,  p-value: 0.01516\n\n\nLinear-log for nonwhite only results in highest adjusted R-squared\n\n\nCode\nlm(st_ec ~ policypriorityscore + log(nonwhite) + hsdiploma + st_soc , data = sd3) |> summary()\n\n\n\nCall:\nlm(formula = st_ec ~ policypriorityscore + log(nonwhite) + hsdiploma + \n    st_soc, data = sd3)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.105985 -0.038466  0.001818  0.030985  0.099538 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          0.99820    0.23277   4.288 0.000103 ***\npolicypriorityscore -0.58543    0.13616  -4.300 9.96e-05 ***\nlog(nonwhite)       -0.03483    0.01247  -2.794 0.007814 ** \nhsdiploma           -0.01259    0.00282  -4.464 5.95e-05 ***\nst_soc               0.34288    0.05219   6.570 6.02e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05578 on 42 degrees of freedom\n  (961 observations deleted due to missingness)\nMultiple R-squared:  0.7119,    Adjusted R-squared:  0.6844 \nF-statistic: 25.94 on 4 and 42 DF,  p-value: 7.146e-11\n\n\n##6\nTwo Period Panel data comparing 1990 and 2000\n\n\nCode\nsd1990 <- subset(sd2, year = 1990, na.rm = TRUE ) \n\nsd2000 <- subset(sd2, year = 1990, na.rm = TRUE ) \n\n\n\n\nCode\ndiff_ideo <- sd2000$ideo - sd1990$ideo\ndiff_econdev<- sd2000$econdev - sd1990$econdev\n\n\n\n\nCode\ninstall.packages(\"AER\")\n\n\nInstalling package into 'C:/Users/srika/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n\nCode\ninstall.packages(\"plm\")\n\n\nInstalling package into 'C:/Users/srika/AppData/Local/R/win-library/4.2'\n(as 'lib' is unspecified)\n\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n\n\n\nCode\nlibrary(AER)\n\n\nError in library(AER): there is no package called 'AER'\n\n\nCode\nlibrary(plm)\n\n\nError in library(plm): there is no package called 'plm'\n\n\nEstimating a regression using differenced data\n\n\nCode\nideo_mod <- lm(diff_ideo ~ diff_econdev)\n\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\nError in coeftest(ideo_mod, vcov = vcovHC, type = \"HC1\"): could not find function \"coeftest\"\n\n\n\n\nCode\n# plot the differenced data\nplot(x = diff_econdev, \n     y = diff_ideo, \n     xlab = \"Change in econdev\",\n     ylab = \"Change in ideo\",\n     main = \"Changes in Economic Development Agency Presence and Ideology in 1990-2000\",\n     xlim = c(-0.6, 0.6),\n     ylim = c(-1.5, 1),\n     pch = 20, \n     col = \"steelblue\")\n\n\n\n\n\nCode\n# add the regression line to plot\nabline(ideo_mod, lwd = 1.5)\n\n\nError in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): 'a' and 'b' must be finite\n\n\n##7\nEstimating a combined time and entity fixed effects regression model\n\n\nCode\nideo_lm_mod <- lm(ideo ~ econdev + state + year - 1, data = sd2)\nideo_lm_mod\n\n\n\nCall:\nlm(formula = ideo ~ econdev + state + year - 1, data = sd2)\n\nCoefficients:\n            econdev         stateAlabama         stateArizona  \n          0.0001636           -1.2154481           -1.1040309  \n      stateArkansas      stateCalifornia        stateColorado  \n         -1.1651876           -1.0136333           -1.0488766  \n   stateConnecticut        stateDelaware         stateFlorida  \n         -0.9902822           -1.0545878           -1.0874490  \n       stateGeorgia           stateIdaho        stateIllinois  \n         -1.1408023           -1.2246658           -1.0367910  \n       stateIndiana            stateIowa          stateKansas  \n         -1.1247901           -1.1017004           -1.1408348  \n      stateKentucky       stateLouisiana           stateMaine  \n         -1.1019753           -1.1939602           -1.0447755  \n      stateMaryland   stateMassachusetts        stateMichigan  \n         -0.9994727           -0.9369923           -1.0582279  \n     stateMinnesota     stateMississippi        stateMissouri  \n         -1.0541859           -1.2190042           -1.1057527  \n       stateMontana        stateNebraska          stateNevada  \n         -1.1091557           -1.1442449           -1.0049900  \n stateNew Hampshire      stateNew Jersey      stateNew Mexico  \n         -1.0215205           -0.9929828           -1.1021508  \n      stateNew York  stateNorth Carolina    stateNorth Dakota  \n         -0.9780673           -1.1645759           -1.1695092  \n          stateOhio        stateOklahoma          stateOregon  \n         -1.0620687           -1.2172240           -1.0312511  \n  statePennsylvania    stateRhode Island  stateSouth Carolina  \n         -1.0576267           -0.9651297           -1.1801724  \n  stateSouth Dakota       stateTennessee           stateTexas  \n         -1.2298914           -1.1428635           -1.1739866  \n          stateUtah         stateVermont        stateVirginia  \n         -1.1972355           -0.9604917           -1.1262503  \n    stateWashington   stateWest Virginia       stateWisconsin  \n         -1.0045938           -1.0731507           -1.0857281  \n       stateWyoming                 year  \n         -1.1353952            0.0004764  \n\n\n\n\nCode\nideo_mod <- plm(ideo ~ econdev, \n                      data = sd2,\n                      index = c(\"state\", \"year\"), \n                      model = \"within\", \n                      effect = \"twoways\")\n\n\nError in plm(ideo ~ econdev, data = sd2, index = c(\"state\", \"year\"), model = \"within\", : could not find function \"plm\"\n\n\nCode\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\nError in coeftest(ideo_mod, vcov = vcovHC, type = \"HC1\"): could not find function \"coeftest\"\n\n\nChecking the class of state and year\n\n\nCode\nclass(sd2$state)\n\n\n[1] \"character\"\n\n\nCode\nclass(sd2$year)\n\n\n[1] \"integer\"\n\n\nChanging to factors\n\n\nCode\nstate1 <- as.factor(sd2$state)\n\nyear1 <- as.factor(sd2$year)\n\n\n\n\nCode\nideo_mod <- plm(ideo ~ econdev, \n                      data = sd2,\n                      index = c(\"state\", \"year\"), \n                      model = \"within\", \n                      effect = \"twoways\")\n\n\nError in plm(ideo ~ econdev, data = sd2, index = c(\"state\", \"year\"), model = \"within\", : could not find function \"plm\"\n\n\nCode\ncoeftest(ideo_mod, vcov = vcovHC, type = \"HC1\")\n\n\nError in coeftest(ideo_mod, vcov = vcovHC, type = \"HC1\"): could not find function \"coeftest\"\n\n\n\n\nCode\nclass(ideo_lm_mod)\n\n\n[1] \"lm\"\n\n\nResults in very high p-value\n\n\nCode\ncoeftest(ideo_lm_mod, vcov = vcovHC, type = \"HC1\")[1, ]\n\n\nError in coeftest(ideo_lm_mod, vcov = vcovHC, type = \"HC1\"): could not find function \"coeftest\""
  },
  {
    "objectID": "posts/KenDocekal_HW1.html",
    "href": "posts/KenDocekal_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a",
    "href": "posts/KenDocekal_HW1.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nRead in the data from the Excel file:\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b",
    "href": "posts/KenDocekal_HW1.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability distribution of the LungCap, Males and Females, in a box plot:\n\n\nCode\nboxplot(LungCapData$LungCap ~ LungCapData$Gender)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c",
    "href": "posts/KenDocekal_HW1.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLung capacities for smokers and non-smokers, mean and standard deviation:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(mean = mean(LungCap, na.rm = TRUE), sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     7.77  2.73\n2 yes    8.65  1.88\n\n\nResults seem to point to smokers having greater lung capacity which is odd and could indicate factors other than age are influencing lung capacity"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d",
    "href": "posts/KenDocekal_HW1.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”:\nage 13 and lower:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age <=13)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     6.36  2.21\n2 yes    7.20  1.58\n\n\nage 14 to 15:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 14:15)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     8.84 1.36 \n2 yes    8.91 0.865\n\n\nage 16 to 17:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 16:17)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no    10.4   1.73\n2 yes    9.60  1.41\n\n\nage 18 and over:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age >=18)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     11.1  1.56\n2 yes    10.5  1.25"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e",
    "href": "posts/KenDocekal_HW1.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nWhen looking at mean lung capacity of smokers versus non-smokers by age groups we can see lung capacity increasing consistently as age increases. For the two lowest age groups mean capacity is lower for non-smokers although the difference decreases as age increases; this trend is reversed from age 16 onwards as non-smokers overtake smokers in lung capacity. Across all age groups non-smokers also have a greater standard deviation in lung capacity compared to smokers with the age 13 and under non-smoker group having the greatest standard deviation. It is likely that the greater number of age 13 and under respondents is the reason why overall results mirror the distribution seen in the youngest age group."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f",
    "href": "posts/KenDocekal_HW1.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCovariance between lung capacity and age:\n\n\nCode\ncov(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 8.738289\n\n\nA positive covariance is shown which lets us know that as age increases lung capacity also increases.\nCorrelation between lung capacity and age:\n\n\nCode\ncor(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 0.8196749\n\n\nThe correlation coefficient is also positive; similar to the covariance this lets us know that there is a positive relationship between age and lung capacity. Additionally, since .819 is a relatively high score, as a score of 1 would indicate a perfect positive relationship, we know there is a strong relationship where a older respondent would be highly likely to have higher lung capacity and a younger respondent would likely have lower lung capacity."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a-1",
    "href": "posts/KenDocekal_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions:\nCreate data frame:\n\n\nCode\nconvictions<- c(0,1,2,3,4)\nprisoners<- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(convictions, prisoners)\n\ntibble(df)\n\n\n# A tibble: 5 × 2\n  convictions prisoners\n        <dbl>     <dbl>\n1           0       128\n2           1       434\n3           2       160\n4           3        64\n5           4        24\n\n\nProbability of exactly 2 prior convictions:\n\n\nCode\n160/sum(prisoners)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b-1",
    "href": "posts/KenDocekal_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability of fewer than 2 prior convictions (total # of prisoners with less than 2 prior convictions = 562):\n\n\nCode\n562/sum(prisoners)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c-1",
    "href": "posts/KenDocekal_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nProbability of 2 or fewer prior convictions (total # of prisoners with 2 or fewer prior convictions = 722):\n\n\nCode\n722/sum(prisoners)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d-1",
    "href": "posts/KenDocekal_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nProbability of more than 2 prior convictions (total # of prisoners with more than 2 prior convictions = 88):\n\n\nCode\n88/sum(prisoners)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e-1",
    "href": "posts/KenDocekal_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nThe expected value for the number of prior convictions (using the probability of observing each prisoner prior conviction group):\n\n\nCode\ncon1<- c(0,1,2,3,4)\npprob<- c(.158,.536,.198,.079,.028)\n\n\nsum(con1*pprob)\n\n\n[1] 1.281"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f-1",
    "href": "posts/KenDocekal_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nVariance and standard deviation for prior convictions:\n\n\nCode\nvar(prisoners)\n\n\n[1] 25948\n\n\nCode\nsd(prisoners)\n\n\n[1] 161.0838"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html",
    "href": "posts/KenDocekal_HW2.html",
    "title": "HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q1",
    "href": "posts/KenDocekal_HW2.html#q1",
    "title": "HW2",
    "section": "Q1",
    "text": "Q1\n90% Confidence Interval for Bypass:\n18.29 - 19.71\n\n\nCode\nn <- 539\nxbar <- 19 \ns <- 10\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 18.29029\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 19.70971\n\n\n90% Confidence Interval for Angiography:\n17.49 - 18.51\n\n\nCode\nn <- 847\nxbar <- 18 \ns <- 9\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 17.49078\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 18.50922\n\n\nThe confidence interval is narrower for Angiography - 1.02 difference, compared to Bypass - 1.42 difference."
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q2",
    "href": "posts/KenDocekal_HW2.html#q2",
    "title": "HW2",
    "section": "Q2",
    "text": "Q2\nThe proportion point estimate for adult Americans who believe that a college education is essential for success is .55, based on 567 out of the representative sample of 1031 adult Americans surveyed.\nA 95% confidence interval shows that in 95% of cases the observed mean proportion of adult Americans who believe that a college education is essential for success will be between 52% and 58%.\n\n\nCode\nprop.test(567,1031)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q3",
    "href": "posts/KenDocekal_HW2.html#q3",
    "title": "HW2",
    "section": "Q3",
    "text": "Q3\nBased on the range of 30 to 200 we can determine the standard deviation using s = (Maximum – Minimum)/4 resulting in s=42.5.With a 95% significance level we will use 1.96 for the z score. To find the minimum sample size needed - n, we solve for (Zscore*s/margin of error)^2. Our sample size needs to be at least 277.56.\n\n\nCode\nn <- ((1.96)*(42.5)/5)^2\n\nn\n\n\n[1] 277.5556"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q4",
    "href": "posts/KenDocekal_HW2.html#q4",
    "title": "HW2",
    "section": "Q4",
    "text": "Q4"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q5",
    "href": "posts/KenDocekal_HW2.html#q5",
    "title": "HW2",
    "section": "Q5",
    "text": "Q5"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q6",
    "href": "posts/KenDocekal_HW2.html#q6",
    "title": "HW2",
    "section": "Q6",
    "text": "Q6\nCreate a data frame with a column for tax values.\n\n\nCode\ngas_taxes <-  data.frame (first_column  = c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)) \n\n\n\n\nCode\ngas_taxes\n\n\n   first_column\n1         51.27\n2         47.43\n3         38.89\n4         41.95\n5         28.61\n6         41.29\n7         52.19\n8         49.48\n9         35.02\n10        48.13\n11        39.28\n12        54.41\n13        41.66\n14        30.28\n15        18.49\n16        38.72\n17        33.41\n18        45.02\n\n\nCode\nnames(gas_taxes) <- c(\"tax\")\n\n\nUsing a one sample t-test where null hypothesis is mean tax is 45 and alternative hypothesis is true mean is less than 45 we are able to reject the null hypothesis at the 95% confidence level. Results indicate that mean tax was 40.86 and 95% of all observations will find a mean tax less than 44.68; therefore, while a few cities at the upper end of the range had prices near 45 cents per gallon, this was not usual and the average tax per gallon of gas in the US in 2005 was less than 45 cents.\n\n\nCode\nt.test(gas_taxes$tax, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes$tax\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html",
    "href": "posts/KenDocekal_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q1",
    "href": "posts/KenDocekal_HW3.html#q1",
    "title": "HW3",
    "section": "Q1",
    "text": "Q1\nLoading in data\n\n\nCode\nlibrary(alr4) \n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\nlibrary(smss) \n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\n\n\nCode\ndata('UN11', package = 'alr4')\n\n\n\n\nCode\nhead(UN11)\n\n\n                region  group fertility   ppgdp lifeExpF pctUrban\nAfghanistan       Asia  other     5.968   499.0    49.49       23\nAlbania         Europe  other     1.525  3677.2    80.40       53\nAlgeria         Africa africa     2.142  4473.0    75.00       67\nAngola          Africa africa     5.135  4321.9    53.17       59\nAnguilla     Caribbean  other     2.000 13750.1    81.10      100\nArgentina   Latin Amer  other     2.172  9162.1    79.89       93"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q2",
    "href": "posts/KenDocekal_HW3.html#q2",
    "title": "HW3",
    "section": "Q2",
    "text": "Q2"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q3",
    "href": "posts/KenDocekal_HW3.html#q3",
    "title": "HW3",
    "section": "Q3",
    "text": "Q3\nLoading in water data.\n\n\nCode\ndata('water', package = 'alr4')\n\n\nLooking at the scatter plot matrix shows relationships between variables for year, precipitation at six Sierra Nevada mountain sites - APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE, and stream runoff volume near Bishop CA - BSAAM.\nFirst column shows precipitation then runoff (y-axis) by year(x-axis). We can see that for most sites precipitation has a somewhat wide distribution confined towards the lower end of the range with some outliners. OPRC precipitation is has greater spread while OPSLAKE and BSAAM show somewhat of a convex relationship.\nLooking at the last row comparing runoff (y-axis) to precipitation (x-axis); there is minimal correlation between precipitation from APMAM, APSAB, and APSLAKE and runoff levels but a strong positive linear correlation with precipitation from OPBPC, OPRC, and OPSLAKE sites. OPBPC, OPRC, and OPSLAKE sites’ greater correlation with BSAAM implies that these sites may be closer or more influential to stream runoff volume near Bishop CA.\nWhen focusing on the relationships between precipitation across sites there seems to be two groupings with high correlations; APMAM, APSAB, and APSLAKE all show fairly strong positive linear relationships with each other as do OPBPC, OPRC, and OPSLAKE. Across these groups of variables however the relationship is less clear and values are generally clustered among the lower values.This implies that sites based on these two groupings may share closer geographic proximity and are therefore more similarly affected by precipitation levels.\n\n\nCode\npairs(water)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q4",
    "href": "posts/KenDocekal_HW3.html#q4",
    "title": "HW3",
    "section": "Q4",
    "text": "Q4\nLoading in Rateprof data.\n\n\nCode\ndata('Rateprof', package = 'alr4')\n\n\nSpecifying rating variables - quality, helpfulness, clarity, easiness, raterInterest.\n\n\nCode\nRateprof1 = subset(Rateprof, select = c(quality, helpfulness, clarity, easiness, raterInterest))\n\nhead(Rateprof1)\n\n\n   quality helpfulness  clarity easiness raterInterest\n1 4.636364    4.636364 4.636364 4.818182      3.545455\n2 4.318182    4.545455 4.090909 4.363636      4.000000\n3 4.790698    4.720930 4.860465 4.604651      3.432432\n4 4.250000    4.458333 4.041667 2.791667      3.181818\n5 4.684211    4.684211 4.684211 4.473684      4.214286\n6 4.233333    4.266667 4.200000 4.533333      3.916667\n\n\nThe scatter plot matrix shows very strong positive linear relationships between quality, helpfulness, and clarity. Easiness is also positively correlated with these three variables but less strongly. raterInterest is even less strongly correlated with quality, helpfulness, clarity, and easiness (especially with easiness) but there still seems to be a slightly positive linear relationship.\n\n\nCode\npairs(Rateprof1)"
  },
  {
    "objectID": "posts/KenDocekal_HW3.html#q5",
    "href": "posts/KenDocekal_HW3.html#q5",
    "title": "HW3",
    "section": "Q5",
    "text": "Q5\nLoading in student.survey data.\n\n\nCode\ndata('student.survey', package = 'smss')\n\n\nReviewing variables; pi is political ideology, re is religiosity, hi is high school GPA, and tv is average hours of TV watching per week.\n\n\nCode\n?student.survey\n\n\nstarting httpd help server ... done\n\n\nCode\nview(student.survey)"
  },
  {
    "objectID": "posts/KenDocekal_HW4.html",
    "href": "posts/KenDocekal_HW4.html",
    "title": "HW4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(AER)\n\n\nError in library(AER): there is no package called 'AER'\n\n\nCode\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW4.html#q1",
    "href": "posts/KenDocekal_HW4.html#q1",
    "title": "HW4",
    "section": "Q1",
    "text": "Q1"
  },
  {
    "objectID": "posts/KenDocekal_HW4.html#q2",
    "href": "posts/KenDocekal_HW4.html#q2",
    "title": "HW4",
    "section": "Q2",
    "text": "Q2\n\n\nCode\nlibrary(alr4) \n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata('salary', package = 'alr4')\n\n\n\n\nCode\nsummary(salary)\n\n\n     degree      rank        sex          year            ysdeg      \n Masters:34   Asst :18   Male  :38   Min.   : 0.000   Min.   : 1.00  \n PhD    :18   Assoc:14   Female:14   1st Qu.: 3.000   1st Qu.: 6.75  \n              Prof :20               Median : 7.000   Median :15.50  \n                                     Mean   : 7.481   Mean   :16.12  \n                                     3rd Qu.:11.000   3rd Qu.:23.25  \n                                     Max.   :25.000   Max.   :35.00  \n     salary     \n Min.   :15000  \n 1st Qu.:18247  \n Median :23719  \n Mean   :23798  \n 3rd Qu.:27259  \n Max.   :38045  \n\n\nCode\ntibble(salary)\n\n\n# A tibble: 52 × 6\n   degree  rank  sex     year ysdeg salary\n   <fct>   <fct> <fct>  <int> <int>  <int>\n 1 Masters Prof  Male      25    35  36350\n 2 Masters Prof  Male      13    22  35350\n 3 Masters Prof  Male      10    23  28200\n 4 Masters Prof  Female     7    27  26775\n 5 PhD     Prof  Male      19    30  33696\n 6 Masters Prof  Male      16    21  28516\n 7 PhD     Prof  Female     0    32  24900\n 8 Masters Prof  Male      16    18  31909\n 9 PhD     Prof  Male      13    30  31850\n10 PhD     Prof  Male      13    31  32850\n# … with 42 more rows"
  },
  {
    "objectID": "posts/KenDocekal_HW4.html#q3",
    "href": "posts/KenDocekal_HW4.html#q3",
    "title": "HW4",
    "section": "Q3",
    "text": "Q3\n\n\nCode\nlibrary(smss) \n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\ndata('house.selling.price', package = 'smss')\n\n\n\n\nCode\nsummary(house.selling.price)\n\n\n      case            Taxes           Beds       Baths           New      \n Min.   :  1.00   Min.   :  20   Min.   :2   Min.   :1.00   Min.   :0.00  \n 1st Qu.: 25.75   1st Qu.:1178   1st Qu.:3   1st Qu.:2.00   1st Qu.:0.00  \n Median : 50.50   Median :1614   Median :3   Median :2.00   Median :0.00  \n Mean   : 50.50   Mean   :1908   Mean   :3   Mean   :1.96   Mean   :0.11  \n 3rd Qu.: 75.25   3rd Qu.:2238   3rd Qu.:3   3rd Qu.:2.00   3rd Qu.:0.00  \n Max.   :100.00   Max.   :6627   Max.   :5   Max.   :4.00   Max.   :1.00  \n     Price             Size     \n Min.   : 21000   Min.   : 580  \n 1st Qu.: 93225   1st Qu.:1215  \n Median :132600   Median :1474  \n Mean   :155331   Mean   :1629  \n 3rd Qu.:169625   3rd Qu.:1865  \n Max.   :587000   Max.   :4050  \n\n\nCode\ntibble(house.selling.price)\n\n\n# A tibble: 100 × 7\n    case Taxes  Beds Baths   New  Price  Size\n   <int> <int> <int> <int> <int>  <int> <int>\n 1     1  3104     4     2     0 279900  2048\n 2     2  1173     2     1     0 146500   912\n 3     3  3076     4     2     0 237700  1654\n 4     4  1608     3     2     0 200000  2068\n 5     5  1454     3     3     0 159900  1477\n 6     6  2997     3     2     1 499900  3153\n 7     7  4054     3     2     0 265500  1355\n 8     8  3002     3     2     1 289900  2075\n 9     9  6627     5     4     0 587000  3990\n10    10   320     3     2     0  70000  1160\n# … with 90 more rows"
  },
  {
    "objectID": "posts/KenDocekal_HW5.html",
    "href": "posts/KenDocekal_HW5.html",
    "title": "HW5",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(AER)\n\n\nError in library(AER): there is no package called 'AER'\n\n\nCode\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW5.html#q1",
    "href": "posts/KenDocekal_HW5.html#q1",
    "title": "HW5",
    "section": "Q1",
    "text": "Q1\n\n\nCode\nlibrary(smss) \n\n\nWarning: package 'smss' was built under R version 4.2.2\n\n\nCode\ndata('house.selling.price.2', package = 'smss')\n\n\n\n\nCode\nsummary(house.selling.price.2)\n\n\n       P                S              Be              Ba       \n Min.   : 17.50   Min.   :0.40   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 72.90   1st Qu.:1.33   1st Qu.:3.000   1st Qu.:2.000  \n Median : 96.00   Median :1.57   Median :3.000   Median :2.000  \n Mean   : 99.53   Mean   :1.65   Mean   :3.183   Mean   :1.957  \n 3rd Qu.:115.00   3rd Qu.:1.98   3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :309.40   Max.   :3.85   Max.   :5.000   Max.   :3.000  \n      New        \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3011  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\nCode\ntibble(house.selling.price.2)\n\n\n# A tibble: 93 × 5\n       P     S    Be    Ba   New\n   <dbl> <dbl> <int> <int> <int>\n 1  48.5  1.1      3     1     0\n 2  55    1.01     3     2     0\n 3  68    1.45     3     2     0\n 4 137    2.4      3     3     0\n 5 309.   3.3      4     3     1\n 6  17.5  0.4      1     1     0\n 7  19.6  1.28     3     1     0\n 8  24.5  0.74     3     1     0\n 9  34.8  0.78     2     1     0\n10  32    0.97     3     1     0\n# … with 83 more rows\n\n\n\n\nCode\ncor(house.selling.price.2)\n\n\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n\n\nCode\nlm(P ~ S + Be + Ba + New, data = house.selling.price.2) |> summary()\n\n\n\nCall:\nlm(formula = P ~ S + Be + Ba + New, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,    Adjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/KenDocekal_HW5.html#q2",
    "href": "posts/KenDocekal_HW5.html#q2",
    "title": "HW5",
    "section": "Q2",
    "text": "Q2\n\n\nCode\nsummary(trees)\n\n\n     Girth           Height       Volume     \n Min.   : 8.30   Min.   :63   Min.   :10.20  \n 1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \n Median :12.90   Median :76   Median :24.20  \n Mean   :13.25   Mean   :76   Mean   :30.17  \n 3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \n Max.   :20.60   Max.   :87   Max.   :77.00  \n\n\nCode\ntibble(trees)\n\n\n# A tibble: 31 × 3\n   Girth Height Volume\n   <dbl>  <dbl>  <dbl>\n 1   8.3     70   10.3\n 2   8.6     65   10.3\n 3   8.8     63   10.2\n 4  10.5     72   16.4\n 5  10.7     81   18.8\n 6  10.8     83   19.7\n 7  11       66   15.6\n 8  11       75   18.2\n 9  11.1     80   22.6\n10  11.2     75   19.9\n# … with 21 more rows"
  },
  {
    "objectID": "posts/KenDocekal_HW5.html#q3",
    "href": "posts/KenDocekal_HW5.html#q3",
    "title": "HW5",
    "section": "Q3",
    "text": "Q3\n\n\nCode\nlibrary(alr4) \n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\nCode\ndata('florida', package = 'alr4')\n\nsummary(florida)\n\n\n      Gore             Bush           Buchanan     \n Min.   :   788   Min.   :  1316   Min.   :   9.0  \n 1st Qu.:  3055   1st Qu.:  4746   1st Qu.:  46.5  \n Median : 14152   Median : 20196   Median : 114.0  \n Mean   : 43341   Mean   : 43356   Mean   : 258.5  \n 3rd Qu.: 45974   3rd Qu.: 56542   3rd Qu.: 285.5  \n Max.   :386518   Max.   :289456   Max.   :3407.0  \n\n\nCode\ntibble(florida)\n\n\n# A tibble: 67 × 3\n     Gore   Bush Buchanan\n    <int>  <int>    <int>\n 1  47300  34062      262\n 2   2392   5610       73\n 3  18850  38637      248\n 4   3072   5413       65\n 5  97318 115185      570\n 6 386518 177279      789\n 7   2155   2873       90\n 8  29641  35419      182\n 9  25501  29744      270\n10  14630  41745      186\n# … with 57 more rows"
  },
  {
    "objectID": "posts/KPopiela_FinalP2.html",
    "href": "posts/KPopiela_FinalP2.html",
    "title": "KPopiela_Finalp2",
    "section": "",
    "text": "library(poliscidata)\n\nRegistered S3 method overwritten by 'gdata':\n  method         from  \n  reorder.factor gplots\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stats)\nlibrary(ggplot2)\n##Intro and Background Info\nI’ve had more time to think about my final, and I’ve decided to simplify my research topic. Rather than look at ethnic conflict, I’m going to look at how internet usage affects a people’s confidence in their country (institutions and governance). In order to focus on this relationship, I will be using the ‘world’ data set in the ‘poliscidata’ package, and I will be using the following variables: ‘country’, ‘unnetuse’ (internet usage per 100 people), confidence (population’s confidence in their country’s institutions; scaled out of 100), and ‘effectiveness’ (government effectiveness scale).\nBelow is a general presentation of the data I will be using, filtered down as I described.\nThe internet and media consumption play a massive role in politics now; at this point, it’s pretty much expected to find out what is happening in the world on the internet (newspapers, social media, etc.). And as we’ve seen since the 2016 election, the internet also plays a substantial role in domestic politics, whether it be through neutral journalism, activism, whistle-blowing, or misinformation. It is both good and bad, and as Stanford professor Evgeny Morozov writes, the internet’s impact “will depend on individual conditions” such as a given country’s political atmosphere (peaceful, volatile, conservative, liberal, etc.), ease of access to the internet, and how united a population is either in favor or against their current government (“The Internet, Politics and the Politics of Internet Debate”). So, what /is/ the relationship between internet use and domestic politics?\nBased on my personal and academic experience, I hypothesize that while internet usage definitely does affect people’s confidence in their government, I do not think that it automatically hinders government effectiveness. If a certain country’s subjects have extremely little confidence in its government, the government would have no legitimacy (and would therefore be ineffective); a government’s power largely comes from its subjects’ willingness to accept it. However, there are instances where a country’s population has little confidence in its government with no effect on its efficacy. But, again, for this project I will be focusing on how internet usage impacts confidence in a given regime.\nNow to transcribe my research question: Does internet usage (social media, news, etc.) have an impact on people’s confidence in their government (‘confidence’)? If so, is said government more or less effective in its duties?\nFirst, lets look at some summary statistics."
  },
  {
    "objectID": "posts/KPopiela_FinalP2.html#hypothesis-testing-analysis",
    "href": "posts/KPopiela_FinalP2.html#hypothesis-testing-analysis",
    "title": "KPopiela_Finalp2",
    "section": "Hypothesis Testing, Analysis",
    "text": "Hypothesis Testing, Analysis\nI will start with some preliminary graphs/visualizations.\n\nggplot(data=world_filter, aes(x=unnetuse,y=confidence, color=regime_type3)) + geom_point() \n\nWarning: Removed 100 rows containing missing values (geom_point).\n\n\n\n\n\nThis scatterplot, which measures the relationship between internet usage and popular confidence in government, has no obvious relationship or correlation. The points are grouped by color based on regime type, but there is no notable increase or decrease in the points’ location as a variable increases or decreases. The vast majority of values are located in the middle of the graph.\n\nggplot(data=world_filter, aes(x=confidence,y=effectiveness, color=regime_type3)) + geom_point() \n\nWarning: Removed 99 rows containing missing values (geom_point).\n\n\n\n\n\nWhile there still isn’t a traditional positive or negative relationship between ‘confidence’ and ‘effectiveness’, it is noteworthy that the majority of values are in the center of the graph. They have a wide range, but they are mostly within 25 and 75 on the x-axis, which does make sense giving that the 1st and 3rd quantiles are included in the initial summary statistics.\n\nggplot(world_filter, aes(x=unnetuse,y=confidence,color=regime_type3)) + geom_point() + stat_smooth(method=\"lm\",col=\"blue\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 100 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 100 rows containing missing values (geom_point).\n\n\n\n\n\nThis visualization is a recreation of the first scatterplot, but with a multiple regression line. The line, at first glance, is straight, but upon closer inspection it ever so slightly slopes down toward the right. This indicates a slightly negative relationship between internet usage (x) and confidence in government institutions (y).\n\nggplot(world_filter, aes(x=unnetuse,y=effectiveness,color=regime_type3)) + geom_point() + stat_smooth(method=\"lm\",col=\"blue\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 15 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 15 rows containing missing values (geom_point).\n\n\n\n\n\nThe above scatterplot, however, depicts a very clear positive relationship between variables ‘unnetuse’ (x) and ‘effectiveness’ (y). This is indicative of a strong relationship between internet use and government effectiveness. There is what appears to be a heavier concentration of points below 50 on the x-axis, but the general trend is pretty clear that government effectiveness increases as nationals’ internet use does. Although ‘unnetuse’ comes from a 2008 survey, I would argue that this trend still holds up; especially due to COVID-19, the main way that politicians engage with their constituents is on the internet (social media). There is not really a culture of “visit your politician and talk to them” culture anymore, the new norm is for sucht things to be done over the phone, over email, or in some form, over the internet.\nMy next step is to go into modeling. I’m going to start with a linear regression model. ‘confidence’ is the dependent variable, and ‘unnetuse’, ‘effectiveness’, and ‘regime_type3’ are all independent.\n\nconfidence_lm <- lm(confidence~unnetuse+effectiveness+regime_type3, data=world_filter)\nconfidence_lm\n\n\nCall:\nlm(formula = confidence ~ unnetuse + effectiveness + regime_type3, \n    data = world_filter)\n\nCoefficients:\n                    (Intercept)                         unnetuse  \n                        43.0725                          -0.5805  \n                  effectiveness  regime_type3Parliamentary democ  \n                         0.7574                         -15.4561  \n regime_type3Presidential democ  \n                       -19.0101  \n\n\nAnd here is a visualization of the above values.\n\nplot(fitted(confidence_lm))\nabline(h=50, lty=2)\n\n\n\n\n\nsummary(confidence_lm)\n\n\nCall:\nlm(formula = confidence ~ unnetuse + effectiveness + regime_type3, \n    data = world_filter)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.097  -6.527   0.075   7.993  40.399 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      43.0725     5.6175   7.668 4.82e-10 ***\nunnetuse                         -0.5805     0.1561  -3.719 0.000498 ***\neffectiveness                     0.7574     0.1700   4.455 4.61e-05 ***\nregime_type3Parliamentary democ -15.4561     6.3577  -2.431 0.018608 *  \nregime_type3Presidential democ  -19.0101     6.1730  -3.080 0.003336 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.37 on 51 degrees of freedom\n  (111 observations deleted due to missingness)\nMultiple R-squared:  0.3376,    Adjusted R-squared:  0.2856 \nF-statistic: 6.497 on 4 and 51 DF,  p-value: 0.0002642\n\n\nI’m honestly not entirely sure what these values mean, but I input what I had into the function and I got results.\n\nAIC(confidence_lm)\n\n[1] 471.6936\n\nBIC(confidence_lm)\n\n[1] 483.8457"
  },
  {
    "objectID": "posts/KPopiela_FinalProposal.html",
    "href": "posts/KPopiela_FinalProposal.html",
    "title": "KPopiela_final p1",
    "section": "",
    "text": "#I will need to do some more thorough testing to make sure I can actually do this, but I'd like to focus my final project on ethnic violence since I know a lot about it (I wrote my undergrad thesis on ethno-religious violence in the Polish-Ukrainian borderlands). I found some data sets for my DACSS-601 intensive final that were pretty useful - I found a lot of information but now that I will have the statistical background I'd like to see if I can go further with it. Specifically in the sense of finding out statistics related to the likelihood of an eruption of ethnic violence in countries that fit specific criteria on paper. I can come up with a different research question for this topic area if my initial idea isn't feasible though.  \n\n#These criteria are as follows:  \n# - The population doesn't have an overwhelming ethnic majority; there are 2+ groups, each with substantial numbers.  \n# - History of socio-political repression by one group against the other(s) when said group has political power/alternating episodes of targeted political repression depending on what group holds a political majority.  \n# - The country/population is in a state of severe socio-political instability (war, territorial conquest, political power vacuum, etc.)  \n\n#To make this a little less challenging, I'm going to simplify things for myself. First, I will narrow the data down geographically and temporally - I'm going to focus on former Yugoslav states and the former USSR (I might change the location though). Second, since the data sets I'll be using are pretty big, I'm also going to look at certain columns based on the criteria I presented above (ethnic groups involved, group(s) being oppressed and by whom, and presence/absence of political instability). \n\n# Research Question: Based on what the data show, (1) is there actually a higher probability of ethnic armed conflict/war when these conditions, and (2) does one particular condition have a greater effect on political stability than the others?\n\n\nlibrary(readr)\nlibrary(poliscidata)\n\nRegistered S3 method overwritten by 'gdata':\n  method         from  \n  reorder.factor gplots\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(foreign)\n\n##Summary Stats/Visuals\n\n#All data sets I will be using are from the Harvard Dataverse and they are as follows:  \n\n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Power Relations dataset\", https://doi.org/10.7910/DVN/NDJUJM, Harvard Dataverse, V1, UNF:5:k4xxXC2ASI204QZ4jqvUrQ== [fileUNF]  \n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Armed Conflict dataset\", https://doi.org/10.7910/DVN/K3OIJQ, Harvard Dataverse, V1  \n#    UCDP/PRIO Armed Conflict Dataset version 22.1. Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).\n\n#I don't need to look at these in any particular order, so I'm just going to present them in the order they are in above. \n\n\n# Data set 1: \"Ethnic Power Relations dataset\"\n\nethnic_power_relations <- MASTER_EPR_v1_IrgFiR\n\nError in eval(expr, envir, enclos): object 'MASTER_EPR_v1_IrgFiR' not found\n\nethnic_power_relations <- ethnic_power_relations %>%\n  select(statename,from,to,group,status,size) %>%\n  filter(statename == c(\"Albania\",\"Croatia\",\"Bosnia and Herzegovina\",\"Yugoslavia\",\"Macedonia\",\"Poland\",\"Ukraine\",\"Russia\",\"Hungary\",\"Romania\",\"Bulgaria\"), from >= 1980) \n\nError in select(., statename, from, to, group, status, size): object 'ethnic_power_relations' not found\n\nethnic_power_relations\n\nError in eval(expr, envir, enclos): object 'ethnic_power_relations' not found\n\n\n\n#Data set 2: \"Ethnic Armed Conflict dataset\"\n#I'm going to use select() to look at \"country\", \"startyr\", \"endyr\", and \"ETHNOWAR\". Then I will use filter() to meet my geographic requirements.\n\nethnic_armed_conflict <- EAC_edPcfy\n\nError in eval(expr, envir, enclos): object 'EAC_edPcfy' not found\n\nethnic_armed_conflict <- ethnic_armed_conflict %>%\n  select(country, startyr, endyr, ETHNOAIMS, ETHNOWAR) %>%\n  filter(country == c(\"Croatia\",\"Yugoslavia\",\"Bosnia and Herzegovina\",\"USSR\",\"Russia\"),startyr >= 1980)\n\nError in select(., country, startyr, endyr, ETHNOAIMS, ETHNOWAR): object 'ethnic_armed_conflict' not found\n\nethnic_armed_conflict\n\nError in eval(expr, envir, enclos): object 'ethnic_armed_conflict' not found\n\n#This data set is based off of another one (Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).) so I will include that as well\n\n#NOTE: I don't know why only 2 of the 5 countries I listed are showing up\n\n\n#Data set 3: \"UCDP/PRIO Armed Conflict Dataset\" version 22.1\n\nUCDP_Prio_AC <- ucdp_prio_acd_221_wKBkVs\n\nError in eval(expr, envir, enclos): object 'ucdp_prio_acd_221_wKBkVs' not found\n\nUCDP_Prio_AC <- UCDP_Prio_AC %>%\n  select(location, side_a, side_b, start_date) %>%\n  filter(location == c(\"Yugoslavia\",\"Croatia\",\"Serbia\",\"Russia\"))\n\nError in select(., location, side_a, side_b, start_date): object 'UCDP_Prio_AC' not found\n\nUCDP_Prio_AC\n\nError in eval(expr, envir, enclos): object 'UCDP_Prio_AC' not found\n\n#NOTE: I don't know why this one is doing the same thing as the previous one.\n\n\n#I'm obviously going to do more in-depth work with all three data sets, these (below) are just kind of a sample of what I will be doing.\n\n\nethnic_power_relations %>%\n  ggplot(mapping=aes(x=group,y=size,col=status)) + geom_point() + coord_flip()\n\nError in ggplot(., mapping = aes(x = group, y = size, col = status)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  summarise(mean(size))\n\nError in summarise(., mean(size)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  count(status)\n\nError in count(., status): object 'ethnic_power_relations' not found"
  },
  {
    "objectID": "posts/KPopiela_HW1.html",
    "href": "posts/KPopiela_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(lsr)\n#Question 1"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "href": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "title": "HW1",
    "section": "1a/1b. What does the distribution of LungCap look like?",
    "text": "1a/1b. What does the distribution of LungCap look like?\n\nHint: Plot a histogram with probability density on the y axis\n\n\nHint: make boxplots separated by gender using the boxplot() function\n\nLungCap <- read_xls(\"_data/LungCapData.xls\")\nhead(LungCap)\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n\nhist(LungCap$LungCap)\n\n\n\n\n\nLungCap_MF <- LungCap %>%\n  arrange(LungCap, Gender) %>%\n  group_by(Gender)\nboxplot(LungCap_MF$LungCap ~ LungCap_MF$Gender)\n\n\n\n#I wanted to change the axis labels to \"Gender\" (x) and \"Lung Capacity\" (y), but after an hour and a half of trying to no avail, I had to call it for my own sanity.\n\n\ncolnames(LungCap)\n\n[1] \"LungCap\"   \"Age\"       \"Height\"    \"Smoke\"     \"Gender\"    \"Caesarean\""
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "HW1",
    "section": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?",
    "text": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\nLungCap_smoke <- LungCapData %>%\n  select(LungCap, Smoke) %>%\n  group_by(Smoke)\n\nError in select(., LungCap, Smoke): object 'LungCapData' not found\n\nhead(LungCap_smoke)\n\nError in head(LungCap_smoke): object 'LungCap_smoke' not found\n\n\n\nsummarise(LungCap_smoke, mean(LungCap))\n\nError in summarise(LungCap_smoke, mean(LungCap)): object 'LungCap_smoke' not found\n\n#The mean lung capacities for non-smokers and smokers is 7.77 and 8.65 respectively. Does this make sense? No. One would expect that the mean lung capacity for non-smokers would be higher, but that is not the case here. Let's do some digging to see what the range of values for smokers' and non-smokers' lung capacity. I also want to look at how many people voted \"yes\" or \"no\"; it could be that fewer people (with higher lung capacity) voted \"yes,\" contributing to the higher mean.\n\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"yes\")\n\nError in filter(., Smoke == \"yes\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"no\")\n\nError in filter(., Smoke == \"no\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\n##Lung capacity for smokers ranges from 3.850 to 13.325, while the range for non-smokers is 0.507 to 14.675. Right off the bat, smokers have a higher minimum value, which prevents the mean from being dragged down during calculation. Non-smokers' minimum value is 0.507, an outlier which does seem to have an effect on this category's mean. \n\n\nLungCap_smoke %>%\n  count(Smoke)\n\nError in count(., Smoke): object 'LungCap_smoke' not found\n\n#Out of 725 respondents only 77 voted yes and 648 voted no, so I was right with my guess as to what caused the difference in mean lung capacity between smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "HW1",
    "section": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.",
    "text": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#To start, I'm going to calculate the range and mean of each of the above age groups, as well as a tally of how many are and aren't smokers.\n\n#a) Less than or equal to 13\nLC_Age13 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age <= 13)\nrange(LC_Age13$LungCap)\n\n[1]  0.507 12.050\n\n#The range of lung capacity values for children under the age of 13 is 0.507 to 12.050. The mean is 6.412.\n\n\nsummarise(LC_Age13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.41\n\n\n\nLC_Age13 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no      401\n2 yes      27\n\n#401 individuals 13 and under responded that they don't smoke, while 27 said they do. Compared to the initial calculations for the whole survey, the mean value is slightly lower, which is likely indicative of the fact that children have smaller lungs than adults and therefore have less lung capacity. Something important to note, however, is that this age group accounts for 428 of the total 725 responses (about 59%).\n\n\n#b) 14 to 15 \nLC_Age145 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 14:15)\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age145$LungCap)\n\n[1]  5.625 12.900\n\n#The minimum and maximum lung capacity values for individuals aged 14-15 are 5.625 and 12.900. The mean is 8.842.\n\n\nsummarise(LC_Age145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.84\n\n\n\nLC_Age145 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       44\n2 yes       8\n\n#Out of the 52 respondents in this age group, 44 stated that they don't smoke and 8 said that they do. The 14-15y/o age group is MUCH smaller than the \"13 and under\" one (it makes up only 12% of total responses). The percentage of smokers to non-smokers in each of the above age groups,is 7% and 18% respectively. If you were to take these percentages at face value without taking sample size into account, it would look as if the 14-15 y/o age group makes up 18% of the total 725 responses. In reality, this sample accounts for 6% of the total, making its impact relatively low.\n\n\n#c) 16 to 17\nLC_Age167 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 16:17)\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age167$LungCap)\n\n[1]  5.675 13.375\n\n#The minimum and maximum lung capacity values for individuals ages 16 to 17 are 5.675 and 13.375. The mean is 10.058.\n\n\nsummarise(LC_Age167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.1\n\n\n\nLC_Age167 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       40\n2 yes       8\n\n#This sample is similar in size to the previous (14-15 year olds) with only 48 responses, and smokers make up 20% of the responses. Now lets discuss the other figures.  \n\n#The mean lung capacity for 16-17 year olds is 10.058, 1.216 units higher than the previous age group, and 3.646 units higher than the \"13 and under\" age group. As of this point in my calculations, the only relationship seems to be between age and lung capacity rather than smoking and lung capacity; this is due to the facts that: 1) not that many people ages 0-17 smoke, and 2) the sample sizes for the 14-17 age group is 100 compared to the 428 responses in the \"13 and under\" group.\n\n\nLC_Age18p <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age >= 18)\nrange(LC_Age18p$LungCap)\n\n[1]  7.750 14.675\n\n#The minimum and maximum range values for the 18+ age group are 7.750 and 14.675. The mean is 10.965.\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\nLC_Age18p %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       65\n2 yes      15\n\n#Ok so what we've learned here is that this survey was HEAVILY focused on kids 13 and younger; although the sample for this age group is larger than the previous 2, it's still only 80 out of 725 responses (about 11% of total respondents). The mean lung capacity for this group is the highest of all of them at 10.965, but this still doesn't seem to show a relationship between smoking and lung capacity. Rather, at least to me, it shows a relationship between age and lung capacity (i.e. stage of lung development and lung capacity).\n\n##1e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n#I will place a comparison of all values produced by the following calculations at the bottom. (I will go through the smoker and non-smoker calculations first)\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"yes\")\nrange(LCu13$LungCap)\n\n[1]  3.850 10.275\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            7.20\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"yes\")\nrange(LCu145$LungCap)\n\n[1]  6.225 11.025\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.36\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"yes\")\nrange(LCu167$LungCap)\n\n[1]  7.550 11.775\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            9.06\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"yes\")\nrange(LCu18p$LungCap)\n\n[1]  8.200 13.325\n\n\n\nsummarise(LCu18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.5\n\n\n\n#Now for the non-smoker calculations.\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"no\")\nrange(LCu13$LungCap)\n\n[1]  0.507 12.050\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.36\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"no\")\nrange(LCu145$LungCap)\n\n[1]  5.625 12.900\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.93\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"no\")\nrange(LCu167$LungCap)\n\n[1]  5.675 13.375\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.3\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"no\")\nrange(LCu18p$LungCap)\n\n[1]  7.750 14.675\n\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\n#Comparison!!  \n  # 13 and under smokers: range = 3.850 to 10.275, mean = 7.202\n  # 13 and under non-smokers: range = 0.507 and 12.050, mean = 6.359 \n    \n  # 14-15 smokers: range = 6.225 and 11.025, mean = 8.359\n  # 14-15 non-smokers: range = 5.625 and 12.900, mean = 8.930  \n  \n  # 16-17 smokers: range = 7.550 and 11.775, mean = 9.063\n  # 16-17 non-smokers: range = 5.675 and 13.375, mean = 10.257  \n    \n  # 18+ smokers: range = 8.200 and 13.325,mean = 10.513 \n  # 18+ non-smokers: range = 7.750 and 14.675, mean = 10.965\n\n\n#The answers I got for smokers vs. non-smokers are obviously different, but I wouldn't say they necessarily convey something different to what I interpreted from 1d. I don't see a relationship between smoking and lung capacity, and I certainly don't see a massive difference in the values comparing the lung capacity of smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "href": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "title": "HW1",
    "section": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.",
    "text": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\ncov(LungCap$LungCap, LungCap$Age)\n\n[1] 8.738289\n\n\n\ncor(LungCap$LungCap, LungCap$Age)\n\n[1] 0.8196749\n\n\n\n#Covariance between lung capacity and age: 8.738.  \n#Correlation between lung capacity and age: 0.820  \n\n#I'm not totally confident in my understanding of covariance yet, but from what I know, it's the positive or negative relationship between two variables and the further the value is from 0, the stronger the relationship is. And the covariance between lung capacity and age is 8.738. Correlation gets stronger the closer the value gets to 1 or -1; the correlation between lung capacity and age for 'LungCap' dataset is 0.820, a figure relatively close to 1, so I would say there is a moderate to strong correlation between the two variables in question here.\n\n#Question 2\n###Let X=number of prior convictions for prisoners at a state prison at which there are 810 inmates."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "HW1",
    "section": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nconrange <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nconrange\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n#To start I just wanted to present a visualization of the frequency of each categorical variable.  \n  # 0 prior convictions = 128  \n  # 1 prior conviction = 434  \n  # 2 prior convictions = 160  \n  # 3 prior convictions = 64  \n  # 4 prior convictions = 24\n\n\nprop.table(table(conrange))[0:2]\n\nconrange\n        0         1 \n0.1580247 0.5358025 \n\n#By combining the probability values for 0 and 1, we can see that the probability of a randomly selected inmate having fewer than 2 prior convictions is 0.694."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "HW1",
    "section": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nprop.table(table(conrange))[0:3]\n\nconrange\n        0         1         2 \n0.1580247 0.5358025 0.1975309 \n\n#By using the same math above, the probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "HW1",
    "section": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nprop.table(table(conrange))[4:5]\n\nconrange\n         3          4 \n0.07901235 0.02962963 \n\n#The probability that a randomly selected inmate has more than 2 prior convictions is 0.108."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "href": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "title": "HW1",
    "section": "2d. What is the expected value for the number of prior convictions?",
    "text": "2d. What is the expected value for the number of prior convictions?\n\nprior_con_range <- c(0,1,2,3,4)\nprobs <- c(0.158,0.535,0.197,0.079,0.029)\nc(prior_con_range %*% probs)\n\n[1] 1.282\n\n#The expected value for the number of prior convictions is 1.282"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "href": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "title": "HW1",
    "section": "2e. Calculate the variance and the standard deviation for the Prior Convictions.",
    "text": "2e. Calculate the variance and the standard deviation for the Prior Convictions.\n\nvar(conrange)\n\n[1] 0.8572937\n\nsd(conrange)\n\n[1] 0.9259016\n\n#The variance and standard deviation for prior convictions are 0.857 and 0.925 respectively."
  },
  {
    "objectID": "posts/KPopiela_HW2.html",
    "href": "posts/KPopiela_HW2.html",
    "title": "KPopiela HW2",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(stats)"
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-1",
    "href": "posts/KPopiela_HW2.html#question-1",
    "title": "KPopiela HW2",
    "section": "Question 1",
    "text": "Question 1\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n#Bypass values\nn_bypass <- 539\nx_bypass <- 19\nsd_bypass <- 10\ndf_bypass <- n_bypass-1\nalpha = 0.10\n\n\n#t-score  \ntscore_bypass = qt(p=alpha/2,df=df_bypass,lower.tail = F)\ntscore_bypass\n\n[1] 1.647691\n\n\n\n#standard error  \nse_bypass <- sd_bypass/sqrt(n_bypass)\nse_bypass\n\n[1] 0.4307305\n\n\n\n#margin of error and confidence interval (bypass)\nmargin_error <- tscore_bypass*se_bypass\nlower_CI <- x_bypass - margin_error\nupper_CI <- lower_CI+margin_error\nprint(c(lower_CI,upper_CI))\n\n[1] 18.29029 19.00000\n\n\n\n#Angiography values  \nn_angio <- 847\nx_angio <- 18\nsd_angio <- 9\ndf_angio <- n_angio-1\n#alpha value remains the same at 0.10 \n\n\n#t-score\ntscore_angio <- qt(p=alpha/2,df=df_angio,lower.tail = F)\ntscore_angio\n\n[1] 1.646657\n\n\n\n#standard error\nse_angio <-sd_angio/sqrt(n_angio)\nse_angio\n\n[1] 0.3092437\n\n\n\n#margin of error and confidence interval (angiography)\nmargin_error_angio <- tscore_angio*se_angio\nangio_lowerCI <- x_angio - margin_error_angio\nangio_upperCI <- angio_lowerCI+margin_error_angio\nprint(c(angio_lowerCI,angio_upperCI))\n\n[1] 17.49078 18.00000\n\n\nThe bypass confidence interval for the true mean wait time is 18.290, 19.000, or 0.710.\nThe angiography confidence interval the true mean wait time is 17.491, 18.000, or 0.509.\nThe confidence interval is narrower for angiography."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-2",
    "href": "posts/KPopiela_HW2.html#question-2",
    "title": "KPopiela HW2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n#Sample proportion/Point estimate\nn <- 1031\nx <- 567\nsample_prop <- x/n\nsample_prop\n\n[1] 0.5499515\n\n\n\n#Margin of error\nmargin_error2 <- qnorm(0.975)*sqrt(sample_prop*(1-sample_prop)/n)\nmargin_error2\n\n[1] 0.03036761\n\n\n\n#95% Confidence Interval\nCI_lower <- sample_prop - margin_error2\nCI_upper <- CI_lower + margin_error2\nprint(c(CI_lower,CI_upper))\n\n[1] 0.5195839 0.5499515\n\n\nThe point estimate p, of the proportion of all adult Americans who believe that college is essential for success is 0.549, or ~55%. The margin of error is 0.030, which lines up, since the confidence interval is 0.519, 0.549."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-3",
    "href": "posts/KPopiela_HW2.html#question-3",
    "title": "KPopiela HW2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n#Since most formulas require a value for sample size (n), whichever one I use will have to be reorganized: the confidence interval formula. But because I am looking for n, it has to read z*(s/5)^2=n.\n\nf <-function(n, z = 1.96, s = 42.5) {\n  res <- z*s/sqrt(n)\n  return(res)\n}\n\nvec <- vapply(1:300, FUN = f, FUN.VALUE = 5.0)\nwhich(vec < 5) [1]\n\n[1] 278\n\n#The sample contains at least 278 people."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-4",
    "href": "posts/KPopiela_HW2.html#question-4",
    "title": "KPopiela HW2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nC. Report and interpret the P-value for H a: μ > 500.\n####(Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n#In order to test whether or not the mean income for female employees differs from $500/week, we must first condect a one-sample, two-sided significance test.\n\n#We can also assume the following:  \n#1. The sample is random and the population has a normal distribution  \n#2. The mean income for all senior-level workers = $500/week  \n#3. From the random sample of 9 female employees, the mean income = $410/week  \n#4. Standard deviation = 90  \n#5. Null Hypothesis: H0: μ = 500  \n#6. Alternative Hypothesis: Ha: μ ≠ 500  \n\n\n#Test statistic\nybar <- 410\nmean <- 500\ns <- 90\nn <- 9\n(ybar - mean)/(s/sqrt(n))\n\n[1] -3\n\n#The test statistic value is -3\n\n\n#P-value\nn <- 9 \ndf_n <- (n - 1)  \nt_test <- (410 - 500)/(90/sqrt(9))  \np_value <- pt(t_test, df_n)*2  \np_value\n\n[1] 0.01707168\n\n#P-value is 0.017. If we hold to the assumption that a=0.05, we can easily see that 0.017 < 0.05, which means the null hypothesis can be rejected. Therefore, there is enough statistical evidence to support the claim that the mean income for female employees differs from the overall mean of $500/week.\n\n\n\nB. Report the P-value for Ha : μ < 500. Interpret.\n\n#Hypotheses\n    #H0:mu = $500/week  \n    #Ha:mu = <$500/week  \n    #P-value = p(t<t_test)*p(t<-3)\n\n\n#P-value for Ha:my > 500\nq <- -3\nleft_p_value <- pt(q,df_n,lower.tail=TRUE,log.p=FALSE)\nleft_p_value\n\n[1] 0.008535841\n\n#P-value for Ha:my > 500 is 0.0085. This can be rounded up to 0.01, which indicates that there's strong evidence against the mean weekly income being $500+\n\n\n#P-value for H0:mu < 500\nright_p_value <- pt(q,df_n,lower.tail = FALSE,log.p=FALSE)\nright_p_value\n\n[1] 0.9914642\n\n#The p-value for H0:mu < 500 is 0.99, indicating strong evidence in favor of the null hypothesis. This contradicts the claim that mean mu > 500. To make sure my findings are correct, I must confrim that the sum of each p-value totals to 1. I could code this but it's not hard to tell that 0.01 + 0.99 = 1."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-5",
    "href": "posts/KPopiela_HW2.html#question-5",
    "title": "KPopiela HW2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 ,with se = 10.0\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n#Let's start with Jones and confirming that t=1.95 and the p-value = 0.051\n\n#t-test\nt_testj <- (519.5-500)/10.0\nt_testj\n\n[1] 1.95\n\n#The t-test value, is in fact 1.95\n\n\n#P-value\nn5 <- 1000\ndf_5 <- (n5-1)\n\npvaluej <- pt(t_testj, df_5,lower.tail = FALSE,log.p = FALSE)*2\npvaluej\n\n[1] 0.05145555\n\n#Like the t-test value, the p-value is also accurate to the question at 0.051.\n\n\n#Now lets move onto Smith with t = 1.97 and p-value = 0.049\n\n#t-test\nt_testSmith <- (519.7 - 500)/10.0\nt_testSmith\n\n[1] 1.97\n\n#Smith's t-test value is 1.97.\n\n\n#P-value\n\n#sample size n is the same as in the Jones section: n5 <- 1000  \n#df is also the same as the Jones section: df_5 <- (n5-1)  \n\np_valueSmith <- pt(t_testSmith, df_5,lower.tail = FALSE, log.p = FALSE)*2\np_valueSmith\n\n[1] 0.04911426\n\n#Smith's p-value, like the t-test value, is accurate to what the question presents at 0.049\n\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\n\n#In order for a p-value to be statistically significant, it must be greater than 0.05. Smith’s p-value is 0.049 which, while close, is still less than 0.05. Jones’s p-value, however, is statistically significant at 0.051.\n\n\n\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\n#Smith's and Jones's results were extremely similar, but the difference between the two lies right on the threshold of statistical significance; only Jones's results were statistically significant. But given the result values' closeness (0.051 and 0.049), there is moderate evidence against H0."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-6",
    "href": "posts/KPopiela_HW2.html#question-6",
    "title": "KPopiela HW2",
    "section": "Question 6",
    "text": "Question 6\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n#I'm going to start by calculating the t-score to find the upper and lower values in the gas_taxes interval\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ngas_tax_sample <- 18\ndf_gt <- gas_tax_sample - 1\nmean_gt <- mean(gas_taxes)\ntscore_gt <- qt(p=0.05,df=df_gt,lower.tail=FALSE)\ngas_sd <- sd(gas_taxes)\nme_gas_taxes <- qt(0.05,df = df_gt)*gas_sd/sqrt(18)\n\nlower_int_gt<-(mean_gt+me_gas_taxes)\nlower_int_gt\n\n[1] 37.0461\n\n#The lower interval value is 37.046\n\n\nupper_int_gt <- (mean_gt- me_gas_taxes)\nupper_int_gt\n\n[1] 44.67946\n\n#The upper interval value is 44.679\n\n#The average tax/gallon of gas is less than $0.45, so it is within the upper and lower bounds of the confidence interval. However, we will test an alternate outcome via a t-test"
  },
  {
    "objectID": "posts/KPopiela_HW3.html",
    "href": "posts/KPopiela_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "library(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nWarning: package 'smss' was built under R version 4.2.2\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:car':\n\n    recode\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stats)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-1",
    "href": "posts/KPopiela_HW3.html#question-1",
    "title": "HW3",
    "section": "Question 1",
    "text": "Question 1\n\nUnited Nations (Data file: UN11 in alr4) The data in the file UN11 contains several variables, including ppgdp, the gross national product per person in U.S. dollars, and fertility, the birth rate per 1000 females, both from the year 2009. The data are for 199 localities, mostly UN member countries, but also other areas such as Hong Kong that are not independent countries. The data were collected from the United Nations (2011). We will study the dependence of fertility on ppgdp.\n\n1.1. Identify the predictor and the response.\nPredictor: ppgdp\nResponse: fertility\n\n\n1.2 Draw the scatterplot of fertility on the vertical axis versus ppgdp on the horizontal axis and summarize the information in this graph. Does a straight-line mean function seem to be plausible for a summary of this graph?\n\ndata(UN11)\nggplot(data=UN11, aes(x=ppgdp,y=fertility))+geom_point()+\n  geom_smooth(method=\"lm\",se=FALSE)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA straight-line mean function would not be plausible here as the data is not presented linearly\n\n\n1.3 Draw the scatterplot of log(fertility) versus log(ppgdp) using natural logarithms. Does the simple linear regression model seem plausible for a summary of this graph? If you use a different base of logarithms, the shape of the graph won’t change, but the values on the axes will change\n\nggplot(data = UN11, aes(x=log(ppgdp),y=log(fertility))) + geom_point() +\n  geom_smooth(method=\"lm\",se=FALSE)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA simple linear regression model would be much more appropriate"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-2",
    "href": "posts/KPopiela_HW3.html#question-2",
    "title": "HW3",
    "section": "Question 2",
    "text": "Question 2\n\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n2.1 How, if at all, does the slope of the prediction equation change?\nTo account for the conversion rate from USD to GBP, the response and the slope must be divided by 1.33\n\n\n2.2 How, if at all, does the correlation change?\nCorrelation wouldn’t change in this scenario since it isn’t affected by units of measurement"
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-3",
    "href": "posts/KPopiela_HW3.html#question-3",
    "title": "HW3",
    "section": "Question 3",
    "text": "Question 3\n\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\ndata(water)\npairs(water)\n\n\n\n\n\nThe following variables appear to be correlated with each other: OPBPC, OPRC, OPSLAKE. All parts of the matrix with 2 of these variables exhibit a dependence among themselves that is not present between OPBPC, OPRC, and OPSLAKE and APMAM, APSAB, APSLAKE. That being said, though, there also appears to be a correlation among APMAM, APSAB, APSLAKE.\nBSAAM is more closely related to OPBPC, OPRC, and OPSLAKE than to APMAM, APSAB, APSLAKE."
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-4",
    "href": "posts/KPopiela_HW3.html#question-4",
    "title": "HW3",
    "section": "Question 4",
    "text": "Question 4\n\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\ndata(Rateprof)\npairs(Rateprof[c(\"quality\",\"clarity\",\"helpfulness\",\"easiness\",\"raterInterest\")])\n\n\n\n\nThere is a strong positive relationship among the variables “quality”, “clarity”, and “helpfulness.” There appears to be some correlation between “helpfulness” and “easiness”, but the data is more dispersed."
  },
  {
    "objectID": "posts/KPopiela_HW3.html#question-5",
    "href": "posts/KPopiela_HW3.html#question-5",
    "title": "HW3",
    "section": "Question 5",
    "text": "Question 5\n\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable). (You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\n\n(i) y = political ideology and x = religiosity\n\n\n(ii) y = high school GPA and x = hours of TV watching\n\n5.1 Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n\n5.2 Summarize and interpret results of inferential analyses\n\ndata(\"student.survey\")\ncolnames(student.survey)\n\n [1] \"subj\" \"ge\"   \"ag\"   \"hi\"   \"co\"   \"dh\"   \"dr\"   \"tv\"   \"sp\"   \"ne\"  \n[11] \"ah\"   \"ve\"   \"pa\"   \"pi\"   \"re\"   \"ab\"   \"aa\"   \"ld\"  \n\n\nAs per the question, I will be focusing on the following variables: “re” (religiosity) and “pi” (political ideology) for subsection i, and “hi” (high school GPA) and “tv” (hours watching TV) for subsection ii.\n\n#subsection i\nggplot(data=student.survey,aes(x=re,fill=pi))+\n  geom_bar() + labs(x=\"Religiosity\", fill =\"Political Ideology\")\n\n\n\n\nThe above graph is one of several that could have been used; I just used a bar graph with fill colors since it was simple and suited my purposes. I didn’t know how to get additional information about what the variables actually mean, but I’m assuming “religiosity” refers to how often respondents attend religious services or practice their religion. Based on this graph, as “religiousness” increases, conservatism does as well. While not a majority by any means, it is still significant to note that those who identify as very conservative only appear in the bar labelled “every week,” whereas those who identify as very liberal are not even present on the graph to the right of “occasionally.” This, therefore, indicates that those who are heavily liberal-leaning in political ideology are far less likely to go to church/temple/mosque/etc. regularly/frequently than those who are more conservative.\n\n#subsection ii\nggplot(data=student.survey,aes(x=hi, y=tv)) +\n  geom_point() + labs(x=\"High School GPA\", y=\"Hours Watching TV\")\n\n\n\n\nOnce again, this graph is just one of several visualizations that could be used. I chose a scatterplot to reflect individual responses and lessen the visual impact of outliers. While this graph does not show a linear relationship between the two variables, there is a higher concentration of responses with higher GPA’s and lower # of hours watching TV. I will conduct a simple regression model to test whether a linear relationship exists.\nAnd here are some summary statistics for context:\n\nsummary(student.survey[,c('pi', 're', 'hi', 'tv')])\n\n                     pi                re           hi              tv        \n very liberal         : 8   never       :15   Min.   :2.000   Min.   : 0.000  \n liberal              :24   occasionally:29   1st Qu.:3.000   1st Qu.: 3.000  \n slightly liberal     : 6   most weeks  : 7   Median :3.350   Median : 6.000  \n moderate             :10   every week  : 9   Mean   :3.308   Mean   : 7.267  \n slightly conservative: 6                     3rd Qu.:3.625   3rd Qu.:10.000  \n conservative         : 4                     Max.   :4.000   Max.   :37.000  \n very conservative    : 2"
  },
  {
    "objectID": "posts/KPopiela_HW4.html",
    "href": "posts/KPopiela_HW4.html",
    "title": "HW4",
    "section": "",
    "text": "library(smss)\n\nWarning: package 'smss' was built under R version 4.2.2\n\nlibrary(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(stats)\nlibrary(tidyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/KPopiela_HW4.html#question-1",
    "href": "posts/KPopiela_HW4.html#question-1",
    "title": "HW4",
    "section": "Question 1",
    "text": "Question 1\n\nFor recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.\n\n\na) A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.\n\npredicted_sp <- 10536+53.8*1240+2.84*18000\npredicted_sp\n\n[1] 128368\n\n\nGiven the measurements and previous selling price of the house, the predicted selling price is $128,368.\nNow to calculate the residual between the above figure and the actual selling price of $145,000.\n\nresidual <- 145000-predicted_sp\nresidual\n\n[1] 16632\n\n\nThe residual is $16,632, which indicates that the home and property sold for $16,632 above the predicted selling price.\n\n\nb) For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?\nThe coefficient for home size in square feet (x1) is 53.8, so the price of the home would go up by $53.80 for each square foot increase.\n\n\nc) According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?\nThe coefficient for x2 (lot size in square feet) is 2.84, meaning that every square foot increase in property size would add $2.84 to the overall price of the home. For an increase in lot size to have the same impact as a one square foot increase in home size, we have to do some simple division (x1/x2)\n\n53.8/2.84\n\n[1] 18.94366\n\n\nLot size would have to increase by 18.94 square feet to have the same price impact as a one square foot increase in house size."
  },
  {
    "objectID": "posts/KPopiela_HW4.html#question-2",
    "href": "posts/KPopiela_HW4.html#question-2",
    "title": "HW4",
    "section": "Question 2",
    "text": "Question 2\n\n(Data file: salary in alr4 R package). The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.\n\n\na) Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.\n\ndata(salary)\nt.test(salary~sex,data = salary)\n\n\n    Welch Two Sample t-test\n\ndata:  salary by sex\nt = 1.7744, df = 21.591, p-value = 0.09009\nalternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n95 percent confidence interval:\n -567.8539 7247.1471\nsample estimates:\n  mean in group Male mean in group Female \n            24696.79             21357.14 \n\n\nThe mean salary for males is $24,696.79 and the mean salary for females is $21,357.14. Based entirely on sex, the mean salary for men and women is not the same, with a $3,339.65 difference in favor of males.\n\n\nb) Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.\n\nlr_predictors <- lm(salary~degree+rank+sex+year+ysdeg, data=salary)\nlr_predictors\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nCoefficients:\n(Intercept)    degreePhD    rankAssoc     rankProf    sexFemale         year  \n    15746.0       1388.6       5292.4      11118.8       1166.4        476.3  \n      ysdeg  \n     -124.6  \n\n\n\nset.seed(3)\nmf_pay_rate <- data.frame(degree=sample(salary$degree,size=10,replace=T),\n                          rank=sample(salary$rank,size=10,replace=T),\n                          sex=sample(salary$sex,size=10,replace=T),\n                          year=sample(salary$year,size=10,replace=T),\n                          ysdeg=sample(salary$ysdeg,size=10,replace=T))\npredict(lr_predictors,mf_pay_rate)\n\n       1        2        3        4        5        6        7        8 \n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 16837.87 \n       9       10 \n21524.03 28077.52 \n\n\n\nsummary(lr_predictors)\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\nconfint(lr_predictors,'sexFemale')\n\n              2.5 %   97.5 %\nsexFemale -697.8183 3030.565\n\n\nAfter conducting a multiple linear regression (with 95% CI), the difference in salary between males and females in this scenario ranges from $697.82 less than male counterparts to $3030.56 more than male counterparts.\n\n\nc) Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables.\n\nset.seed(3)\nincome_df <-data.frame(degree=sample(salary$degree,size=10,replace=T),\n                          rank=sample(salary$rank,size=10,replace=T),\n                          sex=sample(salary$sex,size=10,replace=T),\n                          year=sample(salary$year,size=10,replace=T),\n                          ysdeg=sample(salary$ysdeg,size=10,replace=T))\npredict(lr_predictors,income_df)\n\n       1        2        3        4        5        6        7        8 \n15368.63 20671.56 14287.78 29407.55 20069.45 32082.22 23606.80 16837.87 \n       9       10 \n21524.03 28077.52 \n\n\n\nsummary(lr_predictors)\n\n\nCall:\nlm(formula = salary ~ degree + rank + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nAccording to this data set, individuals’ salary increases by:\n1) $1388.61 if they have a PhD\n2) $5292.36 if they are an associate professor\n3) $11,118.76 if they are a full/tenured professor\n4) $1166.37 if they are female\n5) $475.31 every year they are employed at their institution\nSalary decreases by $124.57, however, for every year that passes since the individual received their degree. I may have misinterpreted the meaning of this variable (ysdeg), because this doesn’t really make sense if they get a raise for everything else. Anyway, the values representing respondents’ professorial rank and the length of their employment are statistically significant.\n\n\nd) Change the baseline category for the rank variable. Interpret the coefficients related to rank again.\n\nlr_predictors_rank <- lm(salary~rank+sex+degree+year+ysdeg, data=salary)\nset.seed(3) \nincome_rank <- data.frame(rank=sample(salary$rank,size=10,replace=T),\n                  sex=sample(salary$sex,size=10,replace=T),\n                  degree=sample(salary$degree,size=10,replace=T),\n                  year=sample(salary$year,size=10,replace=T),\n                  ysdeg=sample(salary$ysdeg,size=10,replace=T))\npredict(lr_predictors_rank,income_rank)\n\n       1        2        3        4        5        6        7        8 \n25098.78 25963.92 15676.39 24969.76 22624.44 26255.82 16925.83 29123.01 \n       9       10 \n31254.18 28077.52 \n\n\n\nsummary(lr_predictors_rank)\n\n\nCall:\nlm(formula = salary ~ rank + sex + degree + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \ndegreePhD    1388.61    1018.75   1.363    0.180    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\nI am not entirely sure this is right since all of the values remained the same.\n\n\ne) Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts.\n\n\nExclude the variable rank, refit, and summarize how your findings changed, if they did.\n\nlr_predictors3 <- lm(salary~degree+sex+year+ysdeg,data=salary)\n\nset.seed(3)\nincome_df3 <- data.frame(degree=sample(salary$degree,size=10,replace=T),\n                  sex=sample(salary$sex,size=10,replace=T),\n                  year=sample(salary$year,size=10,replace=T),\n                  ysdeg=sample(salary$ysdeg,size=10,replace=T))\npredict(lr_predictors3,income_df3)\n\n       1        2        3        4        5        6        7        8 \n15631.50 25840.16 23116.76 29904.74 30290.05 27114.13 19427.73 25588.74 \n       9       10 \n23098.28 21451.56 \n\n\n\nsummary(lr_predictors3)\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nBy excluding ‘rank’, salary decreases by:\n1) $3299.35 if the individual has a PhD\n2) $1286.54 if the individual\nHowever, salary /increases/ by:\n1) $351.97 for each year the individual is at their current professorial rank\n2) $339.40 every year after the individual receives their PhD\n\n\nf) Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.\n\n\nCreate a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?\n\nlr_predictor4 <- lm(salary~rank+degree+sex+year+ysdeg+year*ysdeg,data=salary)\nsummary(lr_predictor4)\n\n\nCall:\nlm(formula = salary ~ rank + degree + sex + year + ysdeg + year * \n    ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4055.4 -1007.7  -172.6   800.0  9275.7 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16289.506    944.422  17.248  < 2e-16 ***\nrankAssoc    5627.056   1184.705   4.750 2.19e-05 ***\nrankProf    11475.286   1389.241   8.260 1.71e-10 ***\ndegreePhD    1557.213   1028.855   1.514   0.1373    \nsexFemale    1233.531    925.994   1.332   0.1897    \nyear          318.343    174.450   1.825   0.0748 .  \nysdeg        -172.406     89.161  -1.934   0.0596 .  \nyear:ysdeg      7.094      6.578   1.078   0.2867    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2394 on 44 degrees of freedom\nMultiple R-squared:  0.8588,    Adjusted R-squared:  0.8363 \nF-statistic: 38.22 on 7 and 44 DF,  p-value: < 2.2e-16\n\n\nThe slope for ‘year:ysdeg’ is positive, whcih supports the hypothesis that people hired by the new dean make more than those hired beforehand."
  },
  {
    "objectID": "posts/KPopiela_HW4.html#question-3",
    "href": "posts/KPopiela_HW4.html#question-3",
    "title": "HW4",
    "section": "Question 3",
    "text": "Question 3\n\na) Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.\nLet’s start with presenting summary data to see what we’re working with:\n\nlibrary(smss)\ndata(\"house.selling.price\")\nhead(house.selling.price)\n\n  case Taxes Beds Baths New  Price Size\n1    1  3104    4     2   0 279900 2048\n2    2  1173    2     1   0 146500  912\n3    3  3076    4     2   0 237700 1654\n4    4  1608    3     2   0 200000 2068\n5    5  1454    3     3   0 159900 1477\n6    6  2997    3     2   1 499900 3153\n\n\n\nsummary(house.selling.price)\n\n      case            Taxes           Beds       Baths           New      \n Min.   :  1.00   Min.   :  20   Min.   :2   Min.   :1.00   Min.   :0.00  \n 1st Qu.: 25.75   1st Qu.:1178   1st Qu.:3   1st Qu.:2.00   1st Qu.:0.00  \n Median : 50.50   Median :1614   Median :3   Median :2.00   Median :0.00  \n Mean   : 50.50   Mean   :1908   Mean   :3   Mean   :1.96   Mean   :0.11  \n 3rd Qu.: 75.25   3rd Qu.:2238   3rd Qu.:3   3rd Qu.:2.00   3rd Qu.:0.00  \n Max.   :100.00   Max.   :6627   Max.   :5   Max.   :4.00   Max.   :1.00  \n     Price             Size     \n Min.   : 21000   Min.   : 580  \n 1st Qu.: 93225   1st Qu.:1215  \n Median :132600   Median :1474  \n Mean   :155331   Mean   :1629  \n 3rd Qu.:169625   3rd Qu.:1865  \n Max.   :587000   Max.   :4050  \n\n\n\nstr(house.selling.price)\n\n'data.frame':   100 obs. of  7 variables:\n $ case : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Taxes: int  3104 1173 3076 1608 1454 2997 4054 3002 6627 320 ...\n $ Beds : int  4 2 4 3 3 3 3 3 5 3 ...\n $ Baths: int  2 1 2 2 3 2 2 2 4 2 ...\n $ New  : int  0 0 0 0 0 1 0 1 0 0 ...\n $ Price: int  279900 146500 237700 200000 159900 499900 265500 289900 587000 70000 ...\n $ Size : int  2048 912 1654 2068 1477 3153 1355 2075 3990 1160 ...\n\n\nNow I’m going to delve into the regression analysis in which y= selling price (USD) relative to house size, and whether a home is new (1=yes, 0=no).\n\nsummary(lm(Price~Size+New,data=house.selling.price))\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\n\n\nb) Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes.\nIn this scenario, y= -40230.87+116.13x1+57736.28x2. x1 represents house size and x2 represents whether the home is new or not. Now I will conduct a multiple regression analysis to show the relationship between a new home’s selling price and its size.\n\nnew_sp <- lm(formula=Price~New+Size,data=house.selling.price)\nsummary(new_sp)\n\n\nCall:\nlm(formula = Price ~ New + Size, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nNew          57736.283  18653.041   3.095  0.00257 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nAnd now a correlation test to go a step further.\n\ncor.test(house.selling.price$Size,house.selling.price$New)\n\n\n    Pearson's product-moment correlation\n\ndata:  house.selling.price$Size and house.selling.price$New\nt = 4.1212, df = 98, p-value = 7.891e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2032530 0.5399831\nsample estimates:\n      cor \n0.3843277 \n\n\nPredictor variables ‘new’ and ‘size’ have p-values or 0,00257 and 2w-16 respectively. While different, both p-values are statistically significant, which indicates that the null hypothesis can be rejected. By running a correlation test, we can see that the relationship between these 2 variables is weak, as 0.384.\n\n\nc) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\nFirst let’s calculate the predicted selling price of a NEW 3000 sqft house\n\npredicted_spNEW <- -40230.87+116.13*3000+57736\npredicted_spNEW\n\n[1] 365895.1\n\n\nThe predicted selling price for a new house of these measurements is $365,895.10\n\npredicted_spOLD <- -40230.87+116.13*3000\npredicted_spOLD\n\n[1] 308159.1\n\n\nThe predicted selling price for a NOT new 3000 sqft house is $308,159.10\n\n\nd) Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results\nLet’s start by calculating this interaction\n\ninteraction_size <- summary(lm(Price~Size+New+Size*New, data=house.selling.price))\ninteraction_size\n\n\nCall:\nlm(formula = Price ~ Size + New + Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\nWith the interaction term connecting ‘size’ and ‘new’, the estimated price increase for each square foot added to a new home is $61.92. The standard error is $21.69 and the t-test value is 2.855.\n\n\ne) Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.\n\nggplot(data=house.selling.price,aes(x=Size,y=Price, color=New))+\n  geom_point()+\n  geom_smooth(method=\"lm\",se=F)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe scatterplot above shows the relationship between price and house size (sqft) categorized by whether or not the home is new. Newer houses (light blue) have a higher price baseline than those that are “old” (black); most of the older homes are in the bottom left of the graph. There are also a lot more of them. Finally, none of the light blue dots are fully below the regression line, while about half (or more) of those representing older houses are well below the line.\n\n\nf) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.\n\nnew_house3000 <- -22227.81+104.44*3000-78527.50+61.9*3000*1\nnew_house3000\n\n[1] 398264.7\n\n\nThe predicted selling price for a new, 3000 sqft home is $398.264.70\n\nolder_house3000 <- -22227.81+104.4*3000\nolder_house3000\n\n[1] 290972.2\n\n\nThe predicted selling price for an older home of the same size is $290,972.20\n\n\ng) Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.\n\nnew_house1500 <- -22227.81+104.4*1500-78527.50+61.9*1500\nnew_house1500\n\n[1] 148694.7\n\n\nThe predicted selling price for a new, 1500sqft house is $148,694.70\n\nolder_house1500 <- -22227.81+104.4*1500\nolder_house1500\n\n[1] 134372.2\n\n\nThe predicted selling price for an older, 1500sqft home is $134,372.20\nNow to compare to part f. As size doubles in a new home the price increases by more than double, rising to $398,264.70 from $148,694.70 in new homes. For older homes, the same is also true, but with a lower range of values, rising to $290,972.20 from $134,372.20. This heavily indicates that size is much more valuable than other aspects of a given property.\n\n\nh) Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?\nI think both models could work, but if you’re looking specifically at the relationship between ‘size’ and ‘new’ and the selling price, I think the one without the interaction is better. They don’t necessarily need to interact with each other in order to show their relationship to the price of the home."
  },
  {
    "objectID": "posts/KPopiela_HW5.html",
    "href": "posts/KPopiela_HW5.html",
    "title": "HW5",
    "section": "",
    "text": "library(alr4)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: effects\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\nWarning: package 'smss' was built under R version 4.2.2\n\nlibrary(stats)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2"
  },
  {
    "objectID": "posts/KPopiela_HW5.html#question-1",
    "href": "posts/KPopiela_HW5.html#question-1",
    "title": "HW5",
    "section": "Question 1",
    "text": "Question 1\n(Data file: house.selling.price.2 from smss R package)\nFor the house.selling.price.2 data the tables below (not included) show a correlation matrix and a model fit using four predictors of selling price: size (home and lot), # of bedrooms, # of bathrooms, and whether or not the house is new.\n(Hint 1: You should be able to answer A, B, C just using the tables, although you should feel free to load the data in R and work with it if you so choose. They will be consistent with what you see on the tables.\nHint 2: The p-value of a variable in a simple linear regression is the same p-value one would get from a Pearson’s correlation (cor.test). The p-value is a function of the magnitude of the correlation coefficient (the higher the coefficient, the lower the p-value) and of sample size (larger samples lead to smaller p-values). For the correlations shown in the tables, they are between variables of the same length.)\n\ndata(\"house.selling.price.2\")\nnames(house.selling.price.2) <- c('Price','Size','Bedrooms','Bathrooms','New')\n\nsummary(house.selling.price.2)\n\n     Price             Size         Bedrooms       Bathrooms    \n Min.   : 17.50   Min.   :0.40   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 72.90   1st Qu.:1.33   1st Qu.:3.000   1st Qu.:2.000  \n Median : 96.00   Median :1.57   Median :3.000   Median :2.000  \n Mean   : 99.53   Mean   :1.65   Mean   :3.183   Mean   :1.957  \n 3rd Qu.:115.00   3rd Qu.:1.98   3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :309.40   Max.   :3.85   Max.   :5.000   Max.   :3.000  \n      New        \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3011  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n\n\n\na. For backward elimination, which variable would be deleted first? Why?\nIn backward elimination, the variable ‘bedrooms’ would be removed first as it has the highest p-value of the variables.\n\n\nb. For forward selection, which variable would be added first? Why?\nIn forward selection ‘size’ would be added first as it has the lowest p-value at 0.\n\n\nc. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?\n‘BEDS’ may have such a large p-value in the multiple regression model because of its relationship to ‘SIZE’ - the price of a house goes up as its size increases. The extent of the price increase would probably be affected by other variables such as ‘NEW’, since the age of a house also affects its price.\n\n\nd. Using software with these four predictors, find the model that would be selected using each criterion:\n  i. R^2 \n  ii. Adjusted R^2  \n  iii. PRESS  \n  iv. AIC\n  v. BIC  \nBackward elimination, forward selection, and stepwise regression point to one model: the one that excludes the variable ‘Bedrooms’. To confirm these findings, I’m going to compare the model without ‘Bedrooms’ to one that does.\n\nfit_bedrooms <- lm(Price~.,data=house.selling.price.2)\nfit_no_bedrooms <- lm(Price~.-Bedrooms,data=house.selling.price.2)\n\nLets create functions to gather the information needed to move forward with the problem.\n\nr_squared <- function(fit)+\n  summary(fit)$r.squared\nadjusted_rsq <- function(fit)+\n  summary(fit)$adj.r.squared\npress <- function(fit) {\n  pr <- residuals(fit)/(1-lm.influence(fit)$hat)\n  sum(pr^2)\n}\n\nNow lets apply these to the two models (fit_bedrooms and fit_no_bedrooms).\n\nbed_models <- list(fit_bedrooms, fit_no_bedrooms)\ndata.frame(bed_models = c('fit_bedrooms','fit_no_bedrooms'),\n           r_squared=sapply(bed_models,r_squared),\n           adjusted_rsq=sapply(bed_models, adjusted_rsq),\n           press=sapply(bed_models,press),\n           AIC=sapply(bed_models, AIC),\n           BIC=sapply(bed_models,BIC))\n\n       bed_models r_squared adjusted_rsq    press      AIC      BIC\n1    fit_bedrooms 0.8688630    0.8629022 28390.22 790.6225 805.8181\n2 fit_no_bedrooms 0.8681361    0.8636912 27860.05 789.1366 801.7996\n\n\nFor R^2 and Adjusted R^2, the larger the value, the more favorable it is. But for Press, AIC, and BIC the opposite is true: we want the smallest values we can get. With that in mind, the model ‘fit_no_bedrooms’ is more fitting as it has a higher Adjustred R^2 and lower values for Press, AIC, and BIC.\n\n\ne. Explain which model you prefer and why.\nBased on the comparison above, I prefer the model with ‘BEDS’ excluded."
  },
  {
    "objectID": "posts/KPopiela_HW5.html#question-2",
    "href": "posts/KPopiela_HW5.html#question-2",
    "title": "HW5",
    "section": "Question 2",
    "text": "Question 2\n(Data file: ‘trees’ from base R)\n(Data file: trees from base R) From the documentation: “This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular:\n\na. fit a multiple regression model with the Volume as the outcome and Girth and Height as the explanatory variables\n\ndata(trees)\ntree_mrm <- lm(Volume~Girth+Height,data=trees)\nsummary(tree_mrm)\n\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n\n\n\n\nb. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\npar(mfrow=c(2,3))\nplot(tree_mrm,which=1:6)\n\n\n\n\nLooking at the above matrix, I do think it has some violations. For example, the red line in the first plot violates that linearity assumption since its u-shaped rather than straight. Let’s compare with a similar plot.\n\ntree_d2 <- lm(Volume~Girth+I(Girth^2)+Height,data=trees)\nplot(tree_d2,which=1)\n\n\n\n\nUnlike the red line in the first plot of the initial matrix, this one does NOT violate the linearity assumption; this line is much straighter."
  },
  {
    "objectID": "posts/KPopiela_HW5.html#question-3",
    "href": "posts/KPopiela_HW5.html#question-3",
    "title": "HW5",
    "section": "Question 3",
    "text": "Question 3\n(Data file: florida in alr R package)\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan.\n\na. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\ndata(florida)\nfl_lrm <- lm(Buchanan~Bush,data=florida)\npar(mfrow=c(2,3))\nplot(fl_lrm, which=1:6)\n\n\n\n\nIn every plot in this matrix Palm Beach County is an outlier; it sits pretty far from all the other data points.\n\n\nb. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\nfl_log <- lm(log(Buchanan)~log(Bush), data=florida)\npar(mfrow=c(2,3))\nplot(fl_log, which=1:6)\n\n\n\n\nThis model didn’t change my initial opinion about Palm Beach County being an outlier, since that’s still the case. The only difference is that Palm Beach County appears to be less of an ourlier than it was in the first model."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html",
    "href": "posts/MEGHA JOSEPH_BLOG1.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "title": "Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nAccording to statistics,Cardiovascular diseases (CVDs) kill approximately 17 million people globally every year.Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies. People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management. Research question is: Which clinical feature will lead to high cardiovascular risk?"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "title": "Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\n1:Behavioral risk factors will not underline significant predictors of predicting Heart Failure.\n2:Behavioral risk factors will underline significant predictors of predicting Heart Failure ."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "title": "Project Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI am going to analyze a dataset of 299 patients with heart failure collected in 2015.This dataset is comprised of self-reported survey, with 13 clinical features. data.https://www.kaggle.com/datasets/whenamancodes/heart-failure-clinical-records. The important variables of research are ejection fraction, serum creatinine, and smoking.\n.\n\n\nCode\nlibrary(readr)\nmf <- read_csv(\"C:/Users/user/Downloads/Heart Failure Clinical Records.csv\")\nsummary(mf)\n``\n\n\nError: attempt to use zero-length variable name"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "title": "Project Proposal",
    "section": "References",
    "text": "References\n\nSurvival prediction of heart failure patients using machine learning techniques *. (n.d.). Retrieved October 11, 2022, from https://www.sciencedirect.com/science/article/pii/S2352914821002458\n\nMachine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html",
    "href": "posts/MeghaJoseph_hw1.html",
    "title": "HOME WORK1 603",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1",
    "href": "posts/MeghaJoseph_hw1.html#answer-1",
    "title": "HOME WORK1 603",
    "section": "Answer 1",
    "text": "Answer 1\n\n\nCode\nreadD <- read_excel(\"_data/LungCapData.xls\")\nreadD\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (a)",
    "text": "Answer 1 (a)\nDistribution of LungCap:\n\n\nCode\nhist(readD$LungCap)\n\n\n\n\n\nThe distribution is a normal distribution. ## Answer 1 (b)\n\n\nCode\nboxplot(readD$LungCap ~ readD$Gender)\n\n\n\n\n\nThe mean of males appear higher than females."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (c)",
    "text": "Answer 1 (c)\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(readD, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean of smokers is higher than the mean of non smokers and therefore it is not sensible."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (d)",
    "text": "Answer 1 (d)\n\n\nCode\nclass(readD$Age)\n\n\n[1] \"numeric\"\n\n\nCode\nreadD <- mutate(readD, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", \n                                            Age == 14 | Age == 15 ~ \"14 to 15\", \n                                            Age == 16 | Age == 17 ~ \"16 to 17\", \n                                            Age >= 18 ~ \"18 and above\"))\nggplot(readD, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nreadD %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nFrom the above results we can say that people from age group 10 and above smoke."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (f)",
    "text": "Answer 1 (f)\n\n\nCode\ncor(readD$LungCap,readD$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(readD$LungCap,readD$Age)\n\n\n[1] 8.738289\n\n\nFrom the data we can see that the covariance is positive and it shows that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. Therefore as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-2",
    "href": "posts/MeghaJoseph_hw1.html#answer-2",
    "title": "HOME WORK1 603",
    "section": "Answer 2",
    "text": "Answer 2\n\n\nCode\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nC<- data.frame(X, Frequency)\nC\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nC<-rename(C, PriorConvictions=X)\nC\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(C, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\n\n\nCode\nPO<-Frequency/810\nPO\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, PO)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\nAnswer\na: 19.75% b :9.38% c :89.14% d :10.86% e :1.28642 f: variance: 0.8572937 standard deviation: 0.9259016"
  },
  {
    "objectID": "posts/MeghaJoseph_HW2.html",
    "href": "posts/MeghaJoseph_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "##QUESTION1\n\n\nCode\nprocedure <- c('Bypass', 'Angiography')\nsamplesize <- c(539, 847)\nmeanwait <- c(19, 18)\nstandev <- c(10, 9)\n\nsurgdata <- data.frame(procedure, samplesize, meanwait, standev)\n\nsurgdata\n\n\n    procedure samplesize meanwait standev\n1      Bypass        539       19      10\n2 Angiography        847       18       9\n\n\n##QUESTION2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n##QUESTION3\n\n\nCode\nstdevBooks <- (200-30)/4\nmargerrorBooks <- (10/2)\nzBooks <- 1.96\n\nstdevBooks^2 * (zBooks/margerrorBooks)^2\n\n\n[1] 277.5556\n\n\n##QUESTION4 ##A\n\n\nCode\n(410-500)/(90/sqrt(9))\n\n\n[1] -3\n\n\nCode\npt(-3, 8)*2 \n\n\n[1] 0.01707168\n\n\n##B\n\n\nCode\n pt(-3, 8, lower.tail = TRUE)\n\n\n[1] 0.008535841\n\n\n##C\n\n\nCode\n pt(-3, 8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\n##QUESTION5 ##A1\n\n\nCode\n JonesT <- (519.5-500)/10\nJonesT\n\n\n[1] 1.95\n\n\n##A2\n\n\nCode\n JonesP <- pt(1.95, 999, lower.tail = FALSE)*2\nJonesP\n\n\n[1] 0.05145555\n\n\n##A3\n\n\nCode\nSmithT <- (519.7-500)/10\nSmithT\n\n\n[1] 1.97\n\n\n##A4\n\n\nCode\nSmithP <- pt(1.97, 999, lower.tail = FALSE)*2\nSmithP\n\n\n[1] 0.04911426\n\n\n##B\nWith an α-level of .05, the p-values that both Jones (P=.051) and Smith (P=.049) found are very close to equivalent. Although Jones’ P-value is slightly greater than α=.05 and Smith’s P-value is slightly less than α=.05, the proximity of the results should yield the same conclusion. Both P-values provide moderate evidence to reject the null hypothesis and indicate that the mean is not equal to 500. If we were to technically interpret the P-values, then Jones’ test would fail to reject the null hypothesis, and Smith’s test would reject the null hypothesis.\n##C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. For example, a P-value of .009 for a significance level of .05 provides much stronger evidence to reject the null than a P-value of .045, however both values allow for rejection of the null at the significance level .05. In the Jones/Smith example, reporting the results only as “P ≤ 0.05” versus “P > 0.05” will lead to different conclusions about very similar results (rejecting versus failing to reject the null).\n##QUESTION6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html",
    "href": "posts/MeghaJoseph_HW3.html",
    "title": "Home Work 3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-1",
    "href": "posts/MeghaJoseph_HW3.html#question-1",
    "title": "Home Work 3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11) \nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section",
    "href": "posts/MeghaJoseph_HW3.html#section",
    "title": "Home Work 3",
    "section": "1.1.1",
    "text": "1.1.1\nThe predictor variable is ppgdp. The response variable is fertility."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section-1",
    "href": "posts/MeghaJoseph_HW3.html#section-1",
    "title": "Home Work 3",
    "section": "1.1.2",
    "text": "1.1.2"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#section-2",
    "href": "posts/MeghaJoseph_HW3.html#section-2",
    "title": "Home Work 3",
    "section": "1.1.3",
    "text": "1.1.3"
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-2",
    "href": "posts/MeghaJoseph_HW3.html#question-2",
    "title": "Home Work 3",
    "section": "Question 2",
    "text": "Question 2\n\nThe slope of the prediction equation would change. It would be the initial version’s slope divided by 1.33 to account for the change in unit to pounds.\n\nb.The correlation does not change, because it standardizes the slope (thus is not impacted by unit of measure)."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-3",
    "href": "posts/MeghaJoseph_HW3.html#question-3",
    "title": "Home Work 3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\nhead(water)\n\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n\n\n\nCode\n#load dataset \ndata(water)\n\n#scatterplot matrix\npairs(water)\n\n\n\n\n\nCode\n#calculate the summary\nsummary(water)\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\n\nIn this scatterplot matrix, the precipitation levels for the ‘A’ named lakes seem to have a positive (relatively linear) correlation (although unsure how strong) with each other and the ‘O’ named lakes seem to have one as well with each other. The year variable does not appear to have a relationship to any of the variables. Also, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/MeghaJoseph_HW3.html#question-4",
    "href": "posts/MeghaJoseph_HW3.html#question-4",
    "title": "Home Work 3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\n# load dataset, select variables, preview dataset\ndata(Rateprof)\n\nRateprof <- Rateprof %>%\n  select(c(quality, clarity, helpfulness, easiness, raterInterest))  \n\nhead(Rateprof)\n\n\n   quality  clarity helpfulness easiness raterInterest\n1 4.636364 4.636364    4.636364 4.818182      3.545455\n2 4.318182 4.090909    4.545455 4.363636      4.000000\n3 4.790698 4.860465    4.720930 4.604651      3.432432\n4 4.250000 4.041667    4.458333 2.791667      3.181818\n5 4.684211 4.684211    4.684211 4.473684      4.214286\n6 4.233333 4.200000    4.266667 4.533333      3.916667\n\n\n\n\nCode\npairs(Rateprof)\n\n\n\n\n\nReferring to the scatterplot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality.\n##Question 5"
  },
  {
    "objectID": "posts/MeghaJoseph_HW4.html",
    "href": "posts/MeghaJoseph_HW4.html",
    "title": "HOME WORK 4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_HW4.html#question-answer-1",
    "href": "posts/MeghaJoseph_HW4.html#question-answer-1",
    "title": "HOME WORK 4",
    "section": "Question Answer 1",
    "text": "Question Answer 1\n#A\n\n\nCode\n# Predicted selling price\nSP <- function(a, b)\n{-10536 + 53.8*a + 2.84*b}\n\nSP(1240, 18000)\n\n\n[1] 107296\n\n\n\n\nCode\n# Residual\nRSL <- function(r, p)\n  {r- p}\nRSL (145000, 107296)\n\n\n[1] 37704\n\n\nThe predicted selling price is 107,296 dollars and the actual selling price is 145,000 dollars. The residual is 37,704 dollars, meaning that the house was sold for 37,704 dollars greater than predicted."
  },
  {
    "objectID": "posts/MeghaJoseph_HW4.html#question-answer-2",
    "href": "posts/MeghaJoseph_HW4.html#question-answer-2",
    "title": "HOME WORK 4",
    "section": "Question Answer 2",
    "text": "Question Answer 2"
  },
  {
    "objectID": "posts/MeghaJoseph_HW4.html#question-answer-3",
    "href": "posts/MeghaJoseph_HW4.html#question-answer-3",
    "title": "HOME WORK 4",
    "section": "Question Answer 3",
    "text": "Question Answer 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\n#A\n\n\nCode\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth size and New variable positively predict selling price. As we change $ 1 in price, it results in 116.132 change in size and 57736.283 units in New.\n\\[\\\\[0.2in]\\]\n#B\n\n\nCode\nnew_home <- house.selling.price %>% filter(New == 1)\nsummary(lm(Price ~ Size, data = new_home))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new_home)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold_home <- house.selling.price %>% filter(New == 0)\nsummary(lm(Price ~ Size, data = old_home))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old_home)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nFor the filtered data wrt new home and old home, Size positively predicts price (But by a greater value wrt new homes). Adjusted R-squared for the model is also much higher (0.91 vs. 0.58) for new home and old home respectively.\nNew_Price = 166.35 * Size - 100755.31\nOld_Price = 104.438 * Size - 22227.808\n#C\n\n\nCode\nSize <- 3000\nNew_Price = 166.35 * Size - 100755.31\nOld_Price = 104.438 * Size - 22227.808\nsprintf(\"New Price = %f\", New_Price)\n\n\n[1] \"New Price = 398294.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price)\n\n\n[1] \"Old Price = 291086.192000\"\n\n\n#D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n#E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nprice_with_sizeAndNew = ( -22227.81 + 104.44 * Size ) + ( -78527.50 + 61.92 * Size )\nprice_with_size = -22227.81 + 104.44 * Size\n#F\n\n\nCode\nSize <- 3000\nNew_Price_withSizeAndNew = ( -22227.81 + 104.44 * Size ) +( - 78527.50 + 61.92 * Size )\nOld_Price_withSize = -22227.81 + 104.44 * Size\nsprintf(\"New Price = %f\", New_Price_withSizeAndNew)\n\n\n[1] \"New Price = 398324.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price_withSize)\n\n\n[1] \"Old Price = 291092.190000\"\n\n\n#G\n\n\nCode\nSize <- 1500\nNew_Price_withSizeAndNew = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price_withSize = -22227.81 + 104.44 * Size\nsprintf(\"New Price = %f\", New_Price_withSizeAndNew)\n\n\n[1] \"New Price = 148784.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price_withSize)\n\n\n[1] \"Old Price = 134432.190000\"\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger.\n#H When we apply the interaction (having both size and new variable), then we see a significantly large negative coefficient. The adjusted r-squared for the model with Size and New variable combined is 0.7363 and the adjusted r-squared for the first model with just Size variable is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Although both models have almost similar adjusted r-squared value, I would prefer the model with interaction (with Size and New variable) because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/nboonstra_draft.html",
    "href": "posts/nboonstra_draft.html",
    "title": "DACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\nlibrary(plm)\n\n\nError in library(plm): there is no package called 'plm'\n\n\nCode\nlibrary(lmtest)\n\n\nError in library(lmtest): there is no package called 'lmtest'\n\n\nCode\nlibrary(sandwich)\nlibrary(stargazer)\n\n\nError in library(stargazer): there is no package called 'stargazer'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/nboonstra_draft.html#election-data",
    "href": "posts/nboonstra_draft.html#election-data",
    "title": "DACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)",
    "section": "Election Data",
    "text": "Election Data\n\n\nCode\nelections_read <- read_csv(\"_data/mit_election_1976_2020.csv\")\n\nelections_read <- elections_read %>% \n  mutate(party_simplified=str_to_lower(party_simplified)) %>% \n  mutate(state=str_to_title(state)) %>% \n  group_by(party_simplified,state,year) %>% \n  summarise(votes=sum(candidatevotes)) %>% \n  ungroup() %>% \n  pivot_wider(\n    names_from = party_simplified,\n    values_from = votes,\n    values_fill = 0\n  ) %>% \n  mutate(third = libertarian + other) %>% \n  select(!c(libertarian,other)) %>% \n  mutate(totvotes = democrat + republican + third) %>% \n  mutate(dem_rate = democrat/totvotes) %>% \n  mutate(rep_rate = republican/totvotes) %>% \n  mutate(third_rate = third/totvotes)\n\nhead(elections_read,20)\n\n\n# A tibble: 20 × 9\n   state    year democrat republican  third totvotes dem_rate rep_rate third_r…¹\n   <chr>   <dbl>    <dbl>      <dbl>  <dbl>    <dbl>    <dbl>    <dbl>     <dbl>\n 1 Alabama  1976   659170     504070  19610  1182850    0.557    0.426   0.0166 \n 2 Alabama  1980   636730     654192  51007  1341929    0.474    0.488   0.0380 \n 3 Alabama  1984   551899     872849  16965  1441713    0.383    0.605   0.0118 \n 4 Alabama  1988   549506     815576  13394  1378476    0.399    0.592   0.00972\n 5 Alabama  1992   690080     804283 193697  1688060    0.409    0.476   0.115  \n 6 Alabama  1996   662165     769044 103140  1534349    0.432    0.501   0.0672 \n 7 Alabama  2000   692611     941173  32488  1666272    0.416    0.565   0.0195 \n 8 Alabama  2004   693933    1176394  13088  1883415    0.368    0.625   0.00695\n 9 Alabama  2008   813479    1266546  19794  2099819    0.387    0.603   0.00943\n10 Alabama  2012   795696    1255925  22717  2074338    0.384    0.605   0.0110 \n11 Alabama  2016   729547    1318255  75570  2123372    0.344    0.621   0.0356 \n12 Alabama  2020   849624    1441170  32488  2323282    0.366    0.620   0.0140 \n13 Alaska   1976    44058      71555   7961   123574    0.357    0.579   0.0644 \n14 Alaska   1980    41842      86112  30491   158445    0.264    0.543   0.192  \n15 Alaska   1984    62007     138377   7221   207605    0.299    0.667   0.0348 \n16 Alaska   1988    72584     119251   8281   200116    0.363    0.596   0.0414 \n17 Alaska   1992    78294     102000  78212   258506    0.303    0.395   0.303  \n18 Alaska   1996    80380     122746  38494   241620    0.333    0.508   0.159  \n19 Alaska   2000    79004     167398  39158   285560    0.277    0.586   0.137  \n20 Alaska   2004   111025     190889  10684   312598    0.355    0.611   0.0342 \n# … with abbreviated variable name ¹​third_rate\n\n\nThe election data was obtained from the MIT Election Lab on October 10, 2022. These data are for U.S. Presidential elections between 1976 and 2020. Because of the availability of other data (namely turnout data), the cases used below will be, in basic models, elections between 1980 and 2020, and, in robust models, elections between 1996 and 2020."
  },
  {
    "objectID": "posts/nboonstra_draft.html#turnout-data",
    "href": "posts/nboonstra_draft.html#turnout-data",
    "title": "DACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)",
    "section": "Turnout Data",
    "text": "Turnout Data\n\n\nCode\n# 1980 to 2012\nturnout <- read_excel(\"_data/1980-2014 November General Election.xlsx\",skip=1)\n\nturnout <- turnout %>% \n  rename(\"year\" = 1) %>% \n  rename(\"skip2\" = 2) %>% \n  rename(\"skip3\" = 3) %>% \n  rename(\"state\" = 4) %>% \n  rename(\"totballots_vep_rate\" = 5) %>% \n  rename(\"highestoff_vep_rate\" = 6) %>% \n  rename(\"highestoff_vap_rate\" = 7) %>% \n  rename(\"totballots_count\" = 8) %>% \n  rename(\"highestoff_count\" = 9) %>% \n  rename(\"vep_count\" = 10) %>% \n  rename(\"vap_count\" = 11) %>% \n  rename(\"noncitizen_vap_rate\" = 12) %>% \n  rename(\"prison_count\" = 13) %>% \n  rename(\"probation_count\" = 14) %>% \n  rename(\"parole_count\" = 15) %>% \n  rename(\"totfelon_count\" = 16) %>% \n  rename(\"skip17\" = 17) %>% \n  select(!contains(\"skip\")) %>% \n  mutate(\"totfelon_vap_rate\" = (totfelon_count/vap_count)*100)\n\n\n# 2016\nturnout_2016 <- read_excel(\"_data/2016 November General Election.xlsx\",skip=1)\n\nturnout_2016 <- turnout_2016 %>% \n  mutate(year=2016) %>% \n  rename(\"state\" = 1) %>% \n  rename(\"skip2\" = 2) %>% \n  rename(\"skip3\" = 3) %>% \n  rename(\"totballots_vep_rate\" = 4) %>% \n  rename(\"highestoff_vep_rate\" = 5) %>% \n  rename(\"highestoff_vap_rate\" = 6) %>% \n  rename(\"totballots_count\" = 7) %>% \n  rename(\"highestoff_count\" = 8) %>% \n  rename(\"vep_count\" = 9) %>% \n  mutate(vep_count=floor(vep_count)) %>% \n  rename(\"vap_count\" = 10) %>% \n  rename(\"noncitizen_vap_rate\" = 11) %>% \n  rename(\"prison_count\" = 12) %>% \n  rename(\"probation_count\" = 13) %>% \n  rename(\"parole_count\" = 14) %>% \n  rename(\"totfelon_count\" = 15) %>% \n  mutate(totfelon_count = floor(totfelon_count)) %>% \n  rename(\"skip16\" = 16) %>% \n  rename(\"abbrev\" = 17) %>% \n  select(!contains(\"skip\")) %>% \n  mutate(\"totfelon_vap_rate\" = (totfelon_count/vap_count)*100) \n\nelections_read <- turnout_2016 %>% \n  select(c(state,abbrev)) %>% \n  right_join(elections_read)\n\nturnout_2016 <- turnout_2016 %>% \n  select(!contains(\"abbrev\"))\n\n\n# 2020\nturnout_2020 <- read_excel(\"_data/2020 November General Election.xlsx\",skip=1)\n\nturnout_2020 <- turnout_2020 %>% \n  mutate(year=2020) %>% \n  rename(\"state\" = 1) %>% \n  rename(\"skip2\" = 2) %>% \n  rename(\"skip3\" = 3) %>% \n  rename(\"totballots_count\" = 4) %>% \n  rename(\"highestoff_count\" = 5) %>% \n  rename(\"totballots_vep_rate\" = 6) %>% \n  rename(\"highestoff_vep_rate\" = 7) %>% \n  rename(\"vep_count\" = 8) %>% \n  mutate(vep_count = floor(vep_count)) %>% \n  rename(\"vap_count\" = 9) %>% \n  mutate(vap_count = floor(vap_count)) %>% \n  mutate(highestoff_vap_rate = highestoff_count/vap_count) %>% \n  rename(\"noncitizen_vap_rate\" = 10) %>% \n  rename(\"prison_count\" = 11) %>% \n  rename(\"probation_count\" = 12) %>% \n  rename(\"parole_count\" = 13) %>% \n  rename(\"totfelon_count\" = 14) %>% \n  mutate(totfelon_count = floor(totfelon_count)) %>% \n  mutate(totfelon_vap_rate = (totfelon_count/vap_count)*100) %>% \n  rename(\"skip15\" = 15) %>%\n  rename(\"skip16\" = 16) %>% \n  select(!contains(\"skip\")) %>% \n  mutate(state = str_replace(state, \"[*]\", \"\"))\n\n\n# putting it all together\nturnout <- turnout %>% \n  bind_rows(turnout_2016) %>% \n  bind_rows(turnout_2020) %>% \n  mutate(state=str_to_title(state))\n\nelections_read <- elections_read %>% \n  left_join(turnout)\n\nturnout %>% \n  filter(state!=\"United States\") %>% \n  arrange(state,year) %>% \n  head(.,20)\n\n\n# A tibble: 20 × 15\n    year state   totba…¹ highe…² highe…³ totba…⁴ highe…⁵ vep_c…⁶ vap_c…⁷ nonci…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  2020 \" Note…  NA      NA      NA          NA      NA      NA      NA NA     \n 2  1980 \"Alaba…  NA       0.492   0.487      NA 1341929 2726249 2753078  0.004 \n 3  1982 \"Alaba…  NA       0.406   0.401      NA 1128725 2780689 2813033  0.005 \n 4  1984 \"Alaba…  NA       0.509   0.503      NA 1441713 2831099 2867180  0.005 \n 5  1986 \"Alaba…  NA       0.431   0.425      NA 1236230 2869652 2911819  0.006 \n 6  1988 \"Alaba…  NA       0.475   0.467      NA 1378476 2901744 2949934  0.006 \n 7  1990 \"Alaba…  NA       0.411   0.404      NA 1215889 2956385 3012797  0.007 \n 8  1992 \"Alaba…  NA       0.557   0.545      NA 1688060 3030549 3096959  0.009 \n 9  1994 \"Alaba…  NA       0.388   0.379      NA 1201969 3095745 3170782  0.01  \n10  1996 \"Alaba…  NA       0.488   0.475      NA 1534349 3144249 3227688  0.012 \n11  1998 \"Alaba…  NA       0.412   0.4        NA 1317842 3202052 3293917  0.013 \n12  2000 \"Alaba…  NA       0.516   0.501      NA 1672551 3241682 3334576  0.015 \n13  2002 \"Alaba…  NA       0.415   0.404      NA 1364602 3285643 3371451  0.018 \n14  2004 \"Alaba…   0.574   0.572   0.552 1890317 1883415 3292608 3427542  0.019 \n15  2006 \"Alaba…  NA       0.375   0.359      NA 1250401 3338467 3513952  0.023 \n16  2008 \"Alaba…   0.61    0.608   0.584 2105622 2099819 3454510 3595708  0.022 \n17  2010 \"Alaba…   0.433   0.43    0.408 1503232 1494273 3472582 3661784  0.028 \n18  2012 \"Alaba…  NA       0.586   0.56       NA 2074338 3539217 3707440  0.026 \n19  2014 \"Alaba…   0.332   0.329   0.315 1191274 1180413 3588783 3751844  0.025 \n20  2016 \"Alaba…   0.591   0.588   0.563 2134061 2123372 3609447 3770142  0.0249\n# … with 5 more variables: prison_count <dbl>, probation_count <dbl>,\n#   parole_count <dbl>, totfelon_count <dbl>, totfelon_vap_rate <dbl>, and\n#   abbreviated variable names ¹​totballots_vep_rate, ²​highestoff_vep_rate,\n#   ³​highestoff_vap_rate, ⁴​totballots_count, ⁵​highestoff_count, ⁶​vep_count,\n#   ⁷​vap_count, ⁸​noncitizen_vap_rate\n\n\nTurnout data was obtained from the U.S. Elections Project on October 5 (2016 and 2020 sets) and October 11 (1980-2014 dataset), 2022. The U.S. Elections Project is generally considered the most reliable source of turnout data on U.S. general as well as midterm elections."
  },
  {
    "objectID": "posts/nboonstra_draft.html#cost-of-voting-index-covi-data",
    "href": "posts/nboonstra_draft.html#cost-of-voting-index-covi-data",
    "title": "DACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)",
    "section": "Cost of Voting Index (COVI) Data",
    "text": "Cost of Voting Index (COVI) Data\n\n\nCode\ncovi <- read_excel(\"_data/COVI Values 1996-2022 website.xlsx\",\n                   skip=1,\n                 col_types=c(\n                   \"skip\",\"text\",\"numeric\",\"skip\",\"skip\",\"numeric\",\"skip\",\"numeric\",\"skip\",\"skip\",\"skip\"),\n                 col_names=c(\n                   \"abbrev\",\"year\",\"covi_val\",\"covi_covid\"\n                 ))\n\ncovi <- covi %>% \n  filter(year<2022) %>% \n  mutate(covi_covid = case_when(\n    year == 2020 ~ covi_covid,\n    T ~ covi_val\n  ))\n\nelections_read <- elections_read %>% \n  left_join(covi) %>% \n  select(!abbrev)\n\nhead(covi,20)\n\n\n# A tibble: 20 × 4\n   abbrev  year covi_val covi_covid\n   <chr>  <dbl>    <dbl>      <dbl>\n 1 AL      1996 -0.399     -0.399  \n 2 AK      1996  0.664      0.664  \n 3 AZ      1996  0.821      0.821  \n 4 AR      1996  0.346      0.346  \n 5 CA      1996  0.530      0.530  \n 6 CO      1996  0.749      0.749  \n 7 CT      1996  0.0839     0.0839 \n 8 DE      1996  0.359      0.359  \n 9 FL      1996 -0.0565    -0.0565 \n10 GA      1996  0.00691    0.00691\n11 HI      1996  0.312      0.312  \n12 ID      1996 -1.18      -1.18   \n13 IL      1996  0.254      0.254  \n14 IN      1996  0.458      0.458  \n15 IA      1996 -0.0370    -0.0370 \n16 KS      1996  0.317      0.317  \n17 KY      1996  0.484      0.484  \n18 LA      1996  0.434      0.434  \n19 ME      1996 -2.17      -2.17   \n20 MD      1996  0.676      0.676  \n\n\nThe Cost of Voting Index (“COVI”) is an empirical measurement of voting ease and accessibility in the fifty U.S. states. (Unfortunately, this dataset does not include observations for the District of Columbia.) The main substance of the dataset comes from a paper published by professors at Northern Illinois, Jacksonville, and Wuhan Universities (Schraufnagel et al., 2020). The dataset was obtained October 10, 2022.\nBecause ballot access was a central aspect of the theoretical underpinnings of this study, it felt appropriate to include these COVI data in my analysis. As will be seen below, these data do appear to play a significant role in affecting partisan vote outcomes.\nThe covi_covid column differs from the covi_val column only for the year 2020, and accounts for the electoral changes made by many states in response to the COVID-19 pandemic in that year. The values from covi_covid are the ones used for this study."
  },
  {
    "objectID": "posts/nboonstra_draft.html#final-merged-dataset",
    "href": "posts/nboonstra_draft.html#final-merged-dataset",
    "title": "DACSS 603 Fall 2022 Final Project – Draft (Statistical Analysis)",
    "section": "Final Merged Dataset",
    "text": "Final Merged Dataset\n\n\nCode\nelections <- elections_read %>% \n  arrange(state,year) %>% \n  mutate(state = as_factor(state)) %>% \n  mutate(year = as_factor(year)) %>% \n  mutate(dem_win = case_when(\n    democrat > republican & democrat > third ~ 1,\n    T ~ 0\n  )) %>% \n  mutate(dem_rate = dem_rate * 100) %>% \n  mutate(rep_rate = rep_rate * 100) %>% \n  mutate(third_rate = third_rate * 100) %>% \n  mutate(totballots_vep_rate = totballots_vep_rate * 100) %>% \n  mutate(highestoff_vep_rate = highestoff_vep_rate * 100) %>% \n  mutate(highestoff_vap_rate = highestoff_vap_rate * 100) %>% \n  mutate(noncitizen_vap_rate = noncitizen_vap_rate * 100)\n\nhead(elections,20)\n\n\n# A tibble: 20 × 25\n   state   year  democrat repub…¹  third totvo…² dem_r…³ rep_r…⁴ third…⁵ totba…⁶\n   <fct>   <fct>    <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alabama 1976    659170  504070  19610 1182850    55.7    42.6   1.66     NA  \n 2 Alabama 1980    636730  654192  51007 1341929    47.4    48.8   3.80     NA  \n 3 Alabama 1984    551899  872849  16965 1441713    38.3    60.5   1.18     NA  \n 4 Alabama 1988    549506  815576  13394 1378476    39.9    59.2   0.972    NA  \n 5 Alabama 1992    690080  804283 193697 1688060    40.9    47.6  11.5      NA  \n 6 Alabama 1996    662165  769044 103140 1534349    43.2    50.1   6.72     NA  \n 7 Alabama 2000    692611  941173  32488 1666272    41.6    56.5   1.95     NA  \n 8 Alabama 2004    693933 1176394  13088 1883415    36.8    62.5   0.695    57.4\n 9 Alabama 2008    813479 1266546  19794 2099819    38.7    60.3   0.943    61  \n10 Alabama 2012    795696 1255925  22717 2074338    38.4    60.5   1.10     NA  \n11 Alabama 2016    729547 1318255  75570 2123372    34.4    62.1   3.56     59.1\n12 Alabama 2020    849624 1441170  32488 2323282    36.6    62.0   1.40     63.1\n13 Alaska  1976     44058   71555   7961  123574    35.7    57.9   6.44     NA  \n14 Alaska  1980     41842   86112  30491  158445    26.4    54.3  19.2      60.2\n15 Alaska  1984     62007  138377   7221  207605    29.9    66.7   3.48     62.1\n16 Alaska  1988     72584  119251   8281  200116    36.3    59.6   4.14     57.3\n17 Alaska  1992     78294  102000  78212  258506    30.3    39.5  30.3      67.1\n18 Alaska  1996     80380  122746  38494  241620    33.3    50.8  15.9      60.7\n19 Alaska  2000     79004  167398  39158  285560    27.7    58.6  13.7      68.7\n20 Alaska  2004    111025  190889  10684  312598    35.5    61.1   3.42     69.6\n# … with 15 more variables: highestoff_vep_rate <dbl>,\n#   highestoff_vap_rate <dbl>, totballots_count <dbl>, highestoff_count <dbl>,\n#   vep_count <dbl>, vap_count <dbl>, noncitizen_vap_rate <dbl>,\n#   prison_count <dbl>, probation_count <dbl>, parole_count <dbl>,\n#   totfelon_count <dbl>, totfelon_vap_rate <dbl>, covi_val <dbl>,\n#   covi_covid <dbl>, dem_win <dbl>, and abbreviated variable names\n#   ¹​republican, ²​totvotes, ³​dem_rate, ⁴​rep_rate, ⁵​third_rate, …\n\n\nThe final dataframe contains 612 observations (51 states \\(\\times\\) 12 elections) of 25 variables.\n\n\nCode\ncolnames(elections)\n\n\n [1] \"state\"               \"year\"                \"democrat\"           \n [4] \"republican\"          \"third\"               \"totvotes\"           \n [7] \"dem_rate\"            \"rep_rate\"            \"third_rate\"         \n[10] \"totballots_vep_rate\" \"highestoff_vep_rate\" \"highestoff_vap_rate\"\n[13] \"totballots_count\"    \"highestoff_count\"    \"vep_count\"          \n[16] \"vap_count\"           \"noncitizen_vap_rate\" \"prison_count\"       \n[19] \"probation_count\"     \"parole_count\"        \"totfelon_count\"     \n[22] \"totfelon_vap_rate\"   \"covi_val\"            \"covi_covid\"         \n[25] \"dem_win\"            \n\n\n(The party name variables record raw vote totals for each party or party group in a given state and year.)"
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html",
    "href": "posts/nboonstra_final_603_proposal.html",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "",
    "text": "Code\nrm(list=ls())\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#theory",
    "href": "posts/nboonstra_final_603_proposal.html#theory",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Theory",
    "text": "Theory\nA review of even such a small sample of the literature as the works mentioned above will clearly demonstrate that, beyond disagreement over the presence of partisan turnout bias, there is little consensus on the theoretical aspect of such a phenomenon. Before offering my hypothesis, therefore, I would like to briefly address this theoretical side of the argument.\nShaw and Petrocik (2020) take issue with a notion found in turnout bias literature, the notion being “that turnout is endogenous to candidate preference” (p. 53). They cite Downs’ (1957) famous equation, \\(V=(P*B)-C\\), as evidence that it is the intensity of one’s political beliefs, and not their direction, that determines the decision to vote or not, and that therefore turnout is not endogenous to candidate preference.\nI believe this argument misses a subtle nuance that is key to the turnout bias debate. Suppose that not all individuals in a given polity face the same costs to voting; assume, in other words, that a more accurate rendition of Downs’ equation would be \\(V_i=(P*B_i)-C_i\\), in which both cost of voting and the perceived benefit of a preferred candidate’s victory are unique to the individual. For the sake of this argument, the manner in which these costs are distributed is not important; only the fact that there are unequal costs matters. Suppose further that one of the parties in this polity has established itself as being the party that lobbies for a reduction in the cost of voting, particularly for those who face disproportionately high barriers. In a world of rational actors and perfect information, it would follow, ceteris paribus, that an individual who faced disproportionately high costs to voting would support this party, since this party would lobby to improve opportunities for this group. However, the very higher cost of voting that would motivate this individual to support this party could also prevent them from ultimately voting for that candidate in an election. Thus, it could be said that turnout is endogenous to candidate preference – or, more accurately, that the cost of voting is endogenous to both candidate preference and turnout.\nWe can apply this theoretical model to the American case. Certain individuals do face higher barriers to voting; unfortunately, unlike in the model, these barriers do tend to be distributed in a certain manner, often inequitably by race and socioeconomic status. Additionally, it would not be difficult to argue that, of the two major parties, the Democrats have placed themselves in the position of the party lobbying for expanded voting access and reduction of barriers to the ballot box, starting with their role in the Civil Rights movement and corresponding legislation, and continuing to the start of the present Congress and the introduction of H.R. 1, a bill explicitly aimed at expanding voting rights. Thus, while our world is not one of completely perfect information or completely rational actors, and while a number of factors contribute to partisan identity and vote choice, there is a reasonable case to be made that individuals who face barriers to voting, ceteris paribus, would be more likely to support the Democratic Party. Once again, these very barriers to voting that would push individuals toward the Democrats also can restrict them from expressing that preference at the ballot box. Thus, we have our situation of endogeneity between partisan preference and turnout."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "href": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Hypotheses",
    "text": "Hypotheses\nWith the theoretical argument out of the way, I can proceed to out line some of the hypotheses I would like to test with this project.\n\\(H_1\\): Higher turnout will benefit Democrats in state-level Presidential elections.\n\\(H_2\\): Democrats will perform better in state-level Presidential elections as turnout increases relative to the previous election in that state.\nThe distinction of state-level elections is an important one; Shaw and Petrocik (2020) tend to aggregate their data, either by assessing elections on the national level or by aggregating county-level data. In the United States, Presidential elections are conducted at the state level, and I believe that this is the appropriate level of analysis for this analysis."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "href": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Election Data, 1976-2020",
    "text": "Election Data, 1976-2020\nObtained from the MIT Election Project on 10/10/2022.\n\n\nCode\nelection_full <- read_csv(\"./_data/mit_election_1976_2020.csv\")\n\nelection_full <- election_full %>% \n  mutate(party_simplified2 = case_when(\n    party_detailed == \"DEMOCRAT\" ~ \"DEMOCRAT\",\n    party_detailed == \"REPUBLICAN\" ~ \"REPUBLICAN\",\n    party_detailed == \"LIBERTARIAN\" ~ \"LIBERTARIAN\",\n    party_detailed == \"GREEN\" ~ \"GREEN\",\n    party_detailed == \"INDEPENDENT\" ~ \"INDEPENDENT\",\n    TRUE ~ \"OTHER\"\n  )) %>% \n  mutate(party_dem = case_when(\n    party_detailed == \"DEMOCRAT\" ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(election_full, n=20)\n\n\n# A tibble: 20 × 17\n    year state    state…¹ state…² state…³ state…⁴ office candi…⁵ party…⁶ writein\n   <dbl> <chr>    <chr>     <dbl>   <dbl>   <dbl> <chr>  <chr>   <chr>   <lgl>  \n 1  1976 ALABAMA  AL            1      63      41 US PR… \"CARTE… DEMOCR… FALSE  \n 2  1976 ALABAMA  AL            1      63      41 US PR… \"FORD,… REPUBL… FALSE  \n 3  1976 ALABAMA  AL            1      63      41 US PR… \"MADDO… AMERIC… FALSE  \n 4  1976 ALABAMA  AL            1      63      41 US PR… \"BUBAR… PROHIB… FALSE  \n 5  1976 ALABAMA  AL            1      63      41 US PR… \"HALL,… COMMUN… FALSE  \n 6  1976 ALABAMA  AL            1      63      41 US PR… \"MACBR… LIBERT… FALSE  \n 7  1976 ALABAMA  AL            1      63      41 US PR…  <NA>   <NA>    TRUE   \n 8  1976 ALASKA   AK            2      94      81 US PR… \"FORD,… REPUBL… FALSE  \n 9  1976 ALASKA   AK            2      94      81 US PR… \"CARTE… DEMOCR… FALSE  \n10  1976 ALASKA   AK            2      94      81 US PR… \"MACBR… LIBERT… FALSE  \n11  1976 ALASKA   AK            2      94      81 US PR…  <NA>   <NA>    TRUE   \n12  1976 ARIZONA  AZ            4      86      61 US PR… \"FORD,… REPUBL… FALSE  \n13  1976 ARIZONA  AZ            4      86      61 US PR… \"CARTE… DEMOCR… FALSE  \n14  1976 ARIZONA  AZ            4      86      61 US PR… \"MCCAR… INDEPE… FALSE  \n15  1976 ARIZONA  AZ            4      86      61 US PR… \"MACBR… LIBERT… FALSE  \n16  1976 ARIZONA  AZ            4      86      61 US PR… \"CAMEJ… SOCIAL… FALSE  \n17  1976 ARIZONA  AZ            4      86      61 US PR… \"ANDER… AMERIC… FALSE  \n18  1976 ARIZONA  AZ            4      86      61 US PR… \"MADDO… AMERIC… FALSE  \n19  1976 ARIZONA  AZ            4      86      61 US PR…  <NA>   <NA>    TRUE   \n20  1976 ARKANSAS AR            5      71      42 US PR… \"CARTE… DEMOCR… FALSE  \n# … with 7 more variables: candidatevotes <dbl>, totalvotes <dbl>,\n#   version <dbl>, notes <lgl>, party_simplified <chr>,\n#   party_simplified2 <chr>, party_dem <dbl>, and abbreviated variable names\n#   ¹​state_po, ²​state_fips, ³​state_cen, ⁴​state_ic, ⁵​candidate, ⁶​party_detailed\n\n\nCode\ncolnames(election_full)\n\n\n [1] \"year\"              \"state\"             \"state_po\"         \n [4] \"state_fips\"        \"state_cen\"         \"state_ic\"         \n [7] \"office\"            \"candidate\"         \"party_detailed\"   \n[10] \"writein\"           \"candidatevotes\"    \"totalvotes\"       \n[13] \"version\"           \"notes\"             \"party_simplified\" \n[16] \"party_simplified2\" \"party_dem\"        \n\n\nCode\nsummary(election_full)\n\n\n      year         state             state_po           state_fips   \n Min.   :1976   Length:4287        Length:4287        Min.   : 1.00  \n 1st Qu.:1988   Class :character   Class :character   1st Qu.:16.00  \n Median :2000   Mode  :character   Mode  :character   Median :28.00  \n Mean   :1999                                         Mean   :28.62  \n 3rd Qu.:2012                                         3rd Qu.:41.00  \n Max.   :2020                                         Max.   :56.00  \n   state_cen        state_ic        office           candidate        \n Min.   :11.00   Min.   : 1.00   Length:4287        Length:4287       \n 1st Qu.:33.00   1st Qu.:22.00   Class :character   Class :character  \n Median :53.00   Median :42.00   Mode  :character   Mode  :character  \n Mean   :53.67   Mean   :39.75                                        \n 3rd Qu.:81.00   3rd Qu.:61.00                                        \n Max.   :95.00   Max.   :82.00                                        \n party_detailed      writein        candidatevotes       totalvotes      \n Length:4287        Mode :logical   Min.   :       0   Min.   :  123574  \n Class :character   FALSE:3807      1st Qu.:    1177   1st Qu.:  652274  \n Mode  :character   TRUE :477       Median :    7499   Median : 1569180  \n                    NA's :3         Mean   :  311908   Mean   : 2366924  \n                                    3rd Qu.:  199242   3rd Qu.: 3033118  \n                                    Max.   :11110250   Max.   :17500881  \n    version          notes         party_simplified   party_simplified2 \n Min.   :20210113   Mode:logical   Length:4287        Length:4287       \n 1st Qu.:20210113   NA's:4287      Class :character   Class :character  \n Median :20210113                  Mode  :character   Mode  :character  \n Mean   :20210113                                                       \n 3rd Qu.:20210113                                                       \n Max.   :20210113                                                       \n   party_dem     \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.1428  \n 3rd Qu.:0.0000  \n Max.   :1.0000  \n\n\nThis dataframe contains state-level election results for all 50 states and the District of Columbia for the six Presidential elections from 1976 to 2020. (I am currently not sure that I will use that entire date range, particularly because it does not exactly coincide with the turnout data available, but for now I am including the full data set.) Included in the dataframe are candidate vote totals and party affiliations, which I have used to add an extra column, party_dem, which is a dummy variable recording whether or not a given candidate is a Democrat. The data already come in tidy, which is a nice touch; a “case” or row is a given candidate’s performance in a given state’s Presidential election in a given year."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "href": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Turnout data, 1980-2014",
    "text": "Turnout data, 1980-2014\nObtained from the US Elections Project on 10/11/2022.\n\n\nCode\nturnout <- read_excel(\"./_data/1980-2014 November General Election.xlsx\",\n                      skip=2,\n                      col_types=c(\n                        \"numeric\",\"skip\",\"skip\",\"text\",\n                        \"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\"\n                      ),\n                      col_names=c(\n                        \"year\",\"state\",\n                        \"totballots_vep_rate\",\"highestoff_vep_rate\",\"highestoff_vap_rate\",\n                        \"totalballots_count\",\"highestoff_count\",\"vep_count\",\"vap_count\",\n                        \"noncitizen_percent\",\"prison_count\",\"probation_count\",\n                        \"parole_count\",\"totineligible_count\",\"overseas_count\"\n                      ))\n\nhead(turnout,n=20)\n\n\n# A tibble: 20 × 15\n    year state   totba…¹ highe…² highe…³ total…⁴ highe…⁵ vep_c…⁶ vap_c…⁷ nonci…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  2014 United…   0.367   0.36    0.332  8.33e7  8.17e7  2.27e8  2.46e8   0.084\n 2  2014 Alabama   0.332   0.329   0.315  1.19e6  1.18e6  3.59e6  3.75e6   0.025\n 3  2014 Alaska    0.548   0.542   0.51   2.85e5  2.82e5  5.21e5  5.53e5   0.039\n 4  2014 Arizona   0.341   0.334   0.295  1.54e6  1.51e6  4.51e6  5.11e6   0.101\n 5  2014 Arkans…   0.403   0.401   0.375  8.53e5  8.49e5  2.12e6  2.26e6   0.04 \n 6  2014 Califo…   0.307   0.299   0.247  7.51e6  7.32e6  2.44e7  2.96e7   0.168\n 7  2014 Colora…   0.547   0.537   0.494  2.08e6  2.04e6  3.80e6  4.13e6   0.072\n 8  2014 Connec…   0.425   0.423   0.385  1.10e6  1.09e6  2.58e6  2.83e6   0.082\n 9  2014 Delawa…   0.349   0.343   0.318  2.38e5  2.34e5  6.82e5  7.35e5   0.051\n10  2014 Distri…   0.357   0.353   0.32   1.77e5  1.75e5  4.96e5  5.47e5   0.094\n11  2014 Florida   0.433   0.428   0.376  6.03e6  5.95e6  1.39e7  1.58e7   0.106\n12  2014 Georgia   0.386   0.382   0.338  2.60e6  2.57e6  6.73e6  7.60e6   0.071\n13  2014 Hawaii    0.365   0.362   0.329  3.70e5  3.66e5  1.01e6  1.11e6   0.086\n14  2014 Idaho     0.398   0.393   0.365  4.45e5  4.40e5  1.12e6  1.21e6   0.046\n15  2014 Illino…   0.408   0.402   0.366  3.68e6  3.63e6  9.03e6  9.92e6   0.085\n16  2014 Indiana   0.287   0.278   0.267  1.39e6  1.34e6  4.83e6  5.03e6   0.035\n17  2014 Iowa      0.503   0.498   0.473  1.14e6  1.13e6  2.27e6  2.39e6   0.036\n18  2014 Kansas    0.433   0.425   0.398  8.87e5  8.70e5  2.05e6  2.18e6   0.052\n19  2014 Kentuc…   0.449   0.442   0.422  1.46e6  1.44e6  3.25e6  3.41e6   0.027\n20  2014 Louisi…   0.449   0.439   0.415  1.50e6  1.47e6  3.35e6  3.55e6   0.03 \n# … with 5 more variables: prison_count <dbl>, probation_count <dbl>,\n#   parole_count <dbl>, totineligible_count <dbl>, overseas_count <dbl>, and\n#   abbreviated variable names ¹​totballots_vep_rate, ²​highestoff_vep_rate,\n#   ³​highestoff_vap_rate, ⁴​totalballots_count, ⁵​highestoff_count, ⁶​vep_count,\n#   ⁷​vap_count, ⁸​noncitizen_percent\n\n\nCode\ncolnames(turnout)\n\n\n [1] \"year\"                \"state\"               \"totballots_vep_rate\"\n [4] \"highestoff_vep_rate\" \"highestoff_vap_rate\" \"totalballots_count\" \n [7] \"highestoff_count\"    \"vep_count\"           \"vap_count\"          \n[10] \"noncitizen_percent\"  \"prison_count\"        \"probation_count\"    \n[13] \"parole_count\"        \"totineligible_count\" \"overseas_count\"     \n\n\nCode\nsummary(turnout)\n\n\n      year         state           totballots_vep_rate highestoff_vep_rate\n Min.   :1980   Length:936         Min.   :0.0000      Min.   :0.2020     \n 1st Qu.:1988   Class :character   1st Qu.:0.4310      1st Qu.:0.4140     \n Median :1997   Mode  :character   Median :0.5200      Median :0.5010     \n Mean   :1997                      Mean   :0.5125      Mean   :0.4993     \n 3rd Qu.:2006                      3rd Qu.:0.6040      3rd Qu.:0.5840     \n Max.   :2014                      Max.   :0.7880      Max.   :0.7840     \n                                   NA's   :215         NA's   :1          \n highestoff_vap_rate totalballots_count  highestoff_count   \n Min.   :0.1990      Min.   :   122356   Min.   :   117623  \n 1st Qu.:0.3895      1st Qu.:   422851   1st Qu.:   488820  \n Median :0.4770      Median :  1170867   Median :  1236230  \n Mean   :0.4733      Mean   :  3074280   Mean   :  3509231  \n 3rd Qu.:0.5560      3rd Qu.:  2395791   3rd Qu.:  2336586  \n Max.   :0.7390      Max.   :132609063   Max.   :131304731  \n NA's   :1           NA's   :223         NA's   :1          \n   vep_count           vap_count         noncitizen_percent  prison_count    \n Min.   :   270122   Min.   :   277261   Min.   :0.00400    Min.   :      0  \n 1st Qu.:   999644   1st Qu.:  1044366   1st Qu.:0.01500    1st Qu.:   3464  \n Median :  2662524   Median :  2778086   Median :0.03100    Median :  10018  \n Mean   :  7277622   Mean   :  7840064   Mean   :0.04344    Mean   :  39257  \n 3rd Qu.:  4569632   3rd Qu.:  4898253   3rd Qu.:0.06600    3rd Qu.:  24819  \n Max.   :227157964   Max.   :245712915   Max.   :0.18900    Max.   :1605448  \n                                                                             \n probation_count    parole_count    totineligible_count overseas_count   \n Min.   :      0   Min.   :     0   Min.   :      0     Min.   :   6916  \n 1st Qu.:      0   1st Qu.:     0   1st Qu.:   6210     1st Qu.:  43108  \n Median :   7982   Median :  1870   Median :  21329     Median :  89605  \n Mean   :  67542   Mean   : 16227   Mean   :  90039     Mean   : 920963  \n 3rd Qu.:  38902   3rd Qu.:  6592   3rd Qu.:  52525     3rd Qu.:1803021  \n Max.   :2451708   Max.   :637410   Max.   :3363118     Max.   :5345814  \n                                                        NA's   :867      \n\n\nAdditional turnout data are available from the USEP by election from 2000-2020, albeit in their own individual spreadsheets; I may end up merging the 2016 and 2020 spreadsheets into this 1980-2014 set. It is important to note that this dataset includes observations for both Presidential and midterm election years, while I only intend to analyze Presidential elections.\nThis dataset makes distinctions between turnout based on voting-age population (VAP) and voting-eligible population (VEP). The literature generally agrees that VEP is the most reliable and consistent measure. However, given that one of the main differences between the two is the barrier of felony disenfranchisement, a barrier that is often inequitably distributed by race, I may end up using VAP turnout in my analysis; I have not yet decided as of the time of this submission."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "href": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Voter ID data, 2000-2020",
    "text": "Voter ID data, 2000-2020\nObtained from the National Conference of State Legislatures, who kindly provided via email a spreadsheet version of the data on this webpage on 10/11/2022.\n\n\nCode\nvoter_id <- read_excel(\"./_data/voter_id_chronology.xlsx\",\n                      skip = 2,\n                      col_types = c(\"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"skip\"))\n\nvoter_id <- voter_id %>% \n  pivot_longer(cols=c(2:7),\n               names_to=\"year\",\n               values_to=\"id_text\") %>% \n  mutate(id_req = case_when(\n    grepl(\"no id\", id_text, ignore.case = TRUE) ~ 0,\n    TRUE ~ 1\n  )) %>% \n  mutate(id_strict = case_when(\n    grepl(\"Strict\", id_text) ~ 1,\n    TRUE ~ 0\n  )) %>% \n  mutate(id_photo = case_when(\n    grepl(\" photo\", id_text, ignore.case = TRUE) ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(voter_id,n=20)\n\n\n# A tibble: 20 × 6\n   State    year  id_text                 id_req id_strict id_photo\n   <chr>    <chr> <chr>                    <dbl>     <dbl>    <dbl>\n 1 Alabama  2000  No ID required at polls      0         0        0\n 2 Alabama  2004  Non-strict, non-photo        1         0        0\n 3 Alabama  2008  Non-strict, non-photo        1         0        0\n 4 Alabama  2012  Non-strict, non-photo        1         0        0\n 5 Alabama  2016  Non-strict, photo            1         0        1\n 6 Alabama  2020  Non-strict, photo            1         0        1\n 7 Alaska   2000  Non-strict, non-photo        1         0        0\n 8 Alaska   2004  Non-strict, non-photo        1         0        0\n 9 Alaska   2008  Non-strict, non-photo        1         0        0\n10 Alaska   2012  Non-strict, non-photo        1         0        0\n11 Alaska   2016  Non-strict, non-photo        1         0        0\n12 Alaska   2020  Non-strict, non-photo        1         0        0\n13 Arizona  2000  No ID required at polls      0         0        0\n14 Arizona  2004  No ID required at polls      0         0        0\n15 Arizona  2008  Strict non-photo             1         1        0\n16 Arizona  2012  Strict non-photo             1         1        0\n17 Arizona  2016  Strict non-photo             1         1        0\n18 Arizona  2020  Strict non-photo             1         1        0\n19 Arkansas 2000  Non-strict, non-photo        1         0        0\n20 Arkansas 2004  Non-strict, non-photo        1         0        0\n\n\nCode\ncolnames(voter_id)\n\n\n[1] \"State\"     \"year\"      \"id_text\"   \"id_req\"    \"id_strict\" \"id_photo\" \n\n\nCode\nsummary(voter_id)\n\n\n    State               year             id_text              id_req      \n Length:306         Length:306         Length:306         Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :1.0000  \n                                                          Mean   :0.5033  \n                                                          3rd Qu.:1.0000  \n                                                          Max.   :1.0000  \n   id_strict          id_photo     \n Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000  \n Mean   :0.09804   Mean   :0.1928  \n 3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000  \n\n\nGiven that barriers to voting factor into the argument behind my research, I wanted to include data on voter ID laws in my analysis, as a controlling (or other type of) variable. The data here track voter ID laws across all 50 U.S. states and the District of Columbia from 2000 to 2020.\nThese data are surprisingly well balanced when it comes to the occurrence of voter ID laws; 50.33 percent of elections were held under voter-ID laws of some sort. Cases are also specified by whether or not a voter ID law was strict (i.e. required the voter to cast a provisional ballot and verify their identity after Election Day), and whether or not the state required a photo on the identification. Strict voter ID laws are the most rare, occurring in only 9.8 percent of elections in the data set; photo requirements are slightly more common, occurring in 19.28 percent of elections."
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-1",
    "href": "posts/Niharika_HW1.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-data",
    "href": "posts/Niharika_HW1.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/Niharika_HW1.html#a",
    "href": "posts/Niharika_HW1.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/Niharika_HW1.html#b",
    "href": "posts/Niharika_HW1.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/Niharika_HW1.html#c",
    "href": "posts/Niharika_HW1.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/Niharika_HW1.html#d",
    "href": "posts/Niharika_HW1.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/Niharika_HW1.html#e",
    "href": "posts/Niharika_HW1.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/Niharika_HW1.html#f",
    "href": "posts/Niharika_HW1.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-2",
    "href": "posts/Niharika_HW1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-the-table",
    "href": "posts/Niharika_HW1.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/Niharika_HW1.html#a-1",
    "href": "posts/Niharika_HW1.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/Niharika_HW1.html#b-1",
    "href": "posts/Niharika_HW1.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/Niharika_HW1.html#c-1",
    "href": "posts/Niharika_HW1.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/Niharika_HW1.html#d-1",
    "href": "posts/Niharika_HW1.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#e-1",
    "href": "posts/Niharika_HW1.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#f-1",
    "href": "posts/Niharika_HW1.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html",
    "href": "posts/NiyatiSharma_blog1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#introduction",
    "href": "posts/NiyatiSharma_blog1.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nCredit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the leader.So the purpose of credit analysis is to determine the creditworthiness of borrowers by measuring the risk of loss that the lender is exposed to.When calculating the credit risk of a particular borrower, lenders consider various factors like analyze different documents, such as the borrower’s income statement, balance sheet, credit reports, and other documents that reveal the financial situation of the borrower. to evaluate the characteristics of the borrower and conditions of the loan to estimate the probability of default and the subsequent risk of financial loss."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#research-question",
    "href": "posts/NiyatiSharma_blog1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nQ1. How credit risk depends on the age of the person. Q2. Dominating factor on which credit risk depends. Q3. Is credit risk depends on loan_intent?"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#hypothesis",
    "href": "posts/NiyatiSharma_blog1.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nAccording to research credit risk of a particular borrower, lenders consider various factors include the borrower’s capacity to repay are income, character, house ownership, and credit history. Check the relationship between the age, income with credit risk with new dataset."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#dataset",
    "href": "posts/NiyatiSharma_blog1.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThis dataset contains columns simulating credit bureau data, factors on which credit risk depends. The variables of interest for me are income, age, employment length and home ownership.\n\n\nCode\nlibrary(readr)\ndf <- read_csv(\"C:/Users/Lenovo/Downloads/credit_risk_dataset_1.csv\")\n\n\nRows: 32581 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): person_home_ownership, loan_intent, loan_grade, cb_person_default_o...\ndbl (8): person_age, person_income, person_emp_length, loan_amnt, loan_int_r...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nsummary(df)\n\n\n   person_age     person_income     person_home_ownership person_emp_length\n Min.   : 20.00   Min.   :   4000   Length:32581          Min.   :  0.00   \n 1st Qu.: 23.00   1st Qu.:  38500   Class :character      1st Qu.:  2.00   \n Median : 26.00   Median :  55000   Mode  :character      Median :  4.00   \n Mean   : 27.73   Mean   :  66075                         Mean   :  4.79   \n 3rd Qu.: 30.00   3rd Qu.:  79200                         3rd Qu.:  7.00   \n Max.   :144.00   Max.   :6000000                         Max.   :123.00   \n                                                          NA's   :895      \n loan_intent         loan_grade          loan_amnt     loan_int_rate  \n Length:32581       Length:32581       Min.   :  500   Min.   : 5.42  \n Class :character   Class :character   1st Qu.: 5000   1st Qu.: 7.90  \n Mode  :character   Mode  :character   Median : 8000   Median :10.99  \n                                       Mean   : 9589   Mean   :11.01  \n                                       3rd Qu.:12200   3rd Qu.:13.47  \n                                       Max.   :35000   Max.   :23.22  \n                                                       NA's   :3116   \n  loan_status     loan_percent_income cb_person_default_on_file\n Min.   :0.0000   Min.   :0.0000      Length:32581             \n 1st Qu.:0.0000   1st Qu.:0.0900      Class :character         \n Median :0.0000   Median :0.1500      Mode  :character         \n Mean   :0.2182   Mean   :0.1702                               \n 3rd Qu.:0.0000   3rd Qu.:0.2300                               \n Max.   :1.0000   Max.   :0.8300                               \n                                                               \n cb_person_cred_hist_length\n Min.   : 2.000            \n 1st Qu.: 3.000            \n Median : 4.000            \n Mean   : 5.804            \n 3rd Qu.: 8.000            \n Max.   :30.000"
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html",
    "href": "posts/NiyatiSharma_blog2.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.2\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(caret)\n\n\nWarning: package 'caret' was built under R version 4.2.2\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\nlibrary(ROCR)\n\n\nWarning: package 'ROCR' was built under R version 4.2.2\n\n\nCode\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\nWarning: package 'rpart.plot' was built under R version 4.2.2\n\n\nCode\nlibrary(rattle)\n\n\nWarning: package 'rattle' was built under R version 4.2.2\n\n\nLoading required package: bitops\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n\n\nCode\nlibrary(randomForest)\n\n\nWarning: package 'randomForest' was built under R version 4.2.2\n\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nCode\nlibrary(ggplot2)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#introduction",
    "href": "posts/NiyatiSharma_blog2.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nCredit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the leader.So the purpose of credit analysis is to determine the creditworthiness of borrowers by measuring the risk of loss that the lender is exposed to.When calculating the credit risk of a particular borrower, lenders consider various factors like analyze different documents, such as the borrower’s income statement, balance sheet, credit reports, and other documents that reveal the financial situation of the borrower. to evaluate the characteristics of the borrower and conditions of the loan to estimate the probability of default and the subsequent risk of financial loss."
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#research-question",
    "href": "posts/NiyatiSharma_blog2.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nQ1. How credit risk depends on the age of the person? Q2. Does credit risk depends on occupation?\nThe purpose of this project is to take a data set of loan applications and build a predictive model for making a decision as to whether to approve a loan based on the applicant’s profile."
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#hypothesis",
    "href": "posts/NiyatiSharma_blog2.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nAccording to research credit risk of a particular borrower, lenders consider various factors include the borrower’s capacity to repay are income, character, house ownership, and credit history. Check the relationship between the age, income with credit risk with new dataset. For this research, the quantitative data was used. Based on probability techniques, this research has chosen to use random sampling method over the period 2000-2013, based on documents and records of applicants for an Iranian commercial bank. Sample estimation has been done by a pretest sample size of 90 cases and according to the sample size formula, which are derived from legal customers’ profiles. In this study dependent variables are divided in two groups. Good and bad legal customers; the aim of this study is to estimate the important independent variables. In this regard, good customer is a company which repays its loan plus the profit at the due date and in contrast, bad customer is a company which don’t repay at the due date. To differentiate between good and bad customers in our statistical analysis calculations, 0 is illustrating good customers and 1 is illustrating bad customers. Five hypotheses are defined as follows: H1: There is a significant and positive relationship between interest rate and credit risk. H2: There is a significant and positive relationship between due date of repayment and credit risk. H3: There is a significant and positive relationship between history of legal customer relationship with the bank of repayment and credit risk. H4: There is a significant and positive relationship between delay Time of maturity and credit risk. H5: There is a significant and positive relationship between customer in industrial sectors and credit risk."
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#dataset",
    "href": "posts/NiyatiSharma_blog2.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThe data for the analysis is a set of 1000 German credit applications with 20 different attributes of the applicant. The original data is from the UCI Machine Learning Repository but the CSV version used in this analysis can be found from the Penn State University website (https://onlinecourses.science.psu.edu/stat857/node/215).\nThe following code can be used to determine if an applicant is credit worthy and if he (or she) represents a good credit risk to the lender. Several methods are applied to the data to help make this determination. We will look at them in this case.\n\n\nCode\nlibrary(readr)\ncredit <- read_csv(\"_data/german_credit.csv\")\n\n\nRows: 1000 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (21): Creditability, Account Balance, Duration of Credit (month), Paymen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nstr(credit)\n\n\nspc_tbl_ [1,000 × 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Creditability                    : num [1:1000] 1 1 1 1 1 1 1 1 1 1 ...\n $ Account Balance                  : num [1:1000] 1 1 2 1 1 1 1 1 4 2 ...\n $ Duration of Credit (month)       : num [1:1000] 18 9 12 12 12 10 8 6 18 24 ...\n $ Payment Status of Previous Credit: num [1:1000] 4 4 2 4 4 4 4 4 4 2 ...\n $ Purpose                          : num [1:1000] 2 0 9 0 0 0 0 0 3 3 ...\n $ Credit Amount                    : num [1:1000] 1049 2799 841 2122 2171 ...\n $ Value Savings/Stocks             : num [1:1000] 1 1 2 1 1 1 1 1 1 3 ...\n $ Length of current employment     : num [1:1000] 2 3 4 3 3 2 4 2 1 1 ...\n $ Instalment per cent              : num [1:1000] 4 2 2 3 4 1 1 2 4 1 ...\n $ Sex & Marital Status             : num [1:1000] 2 3 2 3 3 3 3 3 2 2 ...\n $ Guarantors                       : num [1:1000] 1 1 1 1 1 1 1 1 1 1 ...\n $ Duration in Current address      : num [1:1000] 4 2 4 2 4 3 4 4 4 4 ...\n $ Most valuable available asset    : num [1:1000] 2 1 1 1 2 1 1 1 3 4 ...\n $ Age (years)                      : num [1:1000] 21 36 23 39 38 48 39 40 65 23 ...\n $ Concurrent Credits               : num [1:1000] 3 3 3 3 1 3 3 3 3 3 ...\n $ Type of apartment                : num [1:1000] 1 1 1 1 2 1 2 2 2 1 ...\n $ No of Credits at this Bank       : num [1:1000] 1 2 1 2 2 2 2 1 2 1 ...\n $ Occupation                       : num [1:1000] 3 3 2 2 2 2 2 2 1 1 ...\n $ No of dependents                 : num [1:1000] 1 2 1 2 1 2 1 2 1 1 ...\n $ Telephone                        : num [1:1000] 1 1 1 1 1 1 1 1 1 1 ...\n $ Foreign Worker                   : num [1:1000] 1 1 1 2 2 2 2 2 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Creditability = col_double(),\n  ..   `Account Balance` = col_double(),\n  ..   `Duration of Credit (month)` = col_double(),\n  ..   `Payment Status of Previous Credit` = col_double(),\n  ..   Purpose = col_double(),\n  ..   `Credit Amount` = col_double(),\n  ..   `Value Savings/Stocks` = col_double(),\n  ..   `Length of current employment` = col_double(),\n  ..   `Instalment per cent` = col_double(),\n  ..   `Sex & Marital Status` = col_double(),\n  ..   Guarantors = col_double(),\n  ..   `Duration in Current address` = col_double(),\n  ..   `Most valuable available asset` = col_double(),\n  ..   `Age (years)` = col_double(),\n  ..   `Concurrent Credits` = col_double(),\n  ..   `Type of apartment` = col_double(),\n  ..   `No of Credits at this Bank` = col_double(),\n  ..   Occupation = col_double(),\n  ..   `No of dependents` = col_double(),\n  ..   Telephone = col_double(),\n  ..   `Foreign Worker` = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#exploratory-analysis",
    "href": "posts/NiyatiSharma_blog2.html#exploratory-analysis",
    "title": "Final Project Proposal",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nBefore starting the modeling phase, it is important to explore the data to get an idea of any patterns or areas of interest.\nThe first thing is to examine how many examples of good and bad credit risk there are.\n\n\nCode\nprint(Credit_new1$Creditability)\n\n\n   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [38] 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [149] 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [223] 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [297] 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [371] 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n [445] 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n [519] 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n [556] 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n [593] 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n [630] 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n [667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n [704] 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [741] 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [778] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [815] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [852] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [889] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [926] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [963] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[1000] 0\n\n\n\n\nCode\ng <- ggplot(Credit_new1, aes(Creditability)) +\n        geom_bar(fill = \"#4EB25A\") +\n        theme(axis.title.x=element_blank()) + \n        theme(axis.title.y=element_blank()) +\n  #      geom_text(aes(label =c(\"Bad\", \"Good\"))) +\n        scale_y_continuous(breaks=seq(0,700,100)) +\n        scale_x_discrete(labels = c(\"Bad\",\"Good\")) +\n        ggtitle(\"Count of Good and Bad Credit Risks\")\ng"
  },
  {
    "objectID": "posts/NiyatiSharma_blog2.html#references",
    "href": "posts/NiyatiSharma_blog2.html#references",
    "title": "Final Project Proposal",
    "section": "References",
    "text": "References\n1.Al-Tamimi, H. and Al-Mazrooei, M. (2007), “Banks’ risk management: a comparison study of UAE national and foreign banks”, The Journal of Risk Finance, Vol. 8 No. 4, pp. 394-409. 2.Angelini, E., di Tollo, G., & Roli, A. (2008). A neural network approach for credit risk evaluation. The Quarterly Review of Economics and Finance, 48(4), 733-755. http://dx.doi.org/10.1016/j.qref.2007.04.001. 3.Beatty, A., and S. Liao. 2011. Do Delays in Expected Loss Recognition Affect Banks’ Willingness to Lend? Journal of Accounting & Economics 52 (1): 1-20. DOI: 10.1016/j.jacceco.2011.02.002 4.Eletter, S. F., & Yaseen, S. G. (2010). Applying Neural Networks for Loan Decisions in the Jordanian Commercial Banking System. International Journal of Computer Science and Network Security, 10(1), 209-214. 5.Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feed forward networks are universal approximators. Neural Networks, 2(5), 359-366.http://dx.doi.org/10.1016/0893-6080(89)90020-8 6.Ghodselahi, A., & Amirmadhi, A. (2011). Application of Artificial Intelligence Techniques for Credit Risk Evaluation. International Journal of Modeling and Optimization, 1(3), 243-249. http://dx.doi.org/10.7763/ IJMO.2011.V1.43 7.Gouvêa, M. A., & Gonçalves, E. B. (2007). Credit Risk Analysis Applying Logistic Regression, Neural Networks and Genetic Algorithms Models. Paper presented at the Production and Operations Management Society (POMS), Dallas, Texas, U.S.A. 8.Hall, M. J. B., Muljawan, D., Suprayogi, & Moorena, L. (2009). Using the artificial neural network to assess bank credit risk: a case study of Indonesia. Applied Financial Economics, 19(22), 1825-1846. http://dx.doi.org/10.1080/09603100903018760 9.Khashman, A. (2010). Neural networks for credit risk evaluation: Investigation of different neural models and learning schemes. Expert Syst. Appl., 37(9), 6233-6239. http://dx.doi.org/10.1016/ j.eswa.2010.02.101 10. Matoussi, H. & Abdelmoula, A. (2009). Using a Neural Network-Based Methodology for Credit–Risk Evaluation of a Tunisian Bank. Middle Eastern Finance and Economics, 4, 117-140"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html",
    "href": "posts/NiyatiSharma_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\n\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-1",
    "href": "posts/NiyatiSharma_HW1.html#question-1",
    "title": "HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "href": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "title": "HW1",
    "section": "Read the data from xls file",
    "text": "Read the data from xls file\n\n\nCode\nRE <- read_excel(\"_data/LungCapData.xls\")\nRE\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n##A\n\n\nCode\nRE %>% \n  ggplot(aes(LungCap))+\n  geom_histogram(bins=20)\n\n\n\n\n\nThe histogram looks close to normal distributed."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b",
    "href": "posts/NiyatiSharma_HW1.html#b",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\nRE %>%\n  ggplot(aes (LungCap, color=Gender)) +\n  geom_boxplot() +\n  theme_classic() \n\n\n\n\n\nThe probability density of the female is higher than the males."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c",
    "href": "posts/NiyatiSharma_HW1.html#c",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\nMean_Smoker <- RE %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_Smoker\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nggplot(RE, aes(LungCap,Smoke))+\n  geom_boxplot()\n\n\n\n\n\nFrom this sample, it appears that smokers have a higher mean lung capacity than non-smokers."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d",
    "href": "posts/NiyatiSharma_HW1.html#d",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\nRE<-RE %>% \n  mutate(Category = as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\nRE\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean Category    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nRE %>%\n  ggplot(aes( LungCap, color = Smoke)) +\n  geom_histogram()+\n  facet_grid(Smoke ~ Category)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe people who smoke are few in age group of “less than or equal to 13”. From the result we can say age is inversely proportional to the lung capacity."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e",
    "href": "posts/NiyatiSharma_HW1.html#e",
    "title": "HW1",
    "section": "E",
    "text": "E\nForm the above data we can say the output are pretty similar that smokers have a lower lung capacity compared to non-smokers"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f",
    "href": "posts/NiyatiSharma_HW1.html#f",
    "title": "HW1",
    "section": "F",
    "text": "F\ncorrelation and covariance between lung capacity and age\n\n\nCode\ncov(RE$LungCap,RE$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(RE$LungCap,RE$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is positive and indicates that age and lung capacity are directly related. Correlation is also positive,from these results we can conclude that the lung capacity increases with age."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-2",
    "href": "posts/NiyatiSharma_HW1.html#question-2",
    "title": "HW1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nx <- c(0:4)\nfreq <- c(128, 434, 160, 64, 24)\nconvictions <- data_frame(x, freq)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nconvictions\n\n\n# A tibble: 5 × 2\n      x  freq\n  <int> <dbl>\n1     0   128\n2     1   434\n3     2   160\n4     3    64\n5     4    24\n\n\n\n\nCode\nconvictions <- convictions %>% mutate(probability = freq/sum(freq))\nconvictions\n\n\n# A tibble: 5 × 3\n      x  freq probability\n  <int> <dbl>       <dbl>\n1     0   128      0.158 \n2     1   434      0.536 \n3     2   160      0.198 \n4     3    64      0.0790\n5     4    24      0.0296"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#a",
    "href": "posts/NiyatiSharma_HW1.html#a",
    "title": "HW1",
    "section": "A",
    "text": "A\nProbability of exactly 2 is 19.75%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b-1",
    "href": "posts/NiyatiSharma_HW1.html#b-1",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\na <-head(convictions,2)\nsum(a$probability)\n\n\n[1] 0.6938272\n\n\nProbability that a randomly selected inmate has fewer than 2 prior convictions : 69.38%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c-1",
    "href": "posts/NiyatiSharma_HW1.html#c-1",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\na <-head(convictions,3)\nsum(a$probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions : 89.13%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d-1",
    "href": "posts/NiyatiSharma_HW1.html#d-1",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\na <-tail(convictions,2)\nsum(a$probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions? : 10.86%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e-1",
    "href": "posts/NiyatiSharma_HW1.html#e-1",
    "title": "HW1",
    "section": "E",
    "text": "E\n\n\nCode\nWE <- weighted.mean(convictions$x,convictions$probability)\nWE\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions : 1.28"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f-1",
    "href": "posts/NiyatiSharma_HW1.html#f-1",
    "title": "HW1",
    "section": "F",
    "text": "F\nThe variance is 0.857 and the standard deviation is 0.925\n\n\nCode\nAB <- (sum(freq*((x-WE)^2)))/(sum(freq)-1)\nAB\n\n\n[1] 0.8572937\n\n\nCode\nsqrt(AB)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html",
    "href": "posts/NiyatiSharma_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\n#| label: setup\n#| warning: false\n#| message: false\n \nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section",
    "href": "posts/NiyatiSharma_HW2.html#section",
    "title": "Homework 2",
    "section": "1",
    "text": "1\nCreating the table with the given data.\n\n\nCode\nSP <- c('Bypass', 'Angiography')\nSS <- c(539, 847)\nMW <- c(19, 18)\nSD <- c(10, 9)\n\nServeyData <- data.frame(SP, SS, MW, SD)\nServeyData\n\n\n           SP  SS MW SD\n1      Bypass 539 19 10\n2 Angiography 847 18  9\n\n\nCalculate Standard error\n\n\nCode\nSE <- SD / sqrt(SS)\nSE\n\n\n[1] 0.4307305 0.3092437\n\n\ncalculate the area of the two tails\n\n\nCode\nCL <- 0.90  \n#area in each tail of the distribution for 90%\ntail_area <- (1-CL)/2\ntail_area\n\n\n[1] 0.05\n\n\ncalculate t-values by using the qt() function\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = SS-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\ncalculate the confidence interval\n\n\nCode\nCI <- c(MW - t_score * SE,\n        MW + t_score * SE)\nprint(CI)\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days and for angiography it is [17.49, 18.51] days. The confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-1",
    "href": "posts/NiyatiSharma_HW2.html#section-1",
    "title": "Homework 2",
    "section": "2",
    "text": "2\nUsing prop.test() to calculate p and the 95% confidence interval.\n\n\nCode\nset.seed(0)\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the point estimate is 0.5195839 - 0.5803191.The point estimate for the proportion of all adult Americans who believe that a college education is essential for success is 0.55."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-2",
    "href": "posts/NiyatiSharma_HW2.html#section-2",
    "title": "Homework 2",
    "section": "3",
    "text": "3\nCalculate the min sample size\n\n\nCode\n# calculate population SD.\nSD <- (200-30)/4\n#margin of error\nME <- (10/2)\n# calculate sample size.\nsamplesize <- ((1.96*SD)/ME)^2\nsamplesize\n\n\n[1] 277.5556\n\n\nthe size of the sample should be 278"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-3",
    "href": "posts/NiyatiSharma_HW2.html#section-3",
    "title": "Homework 2",
    "section": "4",
    "text": "4"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a",
    "href": "posts/NiyatiSharma_HW2.html#a",
    "title": "Homework 2",
    "section": "a",
    "text": "a\ncalculate t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\nt_stats <- (410-500)/(90/sqrt(9))\nt_stats\n\n\n[1] -3\n\n\nCalculate P value\n\n\nCode\np_value <- 2* pt(t_stats, df=8)\np_value\n\n\n[1] 0.01707168\n\n\nThe test statistic is -3 and the p-value is 0.01707168.The p-value is substantially less than .05 is the evidence that we can reject the null hypothesis. There is strong evidence that the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b",
    "href": "posts/NiyatiSharma_HW2.html#b",
    "title": "Homework 2",
    "section": "b",
    "text": "b\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = TRUE)\nPL\n\n\n[1] 0.008535841\n\n\nSince p-value is 0.0085 is less than the alpha level of 0.05, we can reject the null hypothesis. There is evidence that the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c",
    "href": "posts/NiyatiSharma_HW2.html#c",
    "title": "Homework 2",
    "section": "c",
    "text": "c\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = FALSE)\nPL\n\n\n[1] 0.9914642\n\n\nSince p-value is 0.991 is more than the alpha level of 0.05, we cannot reject the null hypothesis. There is evidence that the mean income of female employees is more than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-4",
    "href": "posts/NiyatiSharma_HW2.html#section-4",
    "title": "Homework 2",
    "section": "5",
    "text": "5"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a-1",
    "href": "posts/NiyatiSharma_HW2.html#a-1",
    "title": "Homework 2",
    "section": "a",
    "text": "a\n\n\nCode\n# calculate standard deviation \nStd_Dev <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_jones <- ((519.5-500)/Std_Dev) * sqrt(1000)\nt_jones\n\n\n[1] 1.95\n\n\nCode\n# calculate p-value for Jones.\np_jones <- 2*(pt(q=t_jones, df=999, lower.tail=FALSE))\np_jones\n\n\n[1] 0.05145555\n\n\nCode\n# calculate t for Smith.\nt_smith <- ((519.7-500)/Std_Dev) * sqrt(1000)\nt_smith\n\n\n[1] 1.97\n\n\nCode\n# calculate p-value for Smith.\np_smith <- 2*(pt(q=t_smith, df=999, lower.tail=FALSE))\np_smith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b-1",
    "href": "posts/NiyatiSharma_HW2.html#b-1",
    "title": "Homework 2",
    "section": "b",
    "text": "b\nAt the .05 significance level, we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c-1",
    "href": "posts/NiyatiSharma_HW2.html#c-1",
    "title": "Homework 2",
    "section": "c",
    "text": "c\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-5",
    "href": "posts/NiyatiSharma_HW2.html#section-5",
    "title": "Homework 2",
    "section": "6",
    "text": "6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nHere we can see that the p value for this is .038 which means we can reject the null hypothesis that gas prices are equal to or greater than 45 cents. The mean sample that came up was also within the range of the confidence interval."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-1",
    "href": "posts/NiyatiSharma_HW3.html#answer-1",
    "title": "Homework 3",
    "section": "Answer 1",
    "text": "Answer 1\n\n\nCode\n# Load dataset\ndata(UN11) \nUN11\n\n\n                                        region  group fertility    ppgdp\nAfghanistan                               Asia  other  5.968000    499.0\nAlbania                                 Europe  other  1.525000   3677.2\nAlgeria                                 Africa africa  2.142000   4473.0\nAngola                                  Africa africa  5.135000   4321.9\nAnguilla                             Caribbean  other  2.000000  13750.1\nArgentina                           Latin Amer  other  2.172000   9162.1\nArmenia                                   Asia  other  1.735000   3030.7\nAruba                                Caribbean  other  1.671000  22851.5\nAustralia                              Oceania   oecd  1.949000  57118.9\nAustria                                 Europe   oecd  1.346000  45158.8\nAzerbaijan                                Asia  other  2.148000   5637.6\nBahamas                              Caribbean  other  1.877000  22461.6\nBahrain                                   Asia  other  2.430000  18184.1\nBangladesh                                Asia  other  2.157000    670.4\nBarbados                             Caribbean  other  1.575000  14497.3\nBelarus                                 Europe  other  1.479000   5702.0\nBelgium                                 Europe   oecd  1.835000  43814.8\nBelize                              Latin Amer  other  2.679000   4495.8\nBenin                                   Africa africa  5.078000    741.1\nBermuda                              Caribbean  other  1.760000  92624.7\nBhutan                                    Asia  other  2.258000   2047.2\nBolivia                             Latin Amer  other  3.229000   1977.9\nBosnia and Herzegovina                  Europe  other  1.134000   4477.7\nBotswana                                Africa africa  2.617000   7402.9\nBrazil                              Latin Amer  other  1.800000  10715.6\nBrunei Darussalam                         Asia  other  1.984000  32647.6\nBulgaria                                Europe  other  1.546000   6365.1\nBurkina Faso                            Africa africa  5.750000    519.7\nBurundi                                 Africa africa  4.051000    176.6\nCambodia                                  Asia  other  2.422000    797.2\nCameroon                                Africa africa  4.287000   1206.6\nCanada                           North America   oecd  1.691000  46360.9\nCape Verde                              Africa africa  2.279000   3244.0\nCayman Islands                       Caribbean  other  1.600000  57047.9\nCentral African Republic                Africa africa  4.423000    450.8\nChad                                    Africa africa  5.737000    727.4\nChile                               Latin Amer   oecd  1.832000  11887.7\nChina                                     Asia  other  1.559000   4354.0\nColombia                            Latin Amer  other  2.293000   6222.8\nComoros                                 Africa africa  4.742000    736.6\nCongo                                   Africa africa  4.442000   2665.1\nCook Islands                           Oceania  other  2.530806  12212.1\nCosta Rica                          Latin Amer  other  1.812000   7703.8\nCote dIvoire                            Africa africa  4.224000   1154.1\nCroatia                                 Europe  other  1.501000  13819.5\nCuba                                 Caribbean  other  1.451000   5704.4\nCyprus                                    Asia  other  1.458000  28364.3\nCzech Republic                          Europe   oecd  1.501000  18838.8\nDemocratic Republic of the Congo        Africa africa  5.485000    200.6\nDenmark                                 Europe   oecd  1.885000  55830.2\nDjibouti                                Africa africa  3.589000   1282.6\nDominica                             Caribbean  other  3.000000   7020.8\nDominican Republic                   Caribbean  other  2.490000   5195.4\nEast Timor                                Asia  other  5.918000    706.1\nEcuador                             Latin Amer  other  2.393000   4072.6\nEgypt                                   Africa africa  2.636000   2653.7\nEl Salvador                         Latin Amer  other  2.171000   3425.6\nEquatorial Guinea                       Africa africa  4.980000  16852.4\nEritrea                                 Africa africa  4.243000    429.1\nEstonia                                 Europe   oecd  1.702000  14135.4\nEthiopia                                Africa africa  3.848000    324.6\nFiji                                   Oceania  other  2.602000   3545.7\nFinland                                 Europe   oecd  1.875000  44501.7\nFrance                                  Europe   oecd  1.987000  39545.9\nFrench Polynesia                       Oceania  other  2.033000  24669.0\nGabon                                   Africa africa  3.195000  12468.8\nGambia                                  Africa africa  4.689000    579.1\nGeorgia                                   Asia  other  1.528000   2680.3\nGermany                                 Europe   oecd  1.457000  39857.1\nGhana                                   Africa africa  3.988000   1333.2\nGreece                                  Europe   oecd  1.540000  26503.8\nGreenland                        NorthAtlantic  other  2.217000  35292.7\nGrenada                              Caribbean  other  2.171000   7429.0\nGuatemala                           Latin Amer  other  3.840000   2882.3\nGuinea                                  Africa africa  5.032000    427.5\nGuinea-Bissau                           Africa africa  4.877000    539.4\nGuyana                              Latin Amer  other  2.190000   2996.0\nHaiti                                Caribbean  other  3.159000    612.7\nHonduras                            Latin Amer  other  2.996000   2026.2\nHong Kong                                 Asia  other  1.137000  31823.7\nHungary                                 Europe   oecd  1.430000  12884.0\nIceland                                 Europe  other  2.098000  39278.0\nIndia                                     Asia  other  2.538000   1406.4\nIndonesia                                 Asia  other  2.055000   2949.3\nIran                                      Asia  other  1.587000   5227.1\nIraq                                      Asia  other  4.535000    888.5\nIreland                                 Europe   oecd  2.097000  46220.3\nIsrael                                    Asia   oecd  2.909000  29311.6\nItaly                                   Europe   oecd  1.476000  33877.1\nJamaica                              Caribbean  other  2.262000   4899.0\nJapan                                     Asia   oecd  1.418000  43140.9\nJordan                                    Asia  other  2.889000   4445.3\nKazakhstan                                Asia  other  2.481000   9166.7\nKenya                                   Africa africa  4.623000    801.8\nKiribati                               Oceania  other  3.500000   1468.2\nKuwait                                    Asia  other  2.251000  45430.4\nKyrgyzstan                                Asia  other  2.621000    865.4\nLaos                                      Asia  other  2.543000   1047.6\nLatvia                                  Europe  other  1.506000  10663.0\nLebanon                                   Asia  other  1.764000   9283.7\nLesotho                                 Africa africa  3.051000    980.7\nLiberia                                 Africa africa  5.038000    218.6\nLibya                                   Africa africa  2.410000  11320.8\nLithuania                               Europe  other  1.495000  10975.5\nLuxembourg                              Europe   oecd  1.683000 105095.4\nMacao                                     Asia  other  1.163000  49990.2\nMadagascar                              Africa africa  4.493000    421.9\nMalawi                                  Africa africa  5.968000    357.4\nMalaysia                                  Asia  other  2.572000   8372.8\nMaldives                                  Asia  other  1.668000   4684.5\nMali                                    Africa africa  6.117000    598.8\nMalta                                   Europe  other  1.284000  19599.2\nMarshall Islands                       Oceania  other  4.384466   3069.4\nMauritania                              Africa africa  4.361000   1131.1\nMauritius                               Africa africa  1.590000   7488.3\nMexico                              Latin Amer   oecd  2.227000   9100.7\nMicronesia                             Oceania  other  3.307000   2678.2\nMoldova                                 Europe  other  1.450000   1625.8\nMongolia                                  Asia  other  2.446000   2246.7\nMontenegro                              Europe  other  1.630000   6509.8\nMorocco                                 Africa africa  2.183000   2865.0\nMozambique                              Africa africa  4.713000    407.5\nMyanmar                                   Asia  other  1.939000    876.2\nNamibia                                 Africa africa  3.055000   5124.7\nNauru                                  Oceania  other  3.300000   6190.1\nNepal                                     Asia  other  2.587000    534.7\nNeth Antilles                        Caribbean  other  1.900000  20321.1\nNetherlands                             Europe   oecd  1.794000  46909.7\nNew Caledonia                          Oceania  other  2.091000  35319.5\nNew Zealand                            Oceania   oecd  2.135000  32372.1\nNicaragua                           Latin Amer  other  2.500000   1131.9\nNiger                                   Africa africa  6.925000    357.7\nNigeria                                 Africa africa  5.431000   1239.8\nNorth Korea                               Asia  other  1.988000    504.0\nNorway                                  Europe   oecd  1.948000  84588.7\nOman                                      Asia  other  2.146000  20791.0\nPakistan                                  Asia  other  3.201000   1003.2\nPalau                                  Oceania  other  2.000000  10821.8\nPalestinian Territory                     Asia  other  4.270000   1819.5\nPanama                              Latin Amer  other  2.409000   7614.0\nPapua New Guinea                       Oceania  other  3.799000   1428.4\nParaguay                            Latin Amer  other  2.858000   2771.1\nPeru                                Latin Amer  other  2.410000   5410.7\nPhilippines                               Asia  other  3.050000   2140.1\nPoland                                  Europe   oecd  1.415000  12263.2\nPortugal                                Europe   oecd  1.312000  21437.6\nPuerto Rico                          Caribbean  other  1.757000  26461.0\nQatar                                     Asia  other  2.204000  72397.9\nRepublic of Korea                         Asia  other  1.389000  21052.2\nRomania                                 Europe  other  1.428000   7522.4\nRussian Federation                      Europe  other  1.529000  10351.4\nRwanda                                  Africa africa  5.282000    532.3\nSaint Lucia                          Caribbean  other  1.907000   6677.1\nSamoa                                  Oceania  other  3.763000   3343.3\nSao Tome and Principe                   Africa africa  3.488000   1283.3\nSaudi Arabia                              Asia  other  2.639000  15835.9\nSenegal                                 Africa africa  4.605000   1032.7\nSerbia                                  Europe  other  1.562000   5123.2\nSeychelles                              Africa africa  2.340000  11450.6\nSierra Leone                            Africa africa  4.728000    351.7\nSingapore                                 Asia  other  1.367000  43783.1\nSlovakia                                Europe   oecd  1.372000  15976.0\nSlovenia                                Europe   oecd  1.477000  23109.8\nSolomon Islands                        Oceania  other  4.041000   1193.5\nSomalia                                 Africa africa  6.283000    114.8\nSouth Africa                            Africa africa  2.383000   7254.8\nSpain                                   Europe  other  1.504000  30542.8\nSri Lanka                                 Asia  other  2.235000   2375.3\nSt Vincent and Grenadines            Caribbean  other  1.995000   6171.7\nSudan                                   Africa africa  4.225000   1824.9\nSuriname                            Latin Amer  other  2.266000   7018.0\nSwaziland                               Africa africa  3.174000   3311.2\nSweden                                  Europe   oecd  1.925000  48906.2\nSwitzerland                             Europe   oecd  1.536000  68880.2\nSyria                                     Asia  other  2.772000   2931.5\nTajikistan                                Asia  other  3.162000    816.0\nTanzania                                Africa africa  5.499000    516.0\nTFYR Macedonia                          Europe  other  1.397000   4434.5\nThailand                                  Asia  other  1.528000   4612.8\nTogo                                    Africa africa  3.864000    524.6\nTonga                                  Oceania  other  3.783000   3543.1\nTrinidad and Tobago                  Caribbean  other  1.632000  15205.1\nTunisia                                 Africa africa  1.909000   4222.1\nTurkey                                    Asia   oecd  2.022000  10095.1\nTurkmenistan                              Asia  other  2.316000   4587.5\nTuvalu                                 Oceania  other  3.700000   3187.2\nUganda                                  Africa africa  5.901000    509.0\nUkraine                                 Europe  other  1.483000   3035.0\nUnited Arab Emirates                      Asia  other  1.707000  39624.7\nUnited Kingdom                          Europe   oecd  1.867000  36326.8\nUnited States                    North America   oecd  2.077000  46545.9\nUruguay                             Latin Amer  other  2.043000  11952.4\nUzbekistan                                Asia  other  2.264000   1427.3\nVanuatu                                Oceania  other  3.750000   2963.5\nVenezuela                           Latin Amer  other  2.391000  13502.7\nViet Nam                                  Asia  other  1.750000   1182.7\nYemen                                     Asia  other  4.938000   1437.2\nZambia                                  Africa africa  6.300000   1237.8\nZimbabwe                                Africa africa  3.109000    573.1\n                                 lifeExpF pctUrban\nAfghanistan                      49.49000       23\nAlbania                          80.40000       53\nAlgeria                          75.00000       67\nAngola                           53.17000       59\nAnguilla                         81.10000      100\nArgentina                        79.89000       93\nArmenia                          77.33000       64\nAruba                            77.75000       47\nAustralia                        84.27000       89\nAustria                          83.55000       68\nAzerbaijan                       73.66000       52\nBahamas                          78.85000       84\nBahrain                          76.06000       89\nBangladesh                       70.23000       29\nBarbados                         80.26000       45\nBelarus                          76.37000       75\nBelgium                          82.81000       97\nBelize                           77.81000       53\nBenin                            58.66000       42\nBermuda                          82.30000      100\nBhutan                           69.84000       35\nBolivia                          69.40000       67\nBosnia and Herzegovina           78.40000       49\nBotswana                         51.34000       62\nBrazil                           77.41000       87\nBrunei Darussalam                80.64000       76\nBulgaria                         77.12000       72\nBurkina Faso                     57.02000       27\nBurundi                          52.58000       11\nCambodia                         65.10000       20\nCameroon                         53.56000       59\nCanada                           83.49000       81\nCape Verde                       77.70000       62\nCayman Islands                   83.80000      100\nCentral African Republic         51.30000       39\nChad                             51.61000       28\nChile                            82.35000       89\nChina                            75.61000       48\nColombia                         77.69000       75\nComoros                          63.18000       28\nCongo                            59.33000       63\nCook Islands                     76.24547       76\nCosta Rica                       81.99000       65\nCote dIvoire                     57.71000       51\nCroatia                          80.37000       58\nCuba                             81.33000       75\nCyprus                           82.14000       71\nCzech Republic                   81.00000       74\nDemocratic Republic of the Congo 50.56000       36\nDenmark                          81.37000       87\nDjibouti                         60.04000       76\nDominica                         78.20000       67\nDominican Republic               76.57000       70\nEast Timor                       64.20000       29\nEcuador                          78.91000       68\nEgypt                            75.52000       44\nEl Salvador                      77.09000       65\nEquatorial Guinea                52.91000       40\nEritrea                          64.41000       22\nEstonia                          79.95000       70\nEthiopia                         61.59000       17\nFiji                             72.27000       52\nFinland                          83.28000       85\nFrance                           84.90000       86\nFrench Polynesia                 78.07000       51\nGabon                            64.32000       86\nGambia                           60.30000       59\nGeorgia                          77.31000       53\nGermany                          82.99000       74\nGhana                            65.80000       52\nGreece                           82.58000       62\nGreenland                        71.60000       84\nGrenada                          77.72000       40\nGuatemala                        75.10000       50\nGuinea                           56.39000       36\nGuinea-Bissau                    50.40000       30\nGuyana                           73.45000       29\nHaiti                            63.87000       54\nHonduras                         75.92000       52\nHong Kong                        86.35000      100\nHungary                          78.47000       68\nIceland                          83.77000       94\nIndia                            67.62000       30\nIndonesia                        71.80000       45\nIran                             75.28000       71\nIraq                             72.60000       66\nIreland                          83.17000       62\nIsrael                           84.19000       92\nItaly                            84.62000       69\nJamaica                          75.98000       52\nJapan                            87.12000       67\nJordan                           75.17000       79\nKazakhstan                       72.84000       59\nKenya                            59.16000       23\nKiribati                         63.10000       44\nKuwait                           75.89000       98\nKyrgyzstan                       72.36000       35\nLaos                             69.42000       34\nLatvia                           78.51000       68\nLebanon                          75.07000       87\nLesotho                          48.11000       28\nLiberia                          58.59000       48\nLibya                            77.86000       78\nLithuania                        78.28000       67\nLuxembourg                       82.67000       85\nMacao                            83.80000      100\nMadagascar                       68.61000       31\nMalawi                           55.17000       20\nMalaysia                         76.86000       73\nMaldives                         78.70000       41\nMali                             53.14000       37\nMalta                            82.29000       95\nMarshall Islands                 70.60000       72\nMauritania                       60.95000       42\nMauritius                        76.89000       42\nMexico                           79.64000       78\nMicronesia                       70.17000       23\nMoldova                          73.48000       48\nMongolia                         72.83000       63\nMontenegro                       77.37000       61\nMorocco                          74.86000       59\nMozambique                       51.81000       39\nMyanmar                          67.87000       34\nNamibia                          63.04000       39\nNauru                            57.10000      100\nNepal                            70.05000       19\nNeth Antilles                    79.86000       93\nNetherlands                      82.79000       83\nNew Caledonia                    80.49000       57\nNew Zealand                      82.77000       86\nNicaragua                        77.45000       58\nNiger                            55.77000       17\nNigeria                          53.38000       51\nNorth Korea                      72.12000       60\nNorway                           83.47000       80\nOman                             76.44000       73\nPakistan                         66.88000       36\nPalau                            72.10000       84\nPalestinian Territory            74.81000       74\nPanama                           79.07000       75\nPapua New Guinea                 65.52000       13\nParaguay                         74.91000       62\nPeru                             76.90000       77\nPhilippines                      72.57000       49\nPoland                           80.56000       61\nPortugal                         82.76000       61\nPuerto Rico                      83.20000       99\nQatar                            78.24000       96\nRepublic of Korea                83.95000       83\nRomania                          77.95000       58\nRussian Federation               75.01000       73\nRwanda                           57.13000       19\nSaint Lucia                      77.54000       28\nSamoa                            76.02000       20\nSao Tome and Principe            66.48000       63\nSaudi Arabia                     75.57000       82\nSenegal                          60.92000       43\nSerbia                           77.05000       56\nSeychelles                       78.00000       56\nSierra Leone                     48.87000       39\nSingapore                        83.71000      100\nSlovakia                         79.53000       55\nSlovenia                         82.84000       49\nSolomon Islands                  70.00000       19\nSomalia                          53.38000       38\nSouth Africa                     54.09000       62\nSpain                            84.76000       78\nSri Lanka                        78.40000       14\nSt Vincent and Grenadines        74.73000       50\nSudan                            63.82000       41\nSuriname                         74.18000       70\nSwaziland                        48.54000       21\nSweden                           83.65000       85\nSwitzerland                      84.71000       74\nSyria                            77.72000       56\nTajikistan                       71.23000       26\nTanzania                         60.31000       27\nTFYR Macedonia                   77.14000       59\nThailand                         77.76000       34\nTogo                             59.40000       44\nTonga                            75.38000       24\nTrinidad and Tobago              73.82000       14\nTunisia                          77.05000       68\nTurkey                           76.61000       70\nTurkmenistan                     69.40000       50\nTuvalu                           65.10000       51\nUganda                           55.44000       13\nUkraine                          74.58000       69\nUnited Arab Emirates             78.02000       84\nUnited Kingdom                   82.42000       80\nUnited States                    81.31000       83\nUruguay                          80.66000       93\nUzbekistan                       71.90000       36\nVanuatu                          73.58000       26\nVenezuela                        77.73000       94\nViet Nam                         77.44000       31\nYemen                            67.66000       32\nZambia                           50.04000       36\nZimbabwe                         52.72000       39\n\n\nCode\n# Select variables of focus\nUN11 <- UN11 %>%\nselect(c(ppgdp, fertility)) \n\n# Preview data\nhead(UN11)\n\n\n              ppgdp fertility\nAfghanistan   499.0     5.968\nAlbania      3677.2     1.525\nAlgeria      4473.0     2.142\nAngola       4321.9     5.135\nAnguilla    13750.1     2.000\nArgentina    9162.1     2.172\n\n\n#a. Predictor variable = ppgdp (gross national product per person, in US dollars) Response variable = fertility (birth rate per 1000 females).\n#b. ::: {.cell}\n\nCode\nmm1<-lm((UN11$fertility)~(UN11$ppgdp))\n\n:::\n\n\nCode\nplot((UN11$fertility) ~ (UN11$ppgdp), data=UN11)\nabline(mm1,col=\"blue\")\n\n\n\n\n\nThe graph shows there is a negative relation betweeb fertility and ppgdp. It is not exactly linear because increasing ppgdp only decreases fertility about the 10,000 point. #c. ::: {.cell}\n:::\n\n\nCode\nmm2<-lm(log(UN11$fertility)~log(UN11$ppgdp))\nsummary(mm1)\n\n\n\nCall:\nlm(formula = (UN11$fertility) ~ (UN11$ppgdp))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nUN11$ppgdp  -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\n\n\n\n\n\nCode\nplot(log(UN11$fertility) ~ log(UN11$ppgdp), data=UN11)\nabline(mm2,col=\"blue\")\n\n\n\n\n\nThe log scatterplot shows a relationship that also looks negative slop but a bit closer to the linear regression."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-2",
    "href": "posts/NiyatiSharma_HW3.html#answer-2",
    "title": "Homework 3",
    "section": "Answer 2",
    "text": "Answer 2\n#a Change the currency from American dollars to British pounds i.e 1.66 * American dollar shifts the mean of the x-axis (explanatory variable) to increase while the mean of the y-axis (response variable) remains the same shows that the slope of the prediction equation would change.\n#b The correlation doesn’t change,as the relative values of the variables remain unchanged."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-3",
    "href": "posts/NiyatiSharma_HW3.html#answer-3",
    "title": "Homework 3",
    "section": "Answer 3",
    "text": "Answer 3\n\n\nCode\n# load dataset \ndata(water)\n#create scatterplot matrix\npairs(water)\n\n\n\n\n\nCode\n#calculate the summary\nsummary(water)\n\n\n      Year          APMAM            APSAB           APSLAKE     \n Min.   :1948   Min.   : 2.700   Min.   : 1.450   Min.   : 1.77  \n 1st Qu.:1958   1st Qu.: 4.975   1st Qu.: 3.390   1st Qu.: 3.36  \n Median :1969   Median : 7.080   Median : 4.460   Median : 4.62  \n Mean   :1969   Mean   : 7.323   Mean   : 4.652   Mean   : 4.93  \n 3rd Qu.:1980   3rd Qu.: 9.115   3rd Qu.: 5.685   3rd Qu.: 5.83  \n Max.   :1990   Max.   :18.080   Max.   :11.960   Max.   :13.02  \n     OPBPC             OPRC           OPSLAKE           BSAAM       \n Min.   : 4.050   Min.   : 4.350   Min.   : 4.600   Min.   : 41785  \n 1st Qu.: 7.975   1st Qu.: 7.875   1st Qu.: 8.705   1st Qu.: 59857  \n Median : 9.550   Median :11.110   Median :12.140   Median : 69177  \n Mean   :12.836   Mean   :12.002   Mean   :13.522   Mean   : 77756  \n 3rd Qu.:16.545   3rd Qu.:14.975   3rd Qu.:16.920   3rd Qu.: 92206  \n Max.   :43.370   Max.   :24.850   Max.   :33.070   Max.   :146345  \n\n\nLooking at scatterplots , we can say that APMAN,APSLAKE,APSAB lakes shows positive linear relationship with precipitation while the OPBPC,OPRC,OPSLAKE lakes seems to have one as well with each other. Also, it seems that the stream run-off variable BSAAM has a postive relationship to the OPBPC,OPRC,OPSLAKE lakes but no real notable relationship to the APMAN,APSLAKE,APSAB lakes."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-4",
    "href": "posts/NiyatiSharma_HW3.html#answer-4",
    "title": "Homework 3",
    "section": "Answer 4",
    "text": "Answer 4\n\n\nCode\n# load dataset and review\ndata(Rateprof)\nhead(Rateprof)\n\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\n\nCode\n# select the columns\nRP <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n# generate scatterplots.\npairs(RP)\n\n\n\n\n\nReferring to the scatterplot matrix of the average professor ratings for Quality, helpfulness and clarity have the positive have strong positive correlations with each other. Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/NiyatiSharma_HW3.html#answer-5",
    "href": "posts/NiyatiSharma_HW3.html#answer-5",
    "title": "Homework 3",
    "section": "Answer 5",
    "text": "Answer 5\n\n\nCode\n# load and preview data\ndata(student.survey)\nhead(student.survey)\n\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#answer1",
    "href": "posts/NiyatiSharma_HW4.html#answer1",
    "title": "Homework 4",
    "section": "Answer1",
    "text": "Answer1"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#answer-2",
    "href": "posts/NiyatiSharma_HW4.html#answer-2",
    "title": "Homework 4",
    "section": "Answer 2",
    "text": "Answer 2"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#question-3",
    "href": "posts/NiyatiSharma_HW4.html#question-3",
    "title": "Homework 4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price\n\n\n    case Taxes Beds Baths New  Price Size\n1      1  3104    4     2   0 279900 2048\n2      2  1173    2     1   0 146500  912\n3      3  3076    4     2   0 237700 1654\n4      4  1608    3     2   0 200000 2068\n5      5  1454    3     3   0 159900 1477\n6      6  2997    3     2   1 499900 3153\n7      7  4054    3     2   0 265500 1355\n8      8  3002    3     2   1 289900 2075\n9      9  6627    5     4   0 587000 3990\n10    10   320    3     2   0  70000 1160\n11    11   630    3     2   0  64500 1220\n12    12  1780    3     2   0 167000 1690\n13    13  1630    3     2   0 114600 1380\n14    14  1530    3     2   0 103000 1590\n15    15   930    3     1   0 101000 1050\n16    16   590    2     1   0  70000  770\n17    17  1050    3     2   0  85000 1410\n18    18    20    3     1   0  22500 1060\n19    19   870    2     2   0  90000 1300\n20    20  1320    3     2   0 133000 1500\n21    21  1350    2     1   0  90500  820\n22    22  5616    4     3   1 577500 3949\n23    23   680    2     1   0 142500 1170\n24    24  1840    3     2   0 160000 1500\n25    25  3680    4     2   0 240000 2790\n26    26  1660    3     1   0  87000 1030\n27    27  1620    3     2   0 118600 1250\n28    28  3100    3     2   0 140000 1760\n29    29  2070    2     3   0 148000 1550\n30    30   830    3     2   0  69000 1120\n31    31  2260    4     2   0 176000 2000\n32    32  1760    3     1   0  86500 1350\n33    33  2750    3     2   1 180000 1840\n34    34  2020    4     2   0 179000 2510\n35    35  4900    3     3   1 338000 3110\n36    36  1180    4     2   0 130000 1760\n37    37  2150    3     2   0 163000 1710\n38    38  1600    2     1   0 125000 1110\n39    39  1970    3     2   0 100000 1360\n40    40  2060    3     1   0 100000 1250\n41    41  1980    3     1   0 100000 1250\n42    42  1510    3     2   0 146500 1480\n43    43  1710    3     2   0 144900 1520\n44    44  1590    3     2   0 183000 2020\n45    45  1230    3     2   0  69900 1010\n46    46  1510    2     2   0  60000 1640\n47    47  1450    2     2   0 127000  940\n48    48   970    3     2   0  86000 1580\n49    49   150    2     2   0  50000  860\n50    50  1470    3     2   0 137000 1420\n51    51  1850    3     2   0 121300 1270\n52    52   820    2     1   0  81000  980\n53    53  2050    4     2   0 188000 2300\n54    54   710    3     2   0  85000 1430\n55    55  1280    3     2   0 137000 1380\n56    56  1360    3     2   0 145000 1240\n57    57   830    3     2   0  69000 1120\n58    58   800    3     2   0 109300 1120\n59    59  1220    3     2   0 131500 1900\n60    60  3360    4     3   0 200000 2430\n61    61   210    3     2   0  81900 1080\n62    62   380    2     1   0  91200 1350\n63    63  1920    4     3   0 124500 1720\n64    64  4350    3     3   0 225000 4050\n65    65  1510    3     2   0 136500 1500\n66    66  4154    3     3   0 381000 2581\n67    67  1976    3     2   1 250000 2120\n68    68  3605    3     3   1 354900 2745\n69    69  1400    3     2   0 140000 1520\n70    70   790    2     2   0  89900 1280\n71    71  1210    3     2   0 137000 1620\n72    72  1550    3     2   0 103000 1520\n73    73  2800    3     2   0 183000 2030\n74    74  2560    3     2   0 140000 1390\n75    75  1390    4     2   0 160000 1880\n76    76  5443    3     2   0 434000 2891\n77    77  2850    2     1   0 130000 1340\n78    78  2230    2     2   0 123000  940\n79    79    20    2     1   0  21000  580\n80    80  1510    4     2   0  85000 1410\n81    81   710    3     2   0  69900 1150\n82    82  1540    3     2   0 125000 1380\n83    83  1780    3     2   1 162600 1470\n84    84  2920    2     2   1 156900 1590\n85    85  1710    3     2   1 105900 1200\n86    86  1880    3     2   0 167500 1920\n87    87  1680    3     2   0 151800 2150\n88    88  3690    5     3   0 118300 2200\n89    89   900    2     2   0  94300  860\n90    90   560    3     1   0  93900 1230\n91    91  2040    4     2   0 165000 1140\n92    92  4390    4     3   1 285000 2650\n93    93   690    3     1   0  45000 1060\n94    94  2100    3     2   0 124900 1770\n95    95  2880    4     2   0 147000 1860\n96    96   990    2     2   0 176000 1060\n97    97  3030    3     2   0 196500 1730\n98    98  1580    3     2   0 132200 1370\n99    99  1770    3     2   0  88400 1560\n100  100  1430    3     2   0 127200 1340\n\n\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#a-2",
    "href": "posts/NiyatiSharma_HW4.html#a-2",
    "title": "Homework 4",
    "section": "a",
    "text": "a\n\n\nCode\nsummary(lm(Price ~ Size + New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth size and New variable positively predict selling price. As we change $ 1 in price, it results in 116.132 change in size and 57736.283 units in New.\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#b-2",
    "href": "posts/NiyatiSharma_HW4.html#b-2",
    "title": "Homework 4",
    "section": "b",
    "text": "b\n\n\nCode\nnew_home <- house.selling.price %>% filter(New == 1)\nsummary(lm(Price ~ Size, data = new_home))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new_home)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold_home <- house.selling.price %>% filter(New == 0)\nsummary(lm(Price ~ Size, data = old_home))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old_home)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nFor the filtered data wrt new home and old home, Size positively predicts price (But by a greater value wrt new homes). Adjusted R-squared for the model is also much higher (0.91 vs. 0.58) for new home and old home respectively.\nNew_Price = 166.35 * Size - 100755.31\nOld_Price = 104.438 * Size - 22227.808\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#c-2",
    "href": "posts/NiyatiSharma_HW4.html#c-2",
    "title": "Homework 4",
    "section": "c",
    "text": "c\n\n\nCode\nSize <- 3000\nNew_Price = 166.35 * Size - 100755.31\nOld_Price = 104.438 * Size - 22227.808\nsprintf(\"New Price = %f\", New_Price)\n\n\n[1] \"New Price = 398294.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price)\n\n\n[1] \"Old Price = 291086.192000\"\n\n\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#d-1",
    "href": "posts/NiyatiSharma_HW4.html#d-1",
    "title": "Homework 4",
    "section": "d",
    "text": "d\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#e-1",
    "href": "posts/NiyatiSharma_HW4.html#e-1",
    "title": "Homework 4",
    "section": "e",
    "text": "e\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nprice_with_sizeAndNew = ( -22227.81 + 104.44 * Size ) + ( -78527.50 + 61.92 * Size )\nprice_with_size = -22227.81 + 104.44 * Size\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#f-1",
    "href": "posts/NiyatiSharma_HW4.html#f-1",
    "title": "Homework 4",
    "section": "f",
    "text": "f\n\n\nCode\nSize <- 3000\nNew_Price_withSizeAndNew = ( -22227.81 + 104.44 * Size ) +( - 78527.50 + 61.92 * Size )\nOld_Price_withSize = -22227.81 + 104.44 * Size\nsprintf(\"New Price = %f\", New_Price_withSizeAndNew)\n\n\n[1] \"New Price = 398324.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price_withSize)\n\n\n[1] \"Old Price = 291092.190000\"\n\n\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#g",
    "href": "posts/NiyatiSharma_HW4.html#g",
    "title": "Homework 4",
    "section": "g",
    "text": "g\n\n\nCode\nSize <- 1500\nNew_Price_withSizeAndNew = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price_withSize = -22227.81 + 104.44 * Size\nsprintf(\"New Price = %f\", New_Price_withSizeAndNew)\n\n\n[1] \"New Price = 148784.690000\"\n\n\nCode\nsprintf(\"Old Price = %f\", Old_Price_withSize)\n\n\n[1] \"Old Price = 134432.190000\"\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger.\n\\[\\\\[0.2in]\\]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW4.html#h",
    "href": "posts/NiyatiSharma_HW4.html#h",
    "title": "Homework 4",
    "section": "h",
    "text": "h\nWhen we apply the interaction (having both size and new variable), then we see a significantly large negative coefficient. The adjusted r-squared for the model with Size and New variable combined is 0.7363 and the adjusted r-squared for the first model with just Size variable is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Although both models have almost similar adjusted r-squared value, I would prefer the model with interaction (with Size and New variable) because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/NiyatiSharma_HW5.html#answer1",
    "href": "posts/NiyatiSharma_HW5.html#answer1",
    "title": "Homework 5",
    "section": "Answer1",
    "text": "Answer1"
  },
  {
    "objectID": "posts/NiyatiSharma_HW5.html#answer-2",
    "href": "posts/NiyatiSharma_HW5.html#answer-2",
    "title": "Homework 5",
    "section": "Answer 2",
    "text": "Answer 2"
  },
  {
    "objectID": "posts/NiyatiSharma_HW5.html#answer-3",
    "href": "posts/NiyatiSharma_HW5.html#answer-3",
    "title": "Homework 5",
    "section": "Answer 3",
    "text": "Answer 3"
  },
  {
    "objectID": "posts/Project_Yakub Rabiutheen.html",
    "href": "posts/Project_Yakub Rabiutheen.html",
    "title": "Project Rough Draft Proposal",
    "section": "",
    "text": "Hypopthesis\nThis research project will be testing two hypothesis regarding Britain and France.\n#Colonial Powers Hypopthesis. 1. The Years that France and Britain had more Exports is when the rate of colonization increased. 2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\nCode\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\n\n\nError: `path` does not exist: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls'\n\n\nCode\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv' does not exist.\n\n\nCode\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv' does not exist.\n\n\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\n\n\nCode\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\n\n\nError in colnames(Colonial_Years)[3] <- \"Colonizing Country\": object 'Colonial_Years' not found\n\n\nCode\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\n\n\nError in colnames(Colonial_Years)[4] <- \"Year_Colonization_Began\": object 'Colonial_Years' not found\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\n\n\nError in select(Colonial_Years, \"Colonizing Country\", \"Year_Colonization_Began\"): object 'Colonial_Years' not found\n\n\nCode\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\n\n\nError in filter(Colonial_Years, `Colonizing Country` %in% c(\"F\", \"UK\")): object 'Colonial_Years' not found\n\n\nCode\ntable(Colonial_Years)\n\n\nError in table(Colonial_Years): object 'Colonial_Years' not found\n\n\n\n\nCode\nImports_Exports%>% filter(year < '1960') \n\n\nError in filter(., year < \"1960\"): object 'Imports_Exports' not found\n\n\n\n\nCode\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\n\n\nError in filter(Imports_Exports, stateabb %in% c(\"FRN\", \"UKG\")): object 'Imports_Exports' not found\n\n\n\n\nCode\noptions(scipen = 999)    \n\n\nCreated a Forumula to calculate Trade Surplus and Deficits.\n\n\nCode\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\n\n\nError in eval(expr, envir, enclos): object 'colonial_trade' not found\n\n\nFound a better way to find years that France and Britain were running Trade Deficits.\n\n\nCode\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports < colonial_trade$imports, : object 'colonial_trade' not found\n\n\nI did the inverse to find that the UK has always had a Trade Deficit\n\n\nCode\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports > colonial_trade$imports, : object 'colonial_trade' not found\n\n\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-sidebar-title\"}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-navbar-title\"}\n[Fall 2022 Posts]{.hidden render-id=\"quarto-int-navbar:Fall 2022 Posts\"}\n[Contributors]{.hidden render-id=\"quarto-int-navbar:Contributors\"}\n[DACSS]{.hidden render-id=\"quarto-int-navbar:DACSS\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-metatitle\"}\n[International Trade's influence on War]{.hidden render-id=\"quarto-metadesc\"}\n:::\n\n\n\n\n<!-- -->\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Project Rough Draft Proposal\"\nauthor: \"Yakub Rabiutheen\"\ndescription: \"International Trade's influence on War\"\ndate: \"10/11/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - finalproject1\n  - desriptive statistics \n  - probability\n---\n\n# Research Question\n\nHow has international trade influenced how countries interact with each other? This research project looks specifically at  France and Britain which  are grouped together as Colonial Powers to explore the relationship of Colonialism and international trade. This research project will be looking at data from the Correlates Of War Project, which has international trade data from 1870 to 2015. The cut-off year for this research project will be 1960, as  on December 14,1960, the UN declared Colonialism was a human's right's violation and legally declared Colonialism was over(McWhinney,1960).   \n\n\n# Hypopthesis\n\nThis research project will be testing two hypothesis regarding Britain and France.\n\n#Colonial Powers Hypopthesis.\n1. The Years that France and Britain had more Exports is when the rate of colonization increased.\n2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\n\nquarto-executable-code-5450563D\n\n```r\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\nquarto-executable-code-5450563D\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\nquarto-executable-code-5450563D\nlibrary(dplyr)\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\ntable(Colonial_Years)\nquarto-executable-code-5450563D\nImports_Exports%>% filter(year < '1960') \nquarto-executable-code-5450563D\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\nquarto-executable-code-5450563D\noptions(scipen = 999)    \nCreated a Forumula to calculate Trade Surplus and Deficits.\nquarto-executable-code-5450563D\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\nFound a better way to find years that France and Britain were running Trade Deficits. quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\nI did the inverse to find that the UK has always had a Trade Deficit quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/Quarkume HW1.html#lungcapdate",
    "href": "posts/Quarkume HW1.html#lungcapdate",
    "title": "HW1 Quat",
    "section": "LungCapDate",
    "text": "LungCapDate\n\nUse the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\nInstall Libraries\n\n#install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n#install.packages(\"readxl\")\nlibrary(readxl)\n#install.packages(\"magrittr\")\nlibrary(magrittr)\n\n\nWhat does the distribution of LungCap look like?\nThe distribution of Lung Capacity in the data set looks normally distributed.\n\n\n#histogram of LungCap\nhist(LungCapData$LungCap, xlab = 'LungCap', main = '', freq = F)\n\nError in hist(LungCapData$LungCap, xlab = \"LungCap\", main = \"\", freq = F): object 'LungCapData' not found\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\nLooking at the comparative boxplot males have a higher lung capacity than females.\n\n\nboxplot(LungCapData$LungCap ~ LungCapData$Gender,\n        col = c(\"#FFE0B2\", \"#FFA726\"))\n\nError in eval(predvars, data, env): object 'LungCapData' not found\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense? In comparing the means, the lung capacity for smokers is higher than for nonsmokers.\n\n#Mean Lung capacities of smokers\nLungCapData %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"yes\"): object 'LungCapData' not found\n\n#Mean Lung capacities of non-smokers\nLungCapData %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"no\"): object 'LungCapData' not found\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#new var for Age Groups\nLungCapData$Age_Cat <- cut(LungCapData$Age,\n                           breaks = c(0,13,15,17,25),\n                           labels = c('less than or equal to 13','14 to 15','16 to 17','greater than or equal to 18'))\n\nError in cut(LungCapData$Age, breaks = c(0, 13, 15, 17, 25), labels = c(\"less than or equal to 13\", : object 'LungCapData' not found\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age_Cat, scale=\"free\")\n\nError in ggplot(LungCapData, aes(x = Smoke, y = LungCap)): object 'LungCapData' not found\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here? We see an intervening relationship with age. Where most young children either don’t smoke ar all and have smaller lung capacities because of their size.\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age, scale=\"free\")\n\n--\n  \n\nError: <text>:7:0: unexpected end of input\n5: --\n6:   \n  ^\n\n\nf.Calculate the correlation and correlation between Lung Capacity and Age. (use the cov() and cor() functions in R).\n\n#correlation\nLungCapData %>% \n  summarize(correlation = cor(LungCap, Age))\n\nError in summarize(., correlation = cor(LungCap, Age)): object 'LungCapData' not found\n\n#correlation\nLungCapData %>% \n  summarize(covariance = cov(LungCap, Age))\n\nError in summarize(., covariance = cov(LungCap, Age)): object 'LungCapData' not found"
  },
  {
    "objectID": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "href": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "title": "HW1 Quat",
    "section": "1. Examination of Prison Convictions",
    "text": "1. Examination of Prison Convictions"
  },
  {
    "objectID": "posts/Quarkume HW1.html#prisondata",
    "href": "posts/Quarkume HW1.html#prisondata",
    "title": "HW1 Quat",
    "section": "PrisonData",
    "text": "PrisonData\nData\n\nPrisonData <- tibble(\n  prior_convictions = c(0,1,2,3,4),\n  freq = c(128,434,160,64,24))\n\nPrisonData\n\n# A tibble: 5 × 2\n  prior_convictions  freq\n              <dbl> <dbl>\n1                 0   128\n2                 1   434\n3                 2   160\n4                 3    64\n5                 4    24\n\nnum <- sum (PrisonData$freq)\nnum\n\n[1] 810\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nPrisonData %>% \n  filter(prior_convictions == 2) %>% \n  pull (freq) %>% \n  divide_by (num)\n\n[1] 0.1975309\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions < 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.6938272\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions <= 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.891358\n\n\nd.What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions > 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.108642\n\n\ne. What is the expected value for the number of prior convictions?\n\nsum(prior_convictions*freq)\n\nError in eval(expr, envir, enclos): object 'prior_convictions' not found\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html",
    "href": "posts/RahulGundeti_DACSS603_HW1.html",
    "title": "DACSS603_HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "title": "DACSS603_HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "title": "DACSS603_HW1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nlung <- read_excel(\"C:/Users/gunde/Downloads/LungCapData.xls\")\nlung\n\n\n\n\n  \n\n\n\nThe Lung Capacity data contains 725 rows and 6 columns that determine age, height etc., The key classification parameter is based on smoker vs non-smoker."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "title": "DACSS603_HW1",
    "section": "1_A",
    "text": "1_A\nThe distribution of LungCap looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 40, color = \"red\") +\n  geom_density(color = \"green\") +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution\", x = \"Lung Capcity\", y = \"Probability Density\")\n\n\n\n\n\nThe observations plotted by histogram are closer to mean which suggests that it is a normal distribution."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "title": "DACSS603_HW1",
    "section": "1_B",
    "text": "1_B\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution based on gender\", y = \"Probability Density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male < female."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "title": "DACSS603_HW1",
    "section": "1_C",
    "text": "1_C\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- lung %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nThe table contains the mean lung capacity. The observations suggest that the mean value is higher for smokers than non-smokers. This isn’t entirely correct as the individual biological factors plays a main role. So the data is inadequate to form an opinion."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "title": "DACSS603_HW1",
    "section": "1_D",
    "text": "1_D\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nlung <- mutate(lung, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nlung %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non-smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "title": "DACSS603_HW1",
    "section": "1_E",
    "text": "1_E\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nlung %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nComparing 1_D and 1_E we can find similarity which points that only 10 and above age group smoke."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "title": "DACSS603_HW1",
    "section": "1_F",
    "text": "1_F\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(lung$LungCap, lung$Age)\nCorrelation <- cor(lung$LungCap, lung$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nThe comparison shows that the covariance is positive, indicating that lung capacity and age have a direct relationship. As a result, they are moving in the same direction due to the positive correlation as well. This means that as age increases, lung capacity increases as well, which means they are directly proportional."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "title": "DACSS603_HW1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "title": "DACSS603_HW1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nprior <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nCode\nprior\n\n\n\n\n  \n\n\n\n\n\nCode\nprior <- mutate(prior, Probability = Inmate_count/sum(Inmate_count))\nprior"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "title": "DACSS603_HW1",
    "section": "2_A",
    "text": "2_A\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nprior %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "title": "DACSS603_HW1",
    "section": "2_B",
    "text": "2_B\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions < 2)\nsum(random$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "title": "DACSS603_HW1",
    "section": "2_C",
    "text": "2_C\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions <= 2)\nsum(random$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "title": "DACSS603_HW1",
    "section": "2_D",
    "text": "2_D\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions > 2)\nsum(random$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "title": "DACSS603_HW1",
    "section": "2_E",
    "text": "2_E\nExpected value for the number of prior convictions:\n\n\nCode\nprior <- mutate(prior, Wm = Prior_convitions*Probability)\nev <- sum(prior$Wm)\nev\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "title": "DACSS603_HW1",
    "section": "2_F",
    "text": "2_F\nVariance for the Prior Convictions:\n\n\nCode\nvariance <-sum(((prior$Prior_convitions-ev)^2)*prior$Probability)\nvariance\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html",
    "href": "posts/RahulGundeti_DACSS603_HW2.html",
    "title": "DACSS603_HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "title": "DACSS603_HW2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "title": "DACSS603_HW2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\nsample_size <- c(539, 847)\nmwt <- c(19, 18)\ns_stddev <- c(10, 9)\n\nsurgery <- data.frame(procedure, sample_size, mwt, s_stddev)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstd_error <- s_stddev / sqrt(sample_size)\nstd_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = sample_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nConfidence_Interval <- c(mwt - t_score * std_error,\n        mwt + t_score * std_error)\nConfidence_Interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe above results are obtained by fitting the 90% confidence interval level for the sample mean wait time for both the bypass surgery and the angiograph.\nBypass Surgery mean wait time : 18.29029 and 19.70971 days\nAngiograph mean wait time: 17.49078 and 18.50922 days\nThe comparision shows that the wait time for Angiograph is shorter than that of Bypass Surgery."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "title": "DACSS603_HW2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n95 percent confidence interval: 0.5189682 0.5805580\nThe sample estimate for the point p, from the sample who believes that college is necessary for success is: 0.5499515"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "title": "DACSS603_HW2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary sample size is: 278."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "title": "DACSS603_HW2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "title": "DACSS603_HW2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "title": "DACSS603_HW2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "title": "DACSS603_HW2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "title": "DACSS603_HW2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html",
    "href": "posts/RahulGundeti_DACSS603_HW3.html",
    "title": "DACSS603_HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#question-1",
    "title": "DACSS603_HW3",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\ndata(UN11)\nUN11"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#a",
    "title": "DACSS603_HW3",
    "section": "A",
    "text": "A\nThe predictor variable is ppgdp and the response variable is fertility."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#b",
    "title": "DACSS603_HW3",
    "section": "B",
    "text": "B\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = ppgdp, y = fertility)) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe graph shows an intense negative relationship between a country’s gross national product per person and fertility rate at first, then there appears to be little change in fertility in relationship to ppgdp moving beyond this point. A straight-line mean function does not seem to be an appropriate measure for summary of this graph."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#c",
    "title": "DACSS603_HW3",
    "section": "C",
    "text": "C\n\n\nCode\nUN11 %>%\n  select(c(ppgdp,fertility)) %>%\n  ggplot(aes(x = log(ppgdp), y = log(fertility))) + \n  geom_point()+\n  geom_smooth(method=lm)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe relationship between the variables appears to be negative throughout the graph. The simple linear regression seems plausible for summary of this graph."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#question-2",
    "title": "DACSS603_HW3",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#a-1",
    "title": "DACSS603_HW3",
    "section": "A",
    "text": "A\n\n\nCode\nUN11$british <- 1.33 * UN11$ppgdp\nsummary(lm(fertility ~ british, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nCode\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, the slope of the prediction equation changed."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#b-1",
    "title": "DACSS603_HW3",
    "section": "B",
    "text": "B\n\n\nCode\ncor(UN11$ppgdp, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nCode\ncor(UN11$british, UN11$fertility)\n\n\n[1] -0.4399891\n\n\nThe correlation does not change."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#question-3",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#question-3",
    "title": "DACSS603_HW3",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(water)\npairs(water)\n\n\n\n\n\nFrom the above plot, it seems that the stream run-off variable has a relationship to the ‘O’ named lakes but no real notable relationship to the ‘A’ named lakes."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#question-4",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#question-4",
    "title": "DACSS603_HW3",
    "section": "Question 4",
    "text": "Question 4\n\n\nCode\ndata(Rateprof)\nrate <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\npairs(rate)\n\n\n\n\n\nInterpreting to the scatter plot matrix of the average professor ratings for the topics of quality, clarity, helpfulness, easiness, and rater interest, the variables quality, clarity, and helpfulness appear to each have strong positive correlations with each other. The variable easiness appears to have a much weaker positive correlation with helpfulness, clarity, and quality. Rater interest does not appear to have much of a correlation to any of the other variables.So, we can say that Quality, helpfulness and clarity have the clearest linear relationships with one another and Easiness and raterInterest do not seem to have linear relationships with the other variables."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#question-5",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#question-5",
    "title": "DACSS603_HW3",
    "section": "Question 5",
    "text": "Question 5\n\n\nCode\ndata(student.survey)\nstudent.survey"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#a-2",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#a-2",
    "title": "DACSS603_HW3",
    "section": "A",
    "text": "A\n\n\nCode\nstudent.survey %>%\n  select(c(pi, re)) %>%\n  ggplot() + \n  geom_bar(aes(x = re, fill = pi)) +\n  xlab(\"Religiosity\") +\n  ylab(\"Political ideology\") \n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\n\n\nCode\nstudent.survey %>%\n  select(c(tv, hi)) %>%\n  ggplot(aes(x = tv, y = hi)) + \n  geom_point() +\n  geom_smooth(method=lm) +\n  xlab(\"Average Hours of TV watched per Week\") +\n  ylab(\"High School GPA\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHigh school GPA and TV-watching seem to have a negative relationship."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW3.html#b-2",
    "href": "posts/RahulGundeti_DACSS603_HW3.html#b-2",
    "title": "DACSS603_HW3",
    "section": "B",
    "text": "B\n\n\nCode\nsummary(lm(data = student.survey, formula = as.numeric(pi) ~ as.numeric(re)))\n\n\n\nCall:\nlm(formula = as.numeric(pi) ~ as.numeric(re), data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.9308     0.4252   2.189   0.0327 *  \nas.numeric(re)   0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\nAt a significance level of 0.01, there is a statistically significant association between religiosity and political ideology (as p-value < .01). The correlation is moderate and positive, suggesting that as weekly church attendance increases, political ideology becomes more conservative leaning.\n\n\nCode\nsummary(lm(data = student.survey, formula = hi ~ tv))\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nWith a slope of -0.018, there is a negative association between hours of tv watched per week and high school GPA, meaning that as hours of tv viewing increase, a student’s GPA tends to decrease. There is a statistically significant relationship between hours of tv viewed per week and GPA at a significance level of 0.05. However, the R-squared value is close to 0, which suggests that the regression model does not provide a strong prediction for the observed variables. This is not suprising after looking at the scatter plot with hours of tv watched and GPA, since there does not appear to be a linear trend in the data."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html",
    "href": "posts/RahulGundeti_DACSS603_HW4.html",
    "title": "DACSS603_HW4",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#question-1",
    "title": "DACSS603_HW4",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#a",
    "title": "DACSS603_HW4",
    "section": "A",
    "text": "A\n\n\nCode\nPredicted_selling_price <-  -10536 + 53.8 * 1240 + 2.84 * 18000\nPredicted_selling_price\n\n\n[1] 107296\n\n\n\n\nCode\nResidual <- Predicted_selling_price - 145000\nResidual\n\n\n[1] -37704\n\n\nFrom the above result, we can say that the house was sold for 37704 dollars greater than predicted."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#b",
    "title": "DACSS603_HW4",
    "section": "B",
    "text": "B\nUsing the prediction equation ŷ = -10536 + 53.8x1 + 2.84x2, where x2 equals lot size, the house selling price is expected to increase by 53.8 dollars per each square-foot increase in home size given the lot sized is fixed. This is because a fixed lot size would make 2.84x2 a set number in the prediction equation. Therefore, we would not need to factor in a change in the output based on any input. Then, we are left with the coefficient for the home size variable, which is 53.8. For x1 = 1, representing one square-foot of home size, the output would increase by 53.8 * 1 = 53.8."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#c",
    "title": "DACSS603_HW4",
    "section": "C",
    "text": "C\nFor fixed home size, 53.8 * 1 = 2.84x2\n\n\nCode\nx2 <- 53.8/2.84\nx2\n\n\n[1] 18.94366\n\n\nAn increase in lot size of about 18.94 square-feet would have the same impact as an increase of 1 square-foot in home size on the predicted selling price."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#question-2",
    "title": "DACSS603_HW4",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\ndata(\"salary\")\nsalary"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#a-1",
    "title": "DACSS603_HW4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(salary ~ sex, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8602.8 -4296.6  -100.8  3513.1 16687.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    24697        938  26.330   <2e-16 ***\nsexFemale      -3340       1808  -1.847   0.0706 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5782 on 50 degrees of freedom\nMultiple R-squared:  0.0639,    Adjusted R-squared:  0.04518 \nF-statistic: 3.413 on 1 and 50 DF,  p-value: 0.0706\n\n\nThe null hypothesis would be that mean salary for men and mean salary for women are equal, and the alternative hypothesis would be that the salaries are not equal. I ran a regression with sex as the explanatory variable and salary as the outcome variable. The female coefficient is -3340, which means that women do make less than men not considering any other variables. However, if we consider the other variables and also there is a significance level of 0.07, so we fail to reject the null hypothesis and therefore cannot conclude that there is a difference between mean salaries for men and women."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#b-1",
    "title": "DACSS603_HW4",
    "section": "B",
    "text": "B\n\n\nCode\nmodel <- lm(salary ~ ., data = salary)\nsummary(model)\n\n\n\nCall:\nlm(formula = salary ~ ., data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4045.2 -1094.7  -361.5   813.2  9193.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15746.05     800.18  19.678  < 2e-16 ***\ndegreePhD    1388.61    1018.75   1.363    0.180    \nrankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\nrankProf    11118.76    1351.77   8.225 1.62e-10 ***\nsexFemale    1166.37     925.57   1.260    0.214    \nyear          476.31      94.91   5.018 8.65e-06 ***\nysdeg        -124.57      77.49  -1.608    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2398 on 45 degrees of freedom\nMultiple R-squared:  0.855, Adjusted R-squared:  0.8357 \nF-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nconfint(model)\n\n\n                 2.5 %      97.5 %\n(Intercept) 14134.4059 17357.68946\ndegreePhD    -663.2482  3440.47485\nrankAssoc    2985.4107  7599.31080\nrankProf     8396.1546 13841.37340\nsexFemale    -697.8183  3030.56452\nyear          285.1433   667.47476\nysdeg        -280.6397    31.49105\n\n\nAssuming there is no interaction between sex and other predictors, we can be 95% confident that the difference in salary of women compared to men falls between -697.8183 dollars and 3030.56452 dollars."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#c-1",
    "title": "DACSS603_HW4",
    "section": "C",
    "text": "C\nFor degree as the predictor, a PHD would be expected to increase salary by 1388.61 dollars in reference to a Masters degree salary. However, at a significance level of 0.18, we cannot conclude that degree level has a statistically significant impact on salary.\nFor the rank variable, an Associate can expect a 5292.36 dollar increase in salary compared to Assistant, while a Professor can expect a 11118.76 dollar salary increase compared to Assistant. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary.\nFor the variable of sex, a Female can expect a salary increase of 1166.37 dollars in comparison to Male salary, but the significance level is 0.214, so this is not a statistically significant relationship.\nFor year, a faculty member can expect a salary increase of 476.31 dollars for an increase in 1 year of employment in his/her/their position. Additionally, the level of significance is less than 0.01 so the relationship between year and salary appears to be significant.\nFor the ysdeg variable, an increase in years since earning highest degree can expect a decrease in salary, with a coefficient of -124.57. However, with a 0.115 level of significance, this relationship cannot be found to be statistically significant."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#d",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#d",
    "title": "DACSS603_HW4",
    "section": "D",
    "text": "D\n\n\nCode\nsalary$rank <- relevel(salary$rank, ref = \"Prof\")\nsummary(lm(salary ~ rank, salary))\n\n\n\nCall:\nlm(formula = salary ~ rank, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5209.0 -1819.2  -417.8  1586.6  8386.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29659.0      669.3  44.316  < 2e-16 ***\nrankAsst    -11890.3      972.4 -12.228  < 2e-16 ***\nrankAssoc    -6483.0     1043.0  -6.216 1.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2993 on 49 degrees of freedom\nMultiple R-squared:  0.7542,    Adjusted R-squared:  0.7442 \nF-statistic: 75.17 on 2 and 49 DF,  p-value: 1.174e-15\n\n\nAfter changing the baseline category for the rank variable, an Associate can expect a 6483.0 dollar decrease in salary compared to Professor, while a Assistant can expect a 11890.3 dollar salary decrease compared to Professor. Both ranks have significance levels well below 0.05 and we can determine that rank does have a statistically significant impact on salary."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#e",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#e",
    "title": "DACSS603_HW4",
    "section": "E",
    "text": "E\n\n\nCode\nsummary(lm(salary ~ degree + sex + year + ysdeg, salary))\n\n\n\nCall:\nlm(formula = salary ~ degree + sex + year + ysdeg, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8146.9 -2186.9  -491.5  2279.1 11186.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\ndegreePhD   -3299.35    1302.52  -2.533 0.014704 *  \nsexFemale   -1286.54    1313.09  -0.980 0.332209    \nyear          351.97     142.48   2.470 0.017185 *  \nysdeg         339.40      80.62   4.210 0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3744 on 47 degrees of freedom\nMultiple R-squared:  0.6312,    Adjusted R-squared:  0.5998 \nF-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\nWhen removing the variable “rank”, the coefficient for sex is -1286.54 compared to the above regression that included rank with a coefficient for sex at 1166.37. The new coefficient predicts that a female salary would be 1286.54 less than a male salary, when excluding the variable of rank. However, the significance level is 0.332, which is very high and therefore the results cannot be found to be statistically significant. While the change of the coefficient to negative upon removal of rank is interesting, the significance level would likely prevent these results from holding up in court as an indication of discrimination on the basis of sex."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#f",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#f",
    "title": "DACSS603_HW4",
    "section": "F",
    "text": "F\n\n\nCode\nsalary <- salary %>%\n  mutate(hired = case_when(ysdeg <= 15 ~ \"1\", ysdeg > 15 ~ \"0\"))\nsummary(lm(salary ~ hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ hired, data = salary)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -8294  -3486  -1772   3829  10576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27469.4      913.4  30.073  < 2e-16 ***\nhired1       -7343.5     1291.8  -5.685 6.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4658 on 50 degrees of freedom\nMultiple R-squared:  0.3926,    Adjusted R-squared:  0.3804 \nF-statistic: 32.32 on 1 and 50 DF,  p-value: 6.734e-07\n\n\n\n\nCode\nsummary(lm(salary ~ sex + rank + degree + hired, data = salary))\n\n\n\nCall:\nlm(formula = salary ~ sex + rank + degree + hired, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6187.5 -1750.9  -438.9  1719.5  9362.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  29511.3      784.0  37.640  < 2e-16 ***\nsexFemale     -829.2      997.6  -0.831    0.410    \nrankAsst    -11925.7     1512.4  -7.885 4.37e-10 ***\nrankAssoc    -7100.4     1297.0  -5.474 1.76e-06 ***\ndegreePhD     1126.2     1018.4   1.106    0.275    \nhired1         319.0     1303.8   0.245    0.808    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3023 on 46 degrees of freedom\nMultiple R-squared:  0.7645,    Adjusted R-squared:  0.7389 \nF-statistic: 29.87 on 5 and 46 DF,  p-value: 2.192e-13\n\n\nI created a dummy variable called “hired” which coded those employed for 15 years or less (thus hired by the new Dean) as 1 and those who have been employed for over 15 years as 0. Then, I fit a new regression model and decided to include the variables of sex, rank, degree, and hired. I omitted the year and ysdeg variables to prevent overlapping or multicollinearity. Multicollinearity can be a concern when variables are highly correlated or related in some way. The idea of regression is to observe how each variable partially effects the output while holding the other variables fixed. We cannot reasonably change the year or ysdeg or hired variables individually while holding the other two fixed since they tend to “grow” in similar manners. Since the variable hired is a product of the ysdeg variable, we could not include both.\nBased on the regression model, those hired by the current Dean are predicted to make 319 dollars more than those not hired by the Dean. When it comes to salary, this is a rather insignificant number. Furthermore, the level of significance for the hired variable is .81, which is astronomical and indicates that the relationship between hired and salary is not statistically significant. Based on these factors, I would state that findings do not indicate any favorable treatment by the Dean toward faculty that the Dean specifically hired."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#question-3",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#question-3",
    "title": "DACSS603_HW4",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\ndata(\"house.selling.price\")\nhouse.selling.price"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#a-2",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#a-2",
    "title": "DACSS603_HW4",
    "section": "A",
    "text": "A\n\n\nCode\nsummary(lm(Price ~ Size + New, house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size + New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-205102  -34374   -5778   18929  163866 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -40230.867  14696.140  -2.738  0.00737 ** \nSize           116.132      8.795  13.204  < 2e-16 ***\nNew          57736.283  18653.041   3.095  0.00257 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53880 on 97 degrees of freedom\nMultiple R-squared:  0.7226,    Adjusted R-squared:  0.7169 \nF-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nBoth Size and New significantly positively predict selling price. As each predictor goes up by 1 unit, selling price rises by 116.132 dollars and 57736.283 dollars respectively."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#b-2",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#b-2",
    "title": "DACSS603_HW4",
    "section": "B",
    "text": "B\n\n\nCode\nnew <- house.selling.price %>% \n  filter(New == 1)\nsummary(lm(Price ~ Size, data = new))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = new)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-78606 -16092   -987  20068  76140 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -100755.31   42513.73  -2.370   0.0419 *  \nSize            166.35      17.09   9.735 4.47e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45500 on 9 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9036 \nF-statistic: 94.76 on 1 and 9 DF,  p-value: 4.474e-06\n\n\n\n\nCode\nold <- house.selling.price %>% \n  filter(New == 0)\nsummary(lm(Price ~ Size, data = old))\n\n\n\nCall:\nlm(formula = Price ~ Size, data = old)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -29155   -7297   14159  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15708.186  -1.415    0.161    \nSize           104.438      9.538  10.950   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52620 on 87 degrees of freedom\nMultiple R-squared:  0.5795,    Adjusted R-squared:  0.5747 \nF-statistic: 119.9 on 1 and 87 DF,  p-value: < 2.2e-16\n\n\nSize significantly positively predicts price for both new and old houses, but by a greater magnitude for new houses. Adjusted R-squared for the model is also much higher (0.91 vs. 0.58).\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#c-2",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#c-2",
    "title": "DACSS603_HW4",
    "section": "C",
    "text": "C\n\n\nCode\nSize <- 3000\nNew_Price = 166 * Size - 100755.31\nOld_Price = 104 * Size - 22227.808\nNew_Price\n\n\n[1] 397244.7\n\n\nCode\nOld_Price\n\n\n[1] 289772.2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#d-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#d-1",
    "title": "DACSS603_HW4",
    "section": "D",
    "text": "D\n\n\nCode\nsummary(lm(Price ~ Size*New, data = house.selling.price))\n\n\n\nCall:\nlm(formula = Price ~ Size * New, data = house.selling.price)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-175748  -28979   -6260   14693  192519 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -22227.808  15521.110  -1.432  0.15536    \nSize           104.438      9.424  11.082  < 2e-16 ***\nNew         -78527.502  51007.642  -1.540  0.12697    \nSize:New        61.916     21.686   2.855  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52000 on 96 degrees of freedom\nMultiple R-squared:  0.7443,    Adjusted R-squared:  0.7363 \nF-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#e-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#e-1",
    "title": "DACSS603_HW4",
    "section": "E",
    "text": "E\nThe predicted selling price, based on the new regression that includes interaction between Size and Newness, would look like:\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#f-1",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#f-1",
    "title": "DACSS603_HW4",
    "section": "F",
    "text": "F\n\n\nCode\nSize <- 3000\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 398324.7\n\n\nCode\nOld_Price\n\n\n[1] 291092.2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#g",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#g",
    "title": "DACSS603_HW4",
    "section": "G",
    "text": "G\n\n\nCode\nSize <- 1500\nNew_Price = -22227.81 + 104.44 * Size - 78527.50 * 1 + 61.92 * Size * 1\nOld_Price = -22227.81 + 104.44 * Size\nNew_Price\n\n\n[1] 148784.7\n\n\nCode\nOld_Price\n\n\n[1] 134432.2\n\n\nAs size of home goes up, the difference in predicted selling prices between old and new homes becomes larger."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW4.html#h",
    "href": "posts/RahulGundeti_DACSS603_HW4.html#h",
    "title": "DACSS603_HW4",
    "section": "H",
    "text": "H\nThe prediction model with interaction has a significantly large negative coefficient for the New variable. The adjusted r-squared for the model with interaction is 0.7363 and the adjusted r-squared for the first model without interaction is 0.7169. The increase in the adjusted r-squared with the interaction model could be due to an additional variable or could indicate a slightly better fit for the prediction of the data. Since the models do have similar adjusted r-squared values, I would prefer the model with interaction because the regression indicates that the interaction term is statistically significant to selling price prediction, so I feel it is necessary to utilize an equation that factors for this."
  },
  {
    "objectID": "posts/Rough_Draft_Project_Yakub Rabiutheen.html",
    "href": "posts/Rough_Draft_Project_Yakub Rabiutheen.html",
    "title": "Project Rough Draft Proposal",
    "section": "",
    "text": "The Research Question in this Project explores the Democratic Outcomes of Ex-French and British Colonies. How do French and British Colonies differ in terms of their Democratic ranking?\nIn terms of Colonial rule, Patrick Ziltener and Daniel K�nzler have come up with a scale to categorize the different levels of colonial rule(Ziltener & Kunzler,2013).\n0 = no colonial domination / not applicable 1 = semi-colonialism 2 = indirect rule with little interference in internal affairs v 3 = indirect rule with strong interference in internal affairs 4 = direct rule\nThis Project looks at countries with ranking 2-4, as Semi-Colonial Countries are not official colonies.\nFor both former French and Former British Colonies, there are 3 categories of Colonial Rule for both of them.\n\nWeakly Influenced Colonies\nStrongly Influenced Colonies\nDirectly Ruled Colonies."
  },
  {
    "objectID": "posts/Rough_Draft_Project_Yakub Rabiutheen.html#all-summary-tables",
    "href": "posts/Rough_Draft_Project_Yakub Rabiutheen.html#all-summary-tables",
    "title": "Project Rough Draft Proposal",
    "section": "All Summary Tables",
    "text": "All Summary Tables\nAs shown below, Directly Ruled Colonies had average of Positive Democratic Rankings in contrast to Weak and Strong Colonies that had negative Democratic Rankings on Average.\n\n\nCode\ndirect_rule_french.descriptive\n\n\n  democ autoc logpgp12   avexpr currentinst cons00a cons1 euro1900   logem4\n1     7     0 7.366976       NA         175       1     3        0 5.585449\n2     3     0       NA       NA         171      NA    NA        0 3.258096\n3     3     1 9.049626 6.500000         152       1     2       13 4.359270\n4     4     1 9.685675 7.818182         170       1     2        0 5.634789\n5     4     0 6.974252 6.545455         178       1     1        0 6.180017\n6     6     1 6.499560 5.000000         176       1     3        0 5.991465\n7     0     7 8.198421 6.409091          99       1     1        0 4.941642\n  mortality extmort4\n1        NA       NA\n2        NA       NA\n3        NA     78.2\n4       280       NA\n5       483       NA\n6       400     78.1\n7       140    140.0\n\n\nCode\nweak_rule_french.descriptive\n\n\n  democ autoc logpgp12 avexpr currentinst cons00a cons1 euro1900   logem4\n1     7     0 7.572484      6         166       1     3        0 5.103883\n2     1     3 7.308457     NA         184       1     1        0 5.634789\n  mortality extmort4\n1    164.66   164.66\n2        NA       NA\n\n\nCode\nstrong_rule_french.descriptive\n\n\n  democ autoc logpgp12   avexpr currentinst cons00a cons1 euro1900   logem4\n1     7     1 7.321831 4.454545         153       1     3        0 5.634789\n2     1     5 8.554794 7.090909          97       1     1        1 4.359270\n3     6     1 7.102072 4.000000         151       1     3        0 7.986165\n4     0     2 7.864489       NA         167       1     5        0 5.634789\n5     1     3 6.957467 6.909091         156       1     3        0 6.504288\n6     7     0 9.189585 6.454545          50       1     1        3 4.143135\n  mortality extmort4\n1     280.0      280\n2      78.2       NA\n3    2940.0     2940\n4        NA       NA\n5     668.0       NA\n6      63.0       NA\n\n\n##Regression Analysis Democracy vs Autocracy for Durability\n\n\nCode\nunique((strong_rule_french$`Country Name`))\n\n\n[1] \"Burkina Faso\" \"Morocco\"      \"Mali\"         \"Mauritania\"   \"Togo\"        \n[6] \"Tunisia\"     \n\n\n\n\nCode\nunique(direct_rule_french$`Country Name`)\n\n\n[1] \"Benin\"    \"Djibouti\" \"Algeria\"  \"Gabon\"    \"Guinea\"   \"Niger\"    \"Vietnam\" \n\n\n\n\nCode\nunique(weak_rule_french$`Country Name`)\n\n\n[1] \"Senegal\" \"Chad\"   \n\n\n\n\nCode\ngeneral_french.fit = lm(durable~democ+autoc, data=polity_french)\ndirect_rule_french.fit = lm(durable~democ+autoc, data=direct_rule_french)\nstrong_rule_french.fit= lm(durable~democ+autoc, data=strong_rule_french)\nweak_rule_french.fit = lm(durable~democ+autoc, data=weak_rule_french)\n\n\n\n\nCode\ngeneral_french.fit\n\n\n\nCall:\nlm(formula = durable ~ democ + autoc, data = polity_french)\n\nCoefficients:\n(Intercept)        democ        autoc  \n    12.2290      -0.5533       0.7022  \n\n\nAs shown below, the former UK colonies had much greater stability under Democracy than Autocracy, Post-Independence. In contrast to former French Colonies. This is relevant as if Democracy is shown to bring less stablity in a post-colonial country that might create a preference for autocracy.\n\n\nCode\ngeneral_british.fit\n\n\n\nCall:\nlm(formula = durable ~ democ + autoc, data = polity_british)\n\nCoefficients:\n(Intercept)        democ        autoc  \n    24.8874       1.3707      -0.9902  \n\n\nDirect French Ruled Ex Colonies had much more stability Post-Independence under an Autocracy than a Democratic Government.\n\n\nCode\ndirect_rule_french.fit\n\n\n\nCall:\nlm(formula = durable ~ democ + autoc, data = direct_rule_french)\n\nCoefficients:\n(Intercept)        democ        autoc  \n     6.7551       0.7666       7.7016  \n\n\nFrench Colonies that were Strongly Ruled had more Stability in a Autocracy than a Democracy.\n\n\nCode\nstrong_rule_french.fit\n\n\n\nCall:\nlm(formula = durable ~ democ + autoc, data = strong_rule_french)\n\nCoefficients:\n(Intercept)        democ        autoc  \n    -14.928        1.594       12.791  \n\n\nFrench Colonies that were Weakly Ruled had more Stability in a Autocracy than a Democracy.\n\n\nCode\nweak_rule_french.fit\n\n\n\nCall:\nlm(formula = durable ~ democ + autoc, data = weak_rule_french)\n\nCoefficients:\n(Intercept)        democ        autoc  \n     27.333       -1.333           NA  \n\n\n##Conclusion\nMy findings have found that on average British Colonies had better Democratic Outcomes than French Colonies, when divided into groups, Direct Rule Colonies that were British had positive Democratic outcomes on average compared to Weak and Strong Ruled Colonies. My analysis of French Colonies had similar findings that directly ruled colonies had better Democratic outcomes compared to Weak and Strong ruled colonies. For both French and British Colonies, Strong Ruled Colonies had the worst average Democratic Rankings."
  },
  {
    "objectID": "posts/Rough_Draft_Project_Yakub Rabiutheen.html#all-summary-tables-1",
    "href": "posts/Rough_Draft_Project_Yakub Rabiutheen.html#all-summary-tables-1",
    "title": "Project Rough Draft Proposal",
    "section": "All Summary Tables",
    "text": "All Summary Tables\nAs shown below, Directly Ruled Colonies had average of Positive Democratic Rankings in contrast to Weak and Strong Colonies that had negative Democratic Rankings on Average. quarto-executable-code-5450563D\ndirect_rule_french.descriptive\nweak_rule_french.descriptive\nstrong_rule_french.descriptive\n##Regression Analysis Democracy vs Autocracy for Durability\nquarto-executable-code-5450563D\nunique((strong_rule_french$`Country Name`))\nquarto-executable-code-5450563D\nunique(direct_rule_french$`Country Name`)\nquarto-executable-code-5450563D\nunique(weak_rule_french$`Country Name`)\nquarto-executable-code-5450563D\ngeneral_french.fit = lm(durable~democ+autoc, data=polity_french)\ndirect_rule_french.fit = lm(durable~democ+autoc, data=direct_rule_french)\nstrong_rule_french.fit= lm(durable~democ+autoc, data=strong_rule_french)\nweak_rule_french.fit = lm(durable~democ+autoc, data=weak_rule_french)\nquarto-executable-code-5450563D\ngeneral_french.fit\nAs shown below, the former UK colonies had much greater stability under Democracy than Autocracy, Post-Independence. In contrast to former French Colonies. This is relevant as if Democracy is shown to bring less stablity in a post-colonial country that might create a preference for autocracy. quarto-executable-code-5450563D\ngeneral_british.fit\nDirect French Ruled Ex Colonies had much more stability Post-Independence under an Autocracy than a Democratic Government.\nquarto-executable-code-5450563D\ndirect_rule_french.fit\nFrench Colonies that were Strongly Ruled had more Stability in a Autocracy than a Democracy.\nquarto-executable-code-5450563D\nstrong_rule_french.fit\nFrench Colonies that were Weakly Ruled had more Stability in a Autocracy than a Democracy. quarto-executable-code-5450563D\nweak_rule_french.fit\n##Conclusion\nMy findings have found that on average British Colonies had better Democratic Outcomes than French Colonies, when divided into groups, Direct Rule Colonies that were British had positive Democratic outcomes on average compared to Weak and Strong Ruled Colonies. My analysis of French Colonies had similar findings that directly ruled colonies had better Democratic outcomes compared to Weak and Strong ruled colonies. For both French and British Colonies, Strong Ruled Colonies had the worst average Democratic Rankings."
  },
  {
    "objectID": "posts/shelton_final1.html",
    "href": "posts/shelton_final1.html",
    "title": "Final Project: Part 1",
    "section": "",
    "text": "Homelessness is a complex living situation with several qualifying conditions; at its most simple state, the U.S Dept. of Housing and Urban Development defines it as lacking a fixed, regular nighttime residence (not a shelter) or having a nighttime residence not designed for human accommodation1.\nOn a single night in 2020, over 500,0002 people experienced homelessness in the United States. Florida, with the third largest state population , had the fourth largest homeless population of 2020 with 27,4872.\nFlorida counties represent a large age range and varying demographic profiles; the state is a hub to a variety of industries including tourism, defense, agriculture, and information technology. Investigating homelessness in Florida counties with robust data can lead to several conclusions about who is being impacted where, and how state policy is failing groups of a diverse population.\n\nResearch QuestionHypothesisIntroduction to DataImprovementsCodebookReferences\n\n\nCarole Zugazaga’s 2004 study of 54 single homeless men, 54 single homeless women, and 54 homeless women with children in the Central Florida area investigated stressful life events common among homeless people. The interviews revealed that women were more likely to have been sexually or physically assaulted, while men were more likely to have been incarcerated or abuse drugs/alcohol. Homeless women with children were more likely to be in foster care as a youth.\nNearly a decade later,county-level data can be used to investigate the relationship between Zugazaga’s reported stressful life events (incarceration, drug arrests, poverty, forcible sex…)3 and homelessness counts.\n\n\n\n\n\n\nResearch Question\n\n\n\nDo particular life stressors increase a population’s vulnerability to homelessness?\n\n\n\n\nHomelessness is not a new issue in the United States, yet homeless policy targets elimination via criminalization rather than prevention. Despite state and federal governments being aware of the circumstances that increase vulnerability to homelessness for decades, I anticipate all of the variables to remain significant in a model relating stressors to Florida homelessness counts 2018-2020.\n\n\n\n\n\n\nResearch Hypothesis\n\n\n\nH0: All stressors are insignificant in predicting homelessness counts ( Bi = 0 for i=0,1,2,…n )\nHA: At least one stressor Bi is significant in predicting homelessness counts\n\n\n\n\nThe data florida_1820.csv4 describes population, homelessness counts, poverty counts and several other demographic indicators3 at the county level for 2018-2020. All 67 Florida counties have observations for the 3 years giving us 201 observations of 15 variables. Each observation provides a count of each variables from a single county for a year within 2018-2020.\nThe data were collected from the Florida Department of Health. Variable names3 were used as search indicators to produce counts for Florida counties. Unfortunately, we cannot accurately analyze the effect of COVID-19 as data is incomplete for the majority of counties in 2021.\n\n\n\n\n\n\nIntro to Data\n\n\n\n\n\n\n\n\n\n  \n\n\n\n    County               Year      Homeless (Count)   Population     \n Length:201         Min.   :2018   Min.   :   0.0   Min.   :   8367  \n Class :character   1st Qu.:2018   1st Qu.:  11.0   1st Qu.:  28089  \n Mode  :character   Median :2019   Median : 151.0   Median : 130642  \n                    Mean   :2019   Mean   : 427.8   Mean   : 317746  \n                    3rd Qu.:2020   3rd Qu.: 563.0   3rd Qu.: 367471  \n                    Max.   :2020   Max.   :3516.0   Max.   :2864600  \n                                                                     \n Unemployment Rate   Median Inc    Incarceration (Rateper1000) Poverty (Count) \n Min.   : 2.100    Min.   :34583   Min.   : 0.60               Min.   :   906  \n 1st Qu.: 3.400    1st Qu.:41401   1st Qu.: 2.50               1st Qu.:  4901  \n Median : 4.000    Median :50640   Median : 3.40               Median : 16210  \n Mean   : 4.697    Mean   :51116   Mean   : 3.84               Mean   : 42922  \n 3rd Qu.: 5.600    3rd Qu.:58093   3rd Qu.: 4.50               3rd Qu.: 46034  \n Max.   :13.500    Max.   :83803   Max.   :18.60               Max.   :482656  \n                                                                               \n Drug Arrests (Count) Relocated (Rate) Sub Abuse Enrollment (Count)\n Min.   :   13        Min.   : 4.689   Min.   :   5.0              \n 1st Qu.:  225        1st Qu.:11.244   1st Qu.:  76.0              \n Median :  729        Median :12.700   Median : 250.0              \n Mean   : 1558        Mean   :13.288   Mean   : 877.6              \n 3rd Qu.: 1903        3rd Qu.:14.544   3rd Qu.:1030.0              \n Max.   :13038        Max.   :22.553   Max.   :6272.0              \n                                                                   \n Adult Pysch Beds (Count) Severe Housing Problems (Rate) Forcible Sex (Count)\n Min.   :  0.00           Min.   : 9.6                   Min.   :   0.0      \n 1st Qu.:  0.00           1st Qu.:13.3                   1st Qu.:  14.0      \n Median :  0.00           Median :15.4                   Median :  45.0      \n Mean   : 66.26           Mean   :15.8                   Mean   : 170.5      \n 3rd Qu.: 84.00           3rd Qu.:17.3                   3rd Qu.: 225.0      \n Max.   :778.00           Max.   :29.8                   Max.   :1408.0      \n                          NA's   :134                                        \n Foster Care (Count)\n Min.   :   3.0     \n 1st Qu.:  33.0     \n Median : 153.0     \n Mean   : 326.1     \n 3rd Qu.: 353.0     \n Max.   :2289.0     \n                    \n\n\n\n\n  \n\n\n\n\n\n\nExpanding Intro to Data exposes summary statistics including mean, range, quantiles, and standard deviation for all 15 variables. The table below the summaries provides arranged figures for basic parameters of interest grouped by county.\nLATER: Plots, Isolate more variables of interest with grouping, group by year?\n\n\nWhile the data is great illustration of homelessness in Florida by county, there are improvements that could be made to both data collection and the research question itself to further the study.\nData:\n\nUnfortunately, FL Health Charts did not provide demographic breakdown for the homeless population (Age, Sex, Race), which would drastically widen the scope of the analysis, leading to far more interesting conclusions.\nThere is only have data for a three year period; this is too small of a range to make a strong statement about the impact of homeless policy on Florida counties or how the relevance of certain stressors has changed over time. For a more in depth study I would begin with a 10 year range.\n\nResearch Question:\n\nDemographic breakdown of stressors’ impact (Age, Sex, Race)\nExtend the question to the entire country, providing a breakdown by state\nCompare to foreign countries to contrast governments’ approaches to homelessness and leading causes of homelessness around the world.\n\n\n\nLATER: Variable Definitions and Collection Methods here\n\n\nLater: Carol Zugazaga\n\n\n\n\n\n1.) Homeless Definition\n2.) US Interagency Council on Homelessness\n3.) Explanation of variables and collection method in Codebook tab\n4.) This data was cleaned and put in a tidy format in another script; manipulations were messy and inefficient (brute force) so I did not include the cleaning file."
  },
  {
    "objectID": "posts/shelton_final2.html",
    "href": "posts/shelton_final2.html",
    "title": "Final Project: Part 2 (Update)",
    "section": "",
    "text": "Homelessness is a complex living situation with several qualifying conditions; at its most simple state, the U.S Dept. of Housing and Urban Development defines it as lacking a fixed, regular nighttime residence (not a shelter) or having a nighttime residence not designed for human accommodation1.\nOn a single night in 2020, over 500,0002 people experienced homelessness in the United States. Florida, with the third largest state population , had the fourth largest homeless population of 2020 with 27,4872.\nFlorida counties represent a large age range and varying demographic profiles; the state is a hub to a variety of industries including tourism, defense, agriculture, and information technology. Investigating homelessness in Florida counties with robust data can lead to several conclusions about who is being impacted where, and how state policy is failing groups of a diverse population.\n\nResearch QuestionHypothesisIntroduction to DataRegression Analysis, Diagnostics, and Model SelectionConclusionsImprovementsCodebookReferences\n\n\nCarole Zugazaga’s 2004 study of 54 single homeless men, 54 single homeless women, and 54 homeless women with children in the Central Florida area investigated stressful life events common among homeless people. The interviews revealed that women were more likely to have been sexually or physically assaulted, while men were more likely to have been incarcerated or abuse drugs/alcohol. Homeless women with children were more likely to be in foster care as a youth.\nNearly a decade later,county-level data can be used to investigate the relationship between Zugazaga’s reported stressful life events (incarceration, drug arrests, poverty, forcible sex…)3 and homelessness counts.\n\n\n\n\n\n\nResearch Question\n\n\n\nDo particular life stressors increase a population’s vulnerability to homelessness?\n\n\n\n\nHomelessness is not a new issue in the United States, yet homeless policy targets elimination via criminalization rather than prevention. Despite state and federal governments being aware of the circumstances that increase vulnerability to homelessness for decades, I anticipate all of the variables to remain significant in a model relating stressors to Florida homelessness counts 2018-2020.\n\n\n\n\n\n\nResearch Hypothesis\n\n\n\nH0: All stressors are insignificant in predicting homelessness counts ( Bi = 0 for i=0,1,2,…n )\nHA: At least one stressor Bi is significant in predicting homelessness counts\n\n\n\n\nThe data florida_1820.csv4 describes population, homelessness counts, poverty counts and several other demographic indicators3 at the county level for 2018-2020. All 67 Florida counties have observations for the 3 years giving us 201 observations of 15 variables. Each observation provides a count of each variables from a single county for a year within 2018-2020.\nThe data were collected from the Florida Department of Health. Variable names3 were used as search indicators to produce counts for Florida counties. Unfortunately, we cannot accurately analyze the effect of COVID-19 as data is incomplete for the majority of counties in 2021.\n\n\n\n\n\n\nIntro to Data\n\n\n\n\n\n\n\n\n\n  \n\n\n\n    County               Year      Homeless (Count)   Population     \n Length:201         Min.   :2018   Min.   :   0.0   Min.   :   8367  \n Class :character   1st Qu.:2018   1st Qu.:  11.0   1st Qu.:  28089  \n Mode  :character   Median :2019   Median : 151.0   Median : 130642  \n                    Mean   :2019   Mean   : 427.8   Mean   : 317746  \n                    3rd Qu.:2020   3rd Qu.: 563.0   3rd Qu.: 367471  \n                    Max.   :2020   Max.   :3516.0   Max.   :2864600  \n                                                                     \n Unemployment Rate   Median Inc    Incarceration (Rateper1000) Poverty (Count) \n Min.   : 2.100    Min.   :34583   Min.   : 0.60               Min.   :   906  \n 1st Qu.: 3.400    1st Qu.:41401   1st Qu.: 2.50               1st Qu.:  4901  \n Median : 4.000    Median :50640   Median : 3.40               Median : 16210  \n Mean   : 4.697    Mean   :51116   Mean   : 3.84               Mean   : 42922  \n 3rd Qu.: 5.600    3rd Qu.:58093   3rd Qu.: 4.50               3rd Qu.: 46034  \n Max.   :13.500    Max.   :83803   Max.   :18.60               Max.   :482656  \n                                                                               \n Drug Arrests (Count) Relocated (Rate) Sub Abuse Enrollment (Count)\n Min.   :   13        Min.   : 4.689   Min.   :   5.0              \n 1st Qu.:  225        1st Qu.:11.244   1st Qu.:  76.0              \n Median :  729        Median :12.700   Median : 250.0              \n Mean   : 1558        Mean   :13.288   Mean   : 877.6              \n 3rd Qu.: 1903        3rd Qu.:14.544   3rd Qu.:1030.0              \n Max.   :13038        Max.   :22.553   Max.   :6272.0              \n                                                                   \n Adult Psych Beds (Count) Severe Housing Problems (Rate) Forcible Sex (Count)\n Min.   :  0.00           Min.   : 9.6                   Min.   :   0.0      \n 1st Qu.:  0.00           1st Qu.:13.3                   1st Qu.:  14.0      \n Median :  0.00           Median :15.4                   Median :  45.0      \n Mean   : 66.26           Mean   :15.8                   Mean   : 170.5      \n 3rd Qu.: 84.00           3rd Qu.:17.3                   3rd Qu.: 225.0      \n Max.   :778.00           Max.   :29.8                   Max.   :1408.0      \n                          NA's   :134                                        \n Foster Care (Count)\n Min.   :   3.0     \n 1st Qu.:  33.0     \n Median : 153.0     \n Mean   : 326.1     \n 3rd Qu.: 353.0     \n Max.   :2289.0     \n                    \n\n\n\n\n  \n\n\n\nCOMMENT PLOTS TAB\n\n\n\nExpanding Intro to Data exposes summary statistics including mean, range, quantiles, and standard deviation for all 15 variables. The table below the summaries provides arranged figures for basic parameters of interest grouped by county.\nLATER: Plots, Isolate more variables of interest with grouping, group by year?\n\n\n\n\nWhile over 10 variables are predicting Homeless (Count) across Florida counties, there are still limitations when attempting to comment on the magnitude of an individual stressor. Stressors influence homelessness by driving those in severe situations out of their home or away from their place of origin. Homeless (Count) is not an ideal measure of magnitude as the homeless population migrating to escape or avoid certain stressors would result in counties with low stressor values having a higher homeless population; this effect is left unexplained by the following models.\n\nThe variable Relocated (Rate) is included as an attempt to control for new movement, however this doesn’t completely capture county-to-county migration.\nFL Charts has data that records Population Who Lived in a Different County One Year Earlier, however with the data spanning 2009-2014, using values recorded 4 years prior to our data isn’t desirable either.\nThe most appropriate data to accurately capture county-to-county migration is here via the US Census Bureau. The -In, -Out, -Net... spreadsheet provides totals for each county in the United States and movement to all other US counties; unfortunately, this data is too complex to wrangle into the simple data set florida_1820.csv.\n\n\n\n\n\n\nCode\n# Fit 1: A Linear Regression Model With All Vars\n\n# Checking Linearity of variables not supported by our literature\n\nflorida_matrix <- florida_og %>%\n                    select(-c('County', \n                              'Year', \n                              'Population', \n                              'Poverty (Count)', \n                              'Severe Housing Problems (Rate)',\n                              'Sub Abuse Enrollment (Count)',\n                              'Drug Arrests (Count)',\n                              'Adult Psych Beds (Count)',\n                              'Foster Care (Count)',\n                              'Forcible Sex (Count)' ))%>%\n                      pairs()\n\n\n\n\n\nCode\nflorida_matrix\n\n\nNULL\n\n\nA quick look at variables with a relationship to homelessness not mentioned in Zugazaga’s study, or those that needed further investigation are shown here to confirm a linear approximation is appropriate. A log transformation on Unemployment Rate and Incaceration Rate will be shown in one of the models.\n\n\n\n\n\n\n\n\n\nFit 1: All Variables (No Transformations)\n\n\n\n\n\n\n\nCode\n# Linear relationship appears appropriate for all, possibly attempt log transformation on UE Rate?\n\n# Creating A Linear Model with all variables included: No Transformations\n\n# County Removed as too many levels; improvement: NWFL, NFL, CFL, SWFL, SOFLO categories?\n\nfit1 <- florida_og %>% \n          select(-'County')%>%\n            \n            lm(formula=`Homeless (Count)` ~.)\n\nsummary(fit1)\n\n\n\nCall:\nlm(formula = `Homeless (Count)` ~ ., data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-312.92  -82.67   -3.41   55.84  679.93 \n\nCoefficients: (1 not defined because of singularities)\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -3.913e+02  3.217e+02  -1.216  0.22915    \nYear                                     NA         NA      NA       NA    \nPopulation                       -2.669e-04  5.212e-04  -0.512  0.61074    \n`Unemployment Rate`              -1.965e+01  4.473e+01  -0.439  0.66211    \n`Median Inc`                      3.179e-03  3.175e-03   1.001  0.32111    \n`Incarceration (Rateper1000)`     4.631e+00  1.031e+01   0.449  0.65492    \n`Poverty (Count)`                -3.033e-03  2.758e-03  -1.100  0.27634    \n`Drug Arrests (Count)`           -3.089e-02  3.023e-02  -1.022  0.31150    \n`Relocated (Rate)`               -1.612e+00  8.243e+00  -0.195  0.84574    \n`Sub Abuse Enrollment (Count)`   -1.928e-02  4.347e-02  -0.444  0.65913    \n`Adult Psych Beds (Count)`        4.728e+00  6.623e-01   7.138 2.46e-09 ***\n`Severe Housing Problems (Rate)`  2.453e+01  8.285e+00   2.961  0.00455 ** \n`Forcible Sex (Count)`            3.544e-01  3.090e-01   1.147  0.25652    \n`Foster Care (Count)`             9.531e-01  1.518e-01   6.279 6.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 176.6 on 54 degrees of freedom\n  (134 observations deleted due to missingness)\nMultiple R-squared:  0.9453,    Adjusted R-squared:  0.9332 \nF-statistic: 77.81 on 12 and 54 DF,  p-value: < 2.2e-16\n\n\nCode\nrss1 <- deviance(fit1)\nprint(c('RSS Fit 1', rss1))\n\n\n[1] \"RSS Fit 1\"        \"1683759.60919107\"\n\n\n\nThe first model predicts Homeless (Count) using all variables, without any transformations or interactions. This causes 134 observations to removed as they are missing values for Severe Housing Problems (Rate).\nOnly 4 variables are deemed significant at alpha = 0.05; those without a star (see output) are deemed inconsequential in predicting Homeless (Count).\nLooking at the signs of the predicted insignificant variables, they seem implausible - Increases in variables like Population or Poverty (Count) decreases homelessness? It is clear the movement confounder mentioned above is influencing results. Select transformations or interactions could quell issues.\n\n\n\n\n\n\n\n\n\n\nFit 1: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\nFit 1 does a poor job of obeying the assumptions regarding residuals of linear regression.\nResiduals vs Fitted shows the residuals increasing in size the greater the fitted value is, violating the linearity and independence assumption.\nAs for residuals following an approximately normal distribution, the Q-Q Plot shows a noticeable deviation from the diagonal.\nThere are several points that could be considered outliers due to their residual or leverage value, how greatly they influence the points around them in the model.\n\nObservations 16, 37, and 154 represent Miami-Dade and Broward County - two of the largest and most urbanized regions in the state. Pinellas County (154) is a top 10 county in terms of population.\nMonroe County (130) has large positive residuals, indicating our model greatly under-estimated the number of homeless people in this county.\n\n\n\n\n\n\n\n\n\n\n\nFit 2: All Variables + Interactions + Transformations + Fill all Observations\n\n\n\n\n\n\n\nCode\nfit2 <- florida_og %>% \n          select(-c('County','Year'))%>%\n            mutate(`Unemployment Rate` = log(`Unemployment Rate`),\n                   `Incarceration (Rateper1000)` = log(`Incarceration (Rateper1000)`))%>%\n              fill('Severe Housing Problems (Rate)', .direction=\"down\")%>%\n                \n                lm(formula=`Homeless (Count)` ~ . \n                   + `Unemployment Rate`* `Population`\n                   + `Poverty (Count)`* `Median Inc`)\nsummary(fit2)\n\n\n\nCall:\nlm(formula = `Homeless (Count)` ~ . + `Unemployment Rate` * Population + \n    `Poverty (Count)` * `Median Inc`, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-610.27  -87.32   11.16   80.22  774.27 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      -3.839e+02  1.565e+02  -2.453  0.01511 *  \nPopulation                        1.937e-03  4.650e-04   4.165 4.77e-05 ***\n`Unemployment Rate`              -9.768e+01  4.964e+01  -1.968  0.05057 .  \n`Median Inc`                      1.879e-04  1.482e-03   0.127  0.89922    \n`Incarceration (Rateper1000)`     6.242e+01  3.180e+01   1.963  0.05111 .  \n`Poverty (Count)`                 2.261e-03  2.626e-03   0.861  0.39035    \n`Drug Arrests (Count)`           -4.194e-02  1.805e-02  -2.323  0.02126 *  \n`Relocated (Rate)`                7.769e+00  4.593e+00   1.691  0.09244 .  \n`Sub Abuse Enrollment (Count)`   -2.191e-02  2.286e-02  -0.959  0.33897    \n`Adult Psych Beds (Count)`        3.000e+00  4.594e-01   6.529 6.11e-10 ***\n`Severe Housing Problems (Rate)`  2.332e+01  4.641e+00   5.024 1.18e-06 ***\n`Forcible Sex (Count)`            2.187e-01  1.889e-01   1.158  0.24842    \n`Foster Care (Count)`             4.860e-01  7.659e-02   6.345 1.65e-09 ***\nPopulation:`Unemployment Rate`   -1.223e-04  6.905e-05  -1.771  0.07816 .  \n`Median Inc`:`Poverty (Count)`   -2.388e-07  7.586e-08  -3.147  0.00192 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 168.9 on 186 degrees of freedom\nMultiple R-squared:   0.94, Adjusted R-squared:  0.9355 \nF-statistic: 208.2 on 14 and 186 DF,  p-value: < 2.2e-16\n\n\nCode\nbic2 <- BIC(fit2)\nprint(c('BIC Fit 2:', bic2))\n\n\n[1] \"BIC Fit 2:\"       \"2701.61757023331\"\n\n\nCode\nrss2 <- deviance(fit2)\nprint(c('RSS Fit 2', rss2))\n\n\n[1] \"RSS Fit 2\"        \"5304985.78842245\"\n\n\n\nIn Fit 2, the values from Severe Housing Problems (Rate) were filled down provide values to restore all observations for use in the regression.\nBoth Unemployment Rate and Incarceration Rate were log() transformed to improve linearity with the outcome. Two interaction terms were included.\n\n\n\n\n\n\n\n\n\n\nFit 2: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\nThe appearance of the diagnostic plots improved greatly after the mentioned transformations, with only the Q-Q Plot remaining the same.\nWe still see the influence from the larger counties in the leverage plots. Observation 108, Lee - another high population county, is over-estimated by the model.\n\nAll in Southern Florida, Lee County differs from Broward or Miami-Dade in that it is home to a slightly older population; this may explain the over-estimate.\n\n\n\n\n\n\n\n\n\n\n\nFit 3: Partial Model - Preferred Variables\n\n\n\n\n\n\n\nCode\nfit3 <- florida_og %>% \n          select(-'County')%>%\n            mutate(`Incarceration (Rateper1000)` = log(`Incarceration (Rateper1000)`))%>%\n            #mutate(`Unemployment Rate` = log(`Unemployment Rate`))%>%\n                \n                lm(formula=`Homeless (Count)` ~ \n                   `Poverty (Count)`\n                   + `Median Inc`\n                   + `Incarceration (Rateper1000)`\n                   #+ `Severe Housing Problems (Rate)`\n                   + `Relocated (Rate)`\n                   + `Drug Arrests (Count)`\n                   + `Adult Psych Beds (Count)`\n                   + `Forcible Sex (Count)`\n                   + `Foster Care (Count)`\n                   + `Poverty (Count)`* `Median Inc`)\nsummary(fit3)\n\n\n\nCall:\nlm(formula = `Homeless (Count)` ~ `Poverty (Count)` + `Median Inc` + \n    `Incarceration (Rateper1000)` + `Relocated (Rate)` + `Drug Arrests (Count)` + \n    `Adult Psych Beds (Count)` + `Forcible Sex (Count)` + `Foster Care (Count)` + \n    `Poverty (Count)` * `Median Inc`, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-549.13  -60.37  -10.18   61.85  937.89 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                    -1.243e+02  1.055e+02  -1.178    0.240    \n`Poverty (Count)`              -1.030e-03  2.443e-03  -0.422    0.674    \n`Median Inc`                    8.677e-04  1.587e-03   0.547    0.585    \n`Incarceration (Rateper1000)`   3.614e+01  3.328e+01   1.086    0.279    \n`Relocated (Rate)`              4.206e+00  4.495e+00   0.936    0.351    \n`Drug Arrests (Count)`         -4.258e-03  1.529e-02  -0.279    0.781    \n`Adult Psych Beds (Count)`      3.845e+00  4.142e-01   9.283  < 2e-16 ***\n`Forcible Sex (Count)`          1.002e-01  2.029e-01   0.494    0.622    \n`Foster Care (Count)`           4.662e-01  7.119e-02   6.548 5.24e-10 ***\n`Poverty (Count)`:`Median Inc`  1.472e-08  4.343e-08   0.339    0.735    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.8 on 191 degrees of freedom\nMultiple R-squared:  0.9254,    Adjusted R-squared:  0.9219 \nF-statistic: 263.3 on 9 and 191 DF,  p-value: < 2.2e-16\n\n\nCode\nbic3 <- BIC(fit3)\nprint(c('BIC Fit 3:', bic3))\n\n\n[1] \"BIC Fit 3:\"       \"2718.88871539038\"\n\n\nCode\nrss3 <- deviance(fit3)\nprint(c('RSS Fit 3', rss3))\n\n\n[1] \"RSS Fit 3\"        \"6596216.29584835\"\n\n\n\nA model containing just variables I believed would provide the best fit.\n\n\n\n\n\n\n\n\n\n\nFit 3: Diagnostics\n\n\n\n\n\n\n\n\n\n\n\nThe diagnostic plots of this model leave much to be desired, similarly to Fit 1\n\n\n\n\n\n\n\n\nComparing Residuals Sum Squared, R^2, and BIC to evaluate Fit 2 versus Fit 3, I would select Fit 2 for both prediction and inference. Using all values, including interactions, and a log transformation results in the lowest RSS and BIC, and maximizes the Adjusted R-Squared. It provides the most appropriate diagnostic plots.\n\nInteraction terms: Population:Unemployment Rate and Median Income:Poverty (Count)\n\nI assumed the influence of the unemployment rate would change at different population values. A small unemployment rate of 2% will not have the same effect on the outcome in a county of 100,000 as a 2% rate in a county of 2.5 million.\nOur model found the impact of Unemployment Rate when predicting homelessness in a county diminished as Population increased.\nWhen considering the number of people living below the poverty line, it’s reasonable to believe the influence of number of citizens living below the poverty line will have a greater impact on homelessness in counties with lower median incomes.\nAnother negative slope, the model found the impact of Median Inc to decrease (taking it below zero) as Poverty (Count) increases.\n\nResearch Question:\n\nUsing all of the values rather than just 2019 not only improves Residual Standard Error and Adjusted R-Squared value, it corrects the signs and magnitude of effects. Several more stressors were deemed significant at the 0.05 and 0.10 level.\nAll of Zugazaga’s effects had the correct sign demonstrating their influence in this model, but only Foster Care and Drug Arrests were significant at the 0.05 level as hypothesized. This significance is a comment on the mathematical properties of the model rather than on the real-life influence of the stressors. Incarceration and Forcible Sex are influential situations that can contribute to homelessness.\nDrug Arrests again has a negative slope, a concerning suggestion would be incarceration as a form of drug abuse intervention is decreasing homelessness; however, Incarceration Rate has a large positive slope, dispelling this notion.\n\nThis result speaks more to recidivism rates in Florida’s communities as well as the challenge that is reintegrating into society after release. The negative slope still indicates drug abuse has a role in increasing homeless in Florida counties.\n\n\n\n\n\n\n\n\n\n\n\nWas the Research Question Answered?\n\n\n\n\nAs hypothesized, the model proved several stressors to be significant in predicting Homeless Counts across Florida\nUnfortunately, the study is unable to make a substantial comment on which stressors most increased vulnerability to Homelessness, evaluating magnitude. To do this, deeper demographic variables would need to be included, as well as controlling for stressors as a push factor in homeless migration.\n\n\n\n\n\n\n\n\n\nPrediction vs Inference\n\n\n\n\nThe goal of this brief study was to make inferences regarding stressors’ impact on Homelessness (Counts) in Florida.\nIf prediction was our focus, I would use new 2021 data from FL Charts without the Homeless (Count) column to test the efficacy of Fit 2 as a predictive tool.\n\n\n\n\n\nWhile the data is great illustration of homelessness in Florida by county, there are improvements that could be made to both data collection and the research question itself to further the study.\nData:\n\nUnfortunately, FL Health Charts did not provide demographic breakdown for the homeless population (Age, Sex, Race), which would drastically widen the scope of the analysis, leading to far more interesting conclusions.\nThere is only have data for a three year period; this is too small of a range to make a strong statement about the impact of homeless policy on Florida counties or how the relevance of certain stressors has changed over time. For a more in depth study I would begin with a 10 year range.\n\nResearch Question:\n\nDemographic breakdown of stressors’ impact (Age, Sex, Race)\nExtend the question to the entire country, providing a breakdown by state\nCompare to foreign countries to contrast governments’ approaches to homelessness and leading causes of homelessness around the world.\n\n\n\nVariable Definitions and Collection Methods here\n\n\nCarol Zugazaga R4DS LSR R packages?\n\n\n\n\n\n1.) Homeless Definition\n2.) US Interagency Council on Homelessness\n3.) Explanation of variables and collection method in Codebook tab"
  },
  {
    "objectID": "posts/shelton_HW1.html",
    "href": "posts/shelton_HW1.html",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "1.) Using LungCapData, answer descriptive questions about the data and its distributions.\n2.) Use the given distribution to answer questions about the probability of discrete events.\n\n\n\n\n\n\nCode\n#| include: false\n#| label: Loading in LungCap\n\n og_lungcap <- readxl::read_xls(\"_data/LungCapData.xls\")\n\n# Quick look at dataset\n# glimpse(og_lungcap)\n\n# Variables - 3<dbl> ratio 3<char> (can coerce to logical if needed), \n\n# length(which(is.na(og_lungcap)))\n\n# No missing values to consider\n\n# Descriptive\n# summarytools::dfSummary(og_lungcap)\n\n\nLungCapData: Describes the lung capacity of a population of 725 children aged 3 - 19. It further categorizes the subjects by height, sex, smoking habits, and whether they were birthed using the Caesarean section technique.\nIn the following sections, we’ll use select(), group_by(), filter(), and summarize() to further explore the data and find important relations between variables.\n\n\n\n\n\n\n\n\nLungCap looks to be approximately normally distributed (unimodal, symmetric) with most observations centered around the mean (7.86).\n\n\n\n\n\nCode\nhist_gender <- ggplot(og_lungcap, aes(x=LungCap, y=..density.., fill=Gender)) +\n  geom_histogram(alpha=.5, position=\"identity\", bins=20)+\n  geom_vline(aes(xintercept=mean(LungCap)))\nhist_gender\n\n\n\n\n\nPackage ggplot2 functions ggplot() and geom_histogram() are used to display the LungCap distribution filled by the Gender variable. Both density plots center on the mean, indicating both male and female lung capacity observations are highly concentrated around the mean. The male distribution is shifted slightly to the right of the female distribution, meaning male observations had a higher upper range value than female observations. Males had more observations concentrated to the right of the mean, and the female distribution reciprocated this effect to the left of the mean.\n\n\n\n\n\nCode\nsmokers <- group_by(og_lungcap, Smoke)\nsmokers %>%\n  summarize(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nAfter creating a new dataset smokers by using group_by() on our original data, smokers is piped into a summarize() call. The results surprisingly show that the smoking group had a higher mean lung capacity than the nonsmoking group. This is likely due to a mean age difference within the groups.\n\n\n\n\n\nCode\n# Creating Age Groups Using Case When\n\nsmokers_age <- smokers %>%\n  mutate(AgeGroup = case_when(Age >= 18 ~ \"18+\", \n            Age == 16 | Age == 17 ~ \"16-17\",\n            Age == 14 | Age == 15 ~ \"14-15\",\n            Age <= 13~ \"Under 13\"))\n\n# Mean LungCap by Age and Smoke\n# Must regroup by Smoke again\nsmokers_age %>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(LungCap)`\n  <chr>    <chr>           <dbl>\n1 14-15    no               9.14\n2 14-15    yes              8.39\n3 16-17    no              10.5 \n4 16-17    yes              9.38\n5 18+      no              11.1 \n6 18+      yes             10.5 \n7 Under 13 no               6.36\n8 Under 13 yes              7.20\n\n\nAfter using mutate() to add a column AgeGroup to a copy of smokers, group_by() groups the new dataset by AgeGroup and Smoke before piping it into a summarize() command to find the grouped means of LungCap by AgeGroup and Smoke.\nThe results show that for children above the age of 13, smokers had a lower mean lung capacity than non-smokers. However, for the 13 and under group, we again see results that imply smokers have greater lung capacity than nonsmokers. Let’s investigate further into the relationship between age and lung capacity to explain this quizzical result.\n\n\n\n\n\nCode\ncov(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 8.738289\n\n\nCode\ncor(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 0.8196749\n\n\nCode\n#GGPlot of Age vs Lung\nggplot(og_lungcap, aes(x=Age, y=LungCap)) + geom_point()\n\n\n\n\n\nAge and LungCap have a high covariance which leads to a high correlation (p=0.82). This strong positive value (-1<p<1) indicates these variables “vary greatly” together: when Age is high in the data, so is LungCap. We cannot say that an increase in Age causes an increase Lung capacity without first showing this through regression; however, our results show the variables are highly correlated.\nWe can use knowledge of the human body to infer that as our body ages, our lungs mature. The ages of smokers of the Under 13 group are likely highly left skewed, as I don’t expect many children under 10 to be smoking. This underlying age distribution explains our puzzling results from the previous section.\n\n\nCode\nsmokers_age%>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(Age))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(Age)`\n  <chr>    <chr>       <dbl>\n1 14-15    no          14.5 \n2 14-15    yes         14.6 \n3 16-17    no          16.4 \n4 16-17    yes         16.6 \n5 18+      no          18.5 \n6 18+      yes         18.1 \n7 Under 13 no           9.49\n8 Under 13 yes         11.7 \n\n\n\n\n\n\nFirst, let’s create two vectors: x_val and freq. Then, we’ll use rbind() to create a table.\n\n\nCode\nx_val <-c(0,1,2,3,4)\nfreq <- c(128,434,160,64,24)\nprob <- freq/sum(freq)\n\nxdist <- rbind(x_val,prob)\n\nxdist\n\n\n           [,1]      [,2]      [,3]       [,4]       [,5]\nx_val 0.0000000 1.0000000 2.0000000 3.00000000 4.00000000\nprob  0.1580247 0.5358025 0.1975309 0.07901235 0.02962963\n\n\n\n\n\n\nCode\n# Finding probability of inmate having exactly 2 prior convictions\n\n#Column Index is 3 as the first column is 0\n\n#Surely there is a cleaner way to do this using tidyverse functions rather than base?\n\n# a\na <- xdist['prob',3] \na\n\n\n     prob \n0.1975309 \n\n\n\n\n\n\n\nCode\n#b\nb <- sum(xdist['prob',1:2])\nb\n\n\n[1] 0.6938272\n\n\n\n\n\n\n\nCode\n# c\nc <- a + b\nc\n\n\n    prob \n0.891358 \n\n\n\n\n\n\n\nCode\n#d\nd <- 1 - c\nd\n\n\n    prob \n0.108642 \n\n\n\n\n\n\n\n[1] 1.28642\n\n\n\n\n\n\n\n\n\nCode\n# Var= E(X^2) - E(X)^2\n# Again using brute force because cannot use var() function on the object xdist correctly\nvar_x <-sum((x_val^2)*prob) - ex^2\nvar_x\n\n\n[1] 0.8562353\n\n\n\n\n\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/shelton_HW2.html",
    "href": "posts/shelton_HW2.html",
    "title": "Homework 2 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(warning= FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/shelton_HW2.html#homework-2",
    "href": "posts/shelton_HW2.html#homework-2",
    "title": "Homework 2 Solution",
    "section": "Homework 2",
    "text": "Homework 2\n\nQ1Q2Q3Q4Q5Q6\n\n\n\n90% Confidence Intervals\n\n\nCode\n# Bypass\nn_bp <- 539\nmean_bp <- 19\nsd_bp <- 10\nt_90 <- qt(.05, (n_bp-1), lower.tail=F)\n\n#CI\nupper_bp <- mean_bp + ((sd_bp/sqrt(539))*t_90)\nlower_bp <- mean_bp - ((sd_bp/sqrt(539))*t_90)\n\nci90_bp <- c(lower_bp,upper_bp)\nprint(c(\"90% CI For Mean Bypass Wait\", ci90_bp))\n\n\n[1] \"90% CI For Mean Bypass Wait\" \"18.2902893200424\"           \n[3] \"19.7097106799576\"           \n\n\nCode\n# Angiography\nn_ag <- 847\nmean_ag <- 18\nsd_ag <- 9\nt_90 <- qt(.05, (n_ag-1), lower.tail=F)\n\n#CI\nupper_ag <- mean_ag + (sd_ag/sqrt(539)*t_90)\nlower_ag <- mean_ag - (sd_ag/sqrt(539)*t_90)\n\nci90_ag <- c(lower_ag,upper_ag)\n\nprint(c(\"90% CI For Angiography Wait\", ci90_ag))\n\n\n[1] \"90% CI For Angiography Wait\" \"17.3616612514732\"           \n[3] \"18.6383387485268\"           \n\n\nCode\nprint(c(\"Width Bypass\", upper_bp-lower_bp))\n\n\n[1] \"Width Bypass\"     \"1.41942135991513\"\n\n\nCode\nprint(c(\"Width Angiography\", upper_ag-lower_ag))\n\n\n[1] \"Width Angiography\" \"1.27667749705367\" \n\n\nThe 90% Confidence interval is narrower for the mean Angiography wait time (days) than mean Bypass wait due to the larger sample and smaller standard deviation.\n\n\n\n\nOne Prop Confidence Interval\n\n\nCode\n# College 95% CI\nprop.test(567,1031,conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the true proportion of Americans who believe a college education is essential for success is (0.52,0.58). 95% confidence is not a comment on the proportion itself, rather our method. If we took several samples and created a confidence interval for proportion p, 95% of the intervals would contain the true population proportion.\nBecause our confidence interval does not include .5, we can conclude that at .05 significance, the majority (>0.5) of Americans believe that a college educcation is essential for success.\n\n\n\n\nMargin of Error Calculation\n\n\nCode\n# Margin of Error Calculation\n\nci_95 <- qnorm(.025, lower.tail=F)\n\n# 5 = (170*.25)/sqrt(x)*1.96\n\n(x <- ((170*.25)/5)*ci_95)^2\n\n\n[1] 277.5454\n\n\nTo estimate mean textbook cost per semester within $5 of true value at .05 significance level the financial aid office would need 278 students in their sample.\n\n\n\n\nOne Sample T-Test\n\n\n\n\n\n\na\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees != $500/wk\n\n\nCode\n# womens data\nw_xbar <- 410\nw_n <- 9\nw_sd <- 90\nw_se <- 90/sqrt(w_n)\ntest_stat <- (w_xbar-500)/w_se\ncrit_2sided <- abs(qt(.025,w_n-1))\ncrit_less <- qt(.05, w_n-1,lower.tail=T)\ncrit_greater <- qt(.95,w_n-1,lower.tail=T)\np_value <- pt(test_stat, df=w_n-1, lower.tail=T)\npval_greater <- pt(test_stat, df=w_n-1, lower.tail=F)\n\n# Two Sided 2 Test\nprint('Two-Sided T-Test')\n\n\n[1] \"Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"-3\"                                  \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"2.30600413520417\" \n\n\nCode\nprint(c('p-value', 2*p_value))\n\n\n[1] \"p-value\"            \"0.0170716812337826\"\n\n\np-value = .017; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages differ from $500/week. If female weekly income was equal to 500, we would expect 1.7% of samples to produce a sample mean of 410$ or more extreme.\n\n\n\n\n\n\n\n\nb\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is less than $500/wk\n\n\nCode\nprint('Left-Sided T-Test')\n\n\n[1] \"Left-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_less))\n\n\n[1] \"rejection-region:\" \"-1.8595480375309\" \n\n\nCode\nprint(c('p-value',p_value))\n\n\n[1] \"p-value\"             \"0.00853584061689132\"\n\n\np-value = .009; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages are less than $500/week. If mean female weekly income was equal to 500 , we would expect less than one percent of samples to produce a mean equal to or more extreme (less) than 410.\n\n\n\n\n\n\n\n\nc\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is greater than $500/wk\n\n\nCode\nprint('Right-Sided T-Test')\n\n\n[1] \"Right-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_greater))\n\n\n[1] \"rejection-region:\" \"1.8595480375309\"  \n\n\nCode\nprint(c('p-value',pval_greater))\n\n\n[1] \"p-value\"           \"0.991464159383109\"\n\n\np-value = .991; Fail to reject the null, at alpha=.05 we do nothave sufficient evidence to conclude female employees’ wages are greater than $500/week. If female weekly income was equal to 500, we would expect 99 percent of samples to produce a mean equal to or greater than 410.\n\n\n\n\n\n\n\n\n\n\n\na & b\n\n\n\n\n\nCode\n# jones data\nj_xbar <- 519.5\nj_n <- 1000\nj_se <- 10\nj_test_stat <- (j_xbar-500)/j_se\ncrit_2sided <- abs(qt(.025,j_n-1))\nj_p_value <- pt(j_test_stat, df=j_n-1, lower.tail=F)\n\n\n# Jones Two Sided 2 Test\nprint('Jones Two-Sided T-Test')\n\n\n[1] \"Jones Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', j_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.95\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*j_p_value))\n\n\n[1] \"p-value\"            \"0.0514555476459477\"\n\n\nCode\nprint(c('insignificant at alpha = 0.05'))\n\n\n[1] \"insignificant at alpha = 0.05\"\n\n\nCode\n# smith data\ns_xbar <- 519.7\ns_n <- 1000\ns_se <- 10\ns_test_stat <- (s_xbar-500)/s_se\ncrit_2sided <- abs(qt(.025,j_n-1))\ns_p_value <- pt(s_test_stat, df=s_n-1, lower.tail=F)\n\n\n# Smith Two Sided 2 Test\nprint('Smith Two-Sided T-Test')\n\n\n[1] \"Smith Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', s_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.97\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*s_p_value))\n\n\n[1] \"p-value\"            \"0.0491142565416521\"\n\n\nCode\nprint(c('significant at alpha = 0.05'))\n\n\n[1] \"significant at alpha = 0.05\"\n\n\n\n\n\n\n\n\n\n\nc\n\n\n\nBy not reporting the p-value, we do not understand the strength of the test - how extreme are the findings? In in an example like this, we see nearly identical results produce opposite significance results; language like “statistically significant” can get especially dangerous here to someone who is unfamiliar with basic statistical theory.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, \n               41.95, 28.61, 41.29, \n               52.19, 49.48, 35.02, \n               48.13, 39.28, 54.41, \n               41.66, 30.28, 18.49, \n               38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes; at the 95% confidence level, we have sufficient evidence to reject the null hypothesis mu=45. 45 is not included in our left sided confidence interval, favoring the alternative hypothesis that the average tax on gas in the United States in 2005 was less than 45 cents per gallon."
  },
  {
    "objectID": "posts/shelton_HW3.html",
    "href": "posts/shelton_HW3.html",
    "title": "HW 3 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(alr4)\nlibrary(smss)\nlibrary(GGally)\n\nknitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/shelton_HW3.html#homework-3",
    "href": "posts/shelton_HW3.html#homework-3",
    "title": "HW 3 Solution",
    "section": "Homework 3",
    "text": "Homework 3\n\nQ1Q2Q3Q4Q5\n\n\nWe are using UN11 data:\n\n\n\n\n\n\nUN11\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n1.1.1\n\n\n\nPredictor/Explanatory/IV - ppgdp Per Person GDP\nResponse/DV - fertility Fertility Rate per 1000\n\n\n\n\n\n\n\n\n1.1.2\n\n\n\n\n\n\n\nList of 96\n $ line                      :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                      :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                      :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 11\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                     : chr \"Fertility Rate x ppGDP\"\n $ aspect.ratio              : NULL\n $ axis.title                : NULL\n $ axis.title.x              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom       : NULL\n $ axis.title.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left         : NULL\n $ axis.title.y.right        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom        : NULL\n $ axis.text.y               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left          : NULL\n $ axis.text.y.right         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x              : NULL\n $ axis.ticks.x.top          : NULL\n $ axis.ticks.x.bottom       : NULL\n $ axis.ticks.y              : NULL\n $ axis.ticks.y.left         : NULL\n $ axis.ticks.y.right        : NULL\n $ axis.ticks.length         : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x       : NULL\n $ axis.ticks.length.x.top   : NULL\n $ axis.ticks.length.x.bottom: NULL\n $ axis.ticks.length.y       : NULL\n $ axis.ticks.length.y.left  : NULL\n $ axis.ticks.length.y.right : NULL\n $ axis.line                 : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x               : NULL\n $ axis.line.x.top           : NULL\n $ axis.line.x.bottom        : NULL\n $ axis.line.y               : NULL\n $ axis.line.y.left          : NULL\n $ axis.line.y.right         : NULL\n $ legend.background         : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin             : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing            : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x          : NULL\n $ legend.spacing.y          : NULL\n $ legend.key                : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size           : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height         : NULL\n $ legend.key.width          : NULL\n $ legend.text               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.align         : NULL\n $ legend.title              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.align        : NULL\n $ legend.position           : chr \"right\"\n $ legend.direction          : NULL\n $ legend.justification      : chr \"center\"\n $ legend.box                : NULL\n $ legend.box.just           : NULL\n $ legend.box.margin         : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background     : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing        : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ panel.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.border              : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing             : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ panel.spacing.x           : NULL\n $ panel.spacing.y           : NULL\n $ panel.grid                :List of 6\n  ..$ colour       : chr \"grey92\"\n  ..$ size         : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major          : NULL\n $ panel.grid.minor          :List of 6\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.5\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.major.x        : NULL\n $ panel.grid.major.y        : NULL\n $ panel.grid.minor.x        : NULL\n $ panel.grid.minor.y        : NULL\n $ panel.ontop               : logi FALSE\n $ plot.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ plot.title                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.title.position       : chr \"panel\"\n $ plot.subtitle             :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 5.5points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : num 1\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 5.5points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.caption.position     : chr \"panel\"\n $ plot.tag                  :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.2\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.tag.position         : chr \"topleft\"\n $ plot.margin               : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background          : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ strip.background.x        : NULL\n $ strip.background.y        : NULL\n $ strip.placement           : chr \"inside\"\n $ strip.text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey10\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 4.4points 4.4points 4.4points 4.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x              : NULL\n $ strip.text.y              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.switch.pad.grid     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.switch.pad.wrap     : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ strip.text.y.left         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ x                         : chr \"Per Person GDP (USD)\"\n $ y                         : chr \"Fertility   Rate per 1000\"\n $ caption                   : chr \"Fig 1.1.2\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\nNo, our scatter plot appears to have a negative exponential relationship; as ppgdp increase, fertility decreases at a nonlinear rate.\nNOTE: If anyone knows why it is printing the above output as well as the graph please send a comment on Google Classroom or Piazza!\n\n\n\n\n\n\n\n\n\n1.1.3\n\n\n\n\n\n\n\n\n\n\nNow, after using a log transformation on both x and y, our scatter plot appears to have a negative linear relationship; as ppgdp increase, fertility decreases at a linear rate. This transformation makes these variabels appropriate for linear regression.\n\n\n\n\n\n\n\n\n\n\n\nUSD to GBP\n\n\n\na) The slope of the regression line will be divided by 1.33 to represent the adjustment to GBP. \\(1 USD * (1 GBP/1.33 USD)\\) leaves us with just GBP. The slope is reduced by approximately 25 percent.\nb) The correlation between the explanatory variable and the response will not change.\n\n\n\n\n\n\n\n\n\n\nWater Scatterplot Matrix\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nThe rightmost column uses BSAAM as a response variable to the preciptation values at the various sites over the years 1948-1990. Inspecting the plots on this column closer, we can see that BSAAM has a strong positive correlation with OPSLAKE, OPRC, and OBPBC. There is a much weaker positive relationship between BSAAM and APSLAKE, APSAB, and APSMAM. If we were attempting to predict stream runoff volume, it would be best to observe and use the values of OPSLAKE, OPRC, and OBPBC to create our model. These locations’ precipitation values are highly correlated with stream runoff volume each year.\n\n\n\n\n\n\n\n\nScatterplot Matrix Rateprof\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nUsing ggpairs of the GGally package, we can see the variables that correlate the highest with quality are helpfulness and clarity. Clarity and helpfulness have the third highest correlation with each other. If we were attempting to predict the quality rating of a course, we would strongly consider both clarity and helpfulness ratings.\n\n\n\n\n\n\n\n\n\n\n\nPolitical Ideology x Religiosity\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = pi_num ~ re_no, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         2.2667     0.3394   6.678 1.18e-08 ***\nre_nooccasionally   0.2506     0.4181   0.599 0.551374    \nre_nomost weeks     2.1619     0.6017   3.593 0.000691 ***\nre_noevery week     2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\n\nFirst using a box plot to visualize the relationship between a numeric and a nominal variable, it’s clear the distributions of those that attend church more frequently skew towards conservative values on the 7-point scale provided by the data. This is confirmed by inspecting the summary of the linear regression model relating Political Ideology to Religiosity. Both group means’ coefficients (scores) for those attending church “most weeks” and “weekly” are approximately 2 times greater that the intercept value that respresents those that attend “never”.\n\n\n\n\n\n\n\n\n\nHigh School GPA x TV\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nObserving the results of the linear regression model relating high-school GPA to hours spent watching TV, we see that for every one hour increase in hours of TV/wk, our model predicts high-school GPA to decrease by 0.018305 points. We see the weak negative linear relationship using both geom_point and geom_smooth to depict the regression line and the original data points."
  },
  {
    "objectID": "posts/shelton_HW4.html",
    "href": "posts/shelton_HW4.html",
    "title": "Homework 4 Solution",
    "section": "",
    "text": "Q1Q2Q3\n\n\n\n\n\n\nCode\n(pred_1 <- -10536 + 53.8*(1240) + 2.84*(18000))\n## [1] 107296\n\n(resid_1 <- 145000 - pred_1)\n## [1] 37704\n\n\n\nThe model provided predicts the selling price of a 1,240-sqft house build on a 18,000-sqft lot to be $107,296.\nThe residual (Actual-Predicted) for this prediction is $37,704. The large positive value indicated the model under-predicts selling price for this house.\nThe high residual could be due to a competitive market (cash offers, bidding, etc.), location, or even amenities/renovations within the house, all confounders the model doesn’t account for.\n\n\n\n\n\nWhen lot size is fixed, the model predicts selling price to increase $53.80 for each 1-sqft increase in house size.\n\n\n\n\n\n\\(53.8/2.84 = 18.94\\) For fixed home size, lot size would need to increase by 18.94-sqft to have the same effect as a one unit increase in home size.\n\n\n\n\n\n\n\n\nCode\nsalary <- alr4::salary\n\n(by_sex <- t.test(formula=`salary`~`sex`, data=salary))\n## \n##  Welch Two Sample t-test\n## \n## data:  salary by sex\n## t = 1.7744, df = 21.591, p-value = 0.09009\n## alternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n## 95 percent confidence interval:\n##  -567.8539 7247.1471\n## sample estimates:\n##   mean in group Male mean in group Female \n##             24696.79             21357.14\n\n\nAt alpha=0.05 we do not have sufficient evidence to reject the null hypothesis, H0: There is no difference in Male and Female mean salaries. We cannot conclude a difference exists between male and female salaries.\n\n\n\n\n\nCode\nsalary_1 <- lm(salary ~.,\n               data = salary)\nsummary(salary_1)\n## \n## Call:\n## lm(formula = salary ~ ., data = salary)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4045.2 -1094.7  -361.5   813.2  9193.1 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 15746.05     800.18  19.678  < 2e-16 ***\n## degreePhD    1388.61    1018.75   1.363    0.180    \n## rankAssoc    5292.36    1145.40   4.621 3.22e-05 ***\n## rankProf    11118.76    1351.77   8.225 1.62e-10 ***\n## sexFemale    1166.37     925.57   1.260    0.214    \n## year          476.31      94.91   5.018 8.65e-06 ***\n## ysdeg        -124.57      77.49  -1.608    0.115    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2398 on 45 degrees of freedom\n## Multiple R-squared:  0.855,  Adjusted R-squared:  0.8357 \n## F-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\nprint('95% Confidence Interval:')\n## [1] \"95% Confidence Interval:\"\n\n(confint.lm(salary_1, level = 0.95))\n##                  2.5 %      97.5 %\n## (Intercept) 14134.4059 17357.68946\n## degreePhD    -663.2482  3440.47485\n## rankAssoc    2985.4107  7599.31080\n## rankProf     8396.1546 13841.37340\n## sexFemale    -697.8183  3030.56452\n## year          285.1433   667.47476\n## ysdeg        -280.6397    31.49105\n\n\nWe can see the results of our hypothesis test in (a) are confirmed by the confidence interval for sexFemale which contains 0.\n\n\n\ndegreePhD: 1388.61, p-value 0.180\n\ndegree is a categorical variable with 2 levels: Masters and PhD. Our coefficient of 1388.61 suggests that with all other variables held constant, the model finds the difference in mean salary between PhD recipients and Masters recipients at the university to be $1388.61, with faculty holding a PhD earning more.\nThe p-value of 0.180 indicates that the variable is not significant at the alpha= .05 level, we cannot conclude that degree level is useful in predicting salary at the university (H0: B slope degreePhD = 0). There is not a significant difference between salary of Masters and Phd faculty.\n\nrankAssoc: 5292.36, p-value .00003\n\nrank is a categorical variable with 3 levels: Asst,Assoc, Prof, representing the rank of a faculty member. Asst is the base category in this model, so our coefficient of 5292.36 suggests that with all other variables held constant, there is a $5,292.36 difference in mean salary between Assistant and Associate professors on campus, with associate professors earning more.\nthe p-value is far below our default alpha=0.05. We can reject the null hypothesis that the effect is 0, and conclude that there is a difference between the salaries of the groups Asst and Assoc, the variable is significant in predicting salary.\n\nrankProf: 11118.76, p-value: < 0.005\n\nA coefficient of 11118.76 suggests that with all other variables held constant, there is a $11,118.76 difference in mean salary between Assistant and full Professors on campus, with full Professors earning more.\nthe p-value is far below our default alpha=0.05. We can reject the null hypothesis that the effect is 0, and conclude that there is a difference between the salaries of the groups Asst and Prof, the variable is significant in predicting salary.\n\nsexFemale: 1166.37, p-value 0.214\n\nsex is a categorical varible with two levels: Male and Female. A coefficient of 1166.37 indicated that with all other variables held constant, the model observed a $1,166.37 difference in mean salary between males and females, with females earning more.\nthe p-value of 0.214 is greater than our alpha value, indicating this variable is not useful in predicting salary of faculty members. We cannot conclude there is a significant difference between earnings of different sex levels.\n\nyear: 476.31, p-value < .0005\n\nyear is a continuous variable describing the number of years a faculty member has spent at their current rank. A coefficient of 476.31 suggests that with all other variables held constant, a 1 year increase in experience at a particular rank will result in a $476.31 increase in predicted salary.\nthe p-value is far below our default alpha=0.05. We can reject the null hypothesis that the effect is 0, and conclude the variable is significant in predicting salary.\n\nysdeg: -124.57, p-value 0.115\n\nysdeg is a continuous variable describing the number of years since a faculty member earned their highest degree. A coefficient of -124.57 suggests that with all other variables held constant, a 1 year increase in years since highest degree earned will result in a $124.57 decrease in predicted salary.\nthe p-value of 0.115 is greater than our alpha value, indicating this variable is not useful in predicting salary of faculty members. We cannot conclude there is a significant relationship between salary and ysdeg.\n\n\n\n\n\n\nCode\nrank_redo<- salary %>%\n              mutate(rank=relevel(rank, ref = 'Prof'))\n \n# refit model\nsalary_2 <- lm(salary ~.,\n               data = rank_redo)\nsummary(salary_2)\n## \n## Call:\n## lm(formula = salary ~ ., data = rank_redo)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4045.2 -1094.7  -361.5   813.2  9193.1 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  26864.81    1375.29  19.534  < 2e-16 ***\n## degreePhD     1388.61    1018.75   1.363    0.180    \n## rankAsst    -11118.76    1351.77  -8.225 1.62e-10 ***\n## rankAssoc    -5826.40    1012.93  -5.752 7.28e-07 ***\n## sexFemale     1166.37     925.57   1.260    0.214    \n## year           476.31      94.91   5.018 8.65e-06 ***\n## ysdeg         -124.57      77.49  -1.608    0.115    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2398 on 45 degrees of freedom\n## Multiple R-squared:  0.855,  Adjusted R-squared:  0.8357 \n## F-statistic: 44.24 on 6 and 45 DF,  p-value: < 2.2e-16\n\n\n\nWhile the difference in predicted mean salary remains the same for the Assist group, after changing the reference level of the variable from Assistant to Professor, the relationship between full professors and associate professors has changed. Associate professors are seen to make $5,826.40 less than full professors, a 534 dollar increase (in value) from the model where Assistants are used the point of reference.\n\n\n\n\n\n\nCode\nsalary_3 <- lm(salary ~ . - rank,\n               data = salary)\nsummary(salary_3)\n## \n## Call:\n## lm(formula = salary ~ . - rank, data = salary)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8146.9 -2186.9  -491.5  2279.1 11186.6 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 17183.57    1147.94  14.969  < 2e-16 ***\n## degreePhD   -3299.35    1302.52  -2.533 0.014704 *  \n## sexFemale   -1286.54    1313.09  -0.980 0.332209    \n## year          351.97     142.48   2.470 0.017185 *  \n## ysdeg         339.40      80.62   4.210 0.000114 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3744 on 47 degrees of freedom\n## Multiple R-squared:  0.6312, Adjusted R-squared:  0.5998 \n## F-statistic: 20.11 on 4 and 47 DF,  p-value: 1.048e-09\n\n\n\nWhile sex is still insignificant in the model for predicting salary, we see the value of the coefficient change sign, along with ysdeg.\nTwo variables are now considered significant that were previously in predicting salary when rank was included in the model, ysdeg and degree.\n\n\n\n\n\n\nCode\ndean_edit <- salary %>%\n              mutate(dean = \n                       case_when(`ysdeg` > 15 ~ 'Old',\n                                 `ysdeg` <= 15 ~ 'New'))\nsalary_4 <- lm(salary ~ . - rank - ysdeg,\n               data = dean_edit)\nsummary(salary_4)\n## \n## Call:\n## lm(formula = salary ~ . - rank - ysdeg, data = dean_edit)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10740.1  -2550.1     -3.3   1942.4  11718.3 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  18148.9     1188.2  15.274  < 2e-16 ***\n## degreePhD    -1186.6     1191.2  -0.996 0.324267    \n## sexFemale     -523.5     1355.1  -0.386 0.701017    \n## year           531.4      130.2   4.082 0.000172 ***\n## deanOld       4449.8     1347.2   3.303 0.001834 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3958 on 47 degrees of freedom\n## Multiple R-squared:  0.5878, Adjusted R-squared:  0.5527 \n## F-statistic: 16.75 on 4 and 47 DF,  p-value: 1.338e-08\n\n\n\nI removed rank and ysdeg as both could exhibit multicollinearity with the hiring of the new Dean. Knowing the dean appointed newer graduates upon their appointment would allow ysdeg to predict dean for those with under 15 years of their degree; also one can progress through ranks within 15 years so this could correlate with dean also.\nWith a coefficient for deanOld of around 4500, the model actually predicts that those hired by the Old dean have a higher mean salary as a group compared to those hired by the New dean, contrary to the suggested hypothesis.\n\n\n\n\n\n\n\n\nCode\ndata(\"house.selling.price\")\nhouse <- house.selling.price\n\nhouse_1 <- lm(Price ~ Size+New, data=house)\nsummary(house_1)\n## \n## Call:\n## lm(formula = Price ~ Size + New, data = house)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -205102  -34374   -5778   18929  163866 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -40230.867  14696.140  -2.738  0.00737 ** \n## Size           116.132      8.795  13.204  < 2e-16 ***\n## New          57736.283  18653.041   3.095  0.00257 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 53880 on 97 degrees of freedom\n## Multiple R-squared:  0.7226, Adjusted R-squared:  0.7169 \n## F-statistic: 126.3 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nSize: 116.132, p-value < .005\n\nSize is a continuous variable with a positive coefficient of 116.132 suggesting that for every one sqft increase, predicted Price will increase by $116.13, New held constant.\n\nNew: 57736.28, p-value < .005\n\nNew is a categorical variable with two levels, new=1 and old=0. The coefficient of 57736.28 suggests that as a group, our model predicts new houses to have a mean selling price that is $57,736.28 greater than old houses of the same size.\n\nBoth variables are significant at alpha=0.05, indicating that they are useful in predicting price for our data, their effect has a magnitude different from zero.\n\n\n\nThe full prediction equation follows the form \\(Price = -40230.867 + 116.132*Size + 57736.28*New\\)\nInterpretation of variables, coefficients, and p-values are in (a).\nNew Homes: \\(Price = -40230.867 + 116.132*Size + 57736.28\\)\nOld Homes: \\(Price = -40230.867 + 116.132*Size\\)\n\n\n\n\n\nCode\nnew_3000 <- data.frame(`Size`=3000,`New`=1)\nnew_pred <- predict(house_1, newdata=new_3000)\nprint(c('New 3000 sqft House:',new_pred))\n##                                             1 \n## \"New 3000 sqft House:\"     \"365900.183656625\"\n\nold_3000 <- data.frame(`Size`=3000,`New`=0)\nold_pred <- predict(house_1, newdata=old_3000)\nprint(c('Old 3000 sqft House:',old_pred))\n##                                             1 \n## \"Old 3000 sqft House:\"     \"308163.900855831\"\n\n\n\n\n\n\n\nCode\nhouse_2 <- lm(Price ~ Size+New+ Size*New, data=house)\nsummary(house_2)\n## \n## Call:\n## lm(formula = Price ~ Size + New + Size * New, data = house)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -175748  -28979   -6260   14693  192519 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -22227.808  15521.110  -1.432  0.15536    \n## Size           104.438      9.424  11.082  < 2e-16 ***\n## New         -78527.502  51007.642  -1.540  0.12697    \n## Size:New        61.916     21.686   2.855  0.00527 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 52000 on 96 degrees of freedom\n## Multiple R-squared:  0.7443, Adjusted R-squared:  0.7363 \n## F-statistic: 93.15 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\n\nThe full prediction equation follows the form \\(Price = -22227.808 + 104.438*Size -78527.502*New + 61.916*Size:New\\)\nInterpretation of variables, coefficients, and p-values are in (a).\nNew Homes: \\(Price = -22227.808 + 104.438*Size -78527.502 + 61.916*Size\\)\nOld Homes: \\(Price = -22227.808 + 104.438*Size\\)\n\n\n\n\n\nCode\nnew_3000_int <- data.frame(`Size`=3000,`New`=1)\nnew_pred_int <- predict(house_2, newdata=new_3000_int)\nprint(c('New 3000 sqft House:',new_pred_int))\n##                                             1 \n## \"New 3000 sqft House:\"     \"398307.512638058\"\n\nold_3000_int <- data.frame(`Size`=3000,`New`=0)\nold_pred_int <- predict(house_2, newdata=old_3000_int)\nprint(c('Old 3000 sqft House:',old_pred_int))\n##                                             1 \n## \"Old 3000 sqft House:\"     \"291087.363770394\"\n\n\n\n\n\n\n\nCode\nnew_1500_int <- data.frame(`Size`=1500,`New`=1)\nnew_pred_int2 <- predict(house_2, newdata=new_1500_int)\nprint(c('New 1500 sqft House:',new_pred_int2))\n##                                             1 \n## \"New 1500 sqft House:\"     \"148776.101180188\"\n\nold_1500_int <- data.frame(`Size`=1500,`New`=0)\nold_pred_int2 <- predict(house_2, newdata=old_1500_int)\nprint(c('Old 1500 sqft House:',old_pred_int2))\n##                                             1 \n## \"Old 1500 sqft House:\"     \"134429.777919781\"\n\n\n\nThe difference in price between old and new homes increases with an increase in size. The impact of New has a different magnitude at different sizes.\n\n\n\n\n\nI would select the model with an interaction term has it has a larger Adjusted R-sq value as well as a smaller Residual Standard Error value, indicating a slighly better fit than the model without interactions.\n\nEND TABS"
  },
  {
    "objectID": "posts/ShoshanaBuck- HW1.html",
    "href": "posts/ShoshanaBuck- HW1.html",
    "title": "ShoshanaBuck-HW1",
    "section": "",
    "text": "Question 1\nUse the LungCapData to answer the following questions.\n\n\nCode\n# read in data\nlung_cap<- read_xls(\"_data/LungCapData.xls\")\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\na. What does the distribution of LungCap look like?\n\n\nCode\n#plot histogram\nhist(lung_cap$LungCap)\n\n\n\n\n\nThe histogram shows that the distribution is pretty close to a normal distribution, with an almost a bell shaped curve. Meaning that the data near the mean are more of a frequent occurrence which is true because there are fewer observations near the margins.\n\n\nb. compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\n#Box plot\nggplot(lung_cap, aes (Gender,LungCap)) + geom_boxplot()\n\n\n\n\n\nThe box plot shows that male’s have a slightly higher lung capacity than females. Female’s have more values in the first quartile range and a lower minimum value than male’s. On the other hand male’s have a higher max value and more values in the 3rd quartile range.\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlung_cap %>% \n  group_by(Smoke) %>% \n  summarise(avg_lung_cap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke avg_lung_cap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nThis does not make sense. Smokers have a higher sample mean than non-smokers which intuitively does not make sense because we would assume non-smokers would have a higher lung capacity.\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#categorical variable of age_groups \ndf<- lung_cap %>% \ngroup_by(Smoke,LungCap) %>% \n  summarise(age_group = case_when(Age<=13 ~ \"13 & under\",Age ==14 | Age == 15 ~ \"14 to 15\",Age== 16 | Age == 17 ~ \"16 to 17\", Age>=18 ~ \"18 & older\")) \n\n\n`summarise()` has grouped output by 'Smoke', 'LungCap'. You can override using\nthe `.groups` argument.\n\n\nCode\n#mean of lung capacity with new variable\ndf %>% \n  group_by(Smoke, age_group) %>% \n  summarise(avg_lung_cap = mean(LungCap)) %>% \n  arrange(desc(avg_lung_cap))\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke age_group  avg_lung_cap\n  <chr> <chr>             <dbl>\n1 no    18 & older        11.1 \n2 yes   18 & older        10.5 \n3 no    16 to 17          10.5 \n4 yes   16 to 17           9.38\n5 no    14 to 15           9.14\n6 yes   14 to 15           8.39\n7 yes   13 & under         7.20\n8 no    13 & under         6.36\n\n\nCode\n#histogram\nggplot(df, aes(x = LungCap)) + facet_grid(Smoke ~age_group) +geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nUsing the package ggplot I used the function facet_grids to show a good visualization of the lung capacity between non-smokers and smokers within each age group. Looking at the histograms all age_groups that are non-smokers have a higher sample mean proving that non-smokers have\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\n# Mean of non-smokers 13 & younger\ndf %>% \n  filter(Smoke == 'no' & age_group == '13 & under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n# Mean of smokers 13 & younger\ndf %>% \n  filter(Smoke == 'yes' & age_group == '13 &under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] NaN\n\n\nCode\n#Mean of non-smokers 14 to 15\ndf %>% \n  filter(Smoke == 'no' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.13881\n\n\nCode\n#Mean of smokers 14 to 15\ndf %>% \n  filter(Smoke == 'yes' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.391667\n\n\nCode\n#Mean of non-smokers 16 to 17\ndf %>% \n  filter(Smoke == 'no' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.46981\n\n\nCode\n#Mean of smokers 16 to 17\ndf %>% \n  filter(Smoke == 'yes' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.38375\n\n\nCode\n#Mean of non-smokers 18 & older\ndf %>% \n  filter(Smoke == 'no' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n# Mean of smokers 18 & older\ndf %>% \n  filter(Smoke == 'yes' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\n\nf. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\n#Correlation\ncor(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#Covariance\ncov(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is 0.81 which is pretty close to +1, meaning that the there is a positive relationship between lung capacity and age. The covariance is also high which shows that the two variables of lung capacity and age have a positive relationship. #2 Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n# x= prior convictions \nx<-c(0, 1, 2, 3, 4)\nfrequency<-c(128, 434, 160, 64, 24)\nstate_prison<- data_frame(x,frequency)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nstate_prison\n\n\n# A tibble: 5 × 2\n      x frequency\n  <dbl>     <dbl>\n1     0       128\n2     1       434\n3     2       160\n4     3        64\n5     4        24\n\n\n\n\na. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n# 2 prior convictions.P(2)/total\n160/810\n\n\n[1] 0.1975309\n\n\nThere is a 1.9% probability that a randomly selected inmate has exactly 2 prior convictions.\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#  less than 2 prior convictions. (P(0) + P(1))/total \n(128+434)/810\n\n\n[1] 0.6938272\n\n\nThere is a 6.9% probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n# 2 or fewer prior convictions. (P(0) + P(1) + P(2)) +total\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nThere is a 8.9% probability that a randomly selected inmate has 2 or fewer prior convictions.\n\n\nd. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n# More than 2 prior convictions. (P(3) +P(4)) + total\n(64+24)/810\n\n\n[1] 0.108642\n\n\nThere is a 10.8% probability that a randomly selected inmate has more than 2 prior convictions.\n\n\ne. What is the expected value for the number of prior convictions?\n\n\nCode\n#Prior convictions. ((0*P(0)) +(1*(P(1)) + (2*P(2)) + (3*P(3)) + (4*P(4)))\ndf<-((128*0/810) +(434*1/810) +(160*2/810) +(64*3/810) +(24*4/810)) \nmean(df)\n\n\n[1] 1.28642\n\n\nThe expected value for number of prior convictions is 1.28\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n# variance\nv<- ((0-1.28)^2) *(128/810) +((1-1.28)^2) * (434/810)+((2-1.28)^2) * (160/810)+((3-1.28)^2) *(64/810) +((4-1.28)^2) * (24/810)\nv\n\n\n[1] 0.8562765\n\n\nCode\n# standard deviation\nsd<- sqrt(v)\nsd\n\n\n[1] 0.9253521\n\n\nThe variance is 0.856 and the standard deviation is 0.925."
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html",
    "href": "posts/StephRoberts_finalpart2.html",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(Hmisc)\nlibrary(pscl)\n\n\nError in library(pscl): there is no package called 'pscl'\n\n\nCode\nlibrary(magrittr)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#research-questions",
    "href": "posts/StephRoberts_finalpart2.html#research-questions",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhat risk factors are most predictive of diabetes?\n\nResearch on Diabetes is ongoing and in-depth within the medical field. The prevalence and incidence of diabetes mellitus type 2 (DQ2) have increased consistently for decades, giving way to an increase in mortality related to diabetes. Commonly in the medical field, many risk factors are used to measure a patient’s risk of developing DM2, such as obesity, family history, hypertension and changes in fasting blood sugar levels. Moreno et al. (2018) studied risk parameters for diabetes and concluded “risk of being diabetic rises in patients whose father has suffered an acute myocardial infarction, in those whose mother or father is diabetic and in patients with a high waist perimeter.” Their focus on family history leaves room for studies more focused on individual medical factors, such as blood pressure, BMI, number of pregnancies, etc. That is the aim of this project.\nM. L. M. V. J. A. (2018, June 11). Predictive risk model for the diagnosis of diabetes mellitus type 2 in a follow-up study 15 Years on: Prodi2 study. European journal of public health. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/29897477/\n\nCan the use of regression analysis help predict risk of diabetes based on several medical variables?\n\nOther research, such as a Edlitz & Segal (2022) study titled Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards, does focus on using individual medical factors to predict risk of diabetes through regression. This project aims to conduct similar analysis on different data.\nE. Y. S. E. (2022, June 22). Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards. eLife. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/35731045/"
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#hypothesis",
    "href": "posts/StephRoberts_finalpart2.html#hypothesis",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Hypothesis",
    "text": "Hypothesis\nBody mass index (BMI), glucose, and age are positive predictors of diabetes mellitus type 2.\nBoth hypothesis have been tested in the above mentioned studies. The contribution from this project will be the additional support for or against the hypotheses from the analysis of different data."
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#descriptive-statistics",
    "href": "posts/StephRoberts_finalpart2.html#descriptive-statistics",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe data was collected by the “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database (PIDD). A total of 768 cases are available in PIDD. However, 5 patients had a glucose of 0, 11 patients had a body mass index of 0, 28 others had a diastolic blood pressure of 0, 192 others had skin fold thickness readings of 0, and 140 others had serum insulin levels of 0. After cleaning the data by removing the cases with numbers that are incompatible with life, 392 cases remained. All patients belong to the Pima Indian heritage (subgroup of Native Americans), and are females aged 21 years and above.\nThe datasets consists of 9 medical predictor (independent) variables and one target (dependent) variable, outcome.\nPregnancies: Number of times a woman has been pregnant Glucose: Plasma Glucose concentration of 2 hours in an oral glucose tolerance test BloodPressure: Diastolic Blood Pressure (mm hg) SkinThickness: Triceps skin fold thickness(mm) Insulin: 2 hour serum insulin(mu U/ml) BMI: Body Mass Index ((weight in kg/height in m)^2) Age: Age(years) DiabetesPedigreeFunction: scores likelihood of diabetes based on family history Outcome: 0(doesn’t have diabetes) or 1 (has diabetes)\n\n\nCode\ndf<- read_csv(\"diabetes2.csv\")\n\n\nError: 'diabetes2.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/603_Fall_2022/posts').\n\n\nCode\ndim(df)\n\n\nNULL\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\nCode\nhead(df)\n\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\n\n\nCode\n#check for null entries\nis.null(df)\n\n\n[1] FALSE\n\n\n\n\nCode\n#Check number of 0s in each column\ncolSums(df==0)\n\n\nError in df == 0: comparison (1) is possible only for atomic and list types\n\n\nGlucose, blood pressure, skin thickness, insulin, BMI and Age are not variables that should logically have 0s. Those values, if true, are likely incompatible with life. To reduce the amount of data we exclude, we will only remove observations that have 0s for our explanatory variables. We will remove those cases from analysis.\n\n\nCode\n#Remove rows with 0 in respective columns\ndf <- df[apply(df[c(2,6,8)],1,function(z) !any(z==0)),] \n\n\nError in df[c(2, 6, 8)]: object of type 'closure' is not subsettable\n\n\nCode\n#Verify 0s are gone in selected rows\ncolSums(df==0)\n\n\nError in df == 0: comparison (1) is possible only for atomic and list types\n\n\n\n\nCode\n#Check cleaned data frame\nglimpse(df)\n\n\nfunction (x, df1, df2, ncp, log = FALSE)  \n\n\n\n\nCode\n#Summarize df\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\nAt a glance, this summary is more fitting after having cleaned our data. An average of 3 pregnancies, considering our 21+ female population, makes sense. A mean glucose of 122, blood pressure of 70.66, a BMI of 33, and age of 30.86 are reasonably accurate of our population. The data is clean and ready for analysis.\n\n\nCode\n#Rename columns\ncolnames(df) <- c(\"pregnancies\", \"glucose\", \"bp\", \"skin_thickness\", \"insulin\", \"bmi\", \"dpf\", \"age\", \"diabetes\")\n\n\nError in `colnames<-`(`*tmp*`, value = c(\"pregnancies\", \"glucose\", \"bp\", : attempt to set 'colnames' on an object with less than two dimensions\n\n\nCode\ndim(df)\n\n\nNULL\n\n\nWe have 752 observations and 9 variables with which to work.\nLet’s get familiar with the data set by observing the scatterplot matrix and histograms for each variable.\n\n\nCode\n#Scatterplot matrix\npairs(df)\n\n\nError in as.data.frame.default(x): cannot coerce class '\"function\"' to a data.frame\n\n\nCode\n#Histograms of each variable\nhist.data.frame(df)\n\n\nError in for (v in x) {: invalid for() loop sequence\n\n\nThe scatterplot matix shows that our variables generally lack linear relationships. Logistics regression may be best for analyzing this type of data.\nThe histograms show us the distribution of all our variables. Pregnancies is right skewed with more women having a smaller number of pregnancies and a small number having over 10. Glucose appears to follow a close-to-normal distribution with the mean just over 100. BMI follows a similar pattern with a central value of just over 30. Age is right skewed with the majority of the sample appearing to be in their 20s and slower tapering off to ages in their 50s. This is a good place to start. The only variable we cannot view in a histogram is the outcome variable due to its binary nature as a dummy variable.\n\n\nCode\nbgap <- as.data.frame(df[, c(1,2,6,8,9)])\n\n\nError in df[, c(1, 2, 6, 8, 9)]: object of type 'closure' is not subsettable\n\n\nCode\npairs(bgap)\n\n\nError in pairs(bgap): object 'bgap' not found\n\n\n\n\nCode\n#Plot outcome variable\nggplot(df, aes(x=factor(diabetes))) +\n  geom_bar(fill = \"#0073C2FF\")+\n  scale_x_discrete(labels=c('No Diabetes','Diabetes'))+\n  xlab(\"Outcome\")\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nAbout a third of our sample have diabetes."
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#hypothesis-testing",
    "href": "posts/StephRoberts_finalpart2.html#hypothesis-testing",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\nCode\n#Create training and test data\nset.seed(1)\nbgap %>% \n  nrow() %>% \n  multiply_by(0.7) %>% \n  round() -> training_set_size\n\n\nError in nrow(.): object 'bgap' not found\n\n\nCode\ntrain_indices <- sample(1:nrow(df), training_set_size)\n\n\nError in 1:nrow(df): argument of length 0\n\n\nCode\ntrain <- bgap[train_indices,]\n\n\nError in eval(expr, envir, enclos): object 'bgap' not found\n\n\nCode\ntest <- bgap[-train_indices,]\n\n\nError in eval(expr, envir, enclos): object 'bgap' not found\n\n\nCode\nnrow(train)\n\n\nError in nrow(train): object 'train' not found\n\n\nCode\nnrow(test)\n\n\nError in nrow(test): object 'test' not found\n\n\nCode\nhead(train)\n\n\nError in head(train): object 'train' not found\n\n\nAs a reminder, pregnancies, age, and BMI are all right skewed. So, we will use the logarithm of those variables in our regressions.\n\n\nCode\n#Model using all independent variables\nall_var <- glm(diabetes ~ ., data = train, family = binomial)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\n#Model using BMI, glucose, and age\nbga <- glm(diabetes ~ . -pregnancies, data = train, family = binomial)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\n#Model using BMI and glucose\nbg <- glm(diabetes ~ .-age -pregnancies, data = train, family = binomial)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\n#Model using BMI, glucose, age, and an interaction between pregnancies and age.\nia <- glm(diabetes ~ glucose + bmi + age*pregnancies, data = train, family = binomial)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\n#Model with BMI, glucose, and pregnancies (without age)\nbgp <- glm(diabetes ~ . -age, data = train, family = binomial)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nOur hypothesis involves the BMI, glucose, and age variables. First we can take a look at them individually.\n\n\nCode\n#Generalized linear model can be best for predicting categorical outcome\n#Regression model on BMI\nmbmi <- glm(diabetes ~ bmi, data = train)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\nsummary(mbmi)\n\n\nError in summary(mbmi): object 'mbmi' not found\n\n\nCode\nggplot(data = train, aes(x =bmi, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\nError in ggplot(data = train, aes(x = bmi, y = diabetes)): object 'train' not found\n\n\nInterpretation: The residuals are relatively small. The coefficient estimate suggests that for every 1 unit increase of BMI, the chance of diabetes increases 0.019. The standard errors are small. Our t-values are large compared to our standard error and relatively far from 0. All that, combined with a very small p-value of < 5.15e-11, indicate we can reject the null hypothesis and conclude a relationship between BMI and diabetes.\n\n\nCode\n#Regression model on glucose \nmgluc <- glm(diabetes ~ glucose, data = train)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\nsummary(mgluc)\n\n\nError in summary(mgluc): object 'mgluc' not found\n\n\nCode\nggplot(data = train, aes(x = glucose, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\nError in ggplot(data = train, aes(x = glucose, y = diabetes)): object 'train' not found\n\n\nInterpretation: Interestingly, the values of this regression mirror that of the BMI regression. With a very small p-value, < 2e-16, we can see that the plasma glucose concentrations of the 2 hour oral glucose tolerance test (GGT) are related to our outcome variable, diabetes. This makes sense as GGT is sometimes used as a diagnostic tool for diabetes.\nThe similarities is regression model values between BMI and glucose sparked a curiosity. Let’s see if BMI and glucose are specifically correlated to one another.\n\n\nCode\ncor.test(train$bmi, train$glucose, method = \"pearson\")\n\n\nError in cor.test(train$bmi, train$glucose, method = \"pearson\"): object 'train' not found\n\n\nThis shows a very small p-value of 8.477e-09, which is well under the significance level alpha = 0.5. However, the correlation coefficient suggests a weak positive correlation. This suggests the variables are only weakly related.\nLet’s continue with our regression model hypothesis testing of individual variables.\n\n\nCode\n#Regression model on age\nmage <- glm(diabetes ~ age, data = train)\n\n\nError in is.data.frame(data): object 'train' not found\n\n\nCode\nsummary(mage)\n\n\nError in summary(mage): object 'mage' not found\n\n\nCode\nggplot(data = train, aes(x = age, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\nError in ggplot(data = train, aes(x = age, y = diabetes)): object 'train' not found\n\n\nInterpretation: Similar to the previous variable, age also has small residuals and standard error. It shows a p-value of 1.12e-06, indicating a relationship between age and diabetes.\nThe hypothesis that BMI, glucose tolerance results, and age are positively associated with diabetes is true. Next, we can investigate models that could help predict the outcome using these variables."
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#model-comparisons",
    "href": "posts/StephRoberts_finalpart2.html#model-comparisons",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\n\nCode\n#Summarize model including BMI, glucose, age, and pregnancies\nsummary(all_var)\n\n\nError in summary(all_var): object 'all_var' not found\n\n\nWe can see with the all variable model that collectively, independent variables including pregnancies, glucose, bmi, dpf and even blood pressure have low p-values. That suggests those variables have a relationship with the outcome variable. We can use AIC as an indicator of model fitness, with this model being 517.97. Let’s compare this with the following models.\n\n\nCode\n#Summarize BMI, glucose, and age model\nsummary(bga)\n\n\nError in summary(bga): object 'bga' not found\n\n\nCode\nggplot(train, aes(x=age, y=bmi, color=glucose)) + geom_point() + facet_grid(~diabetes)\n\n\nError in ggplot(train, aes(x = age, y = bmi, color = glucose)): object 'train' not found\n\n\nThe model looking at BMI, age, and glucose as explanatory variables for diabetes has an AIC of 526.6, which is a worse fit than the all variables model.\n\n\nCode\n#Summarize BMI and glucose model\nsummary(bg)\n\n\nError in summary(bg): object 'bg' not found\n\n\nCode\nggplot(train, aes(x=glucose, y=bmi, color=diabetes)) + geom_point() + facet_grid(~diabetes)\n\n\nError in ggplot(train, aes(x = glucose, y = bmi, color = diabetes)): object 'train' not found\n\n\nThis BMI and glucose only model has a worse (higher) AIC, with 536.8, than the 2 previous models. So far, the all variable model appears the best fit.\n\n\nCode\n#Summarize model including interaction between age and pregnancies \n#This was chosen to see if the impact of age on diabetes is different depending on the number of pregnancies\nsummary(ia)\n\n\nError in summary(ia): object 'ia' not found\n\n\nCode\nggplot(train, aes(x=age, y=bmi, color=pregnancies)) + geom_point() + facet_grid(~diabetes)\n\n\nError in ggplot(train, aes(x = age, y = bmi, color = pregnancies)): object 'train' not found\n\n\nWith an AIC of 510.41, this model appears to be the best fit model of those we have compared. This indicated there is an interaction between age and pregnancies that has a relationship with the outcome variable.\n\n\nCode\n#Summarize model including without age\nsummary(bgp)\n\n\nError in summary(bgp): object 'bgp' not found\n\n\nCode\nggplot(train, aes(x=pregnancies, y=bmi, color=glucose)) + geom_point() + facet_grid(~diabetes)\n\n\nError in ggplot(train, aes(x = pregnancies, y = bmi, color = glucose)): object 'train' not found\n\n\nRemoving age entirely increased the AIC of this model. So, the interaction model seems to be the best fit. That is, the model including BMI and glucose and controlling for an interaction between age and pregnancy has the best fit of all models tested.\nWe can use ANOVA to analyze the table of deviance of our regression model.\n\n\nCode\n#ANOVA test on interaction model\nanova(ia, test = \"Chisq\")\n\n\nError in anova(ia, test = \"Chisq\"): object 'ia' not found\n\n\nAll of our variables have small p-values and appear good predictors of the diabetes outcome, with the exception of age. However, age is needed for the interaction variable.\nLet’s check one more thing to ensure we have a good fit.\n\n\nCode\n#Check pseudo R2 for interaction model\npR2(ia)\n\n\nError in pR2(ia): could not find function \"pR2\"\n\n\nWith a McFadden R-squared of 0.27, this model can be considered an excellent fit."
  },
  {
    "objectID": "posts/StephRoberts_finalpart2.html#diagnostics",
    "href": "posts/StephRoberts_finalpart2.html#diagnostics",
    "title": "Final Project: Diabetes Prediction - Part 2",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\nCode\n#Apply model to test data\nfitted.results <- predict(ia ,test,type='response')\n\n\nError in predict(ia, test, type = \"response\"): object 'ia' not found\n\n\nCode\nfitted.results <- ifelse(fitted.results > 0.5,1,0)\n\n\nError in ifelse(fitted.results > 0.5, 1, 0): object 'fitted.results' not found\n\n\nCode\npred <- mean(fitted.results != test$diabetes)\n\n\nError in mean(fitted.results != test$diabetes): object 'fitted.results' not found\n\n\nCode\noutput <- cbind(test, fitted.results)\n\n\nError in cbind(test, fitted.results): object 'test' not found\n\n\nCode\nprint(paste('Accuracy',1-pred))\n\n\nError in paste(\"Accuracy\", 1 - pred): object 'pred' not found\n\n\nCode\nhead(output)\n\n\nError in head(output): object 'output' not found\n\n\nTesting the interaction model on the test data produces the correct outcome 79% of the time.\n\n\nCode\n#Diagnostic plot of model\nplot(ia)\n\n\nError in plot(ia): object 'ia' not found\n\n\nResiduals vs Fitted: The residuals have 2 distinct patterns of behavior. They start at close to 0 and taper off to a negative slope. Another group of residuals starts large and tapers on a negative slope toward 0.\nNormal Q-Q: The Q-Q plot, which indicates level of asymmetry, tells us our data is not a normal distribution. It appears to have a right tail (see the points drifting from the line at the top), which aligns with our individual variable histogram interpretations. It follows that a model combining variables with right skews may also be skewed to the right. However, it also has values away from the line on the bottom left, away from the intercept. This suggests our model also includes a left tail. This distribution seems to have “fat tails.”\nScale-Location: This plots the fitted values of the model along the x-axis and the square root of the standardized residuals along the y-axis. The red line does roughly travel horizontal across the plot. However, there is a clear X pattern among residuals. This suggests our model has heteroscedasticity, which is not ideal.\nResiduals vs Leverage: The points on this graph all fall within Cook’s distance, indicating there are not any influential points in our regression model.\n\nPlan to work on fixing the heteroscedasticity by log transforming BMI, pregnancies, and age when those variables are included in models because they are right skewed data. However, it is taking time figuring out how to do a multiple logistics regression without errors.."
  }
]