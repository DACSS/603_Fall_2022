[
  {
    "objectID": "posts/shelton_HW2.html",
    "href": "posts/shelton_HW2.html",
    "title": "Homework 2 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(warning= FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/shelton_HW2.html#homework-2",
    "href": "posts/shelton_HW2.html#homework-2",
    "title": "Homework 2 Solution",
    "section": "Homework 2",
    "text": "Homework 2\n\nQ1Q2Q3Q4Q5Q6\n\n\n\n90% Confidence Intervals\n\n\nCode\n# Bypass\nn_bp <- 539\nmean_bp <- 19\nsd_bp <- 10\nt_90 <- qt(.05, (n_bp-1), lower.tail=F)\n\n#CI\nupper_bp <- mean_bp + ((sd_bp/sqrt(539))*t_90)\nlower_bp <- mean_bp - ((sd_bp/sqrt(539))*t_90)\n\nci90_bp <- c(lower_bp,upper_bp)\nprint(c(\"90% CI For Mean Bypass Wait\", ci90_bp))\n\n\n[1] \"90% CI For Mean Bypass Wait\" \"18.2902893200424\"           \n[3] \"19.7097106799576\"           \n\n\nCode\n# Angiography\nn_ag <- 847\nmean_ag <- 18\nsd_ag <- 9\nt_90 <- qt(.05, (n_ag-1), lower.tail=F)\n\n#CI\nupper_ag <- mean_ag + (sd_ag/sqrt(539)*t_90)\nlower_ag <- mean_ag - (sd_ag/sqrt(539)*t_90)\n\nci90_ag <- c(lower_ag,upper_ag)\n\nprint(c(\"90% CI For Angiography Wait\", ci90_ag))\n\n\n[1] \"90% CI For Angiography Wait\" \"17.3616612514732\"           \n[3] \"18.6383387485268\"           \n\n\nCode\nprint(c(\"Width Bypass\", upper_bp-lower_bp))\n\n\n[1] \"Width Bypass\"     \"1.41942135991513\"\n\n\nCode\nprint(c(\"Width Angiography\", upper_ag-lower_ag))\n\n\n[1] \"Width Angiography\" \"1.27667749705367\" \n\n\nThe 90% Confidence interval is narrower for the mean Angiography wait time (days) than mean Bypass wait due to the larger sample and smaller standard deviation.\n\n\n\n\nOne Prop Confidence Interval\n\n\nCode\n# College 95% CI\nprop.test(567,1031,conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the true proportion of Americans who believe a college education is essential for success is (0.52,0.58). 95% confidence is not a comment on the proportion itself, rather our method. If we took several samples and created a confidence interval for proportion p, 95% of the intervals would contain the true population proportion.\nBecause our confidence interval does not include .5, we can conclude that at .05 significance, the majority (>0.5) of Americans believe that a college educcation is essential for success.\n\n\n\n\nMargin of Error Calculation\n\n\nCode\n# Margin of Error Calculation\n\nci_95 <- qnorm(.025, lower.tail=F)\n\n# 5 = (170*.25)/sqrt(x)*1.96\n\n(x <- ((170*.25)/5)*ci_95)^2\n\n\n[1] 277.5454\n\n\nTo estimate mean textbook cost per semester within $5 of true value at .05 significance level the financial aid office would need 278 students in their sample.\n\n\n\n\nOne Sample T-Test\n\n\n\n\n\n\na\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees != $500/wk\n\n\nCode\n# womens data\nw_xbar <- 410\nw_n <- 9\nw_sd <- 90\nw_se <- 90/sqrt(w_n)\ntest_stat <- (w_xbar-500)/w_se\ncrit_2sided <- abs(qt(.025,w_n-1))\ncrit_less <- qt(.05, w_n-1,lower.tail=T)\ncrit_greater <- qt(.95,w_n-1,lower.tail=T)\np_value <- pt(test_stat, df=w_n-1, lower.tail=T)\npval_greater <- pt(test_stat, df=w_n-1, lower.tail=F)\n\n# Two Sided 2 Test\nprint('Two-Sided T-Test')\n\n\n[1] \"Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"-3\"                                  \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"2.30600413520417\" \n\n\nCode\nprint(c('p-value', 2*p_value))\n\n\n[1] \"p-value\"            \"0.0170716812337826\"\n\n\np-value = .017; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages differ from $500/week. If female weekly income was equal to 500, we would expect 1.7% of samples to produce a sample mean of 410$ or more extreme.\n\n\n\n\n\n\n\n\nb\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is less than $500/wk\n\n\nCode\nprint('Left-Sided T-Test')\n\n\n[1] \"Left-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_less))\n\n\n[1] \"rejection-region:\" \"-1.8595480375309\" \n\n\nCode\nprint(c('p-value',p_value))\n\n\n[1] \"p-value\"             \"0.00853584061689132\"\n\n\np-value = .009; Reject the null, at alpha=.05 we have sufficient evidence to conclude female employees’ wages are less than $500/week. If mean female weekly income was equal to 500 , we would expect less than one percent of samples to produce a mean equal to or more extreme (less) than 410.\n\n\n\n\n\n\n\n\nc\n\n\n\nH0: mean income of female employees = $500/wk H1: mean income of female employees is greater than $500/wk\n\n\nCode\nprint('Right-Sided T-Test')\n\n\n[1] \"Right-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic:', test_stat))\n\n\n[1] \"test-statistic:\" \"-3\"             \n\n\nCode\nprint(c('rejection-region:',crit_greater))\n\n\n[1] \"rejection-region:\" \"1.8595480375309\"  \n\n\nCode\nprint(c('p-value',pval_greater))\n\n\n[1] \"p-value\"           \"0.991464159383109\"\n\n\np-value = .991; Fail to reject the null, at alpha=.05 we do nothave sufficient evidence to conclude female employees’ wages are greater than $500/week. If female weekly income was equal to 500, we would expect 99 percent of samples to produce a mean equal to or greater than 410.\n\n\n\n\n\n\n\n\n\n\n\na & b\n\n\n\n\n\nCode\n# jones data\nj_xbar <- 519.5\nj_n <- 1000\nj_se <- 10\nj_test_stat <- (j_xbar-500)/j_se\ncrit_2sided <- abs(qt(.025,j_n-1))\nj_p_value <- pt(j_test_stat, df=j_n-1, lower.tail=F)\n\n\n# Jones Two Sided 2 Test\nprint('Jones Two-Sided T-Test')\n\n\n[1] \"Jones Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', j_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.95\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*j_p_value))\n\n\n[1] \"p-value\"            \"0.0514555476459477\"\n\n\nCode\nprint(c('insignificant at alpha = 0.05'))\n\n\n[1] \"insignificant at alpha = 0.05\"\n\n\nCode\n# smith data\ns_xbar <- 519.7\ns_n <- 1000\ns_se <- 10\ns_test_stat <- (s_xbar-500)/s_se\ncrit_2sided <- abs(qt(.025,j_n-1))\ns_p_value <- pt(s_test_stat, df=s_n-1, lower.tail=F)\n\n\n# Smith Two Sided 2 Test\nprint('Smith Two-Sided T-Test')\n\n\n[1] \"Smith Two-Sided T-Test\"\n\n\nCode\nprint(c('test-statistic (use absolute value):', s_test_stat))\n\n\n[1] \"test-statistic (use absolute value):\"\n[2] \"1.97\"                                \n\n\nCode\nprint(c('rejection-region:', crit_2sided))\n\n\n[1] \"rejection-region:\" \"1.96234146113345\" \n\n\nCode\nprint(c('p-value', 2*s_p_value))\n\n\n[1] \"p-value\"            \"0.0491142565416521\"\n\n\nCode\nprint(c('significant at alpha = 0.05'))\n\n\n[1] \"significant at alpha = 0.05\"\n\n\n\n\n\n\n\n\n\n\nc\n\n\n\nBy not reporting the p-value, we do not understand the strength of the test - how extreme are the findings? In in an example like this, we see nearly identical results produce opposite significance results; language like “statistically significant” can get especially dangerous here to someone who is unfamiliar with basic statistical theory.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, \n               41.95, 28.61, 41.29, \n               52.19, 49.48, 35.02, \n               48.13, 39.28, 54.41, \n               41.66, 30.28, 18.49, \n               38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes; at the 95% confidence level, we have sufficient evidence to reject the null hypothesis mu=45. 45 is not included in our left sided confidence interval, favoring the alternative hypothesis that the average tax on gas in the United States in 2005 was less than 45 cents per gallon."
  },
  {
    "objectID": "posts/Quarkume HW1.html#lungcapdate",
    "href": "posts/Quarkume HW1.html#lungcapdate",
    "title": "HW1 Quat",
    "section": "LungCapDate",
    "text": "LungCapDate\n\nUse the LungCapData to answer the following questions. (Hint: Using dplyr, especially group_by() and summarize() can help you answer the following questions relatively efficiently.)\n\nInstall Libraries\n\n#install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n#install.packages(\"readxl\")\nlibrary(readxl)\n#install.packages(\"magrittr\")\nlibrary(magrittr)\n\n\nWhat does the distribution of LungCap look like?\nThe distribution of Lung Capacity in the data set looks normally distributed.\n\n\n#histogram of LungCap\nhist(LungCapData$LungCap, xlab = 'LungCap', main = '', freq = F)\n\nError in hist(LungCapData$LungCap, xlab = \"LungCap\", main = \"\", freq = F): object 'LungCapData' not found\n\n\n\nCompare the probability distribution of the LungCap with respect to Males and Females?\nLooking at the comparative boxplot males have a higher lung capacity than females.\n\n\nboxplot(LungCapData$LungCap ~ LungCapData$Gender,\n        col = c(\"#FFE0B2\", \"#FFA726\"))\n\nError in eval(predvars, data, env): object 'LungCapData' not found\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense? In comparing the means, the lung capacity for smokers is higher than for nonsmokers.\n\n#Mean Lung capacities of smokers\nLungCapData %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"yes\"): object 'LungCapData' not found\n\n#Mean Lung capacities of non-smokers\nLungCapData %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\nError in filter(., Smoke == \"no\"): object 'LungCapData' not found\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#new var for Age Groups\nLungCapData$Age_Cat <- cut(LungCapData$Age,\n                           breaks = c(0,13,15,17,25),\n                           labels = c('less than or equal to 13','14 to 15','16 to 17','greater than or equal to 18'))\n\nError in cut(LungCapData$Age, breaks = c(0, 13, 15, 17, 25), labels = c(\"less than or equal to 13\", : object 'LungCapData' not found\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age_Cat, scale=\"free\")\n\nError in ggplot(LungCapData, aes(x = Smoke, y = LungCap)): object 'LungCapData' not found\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here? We see an intervening relationship with age. Where most young children either don’t smoke ar all and have smaller lung capacities because of their size.\n\nggplot(LungCapData, aes(x=Smoke, y=LungCap)) + \n    geom_boxplot() +\n  facet_wrap(~Age, scale=\"free\")\n\n--\n  \n\nError: <text>:7:0: unexpected end of input\n5: --\n6:   \n  ^\n\n\nf.Calculate the correlation and correlation between Lung Capacity and Age. (use the cov() and cor() functions in R).\n\n#correlation\nLungCapData %>% \n  summarize(correlation = cor(LungCap, Age))\n\nError in summarize(., correlation = cor(LungCap, Age)): object 'LungCapData' not found\n\n#correlation\nLungCapData %>% \n  summarize(covariance = cov(LungCap, Age))\n\nError in summarize(., covariance = cov(LungCap, Age)): object 'LungCapData' not found"
  },
  {
    "objectID": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "href": "posts/Quarkume HW1.html#examination-of-prison-convictions",
    "title": "HW1 Quat",
    "section": "1. Examination of Prison Convictions",
    "text": "1. Examination of Prison Convictions"
  },
  {
    "objectID": "posts/Quarkume HW1.html#prisondata",
    "href": "posts/Quarkume HW1.html#prisondata",
    "title": "HW1 Quat",
    "section": "PrisonData",
    "text": "PrisonData\nData\n\nPrisonData <- tibble(\n  prior_convictions = c(0,1,2,3,4),\n  freq = c(128,434,160,64,24))\n\nPrisonData\n\n# A tibble: 5 × 2\n  prior_convictions  freq\n              <dbl> <dbl>\n1                 0   128\n2                 1   434\n3                 2   160\n4                 3    64\n5                 4    24\n\nnum <- sum (PrisonData$freq)\nnum\n\n[1] 810\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nPrisonData %>% \n  filter(prior_convictions == 2) %>% \n  pull (freq) %>% \n  divide_by (num)\n\n[1] 0.1975309\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions < 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.6938272\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions <= 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.891358\n\n\nd.What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nPrisonData %>% \n  filter(prior_convictions > 2) %>% \n  pull (freq) %>% \n  sum() %>%\n  divide_by (num)\n\n[1] 0.108642\n\n\ne. What is the expected value for the number of prior convictions?\n\nsum(prior_convictions*freq)\n\nError in eval(expr, envir, enclos): object 'prior_convictions' not found\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html",
    "href": "posts/FinalPart1_ToryBarteloni.html",
    "title": "DACSS 603: Final Part 1",
    "section": "",
    "text": "The concept of political trust has been researched in great depth for decades. That research indicates that a number of factors have at least some impact on a group’s level of trust or confidence in their government. Most of the factors studied are related to the public’s perception of government performance including control over crime, the economy, and the appearance of corruption and scandal. To this point there has been no consensus or holistic model that produces a satisfactory answer to the question why do groups trust and have confidence in their government? In this project we will try to bring us one step closer by examining a model that takes into account several factors."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#setup",
    "href": "posts/FinalPart1_ToryBarteloni.html#setup",
    "title": "DACSS 603: Final Part 1",
    "section": "Setup",
    "text": "Setup\nLoading packages and reading in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_final <- read.csv(\"_data/FinalPart1_ToryBartelloni_data.csv\")"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "href": "posts/FinalPart1_ToryBarteloni.html#summary-of-data",
    "title": "DACSS 603: Final Part 1",
    "section": "Summary of Data",
    "text": "Summary of Data\nFirst things first, I will include a brief look at the data set and then we will look at the specifics.\n\n\nCode\nstr(data_final)\n\n\n'data.frame':   91 obs. of  27 variables:\n $ X                           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Power_Distance              : int  NA NA 49 NA 38 11 NA 80 NA NA ...\n $ Individualism               : int  NA NA 46 NA 90 55 NA 20 NA NA ...\n $ Masculinity                 : int  NA NA 56 NA 61 79 NA 55 NA NA ...\n $ Uncertainty_Avoidance       : int  NA NA 86 NA 51 70 NA 60 NA NA ...\n $ Time_Perspective            : int  61 NA 20 61 21 60 61 47 81 70 ...\n $ Indulgence                  : int  15 65 62 NA 71 63 22 20 15 44 ...\n $ Country_Final               : chr  \"Albania\" \"Andorra\" \"Argentina\" \"Armenia\" ...\n $ CPI.Score.2018              : int  36 NA 40 35 77 76 25 26 44 38 ...\n $ GDP_per_Capita              : num  13653 NA 22066 13654 49309 ...\n $ Homicides_per_100K          : num  2.256 NA 5.143 2.468 0.893 ...\n $ Gov_Exp_Employees           : num  9.35e+10 NA 5.52e+11 3.01e+11 5.37e+10 ...\n $ Gov_Exp_GoodsAndServices    : num  3.66e+10 NA 1.74e+11 2.06e+11 5.37e+10 ...\n $ Gov_Exp_Total               : num  3.92e+11 NA 4.75e+12 1.42e+12 5.09e+11 ...\n $ Gov_Exp_Interest            : num  3.50e+10 NA 7.50e+11 1.58e+11 1.80e+10 ...\n $ Gov_Exp_Subsidies           : num  2.09e+11 NA 3.05e+12 5.44e+11 3.41e+11 ...\n $ Gov_Exp_Military            : num  2.17e+10 NA 1.51e+11 3.24e+11 3.73e+10 ...\n $ Wage_Workers                : num  45.7 NA 73.5 66 83.4 ...\n $ Vulnerable_Employment       : num  51.2 NA 22.7 33.1 10.6 ...\n $ WGI_Control_Corruption      : num  -0.5434 1.231 -0.0837 -0.2038 1.8221 ...\n $ WGI_Government_Effectiveness: num  -0.0333 1.901 -0.0965 -0.1975 1.5649 ...\n $ WGI_Political_Stability     : num  0.1112 1.6022 -0.0914 -0.4134 0.9117 ...\n $ WGI_Regulatory_Quality      : num  0.286 1.227 -0.437 0.256 1.872 ...\n $ WGI_Rule_of_Law             : num  -0.403 1.572 -0.408 -0.157 1.726 ...\n $ WGI_Voice_Accountability    : num  0.1427 1.1101 0.5724 0.0555 1.2674 ...\n $ Gov_Confidence              : num  0.148 0.491 0.314 0.308 0.313 ...\n $ Gov_Confidence_Mean         : num  3.39 2.56 2.94 2.97 2.82 ...\n\n\nCode\nsummary(data_final)\n\n\n       X        Power_Distance   Individualism    Masculinity    \n Min.   : 1.0   Min.   : 11.00   Min.   : 6.00   Min.   :  5.00  \n 1st Qu.:23.5   1st Qu.: 41.00   1st Qu.:24.00   1st Qu.: 39.50  \n Median :46.0   Median : 63.00   Median :41.00   Median : 49.00  \n Mean   :46.0   Mean   : 59.61   Mean   :45.32   Mean   : 49.31  \n 3rd Qu.:68.5   3rd Qu.: 72.00   3rd Qu.:68.50   3rd Qu.: 63.50  \n Max.   :91.0   Max.   :104.00   Max.   :91.00   Max.   :110.00  \n                NA's   :32       NA's   :32      NA's   :32      \n Uncertainty_Avoidance Time_Perspective   Indulgence     Country_Final     \n Min.   :  8.00        Min.   :  0.00   Min.   :  0.00   Length:91         \n 1st Qu.: 51.00        1st Qu.: 31.25   1st Qu.: 28.00   Class :character  \n Median : 68.00        Median : 51.50   Median : 42.50   Mode  :character  \n Mean   : 66.46        Mean   : 50.46   Mean   : 44.42                     \n 3rd Qu.: 85.00        3rd Qu.: 69.00   3rd Qu.: 63.50                     \n Max.   :112.00        Max.   :100.00   Max.   :100.00                     \n NA's   :32            NA's   :21       NA's   :19                         \n CPI.Score.2018  GDP_per_Capita   Homicides_per_100K Gov_Exp_Employees  \n Min.   :17.00   Min.   :  2221   Min.   : 0.2067    Min.   :1.635e+09  \n 1st Qu.:33.00   1st Qu.: 12845   1st Qu.: 0.7453    1st Qu.:1.830e+10  \n Median :44.50   Median : 25641   Median : 1.3927    Median :7.092e+10  \n Mean   :50.14   Mean   : 30218   Mean   : 3.8824    Mean   :7.315e+12  \n 3rd Qu.:71.25   3rd Qu.: 42847   3rd Qu.: 3.7136    3rd Qu.:4.237e+11  \n Max.   :88.00   Max.   :127273   Max.   :28.7367    Max.   :3.726e+14  \n NA's   :3       NA's   :3        NA's   :20         NA's   :17         \n Gov_Exp_GoodsAndServices Gov_Exp_Total       Gov_Exp_Interest    \n Min.   :7.335e+08        Min.   :8.294e+09   Min.   :-1.270e+09  \n 1st Qu.:8.305e+09        1st Qu.:8.688e+10   1st Qu.: 3.377e+09  \n Median :3.493e+10        Median :4.187e+11   Median : 1.754e+10  \n Mean   :5.153e+12        Mean   :4.773e+13   Mean   : 4.768e+12  \n 3rd Qu.:2.275e+11        3rd Qu.:2.338e+12   3rd Qu.: 2.648e+11  \n Max.   :2.510e+14        Max.   :2.295e+15   Max.   : 2.751e+14  \n NA's   :17               NA's   :17          NA's   :16          \n Gov_Exp_Subsidies   Gov_Exp_Military     Wage_Workers   Vulnerable_Employment\n Min.   :2.170e+09   Min.   :0.000e+00   Min.   :15.85   Min.   : 3.30        \n 1st Qu.:4.227e+10   1st Qu.:3.952e+09   1st Qu.:57.30   1st Qu.: 9.09        \n Median :2.930e+11   Median :2.669e+10   Median :77.26   Median :18.87        \n Mean   :2.540e+13   Mean   :9.487e+12   Mean   :71.64   Mean   :24.75        \n 3rd Qu.:9.952e+11   3rd Qu.:1.584e+11   3rd Qu.:86.35   3rd Qu.:36.96        \n Max.   :1.138e+15   Max.   :5.314e+14   Max.   :95.73   Max.   :83.70        \n NA's   :17          NA's   :11          NA's   :2       NA's   :2            \n WGI_Control_Corruption WGI_Government_Effectiveness WGI_Political_Stability\n Min.   :-1.560314      Min.   :-1.7516              Min.   :-2.603781      \n 1st Qu.:-0.533848      1st Qu.:-0.1993              1st Qu.:-0.565488      \n Median : 0.009211      Median : 0.2023              Median : 0.111169      \n Mean   : 0.272002      Mean   : 0.4396              Mean   : 0.007762      \n 3rd Qu.: 1.221506      3rd Qu.: 1.3999              3rd Qu.: 0.775985      \n Max.   : 2.167130      Max.   : 2.2127              Max.   : 1.639301      \n                                                                            \n WGI_Regulatory_Quality WGI_Rule_of_Law   WGI_Voice_Accountability\n Min.   :-2.3622        Min.   :-2.2536   Min.   :-1.7968         \n 1st Qu.:-0.3226        1st Qu.:-0.4966   1st Qu.:-0.4587         \n Median : 0.5280        Median : 0.1570   Median : 0.2624         \n Mean   : 0.4615        Mean   : 0.3214   Mean   : 0.2182         \n 3rd Qu.: 1.3576        3rd Qu.: 1.3373   3rd Qu.: 1.0170         \n Max.   : 2.1601        Max.   : 2.0488   Max.   : 1.6552         \n                                                                  \n Gov_Confidence    Gov_Confidence_Mean\n Min.   :0.08744   Min.   :1.561      \n 1st Qu.:0.24726   1st Qu.:2.502      \n Median :0.39372   Median :2.711      \n Mean   :0.41976   Mean   :2.708      \n 3rd Qu.:0.53634   3rd Qu.:3.020      \n Max.   :0.95441   Max.   :3.448      \n                                      \n\n\nA lot going on there, but we can see that the data was 91 observations fo 27 variables. Each observation in the data is a country and there are observations for a number of potentially useful variables. Choosing the best variables and assessing the power of our test will be important due to the noticeable number of NA values for some of the variables."
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "href": "posts/FinalPart1_ToryBarteloni.html#transparency-international",
    "title": "DACSS 603: Final Part 1",
    "section": "Transparency International",
    "text": "Transparency International\nThe Corruption Perceptions Index (CPI) is created by Transparency International by taking a combination of 13 different data sources including assessments and surveys. These sources are largely comprised of experts and business interests so are not a direct reflection of the general public. The scores from each of the sources are standardized, averaged, and then scaled to provide a score for each of the countries in the data sources. What we end up with is Corruption Perception score between 1-100 for each of the countries.\n\n\nCode\ndata_final %>% ggplot(aes(x=CPI.Score.2018)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(CPI.Score.2018,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(CPI.Score.2018,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(CPI.Score.2018,na.rm=TRUE)+\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(CPI.Score.2018,na.rm=TRUE)-\n                    IQR(CPI.Score.2018,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Corruption Perceptions Index\",\n       subtitle=\"Distribution of CPI 2018\",\n       x=\"CPI Score\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-bank-development-and-governance-indicators",
    "title": "DACSS 603: Final Part 1",
    "section": "World Bank Development and Governance Indicators",
    "text": "World Bank Development and Governance Indicators\nThe World Bank collects data from many different sources to obtain indicators for world development as well as the World Governance Indicators project.\nThe Development Indicators are taken from a wide variety of sources. We will be using two primary indicators: GDP per Capita and Intentional Homicides per 100K people. GDP per capita is derived from the World Bank and OECD National Accounts data while Intentional Homicides are taken from the UN Office on Drugs and Crime’s International Homicide Statistics database.\n\n\nCode\ndata_final %>% ggplot(aes(x=GDP_per_Capita)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(GDP_per_Capita,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(GDP_per_Capita,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(GDP_per_Capita,na.rm=TRUE)+\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(GDP_per_Capita,na.rm=TRUE)-\n                    IQR(GDP_per_Capita,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Gross Domestic Product per Capita\",\n       subtitle=\"Distribution of GDP per capita 2019\",\n       x=\"GDP per Capita\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\ndata_final %>% ggplot(aes(x=Homicides_per_100K)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Homicides_per_100K,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Homicides_per_100K,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Homicides_per_100K,na.rm=TRUE)+\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Homicides_per_100K,na.rm=TRUE)-\n                    IQR(Homicides_per_100K,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Intentional Homicides per 100K Residents\",\n       subtitle=\"Distribution of homicides per 100K 2019\",\n       x=\"Homicides per 100K\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()\n\n\n\n\n\nThe World Governance Indicators are a combination of enterprise, citizen, and expert survey respondents from around the world. They use more than 30 surveys to create their six indicators with each indicator using different surveys and different data from each survey to aggregate to the final indicator. The one I am most interested in at this juncture is Voice and Accountability which attempts to measure the level of political freedom (i.e. freedom of speech, press, etc.) and access to participation in goverance by the public (i.e. free and fair elections).\n\n\nCode\ndata_final %>% ggplot(aes(x=WGI_Voice_Accountability)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(WGI_Voice_Accountability,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(WGI_Voice_Accountability,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(WGI_Voice_Accountability,na.rm=TRUE)+\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(WGI_Voice_Accountability,na.rm=TRUE)-\n                    IQR(WGI_Voice_Accountability,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"World Governance Indicators - Voice and Accountability\",\n       subtitle=\"Distribution of Voice and Accountability 2019\",\n       x=\"Voice and Accountability\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "href": "posts/FinalPart1_ToryBarteloni.html#world-values-survey-and-european-values-survey",
    "title": "DACSS 603: Final Part 1",
    "section": "World Values Survey and European Values Survey",
    "text": "World Values Survey and European Values Survey\nThe World Values Survey and European Values Survey collect data by conducting representative surveys in around 100 countries every five years. Their surveys are specifically designed to gather opinions on values ranging from political to religious to social. One of the questions they consistently ask is for respondents to indicate what level of confidence they have in their government. This will be our dependent variable of interest, Confidence in Government. In the plot below it is displayed as a proportion of respondents that said they had at least some confidence in their government.\n\n\nCode\ndata_final %>% ggplot(aes(x=Gov_Confidence)) +\n  geom_histogram(bins = 20) +\n  geom_vline(aes(xintercept=median(Gov_Confidence,na.rm=TRUE),\n             color=\"Median\"), \n             size=2) +\n  geom_vline(aes(xintercept=mean(Gov_Confidence,na.rm=TRUE),\n                 color=\"Mean\"), \n             size=2) +\n  geom_vline(aes(xintercept=\n                 median(Gov_Confidence,na.rm=TRUE)+\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n geom_vline(aes(xintercept=\n               median(Gov_Confidence,na.rm=TRUE)-\n                    IQR(Gov_Confidence,na.rm=TRUE)/2,\n               color=\"IQR\"),\n               size=1.5) +\n  labs(title=\"Proportion of Population that has Confidence in Government\",\n       subtitle=\"Distribution of Confidence in Government 2017-2020\",\n       x=\"Confidence in Government\",\n       y=element_blank(),\n       colour=element_blank()) +\n  theme_bw()"
  },
  {
    "objectID": "posts/HW2answers-DonnySnyder.html",
    "href": "posts/HW2answers-DonnySnyder.html",
    "title": "Homework 2",
    "section": "",
    "text": "Question 1\n\n\nCode\nbyPassConfInt <- NA\nbyPassConfInt[1] <- 19 + .9*(10/(sqrt(539)))\nbyPassConfInt[2] <- 19 - .9*(10/(sqrt(539)))\n\nangConfInt <- NA\nangConfInt[1] <- 18 + .9*(9/(sqrt(847)))\nangConfInt[2] <- 18 - .9*(9/(sqrt(847)))\n\nprint(byPassConfInt)\n\n\n[1] 19.38766 18.61234\n\n\nCode\nprint(angConfInt)\n\n\n[1] 18.27832 17.72168\n\n\nThe confidence interval is narrower than for angiography than for bypass surgery.\n\n\nQuestion 2\n\n\nCode\npointEstData <- NA\npointEstData[1:567] <- 1\npointEstData[568:1031] <- 0\npointSD <- sd(pointEstData)\npointEst <- 567/1031\npointConfInt <- NA\npointConfInt[1] <- pointEst + .95*(pointSD/(sqrt(1031)))\npointConfInt[2] <- pointEst - .95*(pointSD/(sqrt(1031)))\nprint(pointConfInt)\n\n\n[1] 0.5646779 0.5352251\n\n\nThe confidence interval here suggests that we can assume with 95% confidence that between 56.5% of adult Americans and 53.5% believe that college education is essential for success.\n\n\nQuestion 3\n\n\nCode\npopSD <- (200 - 30)/4\ncriticalVal <- 1.96\nsampSize <- ((popSD * criticalVal)/5)^2\nprint(sampSize)\n\n\n[1] 277.5556\n\n\nThe size of the sample should be 278.\n\n\nQuestion 4a\nNull hypothesis: Womens income does not deviate from the mean income of senior-level workers.\nAlternative hypothesis: Womens income does deviate from the mean income of senior-level workers.\n\n\nCode\ntStat <- (410 - 500)/(90/(sqrt(9)))\ndegreeFree <- 9-1\n2*pt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.01707168\n\n\nCode\npt(-tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.008535841\n\n\nCode\npt(tStat, degreeFree, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\nThe p value of this test statistic and degrees of freedom is 0.017.\n#4b The p-value for the one-tailed test h0 < 500 is 0.0085. This is half because it is only measuring half of the distribution.\n#4c The p-value for the one-tailed test h0 > 500 is 0.9915. This is because this is measuring in the opposite direction of the actual mean.\n#Question 5\n\n\nCode\ntStatJones <- (519.5 - 500)/(10)\ntStatSmith <- (519.7 - 500)/(10)\ndegreeFree <- 1000-1\n\n2*pt(tStatJones,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.05145555\n\n\nCode\n2*pt(tStatSmith,degreeFree, lower.tail = FALSE)\n\n\n[1] 0.04911426\n\n\n#Question 5b As you can see from the printed values, the Smith study is statistically significant while the Jones study is not.\n#Question 5c If you do not report the p-value, you cannot tell how close the p-value is to being significant, so it can get rid of the value of running the study to not report it.\n#Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ntStatGas <- (mean(gas_taxes) - 45)/(sqrt(sd(gas_taxes)/length(gas_taxes)))\ndegreeFree <- length(gas_taxes) - 1\n\n2*pt(-tStatGas,degreeFree, lower.tail = FALSE)\n\n\n[1] 2.341428e-05\n\n\nYes there is enough evidence. The p-value is far below p = 0.05, at 0.0000234."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html",
    "href": "posts/HW1_Saaradhaa.html",
    "title": "Homework 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)"
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a",
    "href": "posts/HW1_Saaradhaa.html#a",
    "title": "Homework 1",
    "section": "1 (a)",
    "text": "1 (a)\nReading in the data:\n\n# load packages.\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(lsr)\n\n# read in data.\ndf <- read_excel(\"_data/LungCapData.xls\")\n\nDistribution of LungCap:\n\nhist(df$LungCap)\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b",
    "href": "posts/HW1_Saaradhaa.html#b",
    "title": "Homework 1",
    "section": "1 (b)",
    "text": "1 (b)\nThe boxplots below show the probability distributions grouped by gender.\n\nboxplot(LungCap~Gender, data = df)\n\n\n\n\nMales appear to have a slightly greater lung capacity than females."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c",
    "href": "posts/HW1_Saaradhaa.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n# check class of Smoke.\nclass(df$Smoke)\n\n[1] \"character\"\n\n# convert Smoke to factor type.\ndf$Smoke <- as.factor(df$Smoke)\n\n# mean lung capacity for smokers.\ndf %>% select(Smoke, LungCap) %>% group_by(Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nIt does not make sense, as I did not expect smokers to have greater mean lung capacities than non-smokers."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d",
    "href": "posts/HW1_Saaradhaa.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n# check class of Age.\nclass(df$Age)\n\n[1] \"numeric\"\n\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n\n\nMost people seem to be non-smokers, and non-smokers seem to have greater lung capacity."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e",
    "href": "posts/HW1_Saaradhaa.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n# check class of AgeGroup.\nclass(df$AgeGroup)\n\n[1] \"character\"\n\n# convert AgeGroup to factor.\ndf$AgeGroup <- as.factor(df$AgeGroup)\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n\n  \n\n\n\nNon-smokers have greater mean lung capacity for ages 14-15, 16-17 and 18 and above. Smokers have greater mean lung capacity for age 13 and below, which is different from 1(d). There might be some extreme outliers affecting the results for those age 13 and below."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f",
    "href": "posts/HW1_Saaradhaa.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n# correlation.\ncor(df$LungCap,df$Age)\n\n[1] 0.8196749\n\n# covariance.\ncov(df$LungCap,df$Age)\n\n[1] 8.738289\n\n\nThe value of 0.82 for correlation indicates a strong positive relationship between lung capacity and age - as age increases, lung capacity increases. The covariance is a little harder to interpret - the positive value reflects a positive relationship between lung capacity and age, but it is hard to assess the strength of the relationship, given that covariance ranges from negative infinity to infinity. I would prefer to use correlation in most cases."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#a-1",
    "href": "posts/HW1_Saaradhaa.html#a-1",
    "title": "Homework 1",
    "section": "2 (a)",
    "text": "2 (a)\n\na <- 160/810\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#b-1",
    "href": "posts/HW1_Saaradhaa.html#b-1",
    "title": "Homework 1",
    "section": "2 (b)",
    "text": "2 (b)\n\nb <- (128+434)/810\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#c-1",
    "href": "posts/HW1_Saaradhaa.html#c-1",
    "title": "Homework 1",
    "section": "2 (c)",
    "text": "2 (c)\n\nc <- (128+434+160)/810\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#d-1",
    "href": "posts/HW1_Saaradhaa.html#d-1",
    "title": "Homework 1",
    "section": "2 (d)",
    "text": "2 (d)\n\nd <- (64+24)/810\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#e-1",
    "href": "posts/HW1_Saaradhaa.html#e-1",
    "title": "Homework 1",
    "section": "2 (e)",
    "text": "2 (e)\n\n# multiply each value of X by its probability and add the products.\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\nThe expected value for the number of prior convictions is 1.2864198. To be more precise, since number of prior convictions should not have decimal places, we can round this down to 1, which is what the line graph showed us as well."
  },
  {
    "objectID": "posts/HW1_Saaradhaa.html#f-1",
    "href": "posts/HW1_Saaradhaa.html#f-1",
    "title": "Homework 1",
    "section": "2 (f)",
    "text": "2 (f)\n\n# calculate required formula for each value of X.\nf1_0 <- ((0-e)^2) * (128/810)\nf1_1 <- ((1-e)^2) * (434/810)\nf1_2 <- ((2-e)^2) * (160/810)\nf1_3 <- ((3-e)^2) * (64/810)\nf1_4 <- ((4-e)^2) * (24/810)\n\n# sum up the above for variance.\nf1 <- f1_0 + f1_1 + f1_2 + f1_3 + f1_4\n\n# square root for SD.\nf2 <- sqrt(f1)\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298. In general, I think it might be more meaningful to calculate mode and proportions when generating descriptive statistics for number of prior convictions."
  },
  {
    "objectID": "posts/Final pt 1.html",
    "href": "posts/Final pt 1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/Final pt 1.html#research-question",
    "href": "posts/Final pt 1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nIn the United States, wage stagnation has become a hot-button issue for many people in various fields of employment. Graduate students have been at the center of this issue in recent years- strikes for wage increases and cost-of-living adjustments have taken place at multiple universities throughout the country. Because PhD students often do not have the time to earn extra income (and their contracts often prohibit them from pursuing work elsewhere), how much they will earn from their stipend is a huge factor in considering where to pursue their research (Powell, 2004; Soar et al., 2022). Knowing how much My research question is: What is the strongest predictor of the value of a PhD stipend?"
  },
  {
    "objectID": "posts/Final pt 1.html#hypothesis",
    "href": "posts/Final pt 1.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nH₀: Cost of living is not the strongest predictor of the value of a PhD stipend.\nH₁: Cost of living is the strongest predictor of the value of a PhD stipend."
  },
  {
    "objectID": "posts/Final pt 1.html#dataset",
    "href": "posts/Final pt 1.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThis dataset is comprised of self-reported survey data collected by PhDStipends.com. Respondents are asked their university, department, academic year, and year in the program. They are also asked whether they receive a 12-month or 9-month salary, gross pay, and required fees. PhDStipends automatically calculators the LW Ratio (living wage ratio), which is the stipend divided by the living wage of the country the university is located in. I will likely need to add additional information for my own analysis.\nThe variables of interest for me are the university, department, and program year.\n\n\nCode\nlibrary(readr)\ncsv <- read_csv(\"~/School/UMASS/DACSS 603/Final Project/csv.csv\")\n\n\nError: '~/School/UMASS/DACSS 603/Final Project/csv.csv' does not exist.\n\n\nCode\nsummary(csv)\n\n\nError in summary(csv): object 'csv' not found\n\n\n\n\nCode\nprint(summarytools::dfSummary(csv,\n                              varnumbers = FALSE,\n                              plain.ascii  = FALSE,\n                              style        = \"grid\",\n                              graph.magnif = 0.70,\n                              valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nError in summarytools::dfSummary(csv, varnumbers = FALSE, plain.ascii = FALSE, : object 'csv' not found\n\n\nBased on this summary, there are some extreme outliers in need of removal, particularly in the Overall Pay column. Interesting, the mean Overall Pay of $27549.4 does not seem unreasonable,."
  },
  {
    "objectID": "posts/Final pt 1.html#references",
    "href": "posts/Final pt 1.html#references",
    "title": "Final Project Proposal",
    "section": "References",
    "text": "References\nLiving Wage Calculator. (n.d.). Retrieved October 10, 2022, from https://livingwage.mit.edu/\nPowell, K. Stipend survival. Nature 428, 102–103 (2004). https://doi.org/10.1038/nj6978-102a\nEmily Roberts & Kyle Roberts. (2022, October 10). PhD stipends Dataset. http://www.phdstipends.com/csv\nSoar, M., Stewart, L., Nissen, S. et al. Sweat Equity: Student Scholarships in Aotearoa New Zealand’s Universities. NZ J Educ Stud (2022). https://doi.org/10.1007/s40841-022-00244-5"
  },
  {
    "objectID": "posts/Homework 2 LJones.html",
    "href": "posts/Homework 2 LJones.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(readr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\nAngiography\n\n\nCode\na_mean <- 18\na_sd <- 9\na_ss <- 847\n\na_se <- a_sd/sqrt(a_ss)\n\na_cl <- 0.90  \na_tail <- (1-a_cl)/2\na_tscore <- qt(p = 1-a_tail, df = a_ss-1)\n\na_ci <- c(a_mean - a_tscore * a_se,\n        a_mean + a_tscore * a_se)\nprint(a_ci)\n\n\n[1] 17.49078 18.50922\n\n\nBypass\n\n\nCode\nb_mean <- 19\nb_sd <- 10\nb_ss <- 539\n\nb_se <- b_sd/sqrt(b_ss)\n\nb_cl <- 0.90  \nb_tail <- (1-b_cl)/2\nb_tscore <- qt(p = 1-b_tail, df = b_ss-1)\n\nb_ci <- c(b_mean - b_tscore * b_se,\n        b_mean + b_tscore * b_se)\nprint(b_ci)\n\n\n[1] 18.29029 19.70971\n\n\n\n\nCode\n#assessing which confidence interval is larger\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nThe confidence interval is more narrow for angiographies.\n\n\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = American adults (population), x = sample (surveyed)\nn = 1031\nx = 567\n\n#use prop.test to find p (confidence interval is 95% by default)\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\np (the proportion of adult Americans who believe that college education is essential for success) is 0.5499515. The 95% confidence interval is [0.5189682, 0.5805580], meaning we can be 95% certain this interval captured the true proportion.\n\n\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculating standard deviation using the given\n(200-30)/4\n\n\n[1] 42.5\n\n\nSince our significance level is 5%, our confidence level is 95%. A 95% confidence level corresponds to a z-score of 1.96. From here we can calculate the ideal sample size.\n\n\nCode\n#solve 5 = 1.96((200-30)*.25)/sqrt(x)\n\n(((170*.25)/5)*1.96)^2\n\n\n[1] 277.5556\n\n\nUMass needs a sample of approximately 278 students to estimate the mean cost of textbooks.\n\n\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\n\nAssumptions: normal distribution, significance level .05 H₀: μ=500 H₁: μ≠500 Test statistic: 410\n\n\nCode\n#given\np_mean = 500\ns_mean = 410\ns_size = 9\ns = 90\n\n#find standard error\nsem = s/sqrt(s_size)\n\n#find t-score\nt_score <- (s_mean - p_mean)/sem\nt_score\n\n\n[1] -3\n\n\nCode\n#find p-value\npvalue = 2 * pt(t_score, df=(s_size - 1))\npvalue\n\n\n[1] 0.01707168\n\n\nBecause the p-value is less than the significance level (.05), we reject the null hypothesis that μ=500.\n\n\n\n\n\nCode\n#calculate the p-value for lower tail only\nltail <- pt(t_score, df=(s_size - 1), lower.tail = TRUE)\nltail\n\n\n[1] 0.008535841\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we have evidence that μ < 50.\n\n\n\n\n\nCode\n#calculate the p-value for upper tail only\nutail <- pt(t_score, df=(s_size - 1), lower.tail = FALSE)\nutail\n\n\n[1] 0.9914642\n\n\nBecause the p-value of the lower tail is less than the significance level (.05), we reject H₀, meaning we do not have evidence that μ > 500.\nChecking my work:\n\n\nCode\nltail + utail\n\n\n[1] 1\n\n\n\n\n\n\nJones and Smith separately conduct studies to test H₀: μ = 500 against H₁ : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\n\nJones\n\n\nCode\n#t-score\nJt_score = (519.5 - 500)/10\nJt_score\n\n\n[1] 1.95\n\n\nCode\n#p-value\nJp = 2 * pt(Jt_score, df=(1000 - 1), lower.tail = FALSE)\nJp\n\n\n[1] 0.05145555\n\n\nSmith\n\n\nCode\n#t-score\nSt_score = (519.7 - 500)/10\nSt_score\n\n\n[1] 1.97\n\n\nCode\n#p-value\nSp = 2 * pt(St_score, df=(1000 - 1), lower.tail = FALSE)\nSp\n\n\n[1] 0.04911426\n\n\n\n\n\nAt α = 0.05, Jones’ result is statistically significant (because the p-value is greater than α) but Smith’s result is not.\n\n\n\nJones’ p-value is only just barely greater than 0.05, and Smith’s p-value is only just barely less than 0.05. It is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected, leading to very different conclusions based on data that is extremely similar.\n\n\n\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\nt.test(gas_taxes, mu = 45.0, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nSince the p-value is 0.03827, we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. However, we do not know from what year(s) the data was collected. Therefore we can conclude with certainty that the average tax per gallon in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/HW1answers_DonnySnyder.html",
    "href": "posts/HW1answers_DonnySnyder.html",
    "title": "Homework 1 - Donny Snyder",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n##b\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = Gender, y = LungCap)) + geom_boxplot()\n\n\n\n\n\nThe probability distribution suggests that the lung capacity of males tends to be higher.\n##c\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity of smokers vs nonsmokers appears to be higher for smokers. This doesn’t really make sense because I’ve been taught to think smokers tend to have reduced lung capacity.\n##d and e\n\n\nCode\nx=1\ndf$AgeGroup <- rep(c(\"NA\"),times=725)\nwhile(x <= 725){\n  if(df$Age[x] <= 13){\n    df$AgeGroup[x] = \"less than or equal to 13\"\n  }\n  else if((df$Age[x] >= 14)&&(df$Age[x] <= 15)){\n    df$AgeGroup[x] = \"14 to 15\"\n  }\n  else if((df$Age[x] >= 16)&&(df$Age[x] <= 17)){\n    df$AgeGroup[x] = \"16 to 17\"\n  }\n  else if(df$Age[x] >= 18){\n    df$AgeGroup[x] = \"greater than 18\"\n  }\nx = x + 1\n}\naggregate(data = df, LungCap~AgeGroup+Smoke, mean)\n\n\n                  AgeGroup Smoke   LungCap\n1                 14 to 15    no  9.138810\n2                 16 to 17    no 10.469805\n3          greater than 18    no 11.068846\n4 less than or equal to 13    no  6.358746\n5                 14 to 15   yes  8.391667\n6                 16 to 17   yes  9.383750\n7          greater than 18   yes 10.513333\n8 less than or equal to 13   yes  7.201852\n\n\nCode\naggregate(data = df,LungCap~AgeGroup+Smoke,length)\n\n\n                  AgeGroup Smoke LungCap\n1                 14 to 15    no     105\n2                 16 to 17    no      77\n3          greater than 18    no      65\n4 less than or equal to 13    no     401\n5                 14 to 15   yes      15\n6                 16 to 17   yes      20\n7          greater than 18   yes      15\n8 less than or equal to 13   yes      27\n\n\nCode\naggregate(data = df,Age~Smoke,mean)\n\n\n  Smoke      Age\n1    no 12.03549\n2   yes 14.77922\n\n\nIt seems like people tend to have a lung capacity that increases with age. However, nonsmokers have a higher lung capacity for each age break down besides less than or equal to 13. It seems like smokers just might tend to be older. I confirmed this by looking at the length and mean ages per group, where you can see a majority of smokers are older, whereas non smokers tend to be younger. The mean age for smokers also tends to be older.\n##f\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity appears to be quite correlated with age. This means that Lung capacity tends to go up as age goes up, and vice versa. This is confirmed also by the covariance.\n#Question 2\n##a\n\n\nCode\nprint((160/810) * 100)\n\n\n[1] 19.75309\n\n\nThe probability is 19.75309% that a randomly selected inmate has exactly 2 prior convictions.\n##b\n\n\nCode\nprint(((434+128)/810) * 100)\n\n\n[1] 69.38272\n\n\nThe probability is 69.38272% that a randomly selected inmate has fewer than 2 prior convictions.\n##c\n\n\nCode\nprint(((160+434+128)/810) * 100)\n\n\n[1] 89.1358\n\n\nThe probability is 89.1358% that a randomly selected inmate has 2 or fewer prior convictions.\n##d\n\n\nCode\nprint(((64+24)/810) * 100)\n\n\n[1] 10.8642\n\n\nThe probability is 10.8642% that a randomly selected inmate has more than 2 prior convictions.\n##e\n\n\nCode\nnewDf <- NA\nnewDf[1:128] <- 0\nnewDf[129:562] <- 1\nnewDf[563:722] <- 2\nnewDf[723:786] <- 3\nnewDf[787:810] <- 4\nnewDf <- as.data.frame(newDf)\nmean(newDf$newDf)\n\n\n[1] 1.28642\n\n\nThe expected value, known as the “mean” when it deals in data that are not probability distributions, is 1.28642. Because I created a vector here, I took the mean, though I also could have calculated the expected value by multiplying the probabilities by the numbers. They are both the same value in this case.\n##f\n\n\nCode\nsd(newDf$newDf)\n\n\n[1] 0.9259016\n\n\nCode\nvar(newDf$newDf)\n\n\n[1] 0.8572937\n\n\nThe variance of prior convictions is 0.8572937, the standard deviation of prior convictions is 0.9259016."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html",
    "href": "posts/MEGHA JOSEPH_BLOG1.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#research-question",
    "title": "Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nAccording to statistics,Cardiovascular diseases (CVDs) kill approximately 17 million people globally every year.Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies. People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management. Research question is: Which clinical feature will lead to high cardiovascular risk?"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#hypothesis",
    "title": "Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\n1:Behavioral risk factors will not underline significant predictors of predicting Heart Failure.\n2:Behavioral risk factors will underline significant predictors of predicting Heart Failure ."
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#descriptive-statistics",
    "title": "Project Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nI am going to analyze a dataset of 299 patients with heart failure collected in 2015.This dataset is comprised of self-reported survey, with 13 clinical features. data.https://www.kaggle.com/datasets/whenamancodes/heart-failure-clinical-records. The important variables of research are ejection fraction, serum creatinine, and smoking.\n.\n\n\nCode\nlibrary(readr)\nmf <- read_csv(\"C:/Users/user/Downloads/Heart Failure Clinical Records.csv\")\nsummary(mf)\n``\n\n\nError: attempt to use zero-length variable name"
  },
  {
    "objectID": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "href": "posts/MEGHA JOSEPH_BLOG1.html#references",
    "title": "Project Proposal",
    "section": "References",
    "text": "References\n\nSurvival prediction of heart failure patients using machine learning techniques *. (n.d.). Retrieved October 11, 2022, from https://www.sciencedirect.com/science/article/pii/S2352914821002458\n\nMachine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5"
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html",
    "href": "posts/HW1_ToryBartelloni.html",
    "title": "DACSS 603: Homework 1",
    "section": "",
    "text": "First, let’s load our packages and read in the data.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\n\nlcdata <- read_xls(\"_data/LungCapData.xls\")"
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1a",
    "href": "posts/HW1_ToryBartelloni.html#q1a",
    "title": "DACSS 603: Homework 1",
    "section": "Q1a",
    "text": "Q1a\nWhat does the distribution of LungCap look like?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_histogram(bins=45) +\n  theme_bw() +\n  labs(x=\"Lung Capacity\", y=\"Frequency\", \n       title = \"Lung Capacity Distribution\")\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1b",
    "href": "posts/HW1_ToryBartelloni.html#q1b",
    "title": "DACSS 603: Homework 1",
    "section": "Q1b",
    "text": "Q1b\nCompare the probability density of the LungCap with respect to Males and Females.\n\n\nCode\nlcdata %>%\n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(group=Gender, fill=Gender)) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(x=\"Lung Capacity\", title = \"Lung Capacity Distribution\",\n       subtitle = \"Comparing lung capacity between genders\")\n\n\n\n\n\nThe boxplot comparison indicates that on average males have larger lung capacity, but it also shows that the range and IQR for each gender are similar and have a significant amount of overlap."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1c",
    "href": "posts/HW1_ToryBartelloni.html#q1c",
    "title": "DACSS 603: Homework 1",
    "section": "Q1c",
    "text": "Q1c\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlcdata %>% \n  ggplot(aes(x=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  theme_bw() +\n  theme (axis.text.y = element_blank ()) +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers\")\n\n\n\n\n\nComparing the distributions shows that Smokers have a higher mean lung capacity and a significantly smaller range and IQR. This does not make sense intuitively so I would want to investigate the data a bit more to understand the possible reasons."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1d",
    "href": "posts/HW1_ToryBartelloni.html#q1d",
    "title": "DACSS 603: Homework 1",
    "section": "Q1d",
    "text": "Q1d\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nlc_with_age_groups <- lcdata %>%\n  mutate(Age_Group = factor(case_when(\n    Age <= 13 ~ \"<=13\",\n    Age %in% c(14,15) ~ \"14-15\",\n    Age %in% c(16,17) ~ \"16-17\",\n    Age >= 18 ~ \">=18\"\n      ),\n    levels = c(\"<=13\",\"14-15\",\"16-17\",\">=18\")\n    )\n  )\n\nlc_with_age_groups %>% \n  ggplot(aes(x=Age_Group,y=LungCap)) +\n  geom_boxplot() +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nComparing age groups shows a consistent and clear increase in lung capacity as ages increase up to and over 18 years old."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1e",
    "href": "posts/HW1_ToryBartelloni.html#q1e",
    "title": "DACSS 603: Homework 1",
    "section": "Q1e",
    "text": "Q1e\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c? What could possibly be going on here?\n\n\nCode\nlc_with_age_groups %>%\n  ggplot(aes(x=Smoke, y=LungCap)) +\n  geom_boxplot(aes(fill=Smoke)) +\n  scale_fill_discrete(labels=c(\"Non-Smoker\", \"Smoker\")) +\n  facet_wrap(~Age_Group) +\n  theme_bw() +\n  labs(title=\"Lung Capacity Distribution\", \n       subtitle = \"Comparing smokers and non-smokers within age groups\",\n       x=\"Age Group\",\n       y=\"Lung Capacity\")\n\n\n\n\n\nOutside of ages 13 and under all ages groups show higher average, range, and IRQ for non-smokers. It seems likely that the youngest age group, 13 and under, has the largest number of observations of non-smokers which is bringing down the overall average and lower end of the range. This effect is what is causing us to see the higher lung capacity in smokers overall, but we can infer that the causal factor is more likely age than smoking."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q1f",
    "href": "posts/HW1_ToryBartelloni.html#q1f",
    "title": "DACSS 603: Homework 1",
    "section": "Q1f",
    "text": "Q1f\nCalculate the correlation and covariance between Lung Capacity and Age (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nknitr::kable(\n  lcdata %>% summarise(Covariance = cov(LungCap, Age),\n                     Correlation = cor(LungCap, Age)),\n  caption = \"Relationship between lung capacity and age.\"\n)\n\n\n\nRelationship between lung capacity and age.\n\n\nCovariance\nCorrelation\n\n\n\n\n8.738289\n0.8196749\n\n\n\n\n\nThe covariance shows us that the relationship is positive and the correlation coefficient shows us that the relationship is a strong, positive relationship. So the older the people in the data the larger the lung capacity was observed, on average."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2a",
    "href": "posts/HW1_ToryBartelloni.html#q2a",
    "title": "DACSS 603: Homework 1",
    "section": "Q2a",
    "text": "Q2a\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nprison_props <-  prison_data %>% group_by(X) %>%\n    summarise(Frequency = Frequency,\n      Proportion = Frequency / sum(prison_data$Frequency))\nknitr::kable(prison_props,\n  caption=\"Proportion of Inmates\"\n\n)\n\n\n\nProportion of Inmates\n\n\nX\nFrequency\nProportion\n\n\n\n\n0\n128\n0.1580247\n\n\n1\n434\n0.5358025\n\n\n2\n160\n0.1975309\n\n\n3\n64\n0.0790123\n\n\n4\n24\n0.0296296\n\n\n\n\n\nBy calculating the proportion of inmates with each number of prior convictions we can see that the probability of randomly selecting an inmate with 2 prior convictions is 0.1975 or about 19.8%."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2b",
    "href": "posts/HW1_ToryBartelloni.html#q2b",
    "title": "DACSS 603: Homework 1",
    "section": "Q2b",
    "text": "Q2b\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nprint(paste(\"Probability of fewer than 2 prior convictions:\",\nsum(filter(prison_props, X < 2)$Proportion)))\n\n\n[1] \"Probability of fewer than 2 prior convictions: 0.693827160493827\"\n\n\nSumming prisoners with zero and one prior conviction provides us a probability that 0.6938 or about 69.4% chance that a randomly selected inmate would have less than 2 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2c",
    "href": "posts/HW1_ToryBartelloni.html#q2c",
    "title": "DACSS 603: Homework 1",
    "section": "Q2c",
    "text": "Q2c\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nprint(paste(\"Probability of 2 or fewer prior convictions:\",\n            sum(filter(prison_props, X <=2)$Proportion)))\n\n\n[1] \"Probability of 2 or fewer prior convictions: 0.891358024691358\"\n\n\nSumming the prisoners with two or fewer prior convictions gives us the probability that 0.89 or about 89% probability that a randomly selected inmate would have two prior convictions or fewer."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2d",
    "href": "posts/HW1_ToryBartelloni.html#q2d",
    "title": "DACSS 603: Homework 1",
    "section": "Q2d",
    "text": "Q2d\nWhat is the probability that a randomly selected inmate has more than two prior convictions?\n\n\nCode\nprint(paste(\"Probability of more than 2 prior convictions:\",\nsum(filter(prison_props, X > 2)$Proportion)))\n\n\n[1] \"Probability of more than 2 prior convictions: 0.108641975308642\"\n\n\nThe probability found for either 3 or 4 prior convictions (there is no inmate with more than 4 prior convictions) is 0.1084 or about 10.8% probability."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2e",
    "href": "posts/HW1_ToryBartelloni.html#q2e",
    "title": "DACSS 603: Homework 1",
    "section": "Q2e",
    "text": "Q2e\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nprint(paste(\"The expected value for prior convictions:\", mean(prison_indi_data$X)))\n\n\n[1] \"The expected value for prior convictions: 1.28641975308642\"\n\n\nBy taking a weighted average or an average of all possible observations to select from the expected value is 1.28642 or about 1.3 prior convictions."
  },
  {
    "objectID": "posts/HW1_ToryBartelloni.html#q2f",
    "href": "posts/HW1_ToryBartelloni.html#q2f",
    "title": "DACSS 603: Homework 1",
    "section": "Q2f",
    "text": "Q2f\nCalculate the variance and standard deviation for Prior Convictions.\n\n\nCode\nknitr::kable(\n  prison_indi_data %>% summarise(Variance = var(X),\n                     \"Standard Deviation\" = sd(X)),\n  caption = \"Spread of Inmate Prior Convictions\"\n)\n\n\n\nSpread of Inmate Prior Convictions\n\n\nVariance\nStandard Deviation\n\n\n\n\n0.8572937\n0.9259016"
  },
  {
    "objectID": "posts/Final Project.html",
    "href": "posts/Final Project.html",
    "title": "Final Project",
    "section": "",
    "text": "Extensive research has been done on climate change and economic changes respectively but there is not a significant amount of research about their relation towards one another. There are research papers that touch on this but in different aspects and focus more on other factors like political aspects. I would like to look a little broader and look at the difference between each climate zone and their economic differences. This can be taken with a grain of salt as there are many factors that could effect the economic situation being left out. The data was pulled from NASA’s POWER data access viewer; here I pulled the data by region since pulling the whole country in one go was unavailable. Thus, I will conduct research on each region respectively and then compare the results.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Is there a relation between climate zone and economic growth?\nB. Do Southern climates have the largest economic growth?"
  },
  {
    "objectID": "posts/Final Project.html#reading-in-the-data",
    "href": "posts/Final Project.html#reading-in-the-data",
    "title": "Final Project",
    "section": "Reading in the data",
    "text": "Reading in the data\n\n## problem 1 how to merge files that are this large together? do we reduce the file sizes or is there another work around\n# May just need to keep regions separated \n# Weather data\n# Reading in all the weather data \nAmherst <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/amherst.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/amherst.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nFlorida <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/flordia.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/flordia.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nIllinois <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/illinois.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/illinois.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nMiddle <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/middle.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/middle.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nNewmexico <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/Newmexico.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/Newmexico.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nNorth <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/North.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/North.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nSouth <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/south.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/south.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nSouthCali <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/SouthCali.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/SouthCali.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nTexas <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/Texas.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/Texas.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nWashington <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/washington.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/washington.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nWestV <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/WestV.csv\", skip = 14)\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/WestV.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\n\nAmherst\nHad trouble with pivot_wider since it would split the values up by each name but then would fill in the values with NA for the other sections. This added a ton of NA values that one looked bad and were hard to deal with. I had to go a more manual way and do it for each part of PARAMETER to get the exact number of rows I needed. This stopped the NA values and got them all lined up so it reduced the size of the document from 500k+ rows to 89290 rows. This is huge in terms of running the data and working with it. Finally, I just merged the data together and then I was able to rename all the columns and start regression analysis.\n\n# Bringing all the month columns into one column\nAmherst <- Amherst %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Amherst' not found\n\n## getting the parameter section broken up into different sections since the normal pivot_longer did not work\nTidy_amherst <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Amherst' not found\n\nTidy_amherst <- Tidy_amherst %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_amherst' not found\n\nTidy_amherst <- Tidy_amherst %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_amherst' not found\n\nt2m <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR): object 'Amherst' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_amherst$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Amherst' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_amherst$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Amherst' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_amherst$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Amherst' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_amherst$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Amherst %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Amherst' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_amherst$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\n# renaming all the variables to easier to digest names\nTidy_amherst <- Tidy_amherst %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_amherst' not found\n\n## Getting them in clean looking order\nTidy_amherst <- Tidy_amherst %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_amherst' not found\n\n## Summary and starting regression information \nsummary(Tidy_amherst)\n\nError in summary(Tidy_amherst): object 'Tidy_amherst' not found\n\nview_amherst <- Tidy_amherst %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_amherst' not found\n\nkable(view_amherst, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nFlorida\n\nFlorida <- Florida %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Florida' not found\n\nTidy_Florida <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Florida' not found\n\nTidy_Florida <- Tidy_Florida %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Florida' not found\n\nTidy_Florida <- Tidy_Florida %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Florida' not found\n\nt2m <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Florida' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Florida$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Florida' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Florida$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Florida' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Florida$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Florida' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Florida$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Florida %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Florida' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Florida$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Florida <- Tidy_Florida %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Florida' not found\n\nTidy_Florida <- Tidy_Florida %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Florida' not found\n\nsummary(Tidy_Florida)\n\nError in summary(Tidy_Florida): object 'Tidy_Florida' not found\n\nTidy_Florida <- Tidy_Florida %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Florida' not found\n\nkable(Tidy_Florida, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Florida Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nIllinois\n\nIllinois <- Illinois %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Illinois' not found\n\nTidy_Illinois <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Illinois' not found\n\nTidy_Illinois <- Tidy_Illinois %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Illinois' not found\n\nTidy_Illinois <- Tidy_Illinois %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Illinois' not found\n\nt2m <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Illinois' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Illinois$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Illinois' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Illinois$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Illinois' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Illinois$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Illinois' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Illinois$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Illinois %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Illinois' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Illinois$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Illinois <- Tidy_Illinois %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Illinois' not found\n\nTidy_Illinois <- Tidy_Illinois %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Illinois' not found\n\nsummary(Tidy_Illinois)\n\nError in summary(Tidy_Illinois): object 'Tidy_Illinois' not found\n\nTidy_Illinois <- Tidy_Illinois %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Illinois' not found\n\nkable(Tidy_Illinois, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nMiddle\n\nMiddle <- Middle %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Middle' not found\n\nTidy_Middle <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Middle' not found\n\nTidy_Middle <- Tidy_Middle %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Middle' not found\n\nTidy_Middle <- Tidy_Middle %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Middle' not found\n\nt2m <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Middle' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Middle$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Middle' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Middle$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Middle' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Middle$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Middle' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Middle$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Middle %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Middle' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Middle$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Middle <- Tidy_Middle %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Middle' not found\n\nTidy_Middle <- Tidy_Middle %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Middle' not found\n\nsummary(Tidy_Middle)\n\nError in summary(Tidy_Middle): object 'Tidy_Middle' not found\n\nTidy_Middle <- Tidy_Middle %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Middle' not found\n\nkable(Tidy_Middle, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nNew Mexico\n\nNewmexico <- Newmexico %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Newmexico' not found\n\nTidy_Newmexico <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Newmexico' not found\n\nTidy_Newmexico <- Tidy_Newmexico %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Newmexico' not found\n\nTidy_Newmexico <- Tidy_Newmexico %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Newmexico' not found\n\nt2m <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Newmexico' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Newmexico$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Newmexico' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Newmexico$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Newmexico' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Newmexico$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Newmexico' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Newmexico$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Newmexico %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Newmexico' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Newmexico$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Newmexico <- Tidy_Newmexico %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Newmexico' not found\n\nTidy_Newmexico <- Tidy_Newmexico %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Newmexico' not found\n\nsummary(Tidy_Newmexico)\n\nError in summary(Tidy_Newmexico): object 'Tidy_Newmexico' not found\n\nTidy_Newmexico <- Tidy_Newmexico %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Newmexico' not found\n\nkable(Tidy_Newmexico, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nNorth\n\nNorth <- North %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'North' not found\n\nTidy_North <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'North' not found\n\nTidy_North <- Tidy_North %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_North' not found\n\nTidy_North <- Tidy_North %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_North' not found\n\nt2m <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'North' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_North$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'North' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_North$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'North' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_North$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'North' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_North$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- North %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'North' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_North$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_North <- Tidy_North %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_North' not found\n\nTidy_North <- Tidy_North %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_North' not found\n\nsummary(Tidy_North)\n\nError in summary(Tidy_North): object 'Tidy_North' not found\n\nTidy_North <- Tidy_North %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_North' not found\n\nkable(Tidy_North, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nSouth\n\nSouth <- South %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'South' not found\n\nTidy_South <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'South' not found\n\nTidy_South <- Tidy_South %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_South' not found\n\nTidy_South <- Tidy_South %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_South' not found\n\nt2m <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'South' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_South$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'South' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_South$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'South' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_South$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'South' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_South$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- South %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'South' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_South$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_South <- Tidy_South %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_South' not found\n\nTidy_South <- Tidy_South %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_South' not found\n\nsummary(Tidy_South)\n\nError in summary(Tidy_South): object 'Tidy_South' not found\n\nTidy_South <- Tidy_South %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_South' not found\n\nkable(Tidy_South, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nSouth California\n\nSouthCali <- SouthCali %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'SouthCali' not found\n\nTidy_SouthCali <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'SouthCali' not found\n\nTidy_SouthCali <- Tidy_SouthCali %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_SouthCali' not found\n\nTidy_SouthCali <- Tidy_SouthCali %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_SouthCali' not found\n\nt2m <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'SouthCali' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_SouthCali$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'SouthCali' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_SouthCali$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'SouthCali' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_SouthCali$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'SouthCali' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_SouthCali$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- SouthCali %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'SouthCali' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_SouthCali$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_SouthCali <- Tidy_SouthCali %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_SouthCali' not found\n\nTidy_SouthCali <- Tidy_SouthCali %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_SouthCali' not found\n\nsummary(Tidy_SouthCali)\n\nError in summary(Tidy_SouthCali): object 'Tidy_SouthCali' not found\n\nTidy_SouthCali <- Tidy_SouthCali %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_SouthCali' not found\n\nkable(Tidy_SouthCali, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nTexas\n\nTexas <- Texas %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Texas' not found\n\nTidy_Texas <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Texas' not found\n\nTidy_Texas <- Tidy_Texas %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Texas' not found\n\nTidy_Texas <- Tidy_Texas %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Texas' not found\n\nt2m <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Texas' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Texas$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Texas' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Texas$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Texas' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Texas$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Texas' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Texas$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Texas %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Texas' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Texas$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Texas <- Tidy_Texas %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Texas' not found\n\nTidy_Texas <- Tidy_Texas %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Texas' not found\n\nsummary(Tidy_Texas)\n\nError in summary(Tidy_Texas): object 'Tidy_Texas' not found\n\nTidy_Texas <- Tidy_Texas %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Texas' not found\n\nkable(Tidy_Texas, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nWashington\n\nWashington <- Washington %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'Washington' not found\n\nTidy_Washington <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'Washington' not found\n\nTidy_Washington <- Tidy_Washington %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_Washington' not found\n\nTidy_Washington <- Tidy_Washington %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_Washington' not found\n\nt2m <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Washington' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_Washington$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Washington' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_Washington$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Washington' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_Washington$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Washington' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_Washington$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- Washington %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'Washington' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_Washington$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_Washington <- Tidy_Washington %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_Washington' not found\n\nTidy_Washington <- Tidy_Washington %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_Washington' not found\n\nsummary(Tidy_Washington)\n\nError in summary(Tidy_Washington): object 'Tidy_Washington' not found\n\nTidy_Washington <- Tidy_Washington %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_Washington' not found\n\nkable(Tidy_Washington, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nWest Virgina\n\nWestV <- WestV %>%\npivot_longer(\n  cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, DEC),\n  names_to = \"MONTH\",\n  values_to = \"Month_Average\",\n)\n\nError in pivot_longer(., cols = c(NOV, JAN, FEB, MAR, APR, MAY, JUN, JUL, : object 'WestV' not found\n\nTidy_WestV <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN) %>%\n  filter(PARAMETER == 'PS')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT, LON, MONTH, ANN): object 'WestV' not found\n\nTidy_WestV <- Tidy_WestV %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'Tidy_WestV' not found\n\nTidy_WestV <- Tidy_WestV %>%\n  select(PS, YEAR, MONTH, LAT, LON, ANN)\n\nError in select(., PS, YEAR, MONTH, LAT, LON, ANN): object 'Tidy_WestV' not found\n\nt2m <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'T2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'WestV' not found\n\nt2m <- t2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 't2m' not found\n\nt2m <- t2m %>%\n  select(T2M, YEAR)\n\nError in select(., T2M, YEAR): object 't2m' not found\n\nTidy_WestV$T2M <- t2m$T2M\n\nError in eval(expr, envir, enclos): object 't2m' not found\n\nrh2m <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'RH2M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'WestV' not found\n\nrh2m <- rh2m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'rh2m' not found\n\nrh2m <- rh2m %>%\n  select(RH2M, YEAR)\n\nError in select(., RH2M, YEAR): object 'rh2m' not found\n\nTidy_WestV$RH2M <- rh2m$RH2M\n\nError in eval(expr, envir, enclos): object 'rh2m' not found\n\nwh10m <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS10M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'WestV' not found\n\nwh10m <- wh10m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh10m' not found\n\nwh10m <- wh10m %>%\n  select(WS10M, YEAR)\n\nError in select(., WS10M, YEAR): object 'wh10m' not found\n\nTidy_WestV$WS10M <- wh10m$WS10M\n\nError in eval(expr, envir, enclos): object 'wh10m' not found\n\nwh50m <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'WS50M')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'WestV' not found\n\nwh50m <- wh50m %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'wh50m' not found\n\nwh50m <- wh50m %>%\n  select(WS50M, YEAR)\n\nError in select(., WS50M, YEAR): object 'wh50m' not found\n\nTidy_WestV$WS50M <- wh50m$WS50M\n\nError in eval(expr, envir, enclos): object 'wh50m' not found\n\nPRECTOTCORR <- WestV %>%\n  select(PARAMETER, Month_Average, YEAR, LAT) %>%\n  filter(PARAMETER == 'PRECTOTCORR')\n\nError in select(., PARAMETER, Month_Average, YEAR, LAT): object 'WestV' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  group_by(PARAMETER) %>%\n  mutate(row = row_number()) %>%\n  tidyr::pivot_wider(names_from = PARAMETER, values_from = Month_Average) %>%\n  select(-row)\n\nError in group_by(., PARAMETER): object 'PRECTOTCORR' not found\n\nPRECTOTCORR <- PRECTOTCORR %>%\n  select(PRECTOTCORR, YEAR)\n\nError in select(., PRECTOTCORR, YEAR): object 'PRECTOTCORR' not found\n\nTidy_WestV$PRECTOTCORR <- PRECTOTCORR$PRECTOTCORR\n\nError in eval(expr, envir, enclos): object 'PRECTOTCORR' not found\n\nTidy_WestV <- Tidy_WestV %>%\n  dplyr::rename(Temperature = T2M) %>%\n  dplyr::rename(Humidity = RH2M) %>%\n  dplyr::rename(Wind_10_meter = WS10M) %>%\n  dplyr::rename(Surface_Pressure = PS) %>%\n  dplyr::rename(Wind_50_meter = WS50M) %>%\n  dplyr::rename(Precipitation = PRECTOTCORR) %>%\n  dplyr::rename(Annual = ANN) %>%\n  dplyr::rename(Latitude = LAT) %>%\n  dplyr::rename(Longitude = LON) %>%\n  dplyr::rename(Month = MONTH) %>%\n  dplyr::rename(Year = YEAR)\n\nError in dplyr::rename(., Temperature = T2M): object 'Tidy_WestV' not found\n\nTidy_WestV <- Tidy_WestV %>%\n  select(Year, Month, Latitude, Longitude, Temperature, Humidity, Precipitation, Surface_Pressure, Wind_10_meter, Wind_50_meter, Annual)\n\nError in select(., Year, Month, Latitude, Longitude, Temperature, Humidity, : object 'Tidy_WestV' not found\n\nsummary(Tidy_WestV)\n\nError in summary(Tidy_WestV): object 'Tidy_WestV' not found\n\nTidy_WestV <- Tidy_WestV %>%\n  slice(1:10)\n\nError in slice(., 1:10): object 'Tidy_WestV' not found\n\nkable(Tidy_WestV, digits = 2, align = \"ccccccc\", col.names = c(\"Year\", \"Month\", \"Latitude\", \"Longitude\", \"Temperature\", \"Humidity\", \"Precipitation\", \"Surface Pressure\", \"Wind 10 Meters\", \"Wind 50 Meters\", \"Annual\"), caption = \"Amherst Data\") %>%\n  kable_styling(font_size = 16)\n\nError in kable_styling(., font_size = 16): could not find function \"kable_styling\"\n\n\n\n\nState Economy data\nThe economic data is pulled from the Bureau of Economic Analysis (Analysis, n.d.). This data is the ins, outs, and the difference between the former two in income by state. The data ranges from 1990 to 2020 and covers every state in the US.\n\n# Reading in economic data\n\nEconomy <- read.csv(\"C:/Users/ethan/Documents/Github Class/603_Fall_2022_homework/Economy.csv\")\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/ethan/Documents/Github\nClass/603_Fall_2022_homework/Economy.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n# Renaming the columns to remove the X\nEconomy <- Economy %>%\n  dplyr::rename('1990' = X1990) %>%\n  dplyr::rename('1991' = X1991) %>%\n  dplyr::rename('1992' = X1992) %>%\n  dplyr::rename('1993' = X1993) %>%\n  dplyr::rename('1994' = X1994) %>%\n  dplyr::rename('1995' = X1995) %>%\n  dplyr::rename('1996' = X1996) %>%\n  dplyr::rename('1997' = X1997) %>%\n  dplyr::rename('1998' = X1998) %>%\n  dplyr::rename('1999' = X1999) %>%\n  dplyr::rename('2000' = X2000) %>%\n  dplyr::rename('2001' = X2001) %>%\n  dplyr::rename('2002' = X2002) %>%\n  dplyr::rename('2003' = X2003) %>%\n  dplyr::rename('2004' = X2004) %>%\n  dplyr::rename('2005' = X2005) %>%\n  dplyr::rename('2006' = X2006) %>%\n  dplyr::rename('2007' = X2007) %>%\n  dplyr::rename('2008' = X2008) %>%\n  dplyr::rename('2009' = X2009) %>%\n  dplyr::rename('2010' = X2010) %>%\n  dplyr::rename('2011' = X2011) %>%\n  dplyr::rename('2012' = X2012) %>%\n  dplyr::rename('2013' = X2013) %>%\n  dplyr::rename('2014' = X2014) %>%\n  dplyr::rename('2015' = X2015) %>%\n  dplyr::rename('2016' = X2016) %>%\n  dplyr::rename('2017' = X2017) %>%\n  dplyr::rename('2018' = X2018) %>%\n  dplyr::rename('2019' = X2019) %>%\n  dplyr::rename('2020' = X2020) \n\nError in dplyr::rename(., `1990` = X1990): object 'Economy' not found\n\n# Pivoting to combine all the years into one column\nEconomy <- Economy %>%\npivot_longer(\n  cols = c('1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020'),\n  names_to = \"Year\",\n  values_to = \"Yearly_Fianace\",\n)\n\nError in pivot_longer(., cols = c(\"1990\", \"1991\", \"1992\", \"1993\", \"1994\", : object 'Economy' not found\n\n## Change from char to numeric\nEconomy$Year <- as.numeric(Economy$Year)\n\nError in eval(expr, envir, enclos): object 'Economy' not found\n\n# Changing the finance column to be in millions\nEconomy <- Economy %>%\n  mutate(Year_Money_Millions = Yearly_Fianace/1000)\n\nError in mutate(., Year_Money_Millions = Yearly_Fianace/1000): object 'Economy' not found\n\nsummary(Economy)\n\nError in summary(Economy): object 'Economy' not found"
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html",
    "href": "posts/FinalProjectProposal_Saaradhaa.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\n# load libraries.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(summarytools)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nPrior research literature in the social sciences has continually stressed the need for more research on the Global South. However, few papers actually focus on it. Hence, I am interested to learn more about this region. A data source that lends itself useful for this is the World Values Survey, a global survey with an easily accessible database.\nI am specifically interested in understanding what drives subjective well-being, which can be interpreted via happiness and life satisfaction (Addai et al., 2013).\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. What predicts happiness and life satisfaction in the Global South?\nB. Do predictors of happiness and life satisfaction differ between the Global North and South?\n\n\nThis project will be useful to better understand motivations and desires in the Global South, reduce inter-cultural tensions and enhance cross-cultural cohesion. Governments can also benefit from this research in terms of policy prioritization to maximize citizens’ well-being."
  },
  {
    "objectID": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "href": "posts/FinalProjectProposal_Saaradhaa.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nPast researchers have studied happiness and life satisfaction in the Global South via the World Values Survey (Addai et al., 2013; Ngamaba, 2016). The studies focused on Ghana and Rwanda respectively. The common predictors of happiness and life satisfaction across both countries were satisfaction with health and income.\nTo the best of my knowledge, few studies comparing well-being in the Global North and South exist. Alba (2019) found that happiness was generally greater in the Global North than the Global South, and indicated that future research should attempt to cover the factors behind this. I think happiness and well-being in the Global North may depend on more subjective measures, given that health and income-related issues should be relatively more accounted for.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nHealth and financial satisfaction will not be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nHealth and financial satisfaction will be statistically significant predictors of happiness and life satisfaction in the Global South.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nPredictors of happiness and life satisfaction will not differ between the Global North and South.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nPredictors of happiness and life satisfaction will differ between the Global North and South."
  },
  {
    "objectID": "posts/Final Project Proposal.html",
    "href": "posts/Final Project Proposal.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "The research questions that I am looking to investigate involve the factors that increase university students’ GPA. These include the following:\n1) Does classroom engagement (i.e., taking notes, attending class, listening) result in a higher GPA in university students?\n2) Does reported studying (i.e., weekly study hours) result in a higher GPA in university students?\n3) Does collaboration between students (i.e., studying together, positive class discussions) result in a higher GPA in university students?\n\n\n\n\n\nFor the first research question, it is reasonable to hypothesize that classroom engagement will have a positive effect on students’ academic achievement. Previous research supports this hypothesis. For example, one study found that classroom engagement, as well as other related factors such as time management and autonomous motivation, are predictors of academic achievement (Fokkens-Bruinsma, et al., 2021). Another study found that attendance in higher education is a small, but still statistically significant, predictor of academic performance (Büchele, 2021). In this study, classroom engagement will be defined as “taking notes, attendance, and frequency of listening.” These measures will be reported by university students via survey.\n\n\n\nIn regards to the second research question, it is hypothesized that students who study more will have a higher GPA. There are many previous studies that support this claim. For instance, one study found that university freshmen who studied more than eight hours a week saw an average increase in GPA of 0.580 (Nelson, 2003). Research has also found that increasing study time leads to an increased GPA (Thibodeaux, et al., 2017). In this study, hours spent studying will be measured through students’ estimated range of hours studied, reported via survey.\n\n\n\nIn response to the third research question, it is hypothesized that student collaboration will have a positive effect on student GPA. There is some research literature that supports this statement. One study found that students who study with their peers achieve significantly higher homework scores (Vargas, et al., 2018). Another study found that university students who had a strong social network and exhibited collaborative behaviors tended to achieve higher grades (Ellis & Han, 2021). Effective student collaboration can also occur during class time, such as through small group discussions. Research has found that students who participate in small group discussions demonstrate an increase in resilience, which has shown to improve academic performance (Torrento-estimo, et al, 2012). In this study, student collaboration will be measured through students’ reported time spent studying with peers, and impact that their class discussions have.\n\n\n\n\nThe dataset used is one retrieved from Kaggle using the link here. The dataset is named, “Higher Education Student Performance Evaluation.” This dataset was used in a self-report survey study conducted by Yılmaz and Sekeroglu (2019).\n\n\nCode\nstudentsurvey <- read.csv(\"student_prediction.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'student_prediction.csv': No such\nfile or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nsummary(studentsurvey)\n\n\nError in summary(studentsurvey): object 'studentsurvey' not found\n\n\nCode\nlibrary(ggplot2)\n\n\nTo begin, it is important to examine the demographic variables through descriptive statistics to observe the sample.\n\n\nTo start, students’ reported gender (1 = female and 2 = male) is plotted in the bar graph below.\n\n\nCode\nggplot(studentsurvey, aes(x = GENDER)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = GENDER)): object 'studentsurvey' not found\n\n\nIn this sample, there are more males than females.\n\n\n\nThe bar graph below plots the students’ reported ages at the time of the survey (1 = 18-21, 2 = 22-25, 3 = 26 or above).\n\n\nCode\nggplot(studentsurvey, aes(x = AGE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = AGE)): object 'studentsurvey' not found\n\n\nThe majority of students are between the ages 18-25, with very few above the age of 26.\n\n\n\nThe bar graph below depicts what type of high school the university students graduated from (1= private, 2 = state, 3 = other).\n\n\nCode\nggplot(studentsurvey, aes(x = HS_TYPE)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = HS_TYPE)): object 'studentsurvey' not found\n\n\nAccording to the graph, most students attended a state (public) high school.\n\n\n\nThe bar graph below demonstrates what percentage of their tuition was paid for by scholarship (1 = None, 2 = 25%, 3 = 50%, 4 = 75%, 5 = Full)\n\n\nCode\nggplot(studentsurvey, aes(x = SCHOLARSHIP)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = SCHOLARSHIP)): object 'studentsurvey' not found\n\n\nMost students have received at least 50% scholarship at this university.\n\n\n\nThe bar graph below depicts how many students work a job outside of their classes (1 = Yes, 2 = No)\n\n\nCode\nggplot(studentsurvey, aes(x = WORK)) + geom_bar()\n\n\nError in ggplot(studentsurvey, aes(x = WORK)): object 'studentsurvey' not found\n\n\nMost students do not have a job while they are studying at university in this sample.\n\n\n\nThis sample may not be representative of the U.S. student population. There are more male than female students, which is not the case at most schools: there is about a 1:2 male to female ratio at U.S. colleges (Leukhina & Smaldone, 2022). The ages of students, however, do align with the ages of current university students: about a third of students in university are ages 24 and under (Hanson, 2022). Additionally, like in the sample, the vast majority of students attended public schools (Riser-Kositsky, 2022). In regards to scholarships, the students at this particular university receive scholarships at significantly higher rates than the rest of the U.S. Only about one in eight students receive a scholarship, and only 5% receive a full scholarship (Scholarship Statistics, 2021). While the enrollment statuses of the students were not given, if all students were full-time students, it would align with research that shows that less than half of full-time students (40%) in U.S. universities work while in school. While this sample may not be entirely representative of the U.S. college student population, analyses of this dataset conducted may provide some insight on factors that improve university students GPA.\n\n\n\n\nBüchele, S. (2021). Evaluating the link between attendance and performance in higher education: the role of classroom engagement dimensions. Assessment & Evaluation in Higher Education, 46(1), 132-150.\nEllis, R., & Han, F. (2021). Assessing university student collaboration in new ways. Assessment & Evaluation in Higher Education, 46(4), 509-524.\nFokkens-Bruinsma, M., Vermue, C., Deinumdataset, J. F., & van Rooij, E. (2021). First-year academic achievement: the role of academic self-efficacy, self-regulated learning and beyond classroom engagement. Assessment & Evaluation in Higher Education, 46(7), 1115-1126.\nHanson, M. (2022, July 26). College Enrollment & Student Demographic Statistics. EducationData.org. Retrieved from https://educationdata.org/college-enrollment-statistics.\nLeukhina, O., & Smaldone, A. (2022, March 14). Why do women outnumber men in college enrollment? Saint Louis Fed Eagle. Retrieved from https://www.stlouisfed.org/on-the-economy/2022/mar/why-women-outnumber-men-college-enrollment#:~:text=When%20the%20fall%20college%20enrollment,seen%20in%20U.S.%20college%20enrollment.\nNational Center for Education Statistics. (2022, May). College Student Employment. Coe - college student employment. Retrieved from https://nces.ed.gov/programs/coe/indicator/ssa/college-student-employment\nNelson, R. (2003). Student Efficiency: A study on the behavior and productive efficiency of college students and the determinants of GPA. Issues in Political Economy, 12, 32-43.\nRiser-Kositsky, M. (2022, August 2). Education statistics: Facts about American Schools. Education Week. Retrieved from https://www.edweek.org/leadership/education-statistics-facts-about-american-schools/2019/01.\nScholarship statistics. ThinkImpact.com. (2021, November 10). Retrieved from https://www.thinkimpact.com/scholarship-statistics/.\nThibodeaux, J., Deutsch, A., Kitsantas, A., & Winsler, A. (2017). First-year college students' time use: Relations with self-regulation and GPA. Journal of Advanced Academics, 28(1), 5-27.\nTorrento-estimo, E., Lourdes, C., & Evidente, L. G. (2012). Collaborative Learning in Small Group Discussions and Its Impact on Resilience Quotient and Academic Performance. JPAIR Multidisciplinary Research Journal, 7(1), 1-1.\nVargas, D. L., Bridgeman, A. M., Schmidt, D. R., Kohl, P. B., Wilcox, B. R., & Carr, L. D. (2018). Correlation between student collaboration network centrality and academic performance. Physical Review Physics Education Research, 14(2), 020112.\nYılmaz, N., & Sekeroglu, B. (2019, August). Student Performance Classification Using Artificial Intelligence Techniques. In International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions (pp. 596-603). Springer, Cham."
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html",
    "href": "posts/Final Project 1_Kaushika Potluri.html",
    "title": "Final Project Submission 1",
    "section": "",
    "text": "the research question that I have been interested in is the impact of education about sex and fertility for women and how that changes the fetility rate. Women’s education raises the value of time spent working in the market and, as a result, the opportunity cost of spending time to take care of their child seems less. Across time and places, there is a clear negative link between women’s education and fertility, although its meaning is ambiguous. Women’s level of education may impact fertility through its effects on children’s health, the number of children desired, and women’s ability to give birth and understanding of various birth control options. Each of these are influenced by local, institutional, and national circumstances. Their relative importance may fluctuate as a society develops economically. Since having children affects how much mothers must pay for childcare, women’s education may also be correlated with fertility. The data was acquired from various years of the National Opinion Resource Center’s General Social Survey. Compared to other women, mothers who stay at home with their kids are less likely to invest more money in their education. The correlation between women’s education and unobservable qualities that are jointly linked with fertility may be even more significant.\n###Hypothesis It can be thought of as the total number of unplanned and intended children. The number of kids a family can have, the number of kids the family desires, and the capability to regulate birth through the availability of modern contraceptives and the knowledge of how to use them are all impacted by advancements in women’s education. The number of children a woman has is halfway between the amount she wants and her level of natural fertility. Age and fertility control are the determining variables.If there was a variation by region in birth control availability, such information might be valuable. However, our data set does not contain geographical information (parameters). My assumption would be that if the level of education increases, the number of children would decrease.\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Final Project 1_Kaushika Potluri.html#loading-in-packages",
    "title": "Final Project Submission 1",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#reading-in-data",
    "title": "Final Project Submission 1",
    "section": "Reading in Data:",
    "text": "Reading in Data:\nThe data was acquired from Professor Sander’s article that he used.\n\n\nCode\nWomendata <-  read.csv(\"_data/data.csv\")"
  },
  {
    "objectID": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "href": "posts/Final Project 1_Kaushika Potluri.html#summary-of-the-data",
    "title": "Final Project Submission 1",
    "section": "Summary of the data",
    "text": "Summary of the data\n\n\nCode\nsummary(Womendata)\n\n\n       X           mnthborn         yearborn          age       \n Min.   :   1   Min.   : 1.000   Min.   :38.00   Min.   :15.00  \n 1st Qu.:1091   1st Qu.: 3.000   1st Qu.:55.00   1st Qu.:20.00  \n Median :2181   Median : 6.000   Median :62.00   Median :26.00  \n Mean   :2181   Mean   : 6.331   Mean   :60.43   Mean   :27.41  \n 3rd Qu.:3271   3rd Qu.: 9.000   3rd Qu.:68.00   3rd Qu.:33.00  \n Max.   :4361   Max.   :12.000   Max.   :73.00   Max.   :49.00  \n                                                                \n    electric          radio              tv             bicycle      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.00000   Median :0.0000  \n Mean   :0.1402   Mean   :0.7018   Mean   :0.09291   Mean   :0.2758  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n NA's   :3        NA's   :2        NA's   :2         NA's   :3       \n      educ             ceb            agefbrth        children     \n Min.   : 0.000   Min.   : 0.000   Min.   :10.00   Min.   : 0.000  \n 1st Qu.: 3.000   1st Qu.: 1.000   1st Qu.:17.00   1st Qu.: 0.000  \n Median : 7.000   Median : 2.000   Median :19.00   Median : 2.000  \n Mean   : 5.856   Mean   : 2.442   Mean   :19.01   Mean   : 2.268  \n 3rd Qu.: 8.000   3rd Qu.: 4.000   3rd Qu.:20.00   3rd Qu.: 4.000  \n Max.   :20.000   Max.   :13.000   Max.   :38.00   Max.   :13.000  \n                                   NA's   :1088                    \n    knowmeth         usemeth          monthfm          yearfm     \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.00   Min.   :50.00  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.: 3.00   1st Qu.:72.00  \n Median :1.0000   Median :1.0000   Median : 6.00   Median :78.00  \n Mean   :0.9633   Mean   :0.5776   Mean   : 6.27   Mean   :76.91  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 9.00   3rd Qu.:83.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :12.00   Max.   :88.00  \n NA's   :7        NA's   :71       NA's   :2282    NA's   :2282   \n     agefm          idlnchld          heduc            agesq       \n Min.   :10.00   Min.   : 0.000   Min.   : 0.000   Min.   : 225.0  \n 1st Qu.:17.00   1st Qu.: 3.000   1st Qu.: 0.000   1st Qu.: 400.0  \n Median :20.00   Median : 4.000   Median : 6.000   Median : 676.0  \n Mean   :20.69   Mean   : 4.616   Mean   : 5.145   Mean   : 826.5  \n 3rd Qu.:23.00   3rd Qu.: 6.000   3rd Qu.: 8.000   3rd Qu.:1089.0  \n Max.   :46.00   Max.   :20.000   Max.   :20.000   Max.   :2401.0  \n NA's   :2282    NA's   :120      NA's   :2405                     \n     urban           urb_educ          spirit          protest      \n Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median : 0.000   Median :0.0000   Median :0.0000  \n Mean   :0.5166   Mean   : 3.469   Mean   :0.4222   Mean   :0.2277  \n 3rd Qu.:1.0000   3rd Qu.: 7.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :20.000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n    catholic         frsthalf          educ0           evermarr     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.1025   Mean   :0.5405   Mean   :0.2078   Mean   :0.4767  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n\n\n\n\nCode\nglimpse(Womendata)\n\n\nRows: 4,361\nColumns: 28\n$ X        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ mnthborn <int> 5, 1, 7, 11, 5, 8, 7, 9, 12, 9, 6, 10, 12, 2, 1, 6, 1, 8, 4, …\n$ yearborn <int> 64, 56, 58, 45, 45, 52, 51, 70, 53, 39, 46, 59, 42, 40, 53, 6…\n$ age      <int> 24, 32, 30, 42, 43, 36, 37, 18, 34, 49, 42, 29, 45, 48, 35, 2…\n$ electric <int> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ radio    <int> 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tv       <int> 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ bicycle  <int> 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…\n$ educ     <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ ceb      <int> 0, 3, 1, 3, 2, 1, 4, 0, 1, 0, 3, 3, 4, 10, 3, 0, 4, 2, 0, 1, …\n$ agefbrth <int> NA, 25, 27, 17, 24, 26, 20, NA, 19, NA, 25, 23, 18, 19, 23, N…\n$ children <int> 0, 3, 1, 2, 2, 1, 4, 0, 1, 0, 3, 3, 2, 8, 3, 0, 4, 2, 0, 1, 0…\n$ knowmeth <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ usemeth  <int> 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1…\n$ monthfm  <int> NA, 11, 6, 1, 3, 11, 5, NA, 7, 11, 6, 1, 1, 10, 1, NA, NA, NA…\n$ yearfm   <int> NA, 80, 83, 61, 66, 76, 78, NA, 72, 61, 70, 84, 66, 66, 74, N…\n$ agefm    <int> NA, 24, 24, 15, 20, 24, 26, NA, 18, 22, 24, 24, 23, 26, 21, N…\n$ idlnchld <int> 2, 3, 5, 3, 2, 4, 4, 4, 4, 4, 3, 6, 6, 4, 3, 4, 5, 1, 2, 3, 2…\n$ heduc    <int> NA, 12, 7, 11, 14, 9, 17, NA, 3, 1, 16, 7, NA, 3, 16, NA, NA,…\n$ agesq    <int> 576, 1024, 900, 1764, 1849, 1296, 1369, 324, 1156, 2401, 1764…\n$ urban    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ urb_educ <int> 12, 13, 5, 4, 11, 7, 16, 10, 5, 4, 15, 7, 0, 4, 12, 7, 7, 5, …\n$ spirit   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0…\n$ protest  <int> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1…\n$ catholic <int> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ frsthalf <int> 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0…\n$ educ0    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ evermarr <int> 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1…\n\n\nWe can see that we have 28 variables and 4361 observations in this dataset. The dependent variable of interest - number of living children Then I will perform data manipulation to tidy the data. The variables of interest are age, yearborn, month born, urban education and many more variables that seem intriguing. Variables like radio, bicycle, electric can be ignored in this.\n###References [1] The effect of women’s schooling on fertility by W Sander · 1992 [2] The Impact of Women’s Schooling on Fertility and Contraceptive Use by M Ainsworth · 1996"
  },
  {
    "objectID": "posts/HW1_EmmaRasmussen.html",
    "href": "posts/HW1_EmmaRasmussen.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlungcap<-read_excel(\"_data/LungCapData.xls\")\nhead(lungcap)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\n#saving a copy of original dataset\nlungcap_orig<-lungcap\n\n#checking for missing values in LungCap\nwhich(is.na(lungcap$LungCap))\n\n\ninteger(0)\n\n\n\n1a.\nThe distribution of LungCapData is plotted as a histogram below.\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_histogram()\n\n\n\n\n\nThe histogram looks approximately normally distributed\n\n\n1b.\nThe probability distribution of LungCap data for males and females is compared using the boxplots below:\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Gender))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity of males appears slightly higher than that of females. The IQR and range for males and females appears similarly spread with a higher average for males.\n\n\n1c.\nBelow the mean and standard deviation of smokers and non-smokers is compared. They are also plotted as a boxplot to help visualize the distribution.\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nlungcap%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(lungcap, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean lung capacity for smokers (8.645) in this sample is higher than that of non-smokers (7.770). This does not make sense. However, the standard deviation of non-smokers (2.726) is much higher than smokers (1.883) so there might be something else going on (see boxplot).\n\n\n1d.\nBelow, means are taken by age groups of smokers/non-smokers. I also created a new age category variable (“AgeCat”) to plot the data by smoking status and age category.\n\n\nCode\n#Mean under 13 and nonsmoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n#Mean under 13 and smoker\nlungcap %>% \n  filter(Age<=13 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 7.201852\n\n\nCode\n#Mean 14-15 and nonsmoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.068018\n\n\nCode\n#Mean 14-15 and smoker\nlungcap %>% \n  filter(Age==14 | Age==15 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.689231\n\n\nCode\n#Mean 16-17 and nonsmoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.30523\n\n\nCode\n#Mean 16-17 and smoker\nlungcap %>% \n  filter(Age==16 | Age==17 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.850385\n\n\nCode\n#Mean over 18 and nonsmoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"no\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n#Mean over 18 and smoker\nlungcap %>% \n  filter(Age>=18 & Smoke==\"yes\") %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\n#creating new variable AgeCat to create boxplots\nlungcap<-lungcap %>% \n  mutate(AgeCat= as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\n\n#new Category AgeCat is the last column\nlungcap\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeCat      \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+geom_boxplot()+facet_grid(Smoke ~ AgeCat)\n\n\n\n\n\n\n\n1e.\nComparing the lung capacities for smokers and non-smokers in different age categories:\nNow we can see that the mean lung capacity for smokers by age group is generally lower than that of nonsmokers. This is true in all categories except for Under 13, which is likely because smokers in that category are going to be older than nonsmokers in that category (i.e. it is more likely that a 12 year old smokes than a 6 year old, and a 12 year old has a larger lung capacity than a 6 year old regardless of smoking status)\nThis explains the first calculation of mean by smoking status (before finding the mean by age categories). Smokers are generally going to be older than non-smokers for this sample (the oldest participant in the sample is 19- see code below), which explains why the mean for smokers versus non-smokers (not separated by age categories) makes it look like smokers have a higher average lung capacity.\n\n\nCode\n#checking how old participants in the sample are\nlungcap %>% \n  summarize(range(Age))\n\n\n# A tibble: 2 × 1\n  `range(Age)`\n         <dbl>\n1            3\n2           19\n\n\n\n\n1f.\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\n#Creating vectors of Age and Lung Capacity from df (lungcap) to apply cov() and cor() functions to\nx<-c(lungcap$Age)\ny<-c(lungcap$LungCap)\n\n\n#Calculating covariance\ncov(x, y)\n\n\n[1] 8.738289\n\n\nCode\n#calculating correlation\ncor(x, y)\n\n\n[1] 0.8196749\n\n\nThe covariance, 8.738 is fairly high and positive, meaning as age increases, so does lung capacity (i.e. age and lung capacity co-vary). The correlation (0.82) is fairly close to one and positive, indicating they correlate fairly closely.\n\n\n2a-f.\nPrior Conviction Data\n\n\nCode\n#creating a data frame\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nprison<- data.frame(X, Frequency)\nprison\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nprison<-rename(prison, PriorConvictions=X)\nprison\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(prison, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\nAnswering the Questions\n\n\nCode\n#creating a vector of probabilities\nprobs<-Frequency/810\nprobs\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, probs)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions? 19.75% probability (or 0.1975)\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69.38% probability\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89.14% probability\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions? 10.86% probability\nWhat is the expected value for the number of prior convictions? 1.28642 prior convictions\nCalculate the variance and the standard deviation for the Prior Convictions. variance: 0.8572937 standard deviation: 0.9259016 prior convictions"
  },
  {
    "objectID": "posts/shelton_final1.html",
    "href": "posts/shelton_final1.html",
    "title": "Final Project: Part 1",
    "section": "",
    "text": "Homelessness is a complex living situation with several qualifying conditions; at its most simple state, the U.S Dept. of Housing and Urban Development defines it as lacking a fixed, regular nighttime residence (not a shelter) or having a nighttime residence not designed for human accommodation1.\nOn a single night in 2020, over 500,0002 people experienced homelessness in the United States. Florida, with the third largest state population , had the fourth largest homeless population of 2020 with 27,4872.\nFlorida counties represent a large age range and varying demographic profiles; the state is a hub to a variety of industries including tourism, defense, agriculture, and information technology. Investigating homelessness in Florida counties with robust data can lead to several conclusions about who is being impacted where, and how state policy is failing groups of a diverse population.\n\nResearch QuestionHypothesisIntroduction to DataImprovementsCodebookReferences\n\n\nCarole Zugazaga’s 2004 study of 54 single homeless men, 54 single homeless women, and 54 homeless women with children in the Central Florida area investigated stressful life events common among homeless people. The interviews revealed that women were more likely to have been sexually or physically assaulted, while men were more likely to have been incarcerated or abuse drugs/alcohol. Homeless women with children were more likely to be in foster care as a youth.\nNearly a decade later,county-level data can be used to investigate the relationship between Zugazaga’s reported stressful life events (incarceration, drug arrests, poverty, forcible sex…)3 and homelessness counts.\n\n\n\n\n\n\nResearch Question\n\n\n\nDo particular life stressors increase a population’s vulnerability to homelessness?\n\n\n\n\nHomelessness is not a new issue in the United States, yet homeless policy targets elimination via criminalization rather than prevention. Despite state and federal governments being aware of the circumstances that increase vulnerability to homelessness for decades, I anticipate all of the variables to remain significant in a model relating stressors to Florida homelessness counts 2018-2020.\n\n\n\n\n\n\nResearch Hypothesis\n\n\n\nH0: All stressors are insignificant in predicting homelessness counts ( Bi = 0 for i=0,1,2,…n )\nHA: At least one stressor Bi is significant in predicting homelessness counts\n\n\n\n\nThe data florida_1820.csv4 describes population, homelessness counts, poverty counts and several other demographic indicators3 at the county level for 2018-2020. All 67 Florida counties have observations for the 3 years giving us 201 observations of 15 variables. Each observation provides a count of each variables from a single county for a year within 2018-2020.\nThe data were collected from the Florida Department of Health. Variable names3 were used as search indicators to produce counts for Florida counties. Unfortunately, we cannot accurately analyze the effect of COVID-19 as data is incomplete for the majority of counties in 2021.\n\n\n\n\n\n\nIntro to Data\n\n\n\n\n\n\n\n\n\n  \n\n\n\n    County               Year      Homeless (Count)   Population     \n Length:201         Min.   :2018   Min.   :   0.0   Min.   :   8367  \n Class :character   1st Qu.:2018   1st Qu.:  11.0   1st Qu.:  28089  \n Mode  :character   Median :2019   Median : 151.0   Median : 130642  \n                    Mean   :2019   Mean   : 427.8   Mean   : 317746  \n                    3rd Qu.:2020   3rd Qu.: 563.0   3rd Qu.: 367471  \n                    Max.   :2020   Max.   :3516.0   Max.   :2864600  \n                                                                     \n Unemployment Rate   Median Inc    Incarceration (Rateper1000) Poverty (Count) \n Min.   : 2.100    Min.   :34583   Min.   : 0.60               Min.   :   906  \n 1st Qu.: 3.400    1st Qu.:41401   1st Qu.: 2.50               1st Qu.:  4901  \n Median : 4.000    Median :50640   Median : 3.40               Median : 16210  \n Mean   : 4.697    Mean   :51116   Mean   : 3.84               Mean   : 42922  \n 3rd Qu.: 5.600    3rd Qu.:58093   3rd Qu.: 4.50               3rd Qu.: 46034  \n Max.   :13.500    Max.   :83803   Max.   :18.60               Max.   :482656  \n                                                                               \n Drug Arrests (Count) Relocated (Rate) Sub Abuse Enrollment (Count)\n Min.   :   13        Min.   : 4.689   Min.   :   5.0              \n 1st Qu.:  225        1st Qu.:11.244   1st Qu.:  76.0              \n Median :  729        Median :12.700   Median : 250.0              \n Mean   : 1558        Mean   :13.288   Mean   : 877.6              \n 3rd Qu.: 1903        3rd Qu.:14.544   3rd Qu.:1030.0              \n Max.   :13038        Max.   :22.553   Max.   :6272.0              \n                                                                   \n Adult Pysch Beds (Count) Severe Housing Problems (Rate) Forcible Sex (Count)\n Min.   :  0.00           Min.   : 9.6                   Min.   :   0.0      \n 1st Qu.:  0.00           1st Qu.:13.3                   1st Qu.:  14.0      \n Median :  0.00           Median :15.4                   Median :  45.0      \n Mean   : 66.26           Mean   :15.8                   Mean   : 170.5      \n 3rd Qu.: 84.00           3rd Qu.:17.3                   3rd Qu.: 225.0      \n Max.   :778.00           Max.   :29.8                   Max.   :1408.0      \n                          NA's   :134                                        \n Foster Care (Count)\n Min.   :   3.0     \n 1st Qu.:  33.0     \n Median : 153.0     \n Mean   : 326.1     \n 3rd Qu.: 353.0     \n Max.   :2289.0     \n                    \n\n\n\n\n  \n\n\n\n\n\n\nExpanding Intro to Data exposes summary statistics including mean, range, quantiles, and standard deviation for all 15 variables. The table below the summaries provides arranged figures for basic parameters of interest grouped by county.\nLATER: Plots, Isolate more variables of interest with grouping, group by year?\n\n\nWhile the data is great illustration of homelessness in Florida by county, there are improvements that could be made to both data collection and the research question itself to further the study.\nData:\n\nUnfortunately, FL Health Charts did not provide demographic breakdown for the homeless population (Age, Sex, Race), which would drastically widen the scope of the analysis, leading to far more interesting conclusions.\nThere is only have data for a three year period; this is too small of a range to make a strong statement about the impact of homeless policy on Florida counties or how the relevance of certain stressors has changed over time. For a more in depth study I would begin with a 10 year range.\n\nResearch Question:\n\nDemographic breakdown of stressors’ impact (Age, Sex, Race)\nExtend the question to the entire country, providing a breakdown by state\nCompare to foreign countries to contrast governments’ approaches to homelessness and leading causes of homelessness around the world.\n\n\n\nLATER: Variable Definitions and Collection Methods here\n\n\nLater: Carol Zugazaga\n\n\n\n\n\n1.) Homeless Definition\n2.) US Interagency Council on Homelessness\n3.) Explanation of variables and collection method in Codebook tab\n4.) This data was cleaned and put in a tidy format in another script; manipulations were messy and inefficient (brute force) so I did not include the cleaning file."
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html",
    "href": "posts/HW1_EthanCampbell.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n(The distribution of LungCap looks as follows:)\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\n\n\n\n(Comparing lung cap by gender)\nHere we notice that males tend to have a higher lung cap compared to females. Females average tends to sit around 8 while males seems to sit closer to 9\n\n\nCode\nboxplot(df$LungCap~df$Gender)\n\n\n\n\n\n\n\n\n(smoker vs non-smoker lung cap)\nInterestingly, none smokers tend to have a lower lung capacity however, I believe this might be due to age. No this does not make sense at first glance and does betray my expectation.\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\n\n\n\n(relation between smoking and lung cap at different age groups)\nThe lung cap starts off higher but takes and dip then rises as the age continues to grow. I believe the trend is the higher age grows the higher the lung cap until it reaches a certain point.\n\n\nCode\n# lung cap is 9.62\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 13) %>%\n  colMeans()\n\n\n      Age   LungCap \n15.609290  9.628757 \n\n\nCode\n# lung cap is 9.04\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  colMeans()\n\n\n      Age   LungCap \n14.533333  9.045417 \n\n\nCode\n# lung cap is 10.24\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  colMeans()\n\n\n     Age  LungCap \n16.44330 10.24588 \n\n\nCode\n# lung cap is 11.26\ndf %>%\n  select(Age, LungCap) %>%\n  filter(Age > 18) %>%\n  colMeans()\n\n\n     Age  LungCap \n19.00000 11.26149 \n\n\n\n\n\n(lung cap for smokers and non smokers broken into age groups)\nWe notice a clear trend that smokers have a lower lung capacity compared to non-smokers\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 13) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.71\n2 yes    9.21\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 14 & Age <= 15) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age >= 16 & Age <= 17) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nCode\ndf %>%\n  select(Age, LungCap, Smoke) %>%\n  group_by(Smoke) %>%\n  filter(Age > 18) %>%\n  summarize_at(vars(LungCap), list(mean = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     11.3\n2 yes    11.3\n\n\n\n\n\n(correlation and covariance between lung capacity and age)\ncorrelation is at .819 meaning they have a positive correlation of about 82%. This means that there is a connection between the two and when one goes up so does the other.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#a-1",
    "href": "posts/HW1_EthanCampbell.html#a-1",
    "title": "Homework 1",
    "section": "2.a",
    "text": "2.a\nprobability of exactly 2 convictions probability = 19.7%\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  filter(X == 2)\n\n\n# A tibble: 1 × 3\n      X  Freq Probability\n  <dbl> <dbl>       <dbl>\n1     2   160       0.197"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#b-1",
    "href": "posts/HW1_EthanCampbell.html#b-1",
    "title": "Homework 1",
    "section": "2.b",
    "text": "2.b\nprobability of fewer than 2 convictions probability = 69.2%\n\n\nCode\nsum(df1$Probability[1:2])\n\n\n[1] 0.6921182"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#c-1",
    "href": "posts/HW1_EthanCampbell.html#c-1",
    "title": "Homework 1",
    "section": "2.c",
    "text": "2.c\nProbability of having 2 or fewer convictions probability = 88.9%\n\n\nCode\nsum(df1$Probability[1:3])\n\n\n[1] 0.8891626"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#d-1",
    "href": "posts/HW1_EthanCampbell.html#d-1",
    "title": "Homework 1",
    "section": "2.d",
    "text": "2.d\nprobability of having more than 2 convictions probability = 11.08%\n\n\nCode\nsum(df1$Probability[4:5])\n\n\n[1] 0.1108374"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#e-1",
    "href": "posts/HW1_EthanCampbell.html#e-1",
    "title": "Homework 1",
    "section": "2.e",
    "text": "2.e\nWhat is the expected value expected value is 1.29 convictions\n\n\nCode\ndf1 %>%\n  select(X, Freq, Probability) %>%\n  mutate(expected_value = (0*0.15763547)+(1*0.53448276)+(2*0.19704433)+(3*0.07881773)+(4*0.03201970))\n\n\n# A tibble: 5 × 4\n      X  Freq Probability expected_value\n  <dbl> <dbl>       <dbl>          <dbl>\n1     0   128      0.158            1.29\n2     1   434      0.534            1.29\n3     2   160      0.197            1.29\n4     3    64      0.0788           1.29\n5     4    26      0.0320           1.29"
  },
  {
    "objectID": "posts/HW1_EthanCampbell.html#f-1",
    "href": "posts/HW1_EthanCampbell.html#f-1",
    "title": "Homework 1",
    "section": "2.f",
    "text": "2.f\nWhat is the variance and standard deviation of the prior convictions Variance = 25810.8 standard deviation = 160.6574\n\n\nCode\nvar(df$Freq)\n\n\n[1] 25810.8\n\n\nCode\nsd(df$Freq)\n\n\n[1] 160.6574"
  },
  {
    "objectID": "posts/Final_Project_1.html",
    "href": "posts/Final_Project_1.html",
    "title": "Final_Project_1",
    "section": "",
    "text": "Research Question : examining the relationship between the maximum heart rate one can achieve during exercise and the likelihood of developing heart disease. Using multiple logistic regression, examining handle the confounding effects of age and gender.\nHypothesis Testing : Is there any statistical difference between the gender and age in terms of heart attack prediction.\n#Loading Dataset\n\n\nCode\nlibrary(readr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ tidyr   1.2.1      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nheart_cleveland_upload <- read_csv(\"heart_cleveland_upload.csv\")\n\n\nRows: 297 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpea...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(heart_cleveland_upload)\n\n\n\n\n  \n\n\n\n\n\nCode\ndim(heart_cleveland_upload)\n\n\n[1] 297  14\n\n\nData set contains 297 Columns and 14 columns\n\n\nCode\ncolnames(heart_cleveland_upload)\n\n\n [1] \"age\"       \"sex\"       \"cp\"        \"trestbps\"  \"chol\"      \"fbs\"      \n [7] \"restecg\"   \"thalach\"   \"exang\"     \"oldpeak\"   \"slope\"     \"ca\"       \n[13] \"thal\"      \"condition\"\n\n\nhere are 13 attributes\nage: age in years sex: sex (1 = male; 0 = female) cp: chest pain type – Value 0: typical angina – Value 1: atypical angina – Value 2: non-anginal pain – Value 3: asymptomatic trestbps: resting blood pressure (in mm Hg on admission to the hospital) chol: serum cholestoral in mg/dl fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) restecg: resting electrocardiographic results – Value 0: normal – Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) – Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria thalach: maximum heart rate achieved exang: exercise induced angina (1 = yes; 0 = no) oldpeak = ST depression induced by exercise relative to rest slope: the slope of the peak exercise ST segment – Value 0: upsloping – Value 1: flat – Value 2: downsloping ca: number of major vessels (0-3) colored by flourosopy thal: 0 = normal; 1 = fixed defect; 2 = reversable defect and the label condition: 0 = no disease, 1 = disease\n\nDescriptive statistics\n\n\nCode\nsummary(heart_cleveland_upload)\n\n\n      age             sex               cp           trestbps    \n Min.   :29.00   Min.   :0.0000   Min.   :0.000   Min.   : 94.0  \n 1st Qu.:48.00   1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:120.0  \n Median :56.00   Median :1.0000   Median :2.000   Median :130.0  \n Mean   :54.54   Mean   :0.6768   Mean   :2.158   Mean   :131.7  \n 3rd Qu.:61.00   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:140.0  \n Max.   :77.00   Max.   :1.0000   Max.   :3.000   Max.   :200.0  \n      chol            fbs            restecg          thalach     \n Min.   :126.0   Min.   :0.0000   Min.   :0.0000   Min.   : 71.0  \n 1st Qu.:211.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:133.0  \n Median :243.0   Median :0.0000   Median :1.0000   Median :153.0  \n Mean   :247.4   Mean   :0.1448   Mean   :0.9966   Mean   :149.6  \n 3rd Qu.:276.0   3rd Qu.:0.0000   3rd Qu.:2.0000   3rd Qu.:166.0  \n Max.   :564.0   Max.   :1.0000   Max.   :2.0000   Max.   :202.0  \n     exang           oldpeak          slope              ca        \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.800   Median :1.0000   Median :0.0000  \n Mean   :0.3266   Mean   :1.056   Mean   :0.6027   Mean   :0.6768  \n 3rd Qu.:1.0000   3rd Qu.:1.600   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :6.200   Max.   :2.0000   Max.   :3.0000  \n      thal         condition     \n Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000  \n Mean   :0.835   Mean   :0.4613  \n 3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :2.000   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(heart_cleveland_upload)\n\n\nRows: 297\nColumns: 14\n$ age       <dbl> 69, 69, 66, 65, 64, 64, 63, 61, 60, 59, 59, 59, 59, 58, 56, …\n$ sex       <dbl> 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n$ cp        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ trestbps  <dbl> 160, 140, 150, 138, 110, 170, 145, 134, 150, 178, 170, 160, …\n$ chol      <dbl> 234, 239, 226, 282, 211, 227, 233, 234, 240, 270, 288, 273, …\n$ fbs       <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ restecg   <dbl> 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, …\n$ thalach   <dbl> 131, 151, 114, 174, 144, 155, 150, 145, 171, 145, 159, 125, …\n$ exang     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ oldpeak   <dbl> 0.1, 1.8, 2.6, 1.4, 1.8, 0.6, 2.3, 2.6, 0.9, 4.2, 0.2, 0.0, …\n$ slope     <dbl> 1, 0, 2, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, …\n$ ca        <dbl> 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, …\n$ thal      <dbl> 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, …\n$ condition <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html",
    "href": "posts/Blog Post 2_Kaushika Potluri.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a-distribution-of-lungcap",
    "title": "Homework 1",
    "section": "1(a) Distribution of LungCap:",
    "text": "1(a) Distribution of LungCap:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe distribution appears to be very similar to a normal distribution, according to the histogram."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b",
    "title": "Homework 1",
    "section": "1(b)",
    "text": "1(b)\nThe boxplots below show the probability distributions grouped by Gender.\n\n\nCode\nboxplot(LungCap~Gender, data=df)\n\n\n\n\n\nLooks like males have a slightly higher lung capacity than females."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c",
    "title": "Homework 1",
    "section": "1 (c)",
    "text": "1 (c)\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nSurprisingly, the mean lung capacity is higher for smokers than it is for non-smokers."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d",
    "title": "Homework 1",
    "section": "1 (d)",
    "text": "1 (d)\n\n\nCode\n# convert Age to categorical variable.\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and lower\", Age == 14 | Age == 15 ~ \"14-15\", Age == 16 | Age == 17 ~ \"16-17\", Age >= 18 ~ \"18 and above\"))\narrange(df, Age)\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean AgeGroup    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <chr>       \n 1   5.88      3   55.9 no    male   no        13 and lower\n 2   0.507     3   51.6 no    female yes       13 and lower\n 3   1.18      3   51.9 no    male   no        13 and lower\n 4   4.7       3   52.7 no    male   no        13 and lower\n 5   5.48      3   52.9 no    male   no        13 and lower\n 6   1.02      3   47   no    female no        13 and lower\n 7   2         3   51   no    female no        13 and lower\n 8   1.68      3   51.9 no    male   no        13 and lower\n 9   4.08      3   53.6 no    male   yes       13 and lower\n10   1.45      3   45.3 no    female no        13 and lower\n# … with 715 more rows\n\n\nCode\n# construct histogram.\nggplot(df, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMajority seem to be non-smokers, and looks like non-smokers seem to have higher lung capacity."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e",
    "title": "Homework 1",
    "section": "1 (e)",
    "text": "1 (e)\n\n\nCode\nclass(df$AgeGroup)\n\n\n[1] \"character\"\n\n\n\n\nCode\ndf$AgeGroup <- as.factor(df$AgeGroup) #converting to factor\n\n# construct table.\ndf %>% select(Smoke, LungCap, AgeGroup) %>% group_by(AgeGroup, Smoke) %>% summarise(mean(LungCap))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup     Smoke `mean(LungCap)`\n  <fct>        <chr>           <dbl>\n1 13 and lower no               6.36\n2 13 and lower yes              7.20\n3 14-15        no               9.14\n4 14-15        yes              8.39\n5 16-17        no              10.5 \n6 16-17        yes              9.38\n7 18 and above no              11.1 \n8 18 and above yes             10.5 \n\n\nThe mean lung capacity for smokers aged 13 and under is greater than that of non-smokers in the same age group which is different from expectation. Non-smokers have higher mean lung capacity for ages 14-15, 16-17 and 18 and above. Either there may be an error or extreme outlier in the data for smokers aged 13 and under."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f",
    "title": "Homework 1",
    "section": "1 (f)",
    "text": "1 (f)\n\n\nCode\ncor(df$LungCap,df$Age)\n\n\n[1] 0.8196749\n\n\n\n\nCode\ncov(df$LungCap,df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity and age have a high positive correlation of 0.82, meaning that as age increases, lung capacity also does. The covariance is a little more challenging to interpret; the positive number indicates a positive association between lung capacity and age, but because covariance varies from negative infinity to infinity, it is difficult to judge the strength of the relationship. In most situations, I would choose to employ correlation."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#section",
    "title": "Homework 1",
    "section": "2",
    "text": "2\n\n\nCode\ndf1 <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(df1, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#a",
    "title": "Homework 1",
    "section": "2(a)",
    "text": "2(a)\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n# A tibble: 5 × 3\n    df1 Inmate_count Probability\n  <int>        <dbl>       <dbl>\n1     0          128      0.158 \n2     1          434      0.536 \n3     2          160      0.198 \n4     3           64      0.0790\n5     4           24      0.0296\n\n\n\n\nCode\nIP %>%\n  filter(df1 == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability is about 19.75%."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#b-1",
    "title": "Homework 1",
    "section": "(b)",
    "text": "(b)\n\n\nCode\ndf2 <- IP %>%\n  filter(df1 < 2)\nsum(df2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272"
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#c-1",
    "title": "Homework 1",
    "section": "2(c)",
    "text": "2(c)\n\n\nCode\ndf3 <- IP %>%\n  filter(df1 <= 2)\nsum(df3$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#d-1",
    "title": "Homework 1",
    "section": "2(d)",
    "text": "2(d)\n\n\nCode\ndf4 <- IP %>%\n  filter(df1 > 2)\nsum(df4$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#e-1",
    "title": "Homework 1",
    "section": "2(e)",
    "text": "2(e)\n\n\nCode\nIP <- mutate(IP, X = df1*Probability)\nexpectedvalue<- sum(IP$X)\nexpectedvalue\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.2864198. We can round this to 1."
  },
  {
    "objectID": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "href": "posts/Blog Post 2_Kaushika Potluri.html#f-1",
    "title": "Homework 1",
    "section": "2(f)",
    "text": "2(f)\n\n\nCode\nvar1 <-sum(((IP$df1-expectedvalue)^2)*IP$Probability)\nvar1\n\n\n[1] 0.8562353\n\n\n\n\nCode\nsqrt(var1)\n\n\n[1] 0.9253298\n\n\nThe variance and the standard deviation for prior convictions are 0.8562353 and 0.9253298 respectively."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html",
    "href": "posts/KarenDetter_HW3.html",
    "title": "HW3",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(alr4)\nlibrary(smss)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.1",
    "href": "posts/KarenDetter_HW3.html#q.1",
    "title": "HW3",
    "section": "Q.1",
    "text": "Q.1\n\n\nCode\n##load data\ndata(UN11)\n\n\n\n1.1.1\nIn this model, the predictor variable is ‘ppgdp’ ($ gross national product per person) and the response variable is ‘fertility’.\n\n\n1.1.2\n\n\nCode\n##draw scatterplot\nplot(x = UN11$ppgdp, y = UN11$fertility)\n\n\n\n\n\nThe relationship between fertility and ppgdp is not exactly linear because increasing gross national product only decreases birth rate until it nears about the 10,000 point; after that, the effect seems to disappear.\n\n\n1.1.3\n\n\nCode\n##draw scatterplot with logs of both variables\nplot(x = log(UN11$ppgdp), y = log(UN11$fertility))\n\n\n\n\n\nThe log transformation scatterplot shows a relationship that looks much closer to that of the linear regression model."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.2",
    "href": "posts/KarenDetter_HW3.html#q.2",
    "title": "HW3",
    "section": "Q.2",
    "text": "Q.2\n\n(a)\nConverting the currency from American dollars to British pounds causes the mean of the x-axis (explanatory variable) to increase while the mean of the y-axis (response variable) remains the same. As a result, the prediction equation line becomes less steep, as each value of x is increased for the identical corresponding y-value.\n\n\n(b)\nThe currency conversion would not change the correlation, as the relative values of the variables remain unchanged."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.3",
    "href": "posts/KarenDetter_HW3.html#q.3",
    "title": "HW3",
    "section": "Q.3",
    "text": "Q.3\n\n\nCode\n##load data\ndata(water)\n##draw scatterplot matrix\npairs(water)\n\n\n\n\n\nThese scatterplots show that when precipitation at OPBPC, OPRC, and OPSLAKE increases, the runoff volume at BSAAM goes up. Precipitation at the other three locations does not seem to have a strong linear relationship with stream runoff volume.\nAlso, precipitation rates at the first three sites seem to be somewhat intercorrelated, as do the rates at the last three sites, indicating that the sites in each set may be closer to each other or share similar geographic features."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.4",
    "href": "posts/KarenDetter_HW3.html#q.4",
    "title": "HW3",
    "section": "Q.4",
    "text": "Q.4\n\n\nCode\n##load data\ndata(\"Rateprof\")\n##draw scatterplot matrix of selected variables\npairs(~Rateprof$quality+Rateprof$helpfulness+Rateprof$clarity+Rateprof$easiness+Rateprof$raterInterest, lwd=2, labels = c(\"QUALITY\", \"HELPFULNESS\", \"CLARITY\", \"EASINESS\", \"Rater INTEREST\"), pch=19, cex = 0.75, col = \"blue\")\n\n\n\n\n\nSurprisingly, it doesn’t seem that reviewers’ ratings of their interest in the subject or the easiness of the course correlate with ratings of the professor’s quality, helpfulness, or clarity. Ratings for those three traits, however, all seem to have linear relationships with each other."
  },
  {
    "objectID": "posts/KarenDetter_HW3.html#q.5",
    "href": "posts/KarenDetter_HW3.html#q.5",
    "title": "HW3",
    "section": "Q.5",
    "text": "Q.5\n\n\nCode\n##load data\ndata(student.survey)\n\n\n\n(i)\n\n\nCode\n##convert factor variables to numeric\npi_conv <- as.numeric(student.survey$pi)\nre_conv <- as.numeric(student.survey$re)\n##run regression analysis\nmodel1 <- lm(pi_conv ~ re_conv, data = student.survey)\nsummary(model1)\n\n\n\nCall:\nlm(formula = pi_conv ~ re_conv, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.81243 -0.87160  0.09882  1.12840  3.09882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9308     0.4252   2.189   0.0327 *  \nre_conv       0.9704     0.1792   5.416 1.22e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.345 on 58 degrees of freedom\nMultiple R-squared:  0.3359,    Adjusted R-squared:  0.3244 \nF-statistic: 29.34 on 1 and 58 DF,  p-value: 1.221e-06\n\n\n\n\n(ii)\n\n\nCode\n##run regression analysis\nmodel2 <- lm(hi ~ tv, data = student.survey)\nsummary(model2)\n\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\n\n(a) & (b)\n\n\nCode\n##visualize relationships in the two models with scatterplots\n##include regression lines of coefficients\n##use jitter plots due to small sample size of 60\nggplot(data = student.survey, aes(x = re, y = pi)) +\n  geom_jitter(color = \"blue\") +\n    geom_abline(intercept = .9308, slope = .9704) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe first regression model shows evidence of a strong, statistically significant effect of religiousness on political ideology, with the p-value of .00000122 being well below the significance threshold of .05. As the level of religiousness increases, political ideology becomes more conservative, with religiousness explaining 34% of the variance in ideology. Because of the small number of observations (n=60), scatterplot points do not appear tightly aligned to the regression line, but there is a clear upward-moving trend.\n\n\nCode\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_jitter(color = \"blue\") +\n  geom_abline(intercept = 3.441353, slope = -0.018305) +\n  geom_smooth(method = 'lm')\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThe effect of hours of tv watched on grade point average is not very strong, with the p-value of .0388 being just below the significance threshold. The relationship between the variables is inverse - mean gpa decreases by .02 for every increase in hours of tv watched. Hours of tv watched per week explain 7% of the variance in grade point averages. The scatterplot, again affected by small sample size, does show a slight trend of gpa decreasing as tv level increases."
  },
  {
    "objectID": "posts/HW 1 DACSS.html",
    "href": "posts/HW 1 DACSS.html",
    "title": "Graphs and Probz",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ readr   2.1.2     ✓ stringr 1.4.0\n✓ purrr   0.3.4     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\n\nlibrary(readxl)\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)                                                              \n\n\nm_lung<-mean(LungCapData$LungCap)\nsd_lung<-sd(LungCapData$LungCap)\n\nhist(LungCapData$LungCap, prob= TRUE, xlim = c(0, 20))\ncurve(dnorm(x, m_lung, sd_lung), add= TRUE,lwd= 2,col= \"blue\")"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-5",
    "href": "posts/HW 1 DACSS.html#question-5",
    "title": "Graphs and Probz",
    "section": "Question 5",
    "text": "Question 5\nDoesnt look like its good being a smoker under the age of 18, or any age. Lung capacity is smaller for these groups\n\nggplot(LungCapData, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()"
  },
  {
    "objectID": "posts/HW 1 DACSS.html#question-6",
    "href": "posts/HW 1 DACSS.html#question-6",
    "title": "Graphs and Probz",
    "section": "Question 6",
    "text": "Question 6\n\ncovar<-cov(LungCapData$LungCap, LungCapData$Age)\nprint(covar)\n\n[1] 8.738289\n\ncorre<-cor(LungCapData$LungCap, LungCapData$Age, method = \"pearson\")\nprint(corre)\n\n[1] 0.8196749"
  },
  {
    "objectID": "posts/HW2_CalebHill.html",
    "href": "posts/HW2_CalebHill.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s load the relevant libraries.\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nFor question 1, we need to construct the 90% confident interval to estimate the actual mean wait time for eahc of the two procedures.\n\n\nCode\ns_mean_b <- 19\ns_sd_b <- 10\ns_size_b <- 539\nstandard_error_b <- s_sd_b / sqrt(s_size_b)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_b <- qt(p = 1 - tail_area, df = s_size_b - 1)\nCI_b <- c(s_mean_b - t_score_b * standard_error_b,\n        s_mean_b + t_score_b * standard_error_b)\nprint(CI_b)\n\n\n[1] 18.29029 19.70971\n\n\nThis is the CI for bypass. The following code chunk is for angiography.\n\n\nCode\ns_mean_a <- 18\ns_sd_a <- 9\ns_size_a <- 847\nstandard_error_a <- s_sd_a / sqrt(s_size_a)\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\nt_score_a <- qt(p = 1 - tail_area, df = s_size_a - 1)\nCI_a <- c(s_mean_a - t_score_a * standard_error_a,\n        s_mean_a + t_score_a * standard_error_a)\nprint(CI_a)\n\n\n[1] 17.49078 18.50922\n\n\nIs the confidence interval narrower for angiograpy or bypass survey? Answer = angiography."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a",
    "href": "posts/HW2_CalebHill.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\n\n\nCode\ns_mean_4a <- 410\ns_sd_4a <- 90\ns_size_4a <- 9\nstandard_error_4a <- s_sd_4a / sqrt(s_size_4a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\nCI_4a <- c(s_mean_4a - t_score_4a * standard_error_4a,\n        s_mean_4a + t_score_4a * standard_error_4a)\nprint(CI_4a)\n\n\n[1] 340.8199 479.1801\n\n\nBased upon the data provided, we can be within a 95% CI that mean income for female employees is less than $500 per week. If Ha : μ < 500, then we can accept the hypothesis, based upon the CI. However, for section B, we’ll report the P-value via the t-score."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b",
    "href": "posts/HW2_CalebHill.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value=pt(q = t_score_4a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.025\n\n\nWith a P-value of 0.025, we can accept the Ha : μ < 500. However, let’s change the lower.tail value to TRUE to see about Ha : μ > 500."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c",
    "href": "posts/HW2_CalebHill.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\n\n\nCode\nt_score_4a <- qt(p = 1 - tail_area, df = s_size_4a - 1)\np_value = pt(q = t_score_4a, df = 8, lower.tail = TRUE)\nprint(p_value)\n\n\n[1] 0.975\n\n\nJust as I thought. We have to reject the second hypothesis, that Ha : μ > 500, as the P-value is 0.975, outside of statistical significance minimum of 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#a-1",
    "href": "posts/HW2_CalebHill.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nFor Jones:\n\n\nCode\ns_mean_5a <- 519.5\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 499.8766 539.1234\n\n\nFor Smith:\n\n\nCode\ns_mean_5a <- 519.7\nstandard_error_5a <- 10\ns_size_5a <- 1000\ns_sd_5a <- standard_error_5a * sqrt(s_size_5a)\nconfidence_level <- 0.95\ntail_area <- (1-confidence_level)/2\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\nprint(t_score_5a)\n\n\n[1] 1.962341\n\n\nCode\nt_score_5a <- qt(p = 1 - tail_area, df = s_size_5a - 1)\np_value = pt(q = t_score_5a, df = 8, lower.tail = FALSE)\nprint(p_value)\n\n\n[1] 0.04267427\n\n\nCode\nCI_5a <- c(s_mean_5a - t_score_5a * standard_error_5a,\n        s_mean_5a + t_score_5a * standard_error_5a)\nprint(CI_5a)\n\n\n[1] 500.0766 539.3234"
  },
  {
    "objectID": "posts/HW2_CalebHill.html#b-1",
    "href": "posts/HW2_CalebHill.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nCode for Section B are the P-values shown for each code chunk. Are they statistically significant? At 0.043 for both, yes as they are below 0.05."
  },
  {
    "objectID": "posts/HW2_CalebHill.html#c-1",
    "href": "posts/HW2_CalebHill.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nThe P-value is the likelihood of finding the particular set of observations if the null hypothesis were true. As the P-value is traditionally use in frequentist statistics, we are only able to ascribe probability to this specific set of observations – which are themselves a set amount of observations.\nTherefore, it can sometimes be misleading to report a P-value as 0.05. CI levels allow a range within the set of observations. We can see this problem best with the above results via Jones and Smith. They do not get the same sample mean, even with similar observations."
  },
  {
    "objectID": "posts/ShoshanaBuck- HW1.html",
    "href": "posts/ShoshanaBuck- HW1.html",
    "title": "ShoshanaBuck-HW1",
    "section": "",
    "text": "Question 1\nUse the LungCapData to answer the following questions.\n\n\nCode\n# read in data\nlung_cap<- read_xls(\"_data/LungCapData.xls\")\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\na. What does the distribution of LungCap look like?\n\n\nCode\n#plot histogram\nhist(lung_cap$LungCap)\n\n\n\n\n\nThe histogram shows that the distribution is pretty close to a normal distribution, with an almost a bell shaped curve. Meaning that the data near the mean are more of a frequent occurrence which is true because there are fewer observations near the margins.\n\n\nb. compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\n#Box plot\nggplot(lung_cap, aes (Gender,LungCap)) + geom_boxplot()\n\n\n\n\n\nThe box plot shows that male’s have a slightly higher lung capacity than females. Female’s have more values in the first quartile range and a lower minimum value than male’s. On the other hand male’s have a higher max value and more values in the 3rd quartile range.\n\n\nc. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlung_cap %>% \n  group_by(Smoke) %>% \n  summarise(avg_lung_cap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke avg_lung_cap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nThis does not make sense. Smokers have a higher sample mean than non-smokers which intuitively does not make sense because we would assume non-smokers would have a higher lung capacity.\n\n\nd. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#categorical variable of age_groups \ndf<- lung_cap %>% \ngroup_by(Smoke,LungCap) %>% \n  summarise(age_group = case_when(Age<=13 ~ \"13 & under\",Age ==14 | Age == 15 ~ \"14 to 15\",Age== 16 | Age == 17 ~ \"16 to 17\", Age>=18 ~ \"18 & older\")) \n\n\n`summarise()` has grouped output by 'Smoke', 'LungCap'. You can override using\nthe `.groups` argument.\n\n\nCode\n#mean of lung capacity with new variable\ndf %>% \n  group_by(Smoke, age_group) %>% \n  summarise(avg_lung_cap = mean(LungCap)) %>% \n  arrange(desc(avg_lung_cap))\n\n\n`summarise()` has grouped output by 'Smoke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Smoke [2]\n  Smoke age_group  avg_lung_cap\n  <chr> <chr>             <dbl>\n1 no    18 & older        11.1 \n2 yes   18 & older        10.5 \n3 no    16 to 17          10.5 \n4 yes   16 to 17           9.38\n5 no    14 to 15           9.14\n6 yes   14 to 15           8.39\n7 yes   13 & under         7.20\n8 no    13 & under         6.36\n\n\nCode\n#histogram\nggplot(df, aes(x = LungCap)) + facet_grid(Smoke ~age_group) +geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nUsing the package ggplot I used the function facet_grids to show a good visualization of the lung capacity between non-smokers and smokers within each age group. Looking at the histograms all age_groups that are non-smokers have a higher sample mean proving that non-smokers have\n\n\ne. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\n# Mean of non-smokers 13 & younger\ndf %>% \n  filter(Smoke == 'no' & age_group == '13 & under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 6.358746\n\n\nCode\n# Mean of smokers 13 & younger\ndf %>% \n  filter(Smoke == 'yes' & age_group == '13 &under') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] NaN\n\n\nCode\n#Mean of non-smokers 14 to 15\ndf %>% \n  filter(Smoke == 'no' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.13881\n\n\nCode\n#Mean of smokers 14 to 15\ndf %>% \n  filter(Smoke == 'yes' & age_group == '14 to 15') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 8.391667\n\n\nCode\n#Mean of non-smokers 16 to 17\ndf %>% \n  filter(Smoke == 'no' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.46981\n\n\nCode\n#Mean of smokers 16 to 17\ndf %>% \n  filter(Smoke == 'yes' & age_group == '16 to 17') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 9.38375\n\n\nCode\n#Mean of non-smokers 18 & older\ndf %>% \n  filter(Smoke == 'no' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 11.06885\n\n\nCode\n# Mean of smokers 18 & older\ndf %>% \n  filter(Smoke == 'yes' & age_group == '18 & older') %>% \n  pull(LungCap) %>% \n  mean()\n\n\n[1] 10.51333\n\n\nCode\nlung_cap\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n\n\nf. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\n#Correlation\ncor(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#Covariance\ncov(lung_cap$LungCap, lung_cap$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is 0.81 which is pretty close to +1, meaning that the there is a positive relationship between lung capacity and age. The covariance is also high which shows that the two variables of lung capacity and age have a positive relationship. #2 Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n# x= prior convictions \nx<-c(0, 1, 2, 3, 4)\nfrequency<-c(128, 434, 160, 64, 24)\nstate_prison<- data_frame(x,frequency)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nstate_prison\n\n\n# A tibble: 5 × 2\n      x frequency\n  <dbl>     <dbl>\n1     0       128\n2     1       434\n3     2       160\n4     3        64\n5     4        24\n\n\n\n\na. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n# 2 prior convictions.P(2)/total\n160/810\n\n\n[1] 0.1975309\n\n\nThere is a 1.9% probability that a randomly selected inmate has exactly 2 prior convictions.\n\n\nb. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#  less than 2 prior convictions. (P(0) + P(1))/total \n(128+434)/810\n\n\n[1] 0.6938272\n\n\nThere is a 6.9% probability that a randomly selected inmate has fewer than 2 prior convictions.\n\n\nc. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n# 2 or fewer prior convictions. (P(0) + P(1) + P(2)) +total\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nThere is a 8.9% probability that a randomly selected inmate has 2 or fewer prior convictions.\n\n\nd. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n# More than 2 prior convictions. (P(3) +P(4)) + total\n(64+24)/810\n\n\n[1] 0.108642\n\n\nThere is a 10.8% probability that a randomly selected inmate has more than 2 prior convictions.\n\n\ne. What is the expected value for the number of prior convictions?\n\n\nCode\n#Prior convictions. ((0*P(0)) +(1*(P(1)) + (2*P(2)) + (3*P(3)) + (4*P(4)))\ndf<-((128*0/810) +(434*1/810) +(160*2/810) +(64*3/810) +(24*4/810)) \nmean(df)\n\n\n[1] 1.28642\n\n\nThe expected value for number of prior convictions is 1.28\n\n\nf. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n# variance\nv<- ((0-1.28)^2) *(128/810) +((1-1.28)^2) * (434/810)+((2-1.28)^2) * (160/810)+((3-1.28)^2) *(64/810) +((4-1.28)^2) * (24/810)\nv\n\n\n[1] 0.8562765\n\n\nCode\n# standard deviation\nsd<- sqrt(v)\nsd\n\n\n[1] 0.9253521\n\n\nThe variance is 0.856 and the standard deviation is 0.925."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html",
    "href": "posts/RahulGundeti_DACSS603_HW1.html",
    "title": "DACSS603_HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-1",
    "title": "DACSS603_HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-data",
    "title": "DACSS603_HW1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nlung <- read_excel(\"C:/Users/gunde/Downloads/LungCapData.xls\")\n\n\nError: `path` does not exist: 'C:/Users/gunde/Downloads/LungCapData.xls'\n\n\nCode\nlung\n\n\nError in eval(expr, envir, enclos): object 'lung' not found\n\n\nThe Lung Capacity data contains 725 rows and 6 columns that determine age, height etc., The key classification parameter is based on smoker vs non-smoker."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a",
    "title": "DACSS603_HW1",
    "section": "1_A",
    "text": "1_A\nThe distribution of LungCap looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 40, color = \"red\") +\n  geom_density(color = \"green\") +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution\", x = \"Lung Capcity\", y = \"Probability Density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'lung' not found\n\n\nThe observations plotted by histogram are closer to mean which suggests that it is a normal distribution."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b",
    "title": "DACSS603_HW1",
    "section": "1_B",
    "text": "1_B\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nlung %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"LungCap Probability Distribution based on gender\", y = \"Probability Density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'lung' not found\n\n\nThe box plot shows that the probability density of the male < female."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c",
    "title": "DACSS603_HW1",
    "section": "1_C",
    "text": "1_C\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- lung %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'lung' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nThe table contains the mean lung capacity. The observations suggest that the mean value is higher for smokers than non-smokers. This isn’t entirely correct as the individual biological factors plays a main role. So the data is inadequate to form an opinion."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d",
    "title": "DACSS603_HW1",
    "section": "1_D",
    "text": "1_D\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nlung <- mutate(lung, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(lung, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'lung' not found\n\n\nCode\nlung %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'lung' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non-smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e",
    "title": "DACSS603_HW1",
    "section": "1_E",
    "text": "1_E\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nlung %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'lung' not found\n\n\nComparing 1_D and 1_E we can find similarity which points that only 10 and above age group smoke."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f",
    "title": "DACSS603_HW1",
    "section": "1_F",
    "text": "1_F\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(lung$LungCap, lung$Age)\n\n\nError in is.data.frame(y): object 'lung' not found\n\n\nCode\nCorrelation <- cor(lung$LungCap, lung$Age)\n\n\nError in is.data.frame(y): object 'lung' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nThe comparison shows that the covariance is positive, indicating that lung capacity and age have a direct relationship. As a result, they are moving in the same direction due to the positive correlation as well. This means that as age increases, lung capacity increases as well, which means they are directly proportional."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#question-2",
    "title": "DACSS603_HW1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#reading-the-table",
    "title": "DACSS603_HW1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nprior <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nprior\n\n\n\n\n  \n\n\n\n\n\nCode\nprior <- mutate(prior, Probability = Inmate_count/sum(Inmate_count))\nprior"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#a-1",
    "title": "DACSS603_HW1",
    "section": "2_A",
    "text": "2_A\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nprior %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#b-1",
    "title": "DACSS603_HW1",
    "section": "2_B",
    "text": "2_B\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions < 2)\nsum(random$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#c-1",
    "title": "DACSS603_HW1",
    "section": "2_C",
    "text": "2_C\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions <= 2)\nsum(random$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#d-1",
    "title": "DACSS603_HW1",
    "section": "2_D",
    "text": "2_D\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\nrandom <- prior %>%\n  filter(Prior_convitions > 2)\nsum(random$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#e-1",
    "title": "DACSS603_HW1",
    "section": "2_E",
    "text": "2_E\nExpected value for the number of prior convictions:\n\n\nCode\nprior <- mutate(prior, Wm = Prior_convitions*Probability)\nev <- sum(prior$Wm)\nev\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "href": "posts/RahulGundeti_DACSS603_HW1.html#f-1",
    "title": "DACSS603_HW1",
    "section": "2_F",
    "text": "2_F\nVariance for the Prior Convictions:\n\n\nCode\nvariance <-sum(((prior$Prior_convitions-ev)^2)*prior$Probability)\nvariance\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW3_EthanCampbell.html",
    "href": "posts/HW3_EthanCampbell.html",
    "title": "Homework 3",
    "section": "",
    "text": "Question 2\nAnnual income, in dollars, is an explanatory variable in a regression analysis. For a British version of the report on the analysis, all responses are converted to British pounds sterling (1 pound equals about 1.33 dollars, as of 2016).\n\n(a)\nHow, if at all, does the slope of the prediction equation change?\nYes, the slope will change by 1.33 since this is rate at which it is changing and the conversion between the two values. The US version will increase by 1.33 times compared to the British version.\n\n\n(b)\nHow, if at all, does the correlation change?\nThe correlation should not change as the scale changes in relation to the amount.\n\n\n\nQuestion 3\nWater runoff in the Sierras (Data file: water in alr4) Can Southern California’s water supply in future years be predicted from past data? One factor affecting water availability is stream runoff. If runoff could be predicted, engineers, planners, and policy makers could do their jobs more efficiently. The data file contains 43 years’ worth of precipitation measurements taken at six sites in the Sierra Nevada mountains (labeled APMAM, APSAB, APSLAKE, OPBPC, OPRC, and OPSLAKE) and stream runoff volume at a site near Bishop, California, labeled BSAAM. Draw the scatterplot matrix for these data and summarize the information available from these plots. (Hint: Use the pairs() function.)\n\ndata(water)\n\nhead(water)\n\n  Year APMAM APSAB APSLAKE OPBPC  OPRC OPSLAKE  BSAAM\n1 1948  9.13  3.58    3.91  4.10  7.43    6.47  54235\n2 1949  5.28  4.82    5.20  7.55 11.11   10.26  67567\n3 1950  4.20  3.77    3.67  9.52 12.20   11.35  66161\n4 1951  4.60  4.46    3.93 11.14 15.15   11.13  68094\n5 1952  7.15  4.99    4.88 16.34 20.05   22.81 107080\n6 1953  9.70  5.65    4.91  8.88  8.15    7.41  67594\n\n# here is the pairs of each variable and we notice some correlation however, it is really hard to get a closer look here. \npairs(water,\n      bg = 'blue')\n\n\n\n# creating the regression to look closer \nbsaam_water <- lm(BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + OPSLAKE, data = water)\n\n# summary\nsummary(bsaam_water)\n\n\nCall:\nlm(formula = BSAAM ~ APMAM + APSAB + APSLAKE + OPBPC + OPRC + \n    OPSLAKE, data = water)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12690  -4936  -1424   4173  18542 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 15944.67    4099.80   3.889 0.000416 ***\nAPMAM         -12.77     708.89  -0.018 0.985725    \nAPSAB        -664.41    1522.89  -0.436 0.665237    \nAPSLAKE      2270.68    1341.29   1.693 0.099112 .  \nOPBPC          69.70     461.69   0.151 0.880839    \nOPRC         1916.45     641.36   2.988 0.005031 ** \nOPSLAKE      2211.58     752.69   2.938 0.005729 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7557 on 36 degrees of freedom\nMultiple R-squared:  0.9248,    Adjusted R-squared:  0.9123 \nF-statistic: 73.82 on 6 and 36 DF,  p-value: < 2.2e-16\n\n\n\nSolution\nHere we see that there are two variables that have a statistically significant relationship with BSAAM. These are OPRC and OPSLAKE each less than .05 however, questions regarding multicollinarity arise with this strong corrleation. The other variables such as the AP variables seem to also be correlated however, it does not appear as strongly as the two before. When looking at the range of residuals we notice a very large difference and this indicates that there may be some large and small outliers. This will effect the bests fitted line and lead to less robust analysis. When we are looking at the R^2 we notice it is very high which means it is fairly well fitted as it is close to 1.00.\n\n\n\nQuestion 4\nProfessor ratings (Data file: Rateprof in alr4) In the website and online forum RateMyProfessors.com, students rate and comment on their instructors. Launched in 1999, the site includes millions of ratings on thousands of instructors. The data file includes the summaries of the ratings of 364 instructors at a large campus in the Midwest (Bleske-Rechek and Fritsch, 2011). Each instructor included in the data had at least 10 ratings over a several year period. Students provided ratings of 1–5 on quality, helpfulness, clarity, easiness of instructor’s courses, and raterInterest in the subject matter covered in the instructor’s courses. The data file provides the averages of these five ratings. Create a scatterplot matrix of these five variables. Provide a brief description of the relationships between the five ratings.\n\ndata(Rateprof)\nhead(Rateprof)\n\n  gender numYears numRaters numCourses pepper discipline              dept\n1   male        7        11          5     no        Hum           English\n2   male        6        11          5     no        Hum Religious Studies\n3   male       10        43          2     no        Hum               Art\n4   male       11        24          5     no        Hum           English\n5   male       11        19          7     no        Hum           Spanish\n6   male       10        15          9     no        Hum           Spanish\n   quality helpfulness  clarity easiness raterInterest sdQuality sdHelpfulness\n1 4.636364    4.636364 4.636364 4.818182      3.545455 0.5518564     0.6741999\n2 4.318182    4.545455 4.090909 4.363636      4.000000 0.9020179     0.9341987\n3 4.790698    4.720930 4.860465 4.604651      3.432432 0.4529343     0.6663898\n4 4.250000    4.458333 4.041667 2.791667      3.181818 0.9325048     0.9315329\n5 4.684211    4.684211 4.684211 4.473684      4.214286 0.6500112     0.8200699\n6 4.233333    4.266667 4.200000 4.533333      3.916667 0.8632717     1.0327956\n  sdClarity sdEasiness sdRaterInterest\n1 0.5045250  0.4045199       1.1281521\n2 0.9438798  0.5045250       1.0744356\n3 0.4129681  0.5407021       1.2369438\n4 0.9990938  0.5882300       1.3322506\n5 0.5823927  0.6117753       0.9749613\n6 0.7745967  0.6399405       0.6685579\n\nRates <- Rateprof %>%\n  select(quality, helpfulness, clarity, easiness, raterInterest)\n\npairs(Rates)\n\n\n\n\n\nSolution\nHere we see a vary of different strengths of correlations these will be discussed one by one. These range from very weak correlation to very strong correlation which is interesting to see. These are all linear and positive.\nQuality ~ Helpfulness - Here we notice a strong correlation as the data is very linear. This data is also showing a very strong correlation in a positive direction.\nQuality ~ Clarity- Here we notice a strong correlation that is positive and linear.\nQuality ~ Easiness- Here we notice a weaker correlation compared to the last too but still a positive linear correlation.\nQuality ~ RaterInterest- Here we notice a weak correlation but still a positive linear correlation.\nEasiness ~ RaterInterst - Shows a very flat line which shows a very weak correlation.\nClarity ~ Easiness- Shows a weak linear correlation.\n\n\n\nQuestion 5\nFor the student.survey data file in the smss package, conduct regression analyses relating (by convention, y denotes the outcome variable, x denotes the explanatory variable) (i) y = political ideology and x = religiosity, (ii) y = high school GPA and x = hours of TV watching.\n(You can use ?student.survey in the R console, after loading the package, to see what each variable means.)\n\ndata(student.survey)\nSS <- student.survey\nhead(SS)\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\n# getting the data that we will be working with and checking to see which each variable means\n#?student.survey\n\nSS <- SS %>%\n  select(hi, tv, pi, re)\n\n\n(a) Graphically portray how the explanatory variable relates to the outcome variable in each of the two cases\n\n# plotting the comparison between political ideology and religiosity\nSS_plot <- plot(pi~re, data = SS)\n\n\n\n# plotting the comparison between high school gpa and hours of tv watching\n\nggplot(data = SS, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n# making a table comparing these variabels\nxtabs(~pi+re, SS)\n\n                       re\npi                      never occasionally most weeks every week\n  very liberal              3            5          0          0\n  liberal                   8           14          1          1\n  slightly liberal          2            1          1          2\n  moderate                  1            8          1          0\n  slightly conservative     1            1          2          2\n  conservative              0            0          2          2\n  very conservative         0            0          0          2\n\nsummary(SS)\n\n       hi              tv                             pi                re    \n Min.   :2.000   Min.   : 0.000   very liberal         : 8   never       :15  \n 1st Qu.:3.000   1st Qu.: 3.000   liberal              :24   occasionally:29  \n Median :3.350   Median : 6.000   slightly liberal     : 6   most weeks  : 7  \n Mean   :3.308   Mean   : 7.267   moderate             :10   every week  : 9  \n 3rd Qu.:3.625   3rd Qu.:10.000   slightly conservative: 6                    \n Max.   :4.000   Max.   :37.000   conservative         : 4                    \n                                  very conservative    : 2                    \n\n\n\n\n(b)\nSummarize and interpret results of inferential analyses.\nHere we notice that the more conservative you are the more likely you are to visit religious service on more occasions. However, this is not a very significant trend and it is hard to say with the graph alone. When looking into the xtabs comparing these two results we notice a similar shift there tends to be a higher number of liberals in the never and occasionally sections and then as it gets into most weeks and every week that number drops off quickly and slowly increases on the conservative side.\nWe are also notice a very clear correlation with the hours of watching tv and the highschool gpa. Here we notice that the more hours spent watching tv the less likelu you are to have a higher gpa. There is a negative slope as it drops off between the 10-15 hour mark."
  },
  {
    "objectID": "posts/FinalPart1_StephRoberts.html",
    "href": "posts/FinalPart1_StephRoberts.html",
    "title": "finalpart1",
    "section": "",
    "text": "Are Women and Racial minorities underrepresented in STEM fields (Study & Career)? A predictive analysis of the likelihood of STEM careers.\n\n\nWomen are significantly underrepresented in STEM (science, technology, engineering, and mathematics) fields in the USA, making up less than a quarter of those working in STEM occupations (Noonan, [2017](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR13 “Noonan, R. Women in STEM: 2017 update (ESA Issue Brief #06-17). Office of the Chief Economist, Economics and Statistics Administration, U.S. Department of Commerce (November 13, 2017). Retrieved from https://www.commerce.gov/news/fact-sheets/2017/11/women-stem-2017-update\n“); Ong, Smith, & Ko, [2018](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR15”Ong, M., Smith, J. M., & Ko, L. T. (2018). Counterspaces for women of color in STEM higher education: marginal and central spaces for persistence and success. Journal of Research in Science Teaching, 55(2), 206–245. https://doi.org/10.1002/tea.21417\n.”)).\nRepresentation of women of color is even lower, with Hispanic, Asian, and African American women each receiving less than 5% of STEM bachelor's degrees in the USA (Ong et al., [2018](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00219-2#ref-CR15 “Ong, M., Smith, J. M., & Ko, L. T. (2018). Counterspaces for women of color in STEM higher education: marginal and central spaces for persistence and success. Journal of Research in Science Teaching, 55(2), 206–245. https://doi.org/10.1002/tea.21417\n.”);\nBy the time students reach college, women are significantly underrepresented in STEM majors — for instance, only around 21% of engineering majors are women and only around 19% of computer and information science majors are women.https://www.aauw.org/resources/research/the-stem-gap/\nThe fact that women and racial minorities are still discriminated and underrepresented in the STEM in the 21st century while mankind is stepping foot on other planets is a topic to be given a serious thought.\nThe above mentioned articles are my motivation to perform this analysis in addition to the 2011 survey by US Department of Commerce showing that women and racial minorities are underrpresented in stem fields in two ways: They represent a disproportionatly small percentage of STEM degree holders, as well as STEM workers. These reports are linked below:\n\n“Women in STEM: A Gender Gap to Innovation”\n“Education Supports Racial and Ethnic Equality in STEM”\n\nThe goal of this project is to build a model to predict likelihood of working in a STEM (Science, Technology, Engineering, and Math) career based on basic demographics: Age, sex, race, state of origin."
  },
  {
    "objectID": "posts/FinalPart1_StephRoberts.html#hypothesis",
    "href": "posts/FinalPart1_StephRoberts.html#hypothesis",
    "title": "finalpart1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\nMy hypothesis: Women and Racial minorities are underrepresented in STEM fields.\nThe above mentioned hypothesis has been tested and proved by many researchers and government survey analysis already. Bus i wish to perform this study again by modifying it by developing regression models to resume the likelihood of STEM careers.\n---\n\nLoading the libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)\n\n\n\n\nReading the raw data\n\n\nCode\npop <- read.csv(\"C:/Users/91955/Desktop/603_Fall_2022/ss13pusa.csv\")\n\n\nWarning in file(file, \"rt\"): cannot open file 'C:/Users/91955/Desktop/\n603_Fall_2022/ss13pusa.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nhead(pop)\n\n\nError in head(pop): object 'pop' not found"
  },
  {
    "objectID": "posts/FinalPart1_StephRoberts.html#descriptive-statistics",
    "href": "posts/FinalPart1_StephRoberts.html#descriptive-statistics",
    "title": "finalpart1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nCode\nsummary(pop)\n\n\nError in summary(pop): object 'pop' not found\n\n\nCode\nnrow(pop)\n\n\nError in nrow(pop): object 'pop' not found\n\n\nCode\nncol(pop)\n\n\nError in ncol(pop): object 'pop' not found\n\n\nCode\nglimpse(pop)\n\n\nError in glimpse(pop): object 'pop' not found\n\n\n\n\nThe data has 1613672 rows and 238 columns. The variables I am interested in are AGEP, SEX, HISP, POBP, RAC1P, SCIENGP, SOCP.\nThe data needs cleaning and rearranging the columns and rows."
  },
  {
    "objectID": "posts/HW1_SteveONeill.html",
    "href": "posts/HW1_SteveONeill.html",
    "title": "Homework 1",
    "section": "",
    "text": "Question 2\nI will make a dataframe from the values provided:\n\n\nCode\nprior_convictions=c(0,1,2,3,4)\nfreq=c(128, 434, 160, 64, 24)\nprisondata <- data.frame(prior_convictions, freq)\nprisondata\n\n\n  prior_convictions freq\n1                 0  128\n2                 1  434\n3                 2  160\n4                 3   64\n5                 4   24\n\n\nAnd add a probability column:\n\n\nCode\nprison_prob <- prisondata %>% mutate(prob = freq/sum(freq))\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\n\n2a.\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\nFrom the table above, the probability is 0.19753086, nearly 20 percent.\n\n\n2b.\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\nhead(prison_prob,2) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.6938272\n\n\nThe probability a randomly selected inmate has has fewer than 2 prior convictions is ~69%.\n\n\n2c.\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\nhead(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1  0.891358\n\n\nThe probability a randomly selected inmate has 2 or fewer convictions is ~89%\n\n\n2d.\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\ntail(prison_prob,3) %>% summarise(sum(prob))\n\n\n  sum(prob)\n1 0.3061728\n\n\nThe probability a randomly selected inmate has more than 2 prior convictions is ~30.6%\n\n\n2e.\nWhat is the expected value of the number of prior convictions?\n\n\nCode\nsum(prison_prob$prior_convictions*prison_prob$prob)\n\n\n[1] 1.28642\n\n\nCode\n#Or another way,\n\nweighted.mean(prison_prob$prior_convictions,prison_prob$prob)\n\n\n[1] 1.28642\n\n\nThe expected value of prior convictions is 1.28642\n\n\n2f\n\n\nCode\nprison_prob\n\n\n  prior_convictions freq       prob\n1                 0  128 0.15802469\n2                 1  434 0.53580247\n3                 2  160 0.19753086\n4                 3   64 0.07901235\n5                 4   24 0.02962963\n\n\nCode\nvar(prison_prob$freq)\n\n\n[1] 25948\n\n\nCode\nsd(prison_prob$freq)\n\n\n[1] 161.0838\n\n\nThe variance among all prior convictions is 25948. The standard deviation among all prior convictions is 161.0838."
  },
  {
    "objectID": "posts/KPopiela_FinalProposal.html",
    "href": "posts/KPopiela_FinalProposal.html",
    "title": "KPopiela_final p1",
    "section": "",
    "text": "#I will need to do some more thorough testing to make sure I can actually do this, but I'd like to focus my final project on ethnic violence since I know a lot about it (I wrote my undergrad thesis on ethno-religious violence in the Polish-Ukrainian borderlands). I found some data sets for my DACSS-601 intensive final that were pretty useful - I found a lot of information but now that I will have the statistical background I'd like to see if I can go further with it. Specifically in the sense of finding out statistics related to the likelihood of an eruption of ethnic violence in countries that fit specific criteria on paper. I can come up with a different research question for this topic area if my initial idea isn't feasible though.  \n\n#These criteria are as follows:  \n# - The population doesn't have an overwhelming ethnic majority; there are 2+ groups, each with substantial numbers.  \n# - History of socio-political repression by one group against the other(s) when said group has political power/alternating episodes of targeted political repression depending on what group holds a political majority.  \n# - The country/population is in a state of severe socio-political instability (war, territorial conquest, political power vacuum, etc.)  \n\n#To make this a little less challenging, I'm going to simplify things for myself. First, I will narrow the data down geographically and temporally - I'm going to focus on former Yugoslav states and the former USSR (I might change the location though). Second, since the data sets I'll be using are pretty big, I'm also going to look at certain columns based on the criteria I presented above (ethnic groups involved, group(s) being oppressed and by whom, and presence/absence of political instability). \n\n# Research Question: Based on what the data show, (1) is there actually a higher probability of ethnic armed conflict/war when these conditions, and (2) does one particular condition have a greater effect on political stability than the others?\n\n\nlibrary(readr)\nlibrary(poliscidata)\n\nError in library(poliscidata): there is no package called 'poliscidata'\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(foreign)\n\n##Summary Stats/Visuals\n\n#All data sets I will be using are from the Harvard Dataverse and they are as follows:  \n\n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Power Relations dataset\", https://doi.org/10.7910/DVN/NDJUJM, Harvard Dataverse, V1, UNF:5:k4xxXC2ASI204QZ4jqvUrQ== [fileUNF]  \n#     Lars-Erik Cederman; Brian Min; Andreas Wimmer, 2010, \"Ethnic Armed Conflict dataset\", https://doi.org/10.7910/DVN/K3OIJQ, Harvard Dataverse, V1  \n#    UCDP/PRIO Armed Conflict Dataset version 22.1. Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).\n\n#I don't need to look at these in any particular order, so I'm just going to present them in the order they are in above. \n\n\n# Data set 1: \"Ethnic Power Relations dataset\"\n\nethnic_power_relations <- MASTER_EPR_v1_IrgFiR\n\nError in eval(expr, envir, enclos): object 'MASTER_EPR_v1_IrgFiR' not found\n\nethnic_power_relations <- ethnic_power_relations %>%\n  select(statename,from,to,group,status,size) %>%\n  filter(statename == c(\"Albania\",\"Croatia\",\"Bosnia and Herzegovina\",\"Yugoslavia\",\"Macedonia\",\"Poland\",\"Ukraine\",\"Russia\",\"Hungary\",\"Romania\",\"Bulgaria\"), from >= 1980) \n\nError in select(., statename, from, to, group, status, size): object 'ethnic_power_relations' not found\n\nethnic_power_relations\n\nError in eval(expr, envir, enclos): object 'ethnic_power_relations' not found\n\n\n\n#Data set 2: \"Ethnic Armed Conflict dataset\"\n#I'm going to use select() to look at \"country\", \"startyr\", \"endyr\", and \"ETHNOWAR\". Then I will use filter() to meet my geographic requirements.\n\nethnic_armed_conflict <- EAC_edPcfy\n\nError in eval(expr, envir, enclos): object 'EAC_edPcfy' not found\n\nethnic_armed_conflict <- ethnic_armed_conflict %>%\n  select(country, startyr, endyr, ETHNOAIMS, ETHNOWAR) %>%\n  filter(country == c(\"Croatia\",\"Yugoslavia\",\"Bosnia and Herzegovina\",\"USSR\",\"Russia\"),startyr >= 1980)\n\nError in select(., country, startyr, endyr, ETHNOAIMS, ETHNOWAR): object 'ethnic_armed_conflict' not found\n\nethnic_armed_conflict\n\nError in eval(expr, envir, enclos): object 'ethnic_armed_conflict' not found\n\n#This data set is based off of another one (Gleditsch, Nils Petter, Peter Wallensteen, Mikael Eriksson, Margareta Sollenberg, and Håvard Strand (2002) Armed Conflict 1946-2001: A New Dataset. Journal of Peace Research 39(5).) so I will include that as well\n\n#NOTE: I don't know why only 2 of the 5 countries I listed are showing up\n\n\n#Data set 3: \"UCDP/PRIO Armed Conflict Dataset\" version 22.1\n\nUCDP_Prio_AC <- ucdp_prio_acd_221_wKBkVs\n\nError in eval(expr, envir, enclos): object 'ucdp_prio_acd_221_wKBkVs' not found\n\nUCDP_Prio_AC <- UCDP_Prio_AC %>%\n  select(location, side_a, side_b, start_date) %>%\n  filter(location == c(\"Yugoslavia\",\"Croatia\",\"Serbia\",\"Russia\"))\n\nError in select(., location, side_a, side_b, start_date): object 'UCDP_Prio_AC' not found\n\nUCDP_Prio_AC\n\nError in eval(expr, envir, enclos): object 'UCDP_Prio_AC' not found\n\n#NOTE: I don't know why this one is doing the same thing as the previous one.\n\n\n#I'm obviously going to do more in-depth work with all three data sets, these (below) are just kind of a sample of what I will be doing.\n\n\nethnic_power_relations %>%\n  ggplot(mapping=aes(x=group,y=size,col=status)) + geom_point() + coord_flip()\n\nError in ggplot(., mapping = aes(x = group, y = size, col = status)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  summarise(mean(size))\n\nError in summarise(., mean(size)): object 'ethnic_power_relations' not found\n\n\n\nethnic_power_relations %>%\n  count(status)\n\nError in count(., status): object 'ethnic_power_relations' not found"
  },
  {
    "objectID": "posts/StephRobertsHW1.html",
    "href": "posts/StephRobertsHW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Homework 1\n##1. Use the LungCapData to answer the following questions. (Hint: Using dplyr, especiallygroup_by() and summarize() can help you answer the following questions relatively efficiently.)\n\n\nCode\ndf<- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nhead(df)\n\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n\n\n#Summarize\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmean(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmedian(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nvar(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nsd(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n\n\nCode\nmin(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\nCode\nmax(df$LungCap)\n\n\nError in df$LungCap: object of type 'closure' is not subsettable\n\n\n#a. What does the distribution of LungCap look like? (Hint: Plot a histogram with probability density on the y axis)\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=0.5,col='black',fill='gray')\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nThe histogram follows a distribution close to normal distibution. In fact, if we change binwidth slightly, it appears even closer to normal distribution.\n\n\nCode\nggplot(df, aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nThis helps illustrate the importance of binwidth and what it can do to our visualization interpretations.\n#b. Compare the probability distribution of the LungCap with respect to Males and Females? (Hint: make boxplots separated by gender using the boxplot() function)\n\n\nCode\nggplot(df, aes(x = LungCap, y = Gender)) +        \n  geom_boxplot()\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nThe distribution of male lung capacity is larger and longer than females’.\n#c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(LungCap) %>%\n  mean() \n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nCode\ndf %>%\n  filter(Smoke == 'no') %>%\n  pull(LungCap) %>%\n  mean()\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nIt does not make sense at face value. In this sample, smokers have a higher mean lung capacity than non-smokers. Let’s check how big each subsample is.\n\n\nCode\nlength(which(df$Smoke == 'yes'))\n\n\nError in df$Smoke: object of type 'closure' is not subsettable\n\n\nCode\nlength(which(df$Smoke == 'no'))\n\n\nError in df$Smoke: object of type 'closure' is not subsettable\n\n\nAs suspected, there are far more, almost 10 times as many, non-smokers. If we could gather data from all the smokers, perhaps our means would look a lot different. Maybe our sample was taken from young people whose lungs have not been long affected by the smoking.\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  pull(Age) %>%\n  median() \n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nAgain, as suspected, our sample of smokers is a young age. Therefore, the lack of difference in lung capacity between smokers and non-smokers is not too surprising.\n#d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Create age groups\ndf <- df %>% \n  mutate(agegroup = case_when(\n    Age <= 13  ~ \"less than or equal to 13\",\n    Age >= 14 & Age <= 15 ~ \"14 to 15\",\n    Age >= 16 & Age <= 17 ~ \"16 TO 17\",\n    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in UseMethod(\"mutate\"): no applicable method for 'mutate' applied to an object of class \"function\"\n\n\nCode\ntable(df$agegroup)\n\n\nError in df$agegroup: object of type 'closure' is not subsettable\n\n\nCode\ndf %>%\n  filter(Smoke == 'yes') %>%\n  ggplot(aes(x=LungCap)) + \n  geom_histogram(binwidth=1,col='black',fill='gray')+\n  facet_wrap(~agegroup)\n\n\nError in UseMethod(\"filter\"): no applicable method for 'filter' applied to an object of class \"function\"\n\n\nThese histograms suggest that participants 13 or younger have smaller lung capacity. The Lung capacity seems to generally increase with age as children grow.\n#e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part c. What could possibly be going on here?\n\n\nCode\nggplot(df, aes(x = LungCap, \n           fill = agegroup)) +\n  geom_density(alpha = 0.4)+\n  facet_wrap(~Smoke)\n\n\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n\n\nThis visualization starts to explain furthermore why there is an unexpected result for lung capacity in smokers vs. non-smokers. As we have deducted, lung capacity generally improves with age (in growing years). However, teenagers approaching adulthood are also a group more likely to have access or influence to smoking cigarettes. It is likely that our smokers account for some of the older participants, who happen to be closer to normal smoking age.\n#f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\ncov(df$LungCap, df$Age) #calculate covariance\n\n\nError in df$Age: object of type 'closure' is not subsettable\n\n\nCode\ncor(df$LungCap, df$Age) #calculate correlation\n\n\nError in df$Age: object of type 'closure' is not subsettable\n\n\nA positive coraviance (8.74) indicates lung capacity and age tend to increase together. The positive correlation relatively close to 1 (0.82) indicates there is a fairly strong correlation between the variables.\n##2. Let X = number of prior convictions for prisoners at a state prison at which there are 810 prisoners.\n\n\nCode\n#create the sample\nx<-rep(c(0,1,2,3,4),times=c(128, 434, 160, 64, 24))\nsample(x, 10)\n\n\n [1] 2 1 3 3 0 1 1 0 1 2\n\n\nCode\n#Verify n of sample\nsum(128, 434, 160, 64, 24)\n\n\n[1] 810\n\n\n\n\nCode\n#Calculate the mean\nmean(x)\n\n\n[1] 1.28642\n\n\nCode\n#Verify the mean\nsample_mean <- (((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/810)\nprint(sample_mean)\n\n\n[1] 1.28642\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016\n\n\n#a. What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\n#probability of 2 convictions?\ndnorm.convict <- dnorm(2, mean(x), sd(x))\nprint(dnorm.convict)\n\n\n[1] 0.3201613\n\n\nThe probability of 2 convications in 0.32.\n#b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\n#probability of <2 convictions\nless.than <- pnorm(2, mean(x), sd(x)) - dnorm.convict\nprint(less.than)\n\n\n[1] 0.4593924\n\n\nThe probability of <2 convictions is 0.46.\n#c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\n#probability of =<2 convictions?\npnorm.convict <- pnorm(2, mean(x), sd(x))\nprint(pnorm.convict)\n\n\n[1] 0.7795537\n\n\nThe probability of less than or equal to 2 convictions is 0.78.\n#d. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\n#probability of >2 convictions?\ngreater.than <- 1 - pnorm.convict\nprint(greater.than)\n\n\n[1] 0.2204463\n\n\nThe probability of greater than 2 convictions is 0.22.\n\n\nCode\n#Verify all probabilities add to 1\nless.than + dnorm.convict + greater.than\n\n\n[1] 1\n\n\n#e. What is the expected value for the number of prior convictions?\n\n\nCode\n# Expected value of a probability distribution  can be found with μ = Σx * P(x), where x = data value and P(x) = probability of data. \n\n#Calculate probabilities of data\np0 <- dnorm(0, mean(x), sd(x))\np0\n\n\n[1] 0.1641252\n\n\nCode\np1 <- dnorm(1, mean(x), sd(x))\np1\n\n\n[1] 0.410739\n\n\nCode\np2 <- dnorm(2, mean(x), sd(x))\np2\n\n\n[1] 0.3201613\n\n\nCode\np3 <- dnorm(3, mean(x), sd(x))\np3\n\n\n[1] 0.07772916\n\n\nCode\np4 <- dnorm(4, mean(x), sd(x))\np4\n\n\n[1] 0.005877753\n\n\nCode\n#Calculate expected value\nev <- sum((0*p0), (1*p1), (2*p2), (3*p3), (4*p4))\nev\n\n\n[1] 1.30776\n\n\nCode\n#The expected value should be close to the mean in a normal distribution\nmean(x)\n\n\n[1] 1.28642\n\n\nThe expected value is 1.31.\n#f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\n#Calculate variance\nvar(x)\n\n\n[1] 0.8572937\n\n\n\n\nCode\n#Calculate the sd\nsd(x)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW3_Saaradhaa.html",
    "href": "posts/HW3_Saaradhaa.html",
    "title": "Homework 3",
    "section": "",
    "text": "Qn 1.1.2\n\n# load libraries.\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(alr4)\n\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(smss)\n\n# load dataset.\ndata(UN11)\n\n# draw scatterplot.\nscatterplot(fertility ~ ppgdp, UN11)\n\n\n\n\nNo, the graph seems curvilinear.\n\n\nQn 1.1.3\n\n# draw scatterplot.\nscatterplot (log(fertility) ~ log(ppgdp), UN11)\n\n\n\n\nYes, the simple linear regression model now seems plausible.\n\n\nQn 2a\nWe can test this using the UN11 dataset since ppgdp is in US dollars.\n\n# create new variable.\nUN11$british <- 1.33*UN11$ppgdp\n\n# check slope.\nsummary(lm(fertility ~ british, UN11))\n\n\nCall:\nlm(formula = fertility ~ british, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nbritish     -2.407e-05  3.500e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\nsummary(lm(fertility ~ ppgdp, UN11))\n\n\nCall:\nlm(formula = fertility ~ ppgdp, data = UN11)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9006 -0.8801 -0.3547  0.6749  3.7585 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.178e+00  1.048e-01  30.331  < 2e-16 ***\nppgdp       -3.201e-05  4.655e-06  -6.877  7.9e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.206 on 197 degrees of freedom\nMultiple R-squared:  0.1936,    Adjusted R-squared:  0.1895 \nF-statistic: 47.29 on 1 and 197 DF,  p-value: 7.903e-11\n\n\nThe magnitude of the slope has reduced very slightly, although adjusted R^2 has not.\n\n\nQn 2b\nWe can test this too.\n\n# correlation with US dollars.\ncor(UN11$ppgdp, UN11$fertility)\n\n[1] -0.4399891\n\n# correlation with British pounds.\ncor(UN11$british, UN11$fertility)\n\n[1] -0.4399891\n\n\nSince we multiplied by a constant, the correlation remains the same.\n\n\nQn 3\n\n# load dataset.\ndata(water)\n\n# generate scatterplots.\npairs(water)\n\n\n\n\nStream runoff (BSAAM) seems to have a positive linear relationship with precipitation at OPSLAKE, OPRC and OPBPC; but not with precipitation at APMAM, APSAB or APSLAKE. Stream runoff also seems to be fairly constant (?) over the years.\n\n\nQn 4\n\n# load dataset.\ndata(Rateprof)\n\n# create subset.\nrateprof <- Rateprof %>% select(quality, helpfulness, clarity, easiness, raterInterest)\n\n# generate scatterplots.\npairs(rateprof)\n\n\n\n\nQuality, helpfulness and clarity have the clearest linear relationships with one another. Easiness and raterInterest do not seem to have linear relationships with the other variables.\n\n\nQn 5a\n\n# load dataset.\ndata(student.survey)\nglimpse(student.survey)\n\nRows: 60\nColumns: 18\n$ subj <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ ge   <fct> m, f, f, f, m, m, m, f, m, m, m, f, m, m, f, f, f, m, m, f, f, f,…\n$ ag   <int> 32, 23, 27, 35, 23, 39, 24, 31, 34, 28, 23, 27, 36, 28, 28, 25, 4…\n$ hi   <dbl> 2.2, 2.1, 3.3, 3.5, 3.1, 3.5, 3.6, 3.0, 3.0, 4.0, 2.3, 3.5, 3.3, …\n$ co   <dbl> 3.5, 3.5, 3.0, 3.2, 3.5, 3.5, 3.7, 3.0, 3.0, 3.1, 2.6, 3.6, 3.5, …\n$ dh   <int> 0, 1200, 1300, 1500, 1600, 350, 0, 5000, 5000, 900, 253, 190, 245…\n$ dr   <dbl> 5.0, 0.3, 1.5, 8.0, 10.0, 3.0, 0.2, 1.5, 2.0, 2.0, 1.5, 3.0, 1.5,…\n$ tv   <dbl> 3, 15, 0, 5, 6, 4, 5, 5, 7, 1, 10, 14, 6, 3, 4, 7, 6, 5, 6, 25, 4…\n$ sp   <int> 5, 7, 4, 5, 6, 5, 12, 3, 5, 1, 15, 3, 15, 10, 3, 6, 7, 9, 12, 0, …\n$ ne   <int> 0, 5, 3, 6, 3, 7, 4, 3, 3, 2, 1, 7, 12, 1, 1, 1, 3, 6, 2, 0, 4, 7…\n$ ah   <int> 0, 6, 0, 3, 0, 0, 2, 1, 0, 1, 1, 0, 5, 2, 0, 0, 10, 10, 2, 2, 1, …\n$ ve   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ pa   <fct> r, d, d, i, i, d, i, i, i, i, r, d, d, i, d, i, i, d, i, d, i, i,…\n$ pi   <ord> conservative, liberal, liberal, moderate, very liberal, liberal, …\n$ re   <ord> most weeks, occasionally, most weeks, occasionally, never, occasi…\n$ ab   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ aa   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ ld   <lgl> FALSE, NA, NA, FALSE, FALSE, NA, FALSE, FALSE, NA, FALSE, FALSE, …\n\n# generate plots.\nboxplot(pi ~ re, student.survey)\n\n\n\nscatterplot(hi ~ tv, student.survey)\n\n\n\n\n\nReligiosity and conservatism seem to have a positive relationship.\nHigh school GPA and TV-watching seem to have a negative relationship.\n\n\n\nQn 5b\n\n# change pi to numeric variable.\nstudent.survey$pi <- as.numeric(student.survey$pi)\n\n# removing ordering in re and rename it.\nlevels(student.survey$re) <- c(\"N\", \"O\", \"M\", \"E\")\nstudent.survey$re <- factor(student.survey$re, ordered = FALSE)\n\n# run regression models.\nsummary(lm(pi ~ re, student.survey))\n\n\nCall:\nlm(formula = pi ~ re, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8889 -0.5172 -0.2667  1.2040  2.7333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2667     0.3394   6.678 1.18e-08 ***\nreO           0.2506     0.4181   0.599 0.551374    \nreM           2.1619     0.6017   3.593 0.000691 ***\nreE           2.6222     0.5543   4.731 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.315 on 56 degrees of freedom\nMultiple R-squared:  0.3872,    Adjusted R-squared:  0.3544 \nF-statistic:  11.8 on 3 and 56 DF,  p-value: 4.282e-06\n\nsummary(lm(hi ~ tv, student.survey))\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.441353   0.085345  40.323   <2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\n\nThose who attended religious services most weeks/every week were significantly more likely to be conservative than those who never did, p < .001. There was no significant difference in political ideology between those who occasionally attended religious services and those who never did.\nWatching less hours of TV per week was associated with higher high-school GPAs, p < .05. That being said, as the R2 is fairly low, hours of TV watching is not a great predictor of high school GPA."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-1",
    "href": "posts/HW1_603_Niharikapola.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-data",
    "href": "posts/HW1_603_Niharikapola.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a",
    "href": "posts/HW1_603_Niharikapola.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b",
    "href": "posts/HW1_603_Niharikapola.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c",
    "href": "posts/HW1_603_Niharikapola.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d",
    "href": "posts/HW1_603_Niharikapola.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e",
    "href": "posts/HW1_603_Niharikapola.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f",
    "href": "posts/HW1_603_Niharikapola.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#question-2",
    "href": "posts/HW1_603_Niharikapola.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "href": "posts/HW1_603_Niharikapola.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#a-1",
    "href": "posts/HW1_603_Niharikapola.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#b-1",
    "href": "posts/HW1_603_Niharikapola.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#c-1",
    "href": "posts/HW1_603_Niharikapola.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#d-1",
    "href": "posts/HW1_603_Niharikapola.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#e-1",
    "href": "posts/HW1_603_Niharikapola.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_603_Niharikapola.html#f-1",
    "href": "posts/HW1_603_Niharikapola.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/KPopiela_HW2.html",
    "href": "posts/KPopiela_HW2.html",
    "title": "KPopiela HW2",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(stats)"
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-1",
    "href": "posts/KPopiela_HW2.html#question-1",
    "title": "KPopiela HW2",
    "section": "Question 1",
    "text": "Question 1\n\nThe time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population. Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n#Bypass values\nn_bypass <- 539\nx_bypass <- 19\nsd_bypass <- 10\ndf_bypass <- n_bypass-1\nalpha = 0.10\n\n\n#t-score  \ntscore_bypass = qt(p=alpha/2,df=df_bypass,lower.tail = F)\ntscore_bypass\n\n[1] 1.647691\n\n\n\n#standard error  \nse_bypass <- sd_bypass/sqrt(n_bypass)\nse_bypass\n\n[1] 0.4307305\n\n\n\n#margin of error and confidence interval (bypass)\nmargin_error <- tscore_bypass*se_bypass\nlower_CI <- x_bypass - margin_error\nupper_CI <- lower_CI+margin_error\nprint(c(lower_CI,upper_CI))\n\n[1] 18.29029 19.00000\n\n\n\n#Angiography values  \nn_angio <- 847\nx_angio <- 18\nsd_angio <- 9\ndf_angio <- n_angio-1\n#alpha value remains the same at 0.10 \n\n\n#t-score\ntscore_angio <- qt(p=alpha/2,df=df_angio,lower.tail = F)\ntscore_angio\n\n[1] 1.646657\n\n\n\n#standard error\nse_angio <-sd_angio/sqrt(n_angio)\nse_angio\n\n[1] 0.3092437\n\n\n\n#margin of error and confidence interval (angiography)\nmargin_error_angio <- tscore_angio*se_angio\nangio_lowerCI <- x_angio - margin_error_angio\nangio_upperCI <- angio_lowerCI+margin_error_angio\nprint(c(angio_lowerCI,angio_upperCI))\n\n[1] 17.49078 18.00000\n\n\nThe bypass confidence interval for the true mean wait time is 18.290, 19.000, or 0.710.\nThe angiography confidence interval the true mean wait time is 17.491, 18.000, or 0.509.\nThe confidence interval is narrower for angiography."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-2",
    "href": "posts/KPopiela_HW2.html#question-2",
    "title": "KPopiela HW2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n#Sample proportion/Point estimate\nn <- 1031\nx <- 567\nsample_prop <- x/n\nsample_prop\n\n[1] 0.5499515\n\n\n\n#Margin of error\nmargin_error2 <- qnorm(0.975)*sqrt(sample_prop*(1-sample_prop)/n)\nmargin_error2\n\n[1] 0.03036761\n\n\n\n#95% Confidence Interval\nCI_lower <- sample_prop - margin_error2\nCI_upper <- CI_lower + margin_error2\nprint(c(CI_lower,CI_upper))\n\n[1] 0.5195839 0.5499515\n\n\nThe point estimate p, of the proportion of all adult Americans who believe that college is essential for success is 0.549, or ~55%. The margin of error is 0.030, which lines up, since the confidence interval is 0.519, 0.549."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-3",
    "href": "posts/KPopiela_HW2.html#question-3",
    "title": "KPopiela HW2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n#Since most formulas require a value for sample size (n), whichever one I use will have to be reorganized: the confidence interval formula. But because I am looking for n, it has to read z*(s/5)^2=n.\n\nf <-function(n, z = 1.96, s = 42.5) {\n  res <- z*s/sqrt(n)\n  return(res)\n}\n\nvec <- vapply(1:300, FUN = f, FUN.VALUE = 5.0)\nwhich(vec < 5) [1]\n\n[1] 278\n\n#The sample contains at least 278 people."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-4",
    "href": "posts/KPopiela_HW2.html#question-4",
    "title": "KPopiela HW2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nC. Report and interpret the P-value for H a: μ > 500.\n####(Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n#In order to test whether or not the mean income for female employees differs from $500/week, we must first condect a one-sample, two-sided significance test.\n\n#We can also assume the following:  \n#1. The sample is random and the population has a normal distribution  \n#2. The mean income for all senior-level workers = $500/week  \n#3. From the random sample of 9 female employees, the mean income = $410/week  \n#4. Standard deviation = 90  \n#5. Null Hypothesis: H0: μ = 500  \n#6. Alternative Hypothesis: Ha: μ ≠ 500  \n\n\n#Test statistic\nybar <- 410\nmean <- 500\ns <- 90\nn <- 9\n(ybar - mean)/(s/sqrt(n))\n\n[1] -3\n\n#The test statistic value is -3\n\n\n#P-value\nn <- 9 \ndf_n <- (n - 1)  \nt_test <- (410 - 500)/(90/sqrt(9))  \np_value <- pt(t_test, df_n)*2  \np_value\n\n[1] 0.01707168\n\n#P-value is 0.017. If we hold to the assumption that a=0.05, we can easily see that 0.017 < 0.05, which means the null hypothesis can be rejected. Therefore, there is enough statistical evidence to support the claim that the mean income for female employees differs from the overall mean of $500/week.\n\n\n\nB. Report the P-value for Ha : μ < 500. Interpret.\n\n#Hypotheses\n    #H0:mu = $500/week  \n    #Ha:mu = <$500/week  \n    #P-value = p(t<t_test)*p(t<-3)\n\n\n#P-value for Ha:my > 500\nq <- -3\nleft_p_value <- pt(q,df_n,lower.tail=TRUE,log.p=FALSE)\nleft_p_value\n\n[1] 0.008535841\n\n#P-value for Ha:my > 500 is 0.0085. This can be rounded up to 0.01, which indicates that there's strong evidence against the mean weekly income being $500+\n\n\n#P-value for H0:mu < 500\nright_p_value <- pt(q,df_n,lower.tail = FALSE,log.p=FALSE)\nright_p_value\n\n[1] 0.9914642\n\n#The p-value for H0:mu < 500 is 0.99, indicating strong evidence in favor of the null hypothesis. This contradicts the claim that mean mu > 500. To make sure my findings are correct, I must confrim that the sum of each p-value totals to 1. I could code this but it's not hard to tell that 0.01 + 0.99 = 1."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-5",
    "href": "posts/KPopiela_HW2.html#question-5",
    "title": "KPopiela HW2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 ,with se = 10.0\n\nA. Show that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n#Let's start with Jones and confirming that t=1.95 and the p-value = 0.051\n\n#t-test\nt_testj <- (519.5-500)/10.0\nt_testj\n\n[1] 1.95\n\n#The t-test value, is in fact 1.95\n\n\n#P-value\nn5 <- 1000\ndf_5 <- (n5-1)\n\npvaluej <- pt(t_testj, df_5,lower.tail = FALSE,log.p = FALSE)*2\npvaluej\n\n[1] 0.05145555\n\n#Like the t-test value, the p-value is also accurate to the question at 0.051.\n\n\n#Now lets move onto Smith with t = 1.97 and p-value = 0.049\n\n#t-test\nt_testSmith <- (519.7 - 500)/10.0\nt_testSmith\n\n[1] 1.97\n\n#Smith's t-test value is 1.97.\n\n\n#P-value\n\n#sample size n is the same as in the Jones section: n5 <- 1000  \n#df is also the same as the Jones section: df_5 <- (n5-1)  \n\np_valueSmith <- pt(t_testSmith, df_5,lower.tail = FALSE, log.p = FALSE)*2\np_valueSmith\n\n[1] 0.04911426\n\n#Smith's p-value, like the t-test value, is accurate to what the question presents at 0.049\n\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\n\n#In order for a p-value to be statistically significant, it must be greater than 0.05. Smith’s p-value is 0.049 which, while close, is still less than 0.05. Jones’s p-value, however, is statistically significant at 0.051.\n\n\n\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\n#Smith's and Jones's results were extremely similar, but the difference between the two lies right on the threshold of statistical significance; only Jones's results were statistically significant. But given the result values' closeness (0.051 and 0.049), there is moderate evidence against H0."
  },
  {
    "objectID": "posts/KPopiela_HW2.html#question-6",
    "href": "posts/KPopiela_HW2.html#question-6",
    "title": "KPopiela HW2",
    "section": "Question 6",
    "text": "Question 6\n\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n#I'm going to start by calculating the t-score to find the upper and lower values in the gas_taxes interval\n\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\ngas_tax_sample <- 18\ndf_gt <- gas_tax_sample - 1\nmean_gt <- mean(gas_taxes)\ntscore_gt <- qt(p=0.05,df=df_gt,lower.tail=FALSE)\ngas_sd <- sd(gas_taxes)\nme_gas_taxes <- qt(0.05,df = df_gt)*gas_sd/sqrt(18)\n\nlower_int_gt<-(mean_gt+me_gas_taxes)\nlower_int_gt\n\n[1] 37.0461\n\n#The lower interval value is 37.046\n\n\nupper_int_gt <- (mean_gt- me_gas_taxes)\nupper_int_gt\n\n[1] 44.67946\n\n#The upper interval value is 44.679\n\n#The average tax/gallon of gas is less than $0.45, so it is within the upper and lower bounds of the confidence interval. However, we will test an alternate outcome via a t-test"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html",
    "href": "posts/FinalProject_ManiShankerKamarapu.html",
    "title": "Final project part 1",
    "section": "",
    "text": "Churning refers to a customer who leaves one company to go to another company. Customer churn introduces not only some loss in income but also other negative effects on the operation of companies. Churn management is the concept of identifying those customers who are intending to move their custom to a competing service provider.\nRisselada et al. (2010) stated that churn management is becoming part of customer relationship management. It is important for companies to consider it as they try to establish long-term relationships with customers and maximize the value of their customer base.\n\n\n\n\n\n\nResearch Questions\n\n\n\nA. Does churn-rate depend on the geographical factors of the customer?\nB. Do non-active members are probable to churn or not?\n\n\nThis project will be useful to better understand more about the customer difficulties and factors and also give us a pretty good idea on the factors effecting the customers to exit and also about the dormant state of the customers."
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#hypothesis",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#hypothesis",
    "title": "Final project part 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nCustomer churn analysis has become a major concern in almost every industry that offers products and services. The model developed will help banks identify clients who are likely to be churners and develop appropriate marketing actions to retain their valuable clients. And this model also supports information about similar customer group to consider which marketing reactions are to be provided. Thus, due to existing customers are retained, it will provide banks with increased profits and revenues.\nGiven the above, we can frame our hypotheses as follows:\n\n\n\n\n\n\nH0A\n\n\n\nGeographical factors will not be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH1A\n\n\n\nGeographical factors will be statistically predict the churn-rate.\n\n\n\n\n\n\n\n\nH0B\n\n\n\nActive members will not churn.\n\n\n\n\n\n\n\n\nH1B\n\n\n\nActive members will churn."
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#loading-libraries",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#loading-libraries",
    "title": "Final project part 1",
    "section": "Loading libraries",
    "text": "Loading libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#reading-the-data-set",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#reading-the-data-set",
    "title": "Final project part 1",
    "section": "Reading the data set",
    "text": "Reading the data set\n\n\nCode\nChurn <- read_csv(\"_data/Churn_Modelling.csv\")\n\n\nRows: 10000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Surname, Geography, Gender\ndbl (11): RowNumber, CustomerId, CreditScore, Age, Tenure, Balance, NumOfPro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nChurn\n\n\n\n\n  \n\n\n\nThis data set includes 10k bank customer data records with 14 attributes including socio-demographic attributes, account level and behavioural attributes.\nAttribute Description 1. Row Number- Number of customers 2. Customer ID- ID of customer 3. Surname- Customer name 4. Credit Score- Score of credit card usage 5. Geography- Location of customer 6. Gender- Customer gender 7. Age- Age of Customer 8. Tenure- The period of having the account in months 9. Balance- Customer main balance 10. NumOfProducts- No of products used by customer 11. HasCrCard- If the customer has a credit card or not 12. IsActiveMember- Customer account is active or not 13. Estimated Salary- Estimated salary of the customer. 14. Exited- Indicate churned or not\n\n\nCode\nstr(Churn)\n\n\nspec_tbl_df [10,000 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ RowNumber      : num [1:10000] 1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : num [1:10000] 15634602 15647311 15619304 15701354 15737888 ...\n $ Surname        : chr [1:10000] \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : num [1:10000] 619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr [1:10000] \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr [1:10000] \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : num [1:10000] 42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : num [1:10000] 2 1 8 1 2 8 7 4 4 2 ...\n $ Balance        : num [1:10000] 0 83808 159661 0 125511 ...\n $ NumOfProducts  : num [1:10000] 1 1 3 2 1 2 2 4 2 1 ...\n $ HasCrCard      : num [1:10000] 1 0 1 0 1 1 1 1 0 1 ...\n $ IsActiveMember : num [1:10000] 1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num [1:10000] 101349 112543 113932 93827 79084 ...\n $ Exited         : num [1:10000] 1 0 1 0 0 1 0 1 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   RowNumber = col_double(),\n  ..   CustomerId = col_double(),\n  ..   Surname = col_character(),\n  ..   CreditScore = col_double(),\n  ..   Geography = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_double(),\n  ..   Tenure = col_double(),\n  ..   Balance = col_double(),\n  ..   NumOfProducts = col_double(),\n  ..   HasCrCard = col_double(),\n  ..   IsActiveMember = col_double(),\n  ..   EstimatedSalary = col_double(),\n  ..   Exited = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "posts/FinalProject_ManiShankerKamarapu.html#descriptive-statistics",
    "href": "posts/FinalProject_ManiShankerKamarapu.html#descriptive-statistics",
    "title": "Final project part 1",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n\nCode\nsummary(Churn)\n\n\n   RowNumber       CustomerId         Surname           CreditScore   \n Min.   :    1   Min.   :15565701   Length:10000       Min.   :350.0  \n 1st Qu.: 2501   1st Qu.:15628528   Class :character   1st Qu.:584.0  \n Median : 5000   Median :15690738   Mode  :character   Median :652.0  \n Mean   : 5000   Mean   :15690941                      Mean   :650.5  \n 3rd Qu.: 7500   3rd Qu.:15753234                      3rd Qu.:718.0  \n Max.   :10000   Max.   :15815690                      Max.   :850.0  \n  Geography            Gender               Age            Tenure      \n Length:10000       Length:10000       Min.   :18.00   Min.   : 0.000  \n Class :character   Class :character   1st Qu.:32.00   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median :37.00   Median : 5.000  \n                                       Mean   :38.92   Mean   : 5.013  \n                                       3rd Qu.:44.00   3rd Qu.: 7.000  \n                                       Max.   :92.00   Max.   :10.000  \n    Balance       NumOfProducts    HasCrCard      IsActiveMember  \n Min.   :     0   Min.   :1.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:     0   1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median : 97199   Median :1.00   Median :1.0000   Median :1.0000  \n Mean   : 76486   Mean   :1.53   Mean   :0.7055   Mean   :0.5151  \n 3rd Qu.:127644   3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :250898   Max.   :4.00   Max.   :1.0000   Max.   :1.0000  \n EstimatedSalary         Exited      \n Min.   :    11.58   Min.   :0.0000  \n 1st Qu.: 51002.11   1st Qu.:0.0000  \n Median :100193.91   Median :0.0000  \n Mean   :100090.24   Mean   :0.2037  \n 3rd Qu.:149388.25   3rd Qu.:0.0000  \n Max.   :199992.48   Max.   :1.0000  \n\n\n\n\nCode\nglimpse(Churn)\n\n\nRows: 10,000\nColumns: 14\n$ RowNumber       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ CustomerId      <dbl> 15634602, 15647311, 15619304, 15701354, 15737888, 1557…\n$ Surname         <chr> \"Hargrave\", \"Hill\", \"Onio\", \"Boni\", \"Mitchell\", \"Chu\",…\n$ CreditScore     <dbl> 619, 608, 502, 699, 850, 645, 822, 376, 501, 684, 528,…\n$ Geography       <chr> \"France\", \"Spain\", \"France\", \"France\", \"Spain\", \"Spain…\n$ Gender          <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Female\", \"Mal…\n$ Age             <dbl> 42, 41, 42, 39, 43, 44, 50, 29, 44, 27, 31, 24, 34, 25…\n$ Tenure          <dbl> 2, 1, 8, 1, 2, 8, 7, 4, 4, 2, 6, 3, 10, 5, 7, 3, 1, 9,…\n$ Balance         <dbl> 0.00, 83807.86, 159660.80, 0.00, 125510.82, 113755.78,…\n$ NumOfProducts   <dbl> 1, 1, 3, 2, 1, 2, 2, 4, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, …\n$ HasCrCard       <dbl> 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …\n$ IsActiveMember  <dbl> 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, …\n$ EstimatedSalary <dbl> 101348.88, 112542.58, 113931.57, 93826.63, 79084.10, 1…\n$ Exited          <dbl> 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …"
  },
  {
    "objectID": "posts/hw1_boonstra.html",
    "href": "posts/hw1_boonstra.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a",
    "href": "posts/hw1_boonstra.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nFirst, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlungcap <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(lungcap$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b",
    "href": "posts/hw1_boonstra.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThese are the boxplots of the distributions for the lung capacity of males and females in the sample:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Gender,y=LungCap)) +\n  geom_boxplot()\n\n\n\n\n\nAccording to these boxplots, it appears that males and females have similar median lung capacities, but that males may be more likely to have a higher lung capacity than females."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c",
    "href": "posts/hw1_boonstra.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\nlungcap %>% \n  group_by(Smoke) %>% \n  summarise(mean_lungcap=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke mean_lungcap\n  <chr>        <dbl>\n1 no            7.77\n2 yes           8.65\n\n\nAccording to this sample, it would appear that smokers have a higher lung capacity than non-smokers. This would appear to be counter-intuitive, as one would likely expect smoking to reduce lung functionality and, by extension, capacity."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d",
    "href": "posts/hw1_boonstra.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nIn order to complete this examination by group, we must create a new nominal variable that groups observations by age; this can be accomplished fairly simply using the mutate() and case_when() functions:\n\n\nCode\nlungcap_age <- lungcap %>% \n  mutate(age_group = case_when(\n    Age <= 13 ~ \"13 and under\",\n    Age == 14 | Age == 15 ~ \"14 to 15\",\n    Age == 16 | Age == 17 ~ \"16 to 17\",\n    Age >= 18 ~ \"18 and older\"\n  ))\n\n\nWith this new dataframe, we can use the group_by() function to calculate mean lung capacity by age group and smoker status:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group,Smoke) %>% \n  summarise(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke `mean(LungCap)`\n  <chr>        <chr>           <dbl>\n1 13 and under no               6.36\n2 13 and under yes              7.20\n3 14 to 15     no               9.14\n4 14 to 15     yes              8.39\n5 16 to 17     no              10.5 \n6 16 to 17     yes              9.38\n7 18 and older no              11.1 \n8 18 and older yes             10.5 \n\n\nAccording to these data, it appears that lung capacity generally increases with age. Interestingly, lung capacity is worse for smokers than it is for non-smokers in every age group except for “13 and under”. This is surprising on the surface, given that, when the data are ungrouped, smokers have a higher lung capacity than non-smokers (see part c). However, this begins to make more sense when we see how much better the “13 and under” group is represented compared to the others in this dataset:\n\n\nCode\nlungcap_age %>% \n  group_by(age_group) %>% \n  count()\n\n\n# A tibble: 4 × 2\n# Groups:   age_group [4]\n  age_group        n\n  <chr>        <int>\n1 13 and under   428\n2 14 to 15       120\n3 16 to 17        97\n4 18 and older    80\n\n\nThis high number of observations compared to other age groups likely plays a significant role in skewing the mean of the entire dataset."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e",
    "href": "posts/hw1_boonstra.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nIt is not clear to me how this part is different from part d; from what I do understand, I believe the question being asked here is addressed in that part."
  },
  {
    "objectID": "posts/hw1_boonstra.html#f",
    "href": "posts/hw1_boonstra.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nIt would appear that lung capacity and age covary together positively, such that a higher age means a higher lung capacity. We can confirm this with a simple visualization:\n\n\nCode\nlungcap %>% \n  ggplot(aes(x=Age,y=LungCap)) +\n  geom_point() +\n  geom_smooth(method='lm')"
  },
  {
    "objectID": "posts/hw1_boonstra.html#a-1",
    "href": "posts/hw1_boonstra.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 160 / 810 = 0.1975309."
  },
  {
    "objectID": "posts/hw1_boonstra.html#b-1",
    "href": "posts/hw1_boonstra.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nThe probability that a randomly selected inmate has less than 2 prior convictions is (128+434) / 810 = 0.6938272."
  },
  {
    "objectID": "posts/hw1_boonstra.html#c-1",
    "href": "posts/hw1_boonstra.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is (128+434+160) / 810 = 0.891358."
  },
  {
    "objectID": "posts/hw1_boonstra.html#d-1",
    "href": "posts/hw1_boonstra.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe probability that a randomly selected inmate has more than 2 prior convictions is (64+24) / 810 = 0.108642."
  },
  {
    "objectID": "posts/hw1_boonstra.html#e-1",
    "href": "posts/hw1_boonstra.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nBefore calculating expected value, we should put together a probability mass function for the prisoners data.\n\n\nCode\nprisoners <- prisoners %>% \n  mutate(prob=freq/810) %>% \n  mutate(expect=prob*priors)\n\nprisoners %>% \n  summarise(sum(expect))\n\n\n  sum(expect)\n1     1.28642\n\n\nThe expected value for the number of prior convictions is about 1.29 priors.\nEDIT: There is a much simpler way to compute this! Rather than using the dataframe I created, storing values and their frequencies, I can create one vector that stores each value a certain number of times, according to the given frequencies:\n\n\nCode\nprisoners_full <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nprisoners_full\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n\nBecause each value now appears as frequently as its “probability” of appearing, taking the mean of this vector also provides the correct expected value.\n\n\nCode\nmean(prisoners_full)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/hw1_boonstra.html#f-1",
    "href": "posts/hw1_boonstra.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCreating this numerical vector also makes the standard deviation calculation extremely simple in R.\n\n\nCode\nsd(prisoners_full)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html",
    "href": "posts/HW2_EthanCampbell.html",
    "title": "Homework 2",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a",
    "href": "posts/HW2_EthanCampbell.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nHere we can get the t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\n# Calculating the t statistic\nT_statistic = (410-500)/(90/(sqrt(9)))\nT_statistic\n\n\n[1] -3\n\n\nCode\n# calculating the p value\n\npvalue = 2* pt(T_statistic, df=8)\n\npvalue\n\n\n[1] 0.01707168\n\n\nCode\n# the p value is showing evidence that we would reject the null hypothesis here since it is < .05."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b",
    "href": "posts/HW2_EthanCampbell.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nTesting to see p value of it being less than 500\n\n\nCode\npvalue_left <- pt(T_statistic, df = 8, lower.tail = TRUE)\npvalue_left\n\n\n[1] 0.008535841\n\n\nCode\n# this is also showing a value smaller than the 5% given which means it is more evidence to reject the null hypothesis"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c",
    "href": "posts/HW2_EthanCampbell.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nTesting to see the p value greater than 500\n\n\nCode\npvalue_right <- pt(T_statistic, df = 8, lower.tail = FALSE)\n\npvalue_right\n\n\n[1] 0.9914642\n\n\nCode\n# Making sure the two values equal 1\nsum(pvalue_left, pvalue_right)\n\n\n[1] 1\n\n\nthis is showing a 99.14% chance of observing if the population mean was less than that 500 mark. This is interesting and we would fail to reject the null hypothesis here since it exceeds the amount specified. This would indicate that they are not getting paid the same amount."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#a-1",
    "href": "posts/HW2_EthanCampbell.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n# Lets run the test and see whats going on\n\nJones <- (519.5-500)/(10)\nSmith= (519.7-500)/(10)\n\n# Here we can see the t stat they are both getting so looking good so far\nJones\n\n\n[1] 1.95\n\n\nCode\nSmith\n\n\n[1] 1.97\n\n\nCode\n# Now to get the P-value\n\nJones_p <- 2*pt(Jones, df= 999, lower.tail = FALSE)\nSmith_p <- 2*pt(Smith, df= 999, lower.tail = FALSE)\n\n# Observing the p values\n\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#b-1",
    "href": "posts/HW2_EthanCampbell.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBased on the basic test of this with a CI of 95% we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/HW2_EthanCampbell.html#c.",
    "href": "posts/HW2_EthanCampbell.html#c.",
    "title": "Homework 2",
    "section": "C.",
    "text": "C.\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/shelton_HW1.html",
    "href": "posts/shelton_HW1.html",
    "title": "Homework 1 Solution",
    "section": "",
    "text": "1.) Using LungCapData, answer descriptive questions about the data and its distributions.\n2.) Use the given distribution to answer questions about the probability of discrete events.\n\n\n\n\n\n\nCode\n#| include: false\n#| label: Loading in LungCap\n\n og_lungcap <- readxl::read_xls(\"_data/LungCapData.xls\")\n\n# Quick look at dataset\n# glimpse(og_lungcap)\n\n# Variables - 3<dbl> ratio 3<char> (can coerce to logical if needed), \n\n# length(which(is.na(og_lungcap)))\n\n# No missing values to consider\n\n# Descriptive\n# summarytools::dfSummary(og_lungcap)\n\n\nLungCapData: Describes the lung capacity of a population of 725 children aged 3 - 19. It further categorizes the subjects by height, sex, smoking habits, and whether they were birthed using the Caesarean section technique.\nIn the following sections, we’ll use select(), group_by(), filter(), and summarize() to further explore the data and find important relations between variables.\n\n\n\n\n\n\n\n\nLungCap looks to be approximately normally distributed (unimodal, symmetric) with most observations centered around the mean (7.86).\n\n\n\n\n\nCode\nhist_gender <- ggplot(og_lungcap, aes(x=LungCap, y=..density.., fill=Gender)) +\n  geom_histogram(alpha=.5, position=\"identity\", bins=20)+\n  geom_vline(aes(xintercept=mean(LungCap)))\nhist_gender\n\n\n\n\n\nPackage ggplot2 functions ggplot() and geom_histogram() are used to display the LungCap distribution filled by the Gender variable. Both density plots center on the mean, indicating both male and female lung capacity observations are highly concentrated around the mean. The male distribution is shifted slightly to the right of the female distribution, meaning male observations had a higher upper range value than female observations. Males had more observations concentrated to the right of the mean, and the female distribution reciprocated this effect to the left of the mean.\n\n\n\n\n\nCode\nsmokers <- group_by(og_lungcap, Smoke)\nsmokers %>%\n  summarize(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nAfter creating a new dataset smokers by using group_by() on our original data, smokers is piped into a summarize() call. The results surprisingly show that the smoking group had a higher mean lung capacity than the nonsmoking group. This is likely due to a mean age difference within the groups.\n\n\n\n\n\nCode\n# Creating Age Groups Using Case When\n\nsmokers_age <- smokers %>%\n  mutate(AgeGroup = case_when(Age >= 18 ~ \"18+\", \n            Age == 16 | Age == 17 ~ \"16-17\",\n            Age == 14 | Age == 15 ~ \"14-15\",\n            Age <= 13~ \"Under 13\"))\n\n# Mean LungCap by Age and Smoke\n# Must regroup by Smoke again\nsmokers_age %>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(LungCap))\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(LungCap)`\n  <chr>    <chr>           <dbl>\n1 14-15    no               9.14\n2 14-15    yes              8.39\n3 16-17    no              10.5 \n4 16-17    yes              9.38\n5 18+      no              11.1 \n6 18+      yes             10.5 \n7 Under 13 no               6.36\n8 Under 13 yes              7.20\n\n\nAfter using mutate() to add a column AgeGroup to a copy of smokers, group_by() groups the new dataset by AgeGroup and Smoke before piping it into a summarize() command to find the grouped means of LungCap by AgeGroup and Smoke.\nThe results show that for children above the age of 13, smokers had a lower mean lung capacity than non-smokers. However, for the 13 and under group, we again see results that imply smokers have greater lung capacity than nonsmokers. Let’s investigate further into the relationship between age and lung capacity to explain this quizzical result.\n\n\n\n\n\nCode\ncov(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 8.738289\n\n\nCode\ncor(og_lungcap$Age, og_lungcap$LungCap)\n\n\n[1] 0.8196749\n\n\nCode\n#GGPlot of Age vs Lung\nggplot(og_lungcap, aes(x=Age, y=LungCap)) + geom_point()\n\n\n\n\n\nAge and LungCap have a high covariance which leads to a high correlation (p=0.82). This strong positive value (-1<p<1) indicates these variables “vary greatly” together: when Age is high in the data, so is LungCap. We cannot say that an increase in Age causes an increase Lung capacity without first showing this through regression; however, our results show the variables are highly correlated.\nWe can use knowledge of the human body to infer that as our body ages, our lungs mature. The ages of smokers of the Under 13 group are likely highly left skewed, as I don’t expect many children under 10 to be smoking. This underlying age distribution explains our puzzling results from the previous section.\n\n\nCode\nsmokers_age%>%\n  group_by(AgeGroup, Smoke) %>%\n    summarize(mean(Age))\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   AgeGroup [4]\n  AgeGroup Smoke `mean(Age)`\n  <chr>    <chr>       <dbl>\n1 14-15    no          14.5 \n2 14-15    yes         14.6 \n3 16-17    no          16.4 \n4 16-17    yes         16.6 \n5 18+      no          18.5 \n6 18+      yes         18.1 \n7 Under 13 no           9.49\n8 Under 13 yes         11.7 \n\n\n\n\n\n\nFirst, let’s create two vectors: x_val and freq. Then, we’ll use rbind() to create a table.\n\n\nCode\nx_val <-c(0,1,2,3,4)\nfreq <- c(128,434,160,64,24)\nprob <- freq/sum(freq)\n\nxdist <- rbind(x_val,prob)\n\nxdist\n\n\n           [,1]      [,2]      [,3]       [,4]       [,5]\nx_val 0.0000000 1.0000000 2.0000000 3.00000000 4.00000000\nprob  0.1580247 0.5358025 0.1975309 0.07901235 0.02962963\n\n\n\n\n\n\nCode\n# Finding probability of inmate having exactly 2 prior convictions\n\n#Column Index is 3 as the first column is 0\n\n#Surely there is a cleaner way to do this using tidyverse functions rather than base?\n\n# a\na <- xdist['prob',3] \na\n\n\n     prob \n0.1975309 \n\n\n\n\n\n\n\nCode\n#b\nb <- sum(xdist['prob',1:2])\nb\n\n\n[1] 0.6938272\n\n\n\n\n\n\n\nCode\n# c\nc <- a + b\nc\n\n\n    prob \n0.891358 \n\n\n\n\n\n\n\nCode\n#d\nd <- 1 - c\nd\n\n\n    prob \n0.108642 \n\n\n\n\n\n\n\n[1] 1.28642\n\n\n\n\n\n\n\n\n\nCode\n# Var= E(X^2) - E(X)^2\n# Again using brute force because cannot use var() function on the object xdist correctly\nvar_x <-sum((x_val^2)*prob) - ex^2\nvar_x\n\n\n[1] 0.8562353\n\n\n\n\n\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/Emily Duryea Homework 1.html",
    "href": "posts/Emily Duryea Homework 1.html",
    "title": "Duryea Homework 1",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")  \n\nhist(df$LungCap)\n\n\n\n\n\nPart A: Plotting probability density histogram\n\n\nCode\nhist(df$LungCap, \n     col=\"yellow\",\n     border=\"black\",\n     prob = TRUE,\n     xlab = \"LungCap\",\n     main = \"Density Plot\")\n\nlines(density(df$LungCap),\n      lwd = 2,\n      col = \"chocolate3\")\n\n\n\n\n\nPart B: Compare the probability distribution of the LungCap with respect to Males and Females\n\n\nCode\nggplot(df, aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"LungCap Probability Distribution for Males and Females\", y = \"Probability density\")\n\n\n\n\n\nPart C: Compare the mean lung capacities for smokers and non-smokers\n\n\nCode\nmean_smoking <- df %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nmean_smoking\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nThe means of smokers vs non smokers does not make sense since non smokers have a lower mean lung cap, when one would think it would be the other way around. However, limited data is provided on the sample, so there could be other factors in play.\nPart D: Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”\n\n\nCode\ndf <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\ndf %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGroup)) +\n  theme_classic() + \n  labs(title = \"LungCap and Smoke based on age groups\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nBased on the histograms, Part D seems to contrast with Part C, since the plots seem to demonstrate non-smokers having higher lung capacity than smokers in all age groups. Additionally, lung capacity appears to decrease with age based on the graph.\nPart E: Compare the lung capacities for smokers and non-smokers within each age group\n\n\nCode\ndf %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(title = \"LungCap and Smoke based on age and smoker vs nonsmoker\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nBased on information gained in Part D and Part E, it appears that lung capacity decreases with age, and, despite the means in Part C, lung capacity is higher for non-smokers.\nPart F: Calculate the correlation and covariance between Lung Capacity and Age\n\n\nCode\nCov_lungcapage <- cov(df$LungCap, df$Age)\nCor_lungcapeage <- cor(df$LungCap, df$Age)\nCov_lungcapage\n\n\n[1] 8.738289\n\n\nCode\nCor_lungcapeage\n\n\n[1] 0.8196749\n\n\nBecause both the covariance and correlation are positive numbers, the relationship between lung capacity and age are positively related, meaning as one increases, the other also increases in a proportional manner.\nQuestion 2\n\n\nCode\nPrior_Convictions <- c(0:4)\nInmate_Number <- c(128, 434, 160, 64, 24)\nip <- tibble(Prior_Convictions, Inmate_Number)\n\nip <- mutate(ip, Probability = Inmate_Number/sum(Inmate_Number))\nip\n\n\n# A tibble: 5 × 3\n  Prior_Convictions Inmate_Number Probability\n              <int>         <dbl>       <dbl>\n1                 0           128      0.158 \n2                 1           434      0.536 \n3                 2           160      0.198 \n4                 3            64      0.0790\n5                 4            24      0.0296\n\n\nPart A: What is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nip %>%\n  filter(Prior_Convictions == 2) %>%\n  select(Probability)\n\n\n# A tibble: 1 × 1\n  Probability\n        <dbl>\n1       0.198\n\n\nThe probability that a randomly selected inmate has exactly two prior convictions is 0.1975309.\nPart B: What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\npartb <- ip %>%\n  filter(Prior_Convictions < 2)\nsum(partb$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability that a randomly selected inmate has fewer than two prior convictions is 0.6938272.\nPart C: What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\npartc <- ip %>%\n  filter(Prior_Convictions <= 2)\nsum(partc$Probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has two or fewer prior convictions is 0.891358.\nPart D: What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\npartd <- ip %>%\n  filter(Prior_Convictions > 2)\nsum(partd$Probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than two prior convictions is 0.108642.\nPart E: What is the expected value for the number of prior convictions?\n\n\nCode\nip <- mutate(ip, vl = Prior_Convictions*Probability)\nparte <- sum(ip$vl)\nparte\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.28642.\nPart F: Calculate the variance and the standard deviation for the Prior Convictions\n\n\nCode\nip_var <-sum(((ip$Prior_Convictions-parte)^2)*ip$Probability)\nip_var\n\n\n[1] 0.8562353\n\n\nCode\nsqrt(ip_var)\n\n\n[1] 0.9253298\n\n\nThe variance for prior convictions is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html",
    "href": "posts/HW2_KarenKimble.html",
    "title": "Kimble HW 2",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(stats)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-1",
    "href": "posts/HW2_KarenKimble.html#question-1",
    "title": "Kimble HW 2",
    "section": "Question 1",
    "text": "Question 1\n\nAngiography\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 9 / sqrt(847)\n\nt_score <- qt(p = 1-tail_area, df = 846)\n\nCI_1 <- c(18 - t_score * standard_error, 18 + t_score * standard_error)\n\nprint(CI_1)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nBypass Surgery\n\n\nCode\nconfidence_level <- 0.90\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 10 / sqrt(539)\n\nt_score <- qt(p = 1-tail_area, df = 538)\n\nCI_2 <- c(19 - t_score * standard_error, 19 + t_score * standard_error)\n\nprint(CI_2)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval for angiography (range of about 1.1) is smaller than the confidence interval for bypass surgery (range of about 1.5) at a confidence level of 90%."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-2",
    "href": "posts/HW2_KarenKimble.html#question-2",
    "title": "Kimble HW 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\n# Point estimate\n\np <- 567/1031\n\np\n\n\n[1] 0.5499515\n\n\nThe point estimate p for the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n# Confidence Interval\n\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_deviation <- sqrt((p * (1-p))/1031)\n\nstandard_error <- standard_deviation / sqrt(1031)\n\nt_score <- qt(p = 1-tail_area, df = 1030)\n\nCI_3 <- c(p - t_score * standard_error, p + t_score * standard_error)\n\nprint(CI_3)\n\n\n[1] 0.5490046 0.5508984\n\n\nThrough this test, I am 95% confident that the true proportion of all adult Americans who believe a college education is essential for success lies between 0.549 and 0.551."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-3",
    "href": "posts/HW2_KarenKimble.html#question-3",
    "title": "Kimble HW 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nsd <- (200-30)/4\n\nx <- 5/(2.26 * sd)\n\nsample_size = (1/x)^2\n\nsample_size\n\n\n[1] 369.0241\n\n\nThe size of the sample should be at least 370 people. Since they want a confidence interval within 5 dollars and they assume the standard deviation is a quarter of the range of 30 dollars to 200 dollars, I was able to use the confidence interval equation to find the missing variable of sample size. The confidence level is 95%, meaning that with a large sample size, the t-score would be around 2.26, allowing me to use the equation and isolate the sample size."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-4",
    "href": "posts/HW2_KarenKimble.html#question-4",
    "title": "Kimble HW 2",
    "section": "Question 4",
    "text": "Question 4\n\nPart A\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\n\n\nCode\nconfidence_level <- 0.95\n\ntail_area <- (1-confidence_level)/2\n\nstandard_error <- 90 / sqrt(9)\n\nt_score <- qt(p = 1-tail_area, df = 8)\n\nCI_4A <- c(410 - t_score * standard_error, 410 + t_score * standard_error)\n\np_value = 2 * pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05\n\n\nCode\nprint(CI_4A)\n\n\n[1] 340.8199 479.1801\n\n\nThe p-value is exactly 0.05, meaning it is not smaller than the alpha value of 0.05 and thus is not statistically significant. We do not have enough evidence to reject the null hypothesis. The confidence interval shows that we are 95% confident the true mean income of female employees lies between 340.82 dollars/week and 479.18 dollars/week.\n\n\nPart B\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = TRUE)\n\np_value\n\n\n[1] 0.975\n\n\nThe p-value for the alternate hypothesis that the true mean income of female employees is greater than 500 dollars/week is 0.975. This value is extremely large and greater than the 0.05 alpha level, meaning there is not statistically significant evidence and thus do not reject the null hypothesis.\n\n\nPart C\n\n\nCode\np_value = pt(q = t_score, df = 8, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.025\n\n\nThe p-value for the alternative hypothesis that the true mean income of female employees is less than 500 dollars/week is 0.025. This value is less than the alpha value of 0.05, meaning it is statistically significant and we can reject the null hypothesis. The true mean income of female employees is likely less than 500 dollars/week."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-5",
    "href": "posts/HW2_KarenKimble.html#question-5",
    "title": "Kimble HW 2",
    "section": "Question 5",
    "text": "Question 5\n\nPart A\n\n\nCode\n# Jones\n\nt_score <- (519.5-500)/(10)\n\nt_score\n\n\n[1] 1.95\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.05145555\n\n\n\n\nCode\n# Smith\n\nt_score <- (519.7-500)/(10)\n\nt_score\n\n\n[1] 1.97\n\n\nCode\np_value <- 2 * pt(q = t_score, df = 999, lower.tail = FALSE)\n\np_value\n\n\n[1] 0.04911426\n\n\n\n\nPart B\nFor Jones’s study, the results were not statistically significant because the p-value of 0.51 is greater than the alpha value of 0.05. For Smith’s study, the results were statistically signficiant because the p-value of 0.49 is less than the alpha value of 0.05.\n\n\nPart C\nReporting the result of a test as P being greater or less than the alpha value can be misleading if the p value is not reported. In both studies, the p-value was .01 away from 0.05, yet in only one study was the result statistically significant. A small p-value may still be meaningful to report because it still shows that there was a relatively small probability of getting the result that one did. Not reporting the p-value when reporting the result and whether or not a hypothesis is rejected leaves an important part of the study out. Someone simply reading that a hypothesis was not rejected without knowing the p-value may assume the p-value was very large even when it was small (such as in the case of Smith vs. Jones), thus leaving out a major aspect of the study."
  },
  {
    "objectID": "posts/HW2_KarenKimble.html#question-6",
    "href": "posts/HW2_KarenKimble.html#question-6",
    "title": "Kimble HW 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, alternative = c(\"less\"), mu = 45)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value of this test is 0.038, which is less than the alpha value of 0.05. This means that there is statistically significant evidence and we can reject the null hypothesis, that the true average gas tax per gallon in the United States is 45 cents. There is significant evidence to suggest that the true average gas tax per gallon in the United States is less than 45 cents."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html",
    "href": "posts/DACSS 603 Final Part 1.html",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2018-2019_School_Demographic_Snapshot.csv' does not exist.\n\n\nCode\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\n\nError in number(x = x, accuracy = accuracy, scale = scale, prefix = prefix, : object 'NYC_2019' not found\n\n\nCode\nNYC_2021 <- read_csv(\"/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n\nError: '/Users/karenkimble/Documents/UMass/SPP/Fall 2022/DACSS 603/DACSS Final Project/NYC School Data/2020-2021_Demographic_Snapshot_School.csv' does not exist.\n\n\nCode\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n\nError in rbind(NYC_2019, NYC_2021): object 'NYC_2019' not found\n\n\nCode\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n\nError in recode(school_data$`% Poverty`, `Above 95%` = \"95%\", `Below 5%` = \"5%\"): object 'school_data' not found\n\n\nCode\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\n\nError in lapply(X = X, FUN = FUN, ...): object 'school_data' not found\n\n\nCode\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found\n\n\nCode\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nError in eval(expr, envir, enclos): object 'school_data' not found"
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#research-question",
    "href": "posts/DACSS 603 Final Part 1.html#research-question",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "href": "posts/DACSS 603 Final Part 1.html#hypothesis",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "href": "posts/DACSS 603 Final Part 1.html#descriptive-statistics",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nError in glimpse(school_data): object 'school_data' not found\n\n\n\n\nCode\nsummary(school_data)\n\n\nError in summary(school_data): object 'school_data' not found\n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/DACSS 603 Final Part 1.html#references",
    "href": "posts/DACSS 603 Final Part 1.html#references",
    "title": "DACSS 603 Final Project - Proposal",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/MeghaJoseph_HW2.html",
    "href": "posts/MeghaJoseph_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "##QUESTION1\n\n\nCode\nprocedure <- c('Bypass', 'Angiography')\nsamplesize <- c(539, 847)\nmeanwait <- c(19, 18)\nstandev <- c(10, 9)\n\nsurgdata <- data.frame(procedure, samplesize, meanwait, standev)\n\nsurgdata\n\n\n    procedure samplesize meanwait standev\n1      Bypass        539       19      10\n2 Angiography        847       18       9\n\n\n##QUESTION2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n##QUESTION3\n\n\nCode\nstdevBooks <- (200-30)/4\nmargerrorBooks <- (10/2)\nzBooks <- 1.96\n\nstdevBooks^2 * (zBooks/margerrorBooks)^2\n\n\n[1] 277.5556\n\n\n##QUESTION4 ##A\n\n\nCode\n(410-500)/(90/sqrt(9))\n\n\n[1] -3\n\n\nCode\npt(-3, 8)*2 \n\n\n[1] 0.01707168\n\n\n##B\n\n\nCode\n pt(-3, 8, lower.tail = TRUE)\n\n\n[1] 0.008535841\n\n\n##C\n\n\nCode\n pt(-3, 8, lower.tail = FALSE)\n\n\n[1] 0.9914642\n\n\n##QUESTION5 ##A1\n\n\nCode\n JonesT <- (519.5-500)/10\nJonesT\n\n\n[1] 1.95\n\n\n##A2\n\n\nCode\n JonesP <- pt(1.95, 999, lower.tail = FALSE)*2\nJonesP\n\n\n[1] 0.05145555\n\n\n##A3\n\n\nCode\nSmithT <- (519.7-500)/10\nSmithT\n\n\n[1] 1.97\n\n\n##A4\n\n\nCode\nSmithP <- pt(1.97, 999, lower.tail = FALSE)*2\nSmithP\n\n\n[1] 0.04911426\n\n\n##B\nWith an α-level of .05, the p-values that both Jones (P=.051) and Smith (P=.049) found are very close to equivalent. Although Jones’ P-value is slightly greater than α=.05 and Smith’s P-value is slightly less than α=.05, the proximity of the results should yield the same conclusion. Both P-values provide moderate evidence to reject the null hypothesis and indicate that the mean is not equal to 500. If we were to technically interpret the P-values, then Jones’ test would fail to reject the null hypothesis, and Smith’s test would reject the null hypothesis.\n##C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. For example, a P-value of .009 for a significance level of .05 provides much stronger evidence to reject the null than a P-value of .045, however both values allow for rejection of the null at the significance level .05. In the Jones/Smith example, reporting the results only as “P ≤ 0.05” versus “P > 0.05” will lead to different conclusions about very similar results (rejecting versus failing to reject the null).\n##QUESTION6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html",
    "href": "posts/KalimahMuhammad_hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyr)\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw2.html#questions",
    "href": "posts/KalimahMuhammad_hw2.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\n1.Cardiac Care Network - Wait Times for Cardiac Surgeries\nPrompt: The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (“Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population.\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures.\n\nBypass Surgery Confidence Interval\n\n\nCode\n#calculate confidence interval for bypass surgery\nmean<- 19 #mean wait time\nsd<-10 #standard deviation\nn <-539 #sample size\nbypass_se <- (sd/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score<- qt(p=1-tail_area, df=n-1) #determine t-score\nbypass_CI <- c(mean - t_score* bypass_se,\n               mean + t_score* bypass_se) #calculate confidence interval\nprint(bypass_CI)\n\n\n[1] 18.29029 19.70971\n\n\nThe confidence interval (CI) for the average wait time for bypass surgery is between 18.29 and 19.71 days.\n\n\nAngiography Confidence Interval\n\n\nCode\n#Calculate cofidence interval for angiography\n#mean= 18, sd=9, n=847\nmean_ag<- 18 #mean wait time\nsd_ag<-9 #standard deviation\nn_ag <-847 #sample size\nag_se <- (sd_ag/sqrt(n)) # calculate sample standard error \nconf_level <-0.9 #establish 90% confidence interval\ntail_area <- (1-conf_level)/2 #calculate tail area\nt_score_ag<- qt(p=1-tail_area, df=n-1) #determine t-score\nag_CI <- c(mean_ag - t_score_ag* ag_se,\n               mean_ag + t_score_ag* ag_se) #calculate confidence interval\nprint(ag_CI)\n\n\n[1] 17.36126 18.63874\n\n\nMeanwhile, the CI for the angiography mean wait time is between 17.36 and 18.63 days.\n\n\nIs the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n19.70971-18.29029 #difference in bypass surgery CI range\n\n\n[1] 1.41942\n\n\nCode\n18.63874-17.36126 #difference in angiography CI range\n\n\n[1] 1.27748\n\n\nThe range in the confidence interval for angiography is 1.28 narrower than the bypass surgery, 1.42.\n\n\n\n\n2. National Center for Public Policy - Is college essential for success?\nPrompt: A survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#proportion of US adults who believe college is essential for success\nprop.test(567,1031,conf.level = 0.95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate of the proportion of all adult Americans who believe that a college education is essential for success is 0.55. The confidence interval set at 95% ranges between 0.52 and 0.58.\n\n\n\n3. Student Sample Size\nPrompt: Suppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within 5 dollars of the true population mean (i.e. they want the confidence interval to have a length of 10 dollars or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between 30 dollars and 200 dollars. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#calculate the sample size\npop_sd<-(200-30)/4\ncritical_value<-1.96 #based off signigicance level of 5\nsample_size<- ((pop_sd*critical_value)/5)^2 \nprint(sample_size)\n\n\n[1] 277.5556\n\n\nThe sample size should be 278 students to estimate the mean cost of textbooks per semester.\n\n\n\n4. Income for Union Workers\nPrompt: According to a union agreement, the mean income for all senior-level workers in a large service company equals 500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. Report the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nsam_mean<-410\nmu<-500\nsam_sd<-90\nn<-9\n\nt_score<- (sam_mean-mu)/(sam_sd/(sqrt(n)))\nprint(t_score)\n\n\n[1] -3\n\n\nCode\nupper_tail<- pt(t_score, df=n-1, lower.tail = FALSE)\nprint(upper_tail)\n\n\n[1] 0.9914642\n\n\nCode\nlower_tail<- pt(t_score, df=n-1, lower.tail = TRUE)\nprint(lower_tail)\n\n\n[1] 0.008535841\n\n\nCode\np_value<- upper_tail + lower_tail\nprint(p_value)\n\n\n[1] 1\n\n\n\n\n\n5. Jones and Smith\nPrompt: Jones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7 with se = 10.0.\n\n\nCode\nmu<- 500 #hypothesized population mean\nj_mean<-519.5 #Jones's mean\ns_mean<-519.7 #Smith's mean\nn=1000 #sample size\nse<-10 #standard error\n\n\nShow that t = 1.95 and P-value = 0.051 for Jones.\n\n\nCode\n#calculate the t-score for Jones\nj_tscore<-(j_mean - mu)/se\nprint(j_tscore)\n\n\n[1] 1.95\n\n\nCode\n#calculate p-value for Jones\nj_pvalue<- pt(j_tscore, df=n-1, lower.tail = FALSE) *2\nprint(j_pvalue)\n\n\n[1] 0.05145555\n\n\nShow that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#calculate the t-score for Smith\ns_tscore<-(s_mean - mu)/se\nprint(s_tscore)\n\n\n[1] 1.97\n\n\nCode\n#calculate p-value for Smith\ns_pvalue<- pt(s_tscore, df=n-1, lower.tail = FALSE) *2\nprint(s_pvalue)\n\n\n[1] 0.04911426\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.” Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIn Smith’s test, the p-value of .049, less than 0.05, coupled with the significance level of 0.05 indicate a statistically significant result to reject the null hypothesis. However, the results from the Jones’s test with a p-value of 0.052, greater than 0.05, indicates the results were not statistically significant and the null was retained.\nTheses results can be misleading as the significance level impacts how the p-values are referenced when under 0.05. P-values over 0.05 will typically retain the null, however p-values under 0.05 are influenced by the significance level to determine whether results are statistically significant to reject the null hypothesis.\n\n\n\n6. US Gas Tax\nPrompt:Are the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n#t-test of average gas taxes from sample cities\nt.test(gas_taxes, alternative = c(\"less\"), mu=45, conf.level = 0.95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nYes, using the t-test to compare the sample mean (40.86) to the hypothesized population mean (45) at the confidence level of 95% resulted in a favorable conclusion that the sample average was less than 45 cents."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html",
    "href": "posts/HW2_RoyYoon.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#bypass-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Bypass Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Bypass Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nbypass_sample_size <- 539\nbypass_mean <- 19\nbypass_sd <- 10"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Standard Error for Bypass",
    "text": "Standard Error for Bypass\n\n\nCode\nbypass_standard_error <- bypass_sd / sqrt(bypass_sample_size)\n\nbypass_standard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-bypass",
    "title": "Homework 2",
    "section": "T-Value Steps for Bypass",
    "text": "T-Value Steps for Bypass\n\n\nCode\nbypass_confidence_level <- 0.90\nbypass_tail_area <- (1 - bypass_confidence_level)/2\n\nbypass_tail_area\n\n\n[1] 0.05\n\n\nCode\nbypass_t_score <- qt(p = 1 - bypass_tail_area, df = bypass_sample_size - 1)\n\nbypass_t_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-bypass-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Bypass Confidence Interval",
    "text": "Calculate Bypass Confidence Interval\n\n\nCode\nbypass_confidence_interval <- c(bypass_mean - (bypass_t_score * bypass_standard_error),\n                                bypass_mean + (bypass_t_score * bypass_standard_error))\n\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Margin of Error for bypass",
    "text": "Margin of Error for bypass\n\n\nCode\nbypass_margin_of_error <- bypass_t_score * bypass_standard_error\n\nbypass_margin_of_error\n\n\n[1] 0.7097107"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "href": "posts/HW2_RoyYoon.html#angiography-sample-size-mean-wait-time-standard-deviation",
    "title": "Homework 2",
    "section": "Angiography Sample Size, Mean Wait Time, Standard Deviation",
    "text": "Angiography Sample Size, Mean Wait Time, Standard Deviation\n\n\nCode\n# Bypass information\nangiography_sample_size <- 847\nangiography_mean <- 18\nangiography_sd <- 9"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#standard-error-for-angiography",
    "title": "Homework 2",
    "section": "Standard Error for Angiography",
    "text": "Standard Error for Angiography\n\n\nCode\nangiography_standard_error <- angiography_sd / sqrt(angiography_sample_size)\n\nangiography_standard_error\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "href": "posts/HW2_RoyYoon.html#t-value-steps-for-angiography",
    "title": "Homework 2",
    "section": "T-Value Steps for Angiography",
    "text": "T-Value Steps for Angiography\n\n\nCode\nangiography_confidence_level <- 0.90\nangiography_tail_area <- (1 - angiography_confidence_level)/2\n\nangiography_tail_area\n\n\n[1] 0.05\n\n\nCode\nangiography_t_score <- qt(p = 1 - angiography_tail_area, df = angiography_sample_size - 1)\n\nangiography_t_score\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "href": "posts/HW2_RoyYoon.html#calculate-angiography-confidence-interval",
    "title": "Homework 2",
    "section": "Calculate Angiography Confidence Interval",
    "text": "Calculate Angiography Confidence Interval\n\n\nCode\nangiography_confidence_interval <- c(angiography_mean - (angiography_t_score * angiography_standard_error),\n                                angiography_mean + (angiography_t_score * angiography_standard_error))\n\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "href": "posts/HW2_RoyYoon.html#margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Margin of Error for Angiography",
    "text": "Margin of Error for Angiography\n\n\nCode\nangiography_margin_of_error <- angiography_t_score * angiography_standard_error\n\nangiography_margin_of_error\n\n\n[1] 0.5092182"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "href": "posts/HW2_RoyYoon.html#comparing-bypass-and-angiogrpahy-confidence-intervals",
    "title": "Homework 2",
    "section": "Comparing Bypass and Angiogrpahy Confidence Intervals",
    "text": "Comparing Bypass and Angiogrpahy Confidence Intervals\n\n\nCode\nangiography_confidence_interval\n\n\n[1] 17.49078 18.50922\n\n\nCode\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\nCode\nbypass_confidence_interval\n\n\n[1] 18.29029 19.70971\n\n\nCode\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\nAngiography has a more narrow confidence interval"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#point-estimate",
    "href": "posts/HW2_RoyYoon.html#point-estimate",
    "title": "Homework 2",
    "section": "Point Estimate",
    "text": "Point Estimate\n\n\nCode\nsample_size <- 1031\n\neducation_essential <- 567\n\npoint_estimate <- education_essential / sample_size\n\npoint_estimate \n\n\n[1] 0.5499515\n\n\nCode\nprop.test(education_essential, sample_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  education_essential out of sample_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate for adult Americans who believe college education is essential is 0.5499515. The 95 percent confidence interval is 0.5189682, 0.5805580. 95% of confidence intervals calculated with this procedure would contain the true mean. With the sampling method repeated, about 95% of the intervals would contain the true mean."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a.",
    "href": "posts/HW2_RoyYoon.html#a.",
    "title": "Homework 2",
    "section": "A.",
    "text": "A.\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nAssumptions:\n\nData is normally distributed; significance level is 5%\nNull Hypothesis (H0): μ = 500\nAlternative Hypothesis(H1): μ < 500, μ > 500"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-standard-error",
    "href": "posts/HW2_RoyYoon.html#a-standard-error",
    "title": "Homework 2",
    "section": "A: Standard Error",
    "text": "A: Standard Error\n\n\nCode\nfemale_standard_dev <- 90\nfemale_sample_size <- 9\nfemale_sample_mean <- 410 \nnull_hypothesis_mean <- 500\n\nfemale_standard_error <- female_standard_dev / sqrt(female_sample_size)\n\nfemale_standard_error\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-t-test",
    "href": "posts/HW2_RoyYoon.html#a-t-test",
    "title": "Homework 2",
    "section": "A: t-test",
    "text": "A: t-test\n\n\nCode\nt_stat <- (female_sample_mean - null_hypothesis_mean) / female_standard_error\n\nt_stat\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a-p-value",
    "href": "posts/HW2_RoyYoon.html#a-p-value",
    "title": "Homework 2",
    "section": "A: p-value",
    "text": "A: p-value\n\n\nCode\np_value <- (pt(t_stat, df = 8)) * 2\n\np_value\n\n\n[1] 0.01707168\n\n\np-value (0.01707168) is smaller than the 5% significance level, so we are able to reject the null hypothesis and favor the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "href": "posts/HW2_RoyYoon.html#b.-report-the-p-value-for-ha-μ-500.-interpret.",
    "title": "Homework 2",
    "section": "B. Report the P-value for Ha : μ < 500. Interpret.",
    "text": "B. Report the P-value for Ha : μ < 500. Interpret.\n\n\nCode\nlow_p_value <- (pt(t_stat, df = 8, lower.tail = TRUE))\nlow_p_value\n\n\n[1] 0.008535841"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "href": "posts/HW2_RoyYoon.html#c.-report-and-interpret-the-p-value-for-h-a-μ-500.-hint-the-p-values-for-the-two-possible-one-sided-tests-must-sum-to-1.",
    "title": "Homework 2",
    "section": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)",
    "text": "C. Report and interpret the P-value for H a: μ > 500. (Hint: The P-values for the two possible one-sided tests must sum to 1.)\n\n\nCode\nup_p_value <- (pt(t_stat, df = 8, lower.tail = FALSE))\nup_p_value\n\n\n[1] 0.9914642\n\n\nCode\n#sanity check from hint: The P-values for the two possible one-sided tests must sum to 1\nlow_p_value + up_p_value\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#a",
    "href": "posts/HW2_RoyYoon.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\n#Jones\n\njones_t_stat <- (jones_sample_mean - h0_mean) / jones_standard_error\n\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- (pt(jones_t_stat, df = 999, lower.tail=FALSE)) * 2\n\njones_p_value\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n\nsmith_t_stat <- (smith_sample_mean - h0_mean) / smith_standard_error\n\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- (pt(smith_t_stat, df = 999, lower.tail=FALSE) ) * 2\n\nsmith_p_value\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#b",
    "href": "posts/HW2_RoyYoon.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nif the p value is less than the significance level, the null hypothesis is rejected.\nsignificance level: α = 0.05\nJones p-value: 0.05145555; Jones p-value > significance level, so the study is not statistically significant\nsmith p-value: 0.04911426; Smith p-value < significance level, so the study is statistically significant"
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#c",
    "href": "posts/HW2_RoyYoon.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\nIf the p value is less than the significance level, the null hypothesis is rejected and we can consider the finding/result statistically insignificant. So blanketing Jone’s study as not statistically significant and and Smith’s study as significant may make sense. However, when you look at the sample means from Jones and Smith, there is not an outstanding difference between them. By the way the mathematics of calculating the p-value worked out, it can seem as though there is a great difference between the numbers reported by Jones and Smith, when their reported numbers are actually very narrow to each other."
  },
  {
    "objectID": "posts/HW2_RoyYoon.html#question-6",
    "href": "posts/HW2_RoyYoon.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nWith a sample mean of 40.86278 and a p-value of 0.03827 which is less than the assumes 0.05 significance level, the null hypothesis is rejected and the alternative hypothesis is favored.\nThus, there is enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html",
    "href": "posts/HW2_SteveONeill.html",
    "title": "Homework 2",
    "section": "",
    "text": "“The time between the date a patient was recommended for heart surgery and the surgery date for cardiac patients in Ontario was collected by the Cardiac Care Network (”Wait Times Data Guide,” Ministry of Health and Long-Term Care, Ontario, Canada, 2006). The sample mean and sample standard deviation for wait times (in days) of patients for two cardiac procedures are given in the accompanying table. Assume that the sample is representative of the Ontario population”\n\n\n\nCode\nbypass <- data.frame(sample_size = 539,\n                     mean_wait_time = 19,\n                     standard_dev = 10)\nbypass \n\n\n  sample_size mean_wait_time standard_dev\n1         539             19           10\n\n\nCode\nangiography <- data.frame(sample_size = 847,\n                     mean_wait_time = 18,\n                     standard_dev = 9)\nangiography\n\n\n  sample_size mean_wait_time standard_dev\n1         847             18            9\n\n\n\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-2",
    "href": "posts/HW2_SteveONeill.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\n\nCode\ncollege_n <- 1031\ncollege_k <- 567\ncollege_p <- college_k/college_n\ncollege_p\n\n\n[1] 0.5499515\n\n\nThe point estimate for the proportion of all adult Americans who believe that a college education is essential for success is simply 0.5499515.\n\n\nCode\ncollege_moe <- qnorm(0.975)*sqrt(college_p*(1-college_p)/college_n)\ncollege_moe\n\n\n[1] 0.03036761\n\n\nCode\ncollege_CI_low <- college_p - college_moe\ncollege_CI_high <- college_p + college_moe\ncollege_CI_low\n\n\n[1] 0.5195839\n\n\nCode\ncollege_CI_high\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the point estimate is [0.5195839, 0.5803191]."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-3",
    "href": "posts/HW2_SteveONeill.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\n\nCode\n#The estimate will be useful if it is within $5 of the true population mean\nbooks_moe <- 5\n#The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200.\nbooks_range <- (200-30)\n#They think that the population standard deviation is about a quarter of this range\nbooks_sd <- books_range / 4\nbooks_sd\n\n\n[1] 42.5\n\n\nA 5% alpha means a 95% confidence level. Conventionally we know the ‘critical value’ for the 95% confidence interval is 1.96\nUnfortunately, I’m missing the equation that lets us find the sample size from these values!"
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-4",
    "href": "posts/HW2_SteveONeill.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\n\n\nCode\n#the mean income for all senior-level workers in a large service company equals $500 per week.\nunion_pop_mean = 500\n#For a random sample of nine female employees,\nunion_sample_size = 9\n# ȳ = $410\nunion_sample_mean = 410\n# and s = 90\nunion_sample_sd = 90\n\n\n\nT-statistic\nAssuming the sample is normally distributed, the formula for a t-statistic is: (sample mean - population mean) / (sample standard deviation / square root of sample size)\n\n\nCode\ntstatistic <- (union_sample_mean - union_pop_mean) / (union_sample_sd / sqrt(union_sample_size))\ntstatistic\n\n\n[1] -3\n\n\nThe t-statistic is -3.\n\n\nHypotheses\nIn this case, the null hypothesis is that the mean income for female employees matches $500/wk. The alternate hypothesis is that it does not equal $500/wk.\npt takes the T-statistic and degrees of freedom and returns the p-value to the left. If we multiply it by two, it becomes ‘two-tailed’:\n\n\nCode\n2 * pt(-3, (union_sample_size - 1))\n\n\n[1] 0.01707168\n\n\nThis two-tailed test returns a p-value of 0.01707168 his is well under the significance level of 5%, meaning that there is only a 1.7 percent chance of receiving as extreme a result as this under the null hypothesis. Therefore the null hypothesis is said to be rejected."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-5",
    "href": "posts/HW2_SteveONeill.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\n\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0\n\n\n\nCode\njones_smith_pop_mean = 500\njones_smith_sample_size = 10000\n\njones_sample_mean = 519.5\nsmith_sample_mean = 519.7\n\njones_smith_se = 10\n\n\n\n\nCode\njones_t_statistic <- (jones_sample_mean - jones_smith_pop_mean) / jones_smith_se\njones_t_statistic\n\n\n[1] 1.95\n\n\nCode\njones_p_value <- 2*pt(jones_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\njones_p_value\n\n\n[1] 0.05120403\n\n\nCode\nsmith_t_statistic <- (smith_sample_mean - jones_smith_pop_mean) / jones_smith_se\nsmith_t_statistic\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value <- 2*pt(smith_t_statistic, (jones_smith_sample_size - 1), lower.tail = FALSE)\nsmith_p_value\n\n\n[1] 0.04886592\n\n\n\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\n\nUsing an alpha of .05, Jones’ p value is not ‘statistically significant’, but Smith’s is.\nBoth Jones and Smith’s results should be presented with the P-values included so that knowledgeable people can see how borderline they are. Alternately, different significance levels could be adopted - or the sample size could be increased."
  },
  {
    "objectID": "posts/HW2_SteveONeill.html#question-6",
    "href": "posts/HW2_SteveONeill.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\ngas_mean <- gas_taxes %>% mean()\n\n\nError in gas_taxes %>% mean(): could not find function \"%>%\"\n\n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\n\nCode\nt.test(gas_taxes, mu = gas_mean, alternative = 'less')\n\n\nError in t.test.default(gas_taxes, mu = gas_mean, alternative = \"less\"): object 'gas_mean' not found\n\n\nWith the information we have, I am not sure if we have enough evidence to conclude about the average tax per gallon in 2005. Otherwise, the p-value is within bounds at .5 and yes, this would be statistically significant."
  },
  {
    "objectID": "posts/Homework 1 LJones.html",
    "href": "posts/Homework 1 LJones.html",
    "title": "Homework 1",
    "section": "",
    "text": "First I’ll load the libraries and read in the data.\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlc <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nThe distribution of lung capacity is as follows:\n\n\nCode\nhist(lc$LungCap)\n\n\n\n\n\nThe histogram appears close to the normal distribution.\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data=lc)\n\n\n\n\n\n\n\n\n\n\nCode\nlc %>%\n  group_by(Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nInterestingly, the mean lung capacity is higher for smokers than it is for non-smokers.\n\n\n\n\n\nCode\nlcbyagegrp <- lc %>% \n  mutate(age_group = case_when(\n    Age <=13 ~ \"13 and Under\",\n    Age >=14 & Age <=15 ~\"14-15\",\n    Age >=16 & Age <=17 ~\"16 - 17\",\n    Age >=18 ~\"18+\")) %>% \n  arrange(age_group, Age)\n\nggplot(lcbyagegrp, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(age_group ~ Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nCode\nlcbyagegrp %>%\n  group_by(age_group, Smoke) %>%\n  summarize(Mean = mean(LungCap))\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   age_group [4]\n  age_group    Smoke  Mean\n  <chr>        <chr> <dbl>\n1 13 and Under no     6.36\n2 13 and Under yes    7.20\n3 14-15        no     9.14\n4 14-15        yes    8.39\n5 16 - 17      no    10.5 \n6 16 - 17      yes    9.38\n7 18+          no    11.1 \n8 18+          yes   10.5 \n\n\n\n\nThe mean lung capacity for smokers aged 13 and under is higher than that of non-smokers in the same age group, which defies expectation. The rest of the age groups meet that expectation. There may be an error or extreme outlier in the data for smokers aged 13 and under.\n\n\n\n\n\n\nCode\nlc %>% cov(Age, LungCap)\n\n\nError in pmatch(use, c(\"all.obs\", \"complete.obs\", \"pairwise.complete.obs\", : object 'LungCap' not found\n\n\n\n\nCode\n#correlation\ncor(lc$LungCap,lc$Age)\n\n\n[1] 0.8196749\n\n\nCode\n#covariance\ncov(lc$LungCap, lc$Age)\n\n\n[1] 8.738289\n\n\nThe correlation is very close to positive 1, indicating a strong positive correlation between between lung capacity and age. The covariance being a positive number indicates a positive relationship.\n\n\n\n\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(X, Frequency)\n\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\n\n\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\ndf2\n\n\n  X Frequency Probability\n1 0       128  0.15802469\n2 1       434  0.53580247\n3 2       160  0.19753086\n4 3        64  0.07901235\n5 4        24  0.02962963\n\n\nThe probability is about 19.75%.\n\n\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\nsum(b2$Probability)\n\n\n[1] 0.6938272\n\n\nThe probability is about 69%.\n\n\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\nsum(c2$Probability)\n\n\n[1] 0.891358\n\n\nThe probability is about 89%.\n\n\n\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\nsum(d2$Probability)\n\n\n[1] 0.108642\n\n\nThe probability is about 10.9%.\n\n\n\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\ne\n\n\n[1] 1.28642\n\n\nThe expected number of prior convictions is about 1.286.\n\n\n\n\n\nCode\n#variance\nvariance <- (sum(Frequency*((X-e)^2)))/(sum(Frequency)-1)\nvariance\n\n\n[1] 0.8572937\n\n\nCode\n#standard deviation\nsd <- sqrt(variance)\nsd\n\n\n[1] 0.9259016\n\n\nThe variance of prior convictions is about 0.857, and the standard deviation (simply, the square root of the variance) is about 0.926."
  },
  {
    "objectID": "posts/KenDocekal_HW2.html",
    "href": "posts/KenDocekal_HW2.html",
    "title": "HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q1",
    "href": "posts/KenDocekal_HW2.html#q1",
    "title": "HW2",
    "section": "Q1",
    "text": "Q1\n90% Confidence Interval for Bypass:\n18.29 - 19.71\n\n\nCode\nn <- 539\nxbar <- 19 \ns <- 10\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 18.29029\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 19.70971\n\n\n90% Confidence Interval for Angiography:\n17.49 - 18.51\n\n\nCode\nn <- 847\nxbar <- 18 \ns <- 9\n\nmargin <- qt(0.95,df=n-1)*s/sqrt(n)\n\nlow <- xbar - margin\nlow\n\n\n[1] 17.49078\n\n\nCode\nhigh <- xbar + margin\nhigh\n\n\n[1] 18.50922\n\n\nThe confidence interval is narrower for Angiography - 1.02 difference, compared to Bypass - 1.42 difference."
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q2",
    "href": "posts/KenDocekal_HW2.html#q2",
    "title": "HW2",
    "section": "Q2",
    "text": "Q2\nThe proportion point estimate for adult Americans who believe that a college education is essential for success is .55, based on 567 out of the representative sample of 1031 adult Americans surveyed.\nA 95% confidence interval shows that in 95% of cases the observed mean proportion of adult Americans who believe that a college education is essential for success will be between 52% and 58%.\n\n\nCode\nprop.test(567,1031)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q3",
    "href": "posts/KenDocekal_HW2.html#q3",
    "title": "HW2",
    "section": "Q3",
    "text": "Q3\nBased on the range of 30 to 200 we can determine the standard deviation using s = (Maximum – Minimum)/4 resulting in s=42.5.With a 95% significance level we will use 1.96 for the z score. To find the minimum sample size needed - n, we solve for (Zscore*s/margin of error)^2. Our sample size needs to be at least 277.56.\n\n\nCode\nn <- ((1.96)*(42.5)/5)^2\n\nn\n\n\n[1] 277.5556"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q4",
    "href": "posts/KenDocekal_HW2.html#q4",
    "title": "HW2",
    "section": "Q4",
    "text": "Q4"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q5",
    "href": "posts/KenDocekal_HW2.html#q5",
    "title": "HW2",
    "section": "Q5",
    "text": "Q5"
  },
  {
    "objectID": "posts/KenDocekal_HW2.html#q6",
    "href": "posts/KenDocekal_HW2.html#q6",
    "title": "HW2",
    "section": "Q6",
    "text": "Q6\nCreate a data frame with a column for tax values.\n\n\nCode\ngas_taxes <-  data.frame (first_column  = c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)) \n\n\n\n\nCode\ngas_taxes\n\n\n   first_column\n1         51.27\n2         47.43\n3         38.89\n4         41.95\n5         28.61\n6         41.29\n7         52.19\n8         49.48\n9         35.02\n10        48.13\n11        39.28\n12        54.41\n13        41.66\n14        30.28\n15        18.49\n16        38.72\n17        33.41\n18        45.02\n\n\nCode\nnames(gas_taxes) <- c(\"tax\")\n\n\nUsing a one sample t-test where null hypothesis is mean tax is 45 and alternative hypothesis is true mean is less than 45 we are able to reject the null hypothesis at the 95% confidence level. Results indicate that mean tax was 40.86 and 95% of all observations will find a mean tax less than 44.68; therefore, while a few cities at the upper end of the range had prices near 45 cents per gallon, this was not usual and the average tax per gallon of gas in the US in 2005 was less than 45 cents.\n\n\nCode\nt.test(gas_taxes$tax, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes$tax\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html",
    "href": "posts/HW1_KarenDetter.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "href": "posts/HW1_KarenDetter.html#plot-histogram-with-probability-density-on-the-y-axis",
    "title": "Homework 1",
    "section": "Plot histogram with probability density on the y axis",
    "text": "Plot histogram with probability density on the y axis\n\n\nCode\nhist(LungCapData$LungCap, freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution - most of the observations are close to the mean, with very few close to the margins (0 and 15)."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "href": "posts/HW1_KarenDetter.html#create-boxplots-separated-by-gender",
    "title": "Homework 1",
    "section": "Create boxplots separated by gender",
    "text": "Create boxplots separated by gender\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, horizontal = TRUE)\n\n\n\n\n\nThe boxplots show that male lung capacity has a wider range than that of females; however, the minimum, median, and maximum values are all higher than those of females. This implies that, as a group, men are likely to have higher lung capacity than women."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "href": "posts/HW1_KarenDetter.html#group-by-smoking-status-and-summarize-mean-lung-capacities",
    "title": "Homework 1",
    "section": "Group by smoking status and summarize mean lung capacities",
    "text": "Group by smoking status and summarize mean lung capacities\n\n\nCode\nlibrary(dplyr)\nLungCapData %>%\ngroup_by(Smoke) %>%\nsummarize(mean = mean(LungCap), n = n())\n\n\n# A tibble: 2 × 3\n  Smoke  mean     n\n  <chr> <dbl> <int>\n1 no     7.77   648\n2 yes    8.65    77\n\n\nIn this dataset, the mean lung capacity of smokers is actually higher than that of non-smokers. Since this is counter to what would be expected, there is likely another variable exerting a confounding effect on lung capacity."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "href": "posts/HW1_KarenDetter.html#create-new-data-frame-with-age-group-category-variables",
    "title": "Homework 1",
    "section": "Create new data frame with age group category variables",
    "text": "Create new data frame with age group category variables\n\n\nCode\nLungCapData_AgeGroups <- LungCapData %>%\nmutate(AgeGroup = case_when(Age <= 13 ~ \"less than or equal to 13\", \n            Age == 14 | Age == 15 ~ \"14 to 15\",\n            Age == 16 | Age == 17 ~ \"16 to 17\",\n            Age >= 18 ~ \"greater than or equal to 18\"))"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "href": "posts/HW1_KarenDetter.html#summarize-mean-lung-capacities-by-age-group-and-smoking-status",
    "title": "Homework 1",
    "section": "Summarize mean lung capacities by age group and smoking status",
    "text": "Summarize mean lung capacities by age group and smoking status\n\n\nCode\nLungCapData_AgeGroups %>%\ngroup_by(AgeGroup, Smoke) %>%\nsummarize(MeanLungCap = mean(LungCap), n = n())\n\n\n`summarise()` has grouped output by 'AgeGroup'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   AgeGroup [4]\n  AgeGroup                    Smoke MeanLungCap     n\n  <chr>                       <chr>       <dbl> <int>\n1 14 to 15                    no           9.14   105\n2 14 to 15                    yes          8.39    15\n3 16 to 17                    no          10.5     77\n4 16 to 17                    yes          9.38    20\n5 greater than or equal to 18 no          11.1     65\n6 greater than or equal to 18 yes         10.5     15\n7 less than or equal to 13    no           6.36   401\n8 less than or equal to 13    yes          7.20    27"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "href": "posts/HW1_KarenDetter.html#calculate-correlation-and-covariance-between-lung-capacity-and-age",
    "title": "Homework 1",
    "section": "Calculate correlation and covariance between lung capacity and age",
    "text": "Calculate correlation and covariance between lung capacity and age\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 8.738289\n\n\nSince the correlation coefficient is close to 1, there is a high degree of correlation between lung capacity and age. The covariance of 8.7, being a positive number, indicates that as age increases, lung capacity increases."
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-data-frame",
    "href": "posts/HW1_KarenDetter.html#create-data-frame",
    "title": "Homework 1",
    "section": "Create data frame",
    "text": "Create data frame\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nFreq <- c(128,434,160,64,24)\nPrisonerData <- data.frame (PriorConv, Freq)\nPrisonerData\n\n\n  PriorConv Freq\n1         0  128\n2         1  434\n3         2  160\n4         3   64\n5         4   24"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has == 2 prior convictions",
    "text": "Calculate probability that an inmate has == 2 prior convictions\nprobability = frequency/n\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-1",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has < 2 prior convictions",
    "text": "Calculate probability that an inmate has < 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n\n\n\nCode\n(128/810) + (434/810)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-2",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has <= 2 prior convictions",
    "text": "Calculate probability that an inmate has <= 2 prior convictions\nprobability = frequency(0)/n + frequency(1)/n + frequency(2)/n\n\n\nCode\n(128/810) + (434/810) + (160/810)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "href": "posts/HW1_KarenDetter.html#calculate-probability-that-an-inmate-has-2-prior-convictions-3",
    "title": "Homework 1",
    "section": "Calculate probability that an inmate has > 2 prior convictions",
    "text": "Calculate probability that an inmate has > 2 prior convictions\nprobability = frequency(3)/n + frequency(4)/n\n\n\nCode\n(64/810) + (24/810)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value-for-number-of-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate expected value for number of prior convictions",
    "text": "Calculate expected value for number of prior convictions"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "href": "posts/HW1_KarenDetter.html#create-a-matrix-of-prior-conviction-values-and-their-probabilities",
    "title": "Homework 1",
    "section": "Create a matrix of prior conviction values and their probabilities",
    "text": "Create a matrix of prior conviction values and their probabilities\n\n\nCode\nPriorConv <- c(0,1,2,3,4)\nProbs <- c(0.1580247, 0.5358025, 0.1975309, 0.07901235, 0.02962963)"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "href": "posts/HW1_KarenDetter.html#calculate-expected-value",
    "title": "Homework 1",
    "section": "Calculate expected value",
    "text": "Calculate expected value\n\n\nCode\nc(PriorConv %*% Probs)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "href": "posts/HW1_KarenDetter.html#calculate-variance-and-standard-deviation-for-prior-convictions",
    "title": "Homework 1",
    "section": "Calculate variance and standard deviation for prior convictions",
    "text": "Calculate variance and standard deviation for prior convictions\n\n\nCode\nvar(PriorConv)\n\n\n[1] 2.5\n\n\nCode\nsd(PriorConv)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/HW1_KarenDetter.html#double-check-values",
    "href": "posts/HW1_KarenDetter.html#double-check-values",
    "title": "Homework 1",
    "section": "Double-check values",
    "text": "Double-check values\n\n\nCode\nsqrt(var(PriorConv)) == sd(PriorConv)\n\n\n[1] TRUE"
  },
  {
    "objectID": "posts/Homework2.html",
    "href": "posts/Homework2.html",
    "title": "Homework 2 - Emily Duryea",
    "section": "",
    "text": "Uploading packages to be used for this assignment:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\n\n\n\nBelow is the code used for setting up the degrees of freedom (df = sample size - 1). Thus, Bypass df would be 539 - 1 = 538, and, Angiography would be 847 - 1 = 846.\n\n\nCode\nBypass_df = 538\nAngio_df = 846\n\n\nBelow is the code for setting up the mean and standard deviation.\n\n\nCode\nBypass_mean = 19\nBypass_sd = 10\nAngio_mean = 18\nAngio_sd = 9\n\n\nThe code for calculating t-score with 90% confidence interval is below.\n\n\nCode\nBypass_tscore <- qt(p = 0.9, df = Bypass_df)\nAngio_tscore <- qt(p = 0.9, df = Angio_df)\n\n\nThe code for calculating the standard error (sd/sqrt(sample size)) is below.\n\n\nCode\nBypass_se <- Bypass_sd/sqrt(539)\nAngio_se <- Angio_sd/sqrt(847)\n\n\nThe code for calculating the margin of error (T-score multiplied by the standard error) is below.\n\n\nCode\nBypass_me <- Bypass_tscore*Bypass_se\nAngio_me <- Angio_tscore*Angio_se\n\n\nBelow is the code for calculating the upper and lower ranges (add mean to margin of error for upper, subtract for lower).\n\n\nCode\nBypass_low <- Bypass_mean - Bypass_me\nBypass_up <- Bypass_mean + Bypass_me\nAngio_low <- Angio_mean - Angio_me\nAngio_up <- Angio_mean + Angio_me\nBypass <- c(Bypass_low, Bypass_up)\nAngio <- c(Angio_low, Angio_up)\nBypass\n\n\n[1] 18.44732 19.55268\n\n\nCode\nAngio\n\n\n[1] 17.60338 18.39662\n\n\nThe 90% confidence interval for Bypass is [18.45, 19.55], and for Angio it is [17.60, 18.40]. Thus, Angio has the narrower confidence interval, which is logical to conclude because it has a larger sample size (847 > 539), which reduces the margin of error. Additionally, the standard deviation is smaller (9<10), which signifies less variance.\n\n\n\n\n\nCode\n# Number who believed college ed is essential for success\nN <- 567\n\n# Total sample size\nS <- 1031\n\n# Calculating point estimate\nPE <- N/S\n\n# Using the function prop.test() to find the confidence interval range & p-value\nprop.test(N, S, PE)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  N out of S, null probability PE\nX-squared = 6.9637e-30, df = 1, p-value = 1\nalternative hypothesis: true p is not equal to 0.5499515\n95 percent confidence interval:\n 0.5194543 0.5800778\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval is [0.519, 0.580], with a p-value of 0.550.\n\n\n\n\n\nCode\n# Calculating confidence interval of 95%\nCI95 <- qnorm(0.025, lower.tail = F)\n# Calculating sample size needed using confidence interval equation\nStudent_sample <- ((170*0.25/5)*CI95)^2\nStudent_sample\n\n\n[1] 277.5454\n\n\nBased on these calculations, the needed sample size would be 278 students for a significance level of 5%.\n\n\n\n\n\n\n\nCode\n# Calculating the standard error (sd = 90 sample = 9)\ncompany_se <- 90/sqrt(9)\ncompany_se\n\n\n[1] 30\n\n\nCode\n# Calculating the t-score\ncompany_tscore <- (410-500)/company_se\ncompany_tscore\n\n\n[1] -3\n\n\nCode\n# Calculating the p-value (df = 9-1 = 8)\ncompany_pvalue <- (pt(q=-3, df=8))*2\ncompany_pvalue\n\n\n[1] 0.01707168\n\n\nIt is possible to reject the null hypothesis, as the p-value is statistically significant (0.017), less than 0.05.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or less\nless_company <- pt(-3, 8)\nless_company\n\n\n[1] 0.008535841\n\n\nThe p-value for the lower tail is 0.00854.\n\n\n\n\n\nCode\n# Calculating the probability of a random sample with a mean of 410 or more\nmore_company <- pt(-3, 8, lower.tail = F)\nmore_company\n\n\n[1] 0.9914642\n\n\nThe p-value for the upper tail is 0.991.\n\n\nCode\ntotal_company <- less_company + more_company\ntotal_company\n\n\n[1] 1\n\n\nThe total of both tails is equal to 1.\n\n\n\n\n\n\n\n\nCode\n# Calculating t-scores\nJones_tscore <- (519.5-500)/10\nJones_tscore\n\n\n[1] 1.95\n\n\nCode\nSmith_tscore <- (519.7-500)/10\nSmith_tscore\n\n\n[1] 1.97\n\n\nCode\n# Calculating p-values\nJones_pvalue <- (pt(q=1.95, df=999, lower.tail=FALSE))*2\nJones_pvalue\n\n\n[1] 0.05145555\n\n\nCode\nSmith_pvalue <- (pt(q=1.97, df=999, lower.tail=FALSE))*2\nSmith_pvalue\n\n\n[1] 0.04911426\n\n\n\n\n\nIf the significance level is 0.05, then Smith has statistically significant study findings, while Jones does not.\n\n\n\nBoth of the studies could be statistically significant, depending on the significance level. For example, a 0.1 significance level would mean Jones also had statistically significant results. In this case, since the t-scores were so similar (0.2 off), it would not be unreasonable for both studies to reject the null hypothesis.\n\n\n\n\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# Getting the mean\nmean_gtaxes <- mean(gas_taxes)\nmean_gtaxes\n\n\n[1] 40.86278\n\n\nCode\n# Conducting t-test\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nBecause the p-value is 0.03827 on a 95% confidence interval, on the 0.05 significance level, it is possible the null hypothesis that gas prices are equal to or greater than $0.45."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html",
    "href": "posts/FinalProjectPart1_DonnySnyder.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question",
    "text": "Research Question\nAffective polarization describes a heightened state of animosity between partisans that has steadily grown from the 1970s to today (Iyengar et al., 2019). Identifying antecedents of affective polarization is essential to creating intervention strategies into this negative state of politics. Levendusky (2009) proposes a social model where individuals making sense of simplified elite cues enables people to understand the relevant identities of the political landscape, which may lead to downstream affective polarization. I intend to expand on this model, testing a construct of construal level, or the level of abstraction to concreteness (Trope & Liberman, 2010) with which partisans perceive partisan groups and group cues. Prior studies suggest that lower construal may serve as an antecedent to affective polarization when partisans view issues in more concrete, group terms (Snyder, Unpublished). This study will expand these models into extant, large scale, political science datasets. Additionally, this project will employ supervised machine learning models to qualitatively code a large-n sample of free response questions."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#hypotheses",
    "title": "Final Project Part 1",
    "section": "Hypotheses",
    "text": "Hypotheses\nI hypothesize that partisans who are qualitatively coded as having a lower construal level will demonstrate higher levels of group/affective polarization, as measured on a feeling thermometer or measures of feelings about political groups - whichever is available in the datasets.\nI hypothesize that using a sentiment analysis, these tendencies may be moderated by valence of their free response, with stronger valence enhancing the effect of construal level on affective polarization."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#datasets",
    "title": "Final Project Part 1",
    "section": "Datasets",
    "text": "Datasets\nI intend to use ANES and/or NAES free response data to provide an initial exploratory analysis. I will qualitatively code these data using a novel construal level paradigm (Snyder, unpublished). i will then use this qualitative coding process to train a supervised machine learning algorithm."
  },
  {
    "objectID": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "href": "posts/FinalProjectPart1_DonnySnyder.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nIyengar, S., Lelkes, Y., Levendusky, M., Malhotra, N., & Westwood, S. J. (2019). The origins and consequences of affective polarization in the United States. Annual Review of Political Science, 22(1), 129-146. Levendusky, M. (2009). The partisan sort: How liberals became Democrats and conservatives became Republicans. University of Chicago Press. Snyder, D. (2022). Keep It Simple Stupid: How Individual Differences in Cue Construal Explain Variations in Affective Polarization. Unpublished Manuscript Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. Psychological review, 117(2), 440."
  },
  {
    "objectID": "posts/HW_1_603.html",
    "href": "posts/HW_1_603.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_603.html#question-1",
    "href": "posts/HW_1_603.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nLung_data<- read_excel(\"C:/Users/manik/Desktop/LungCapData.xls\")\n\n\nError: `path` does not exist: 'C:/Users/manik/Desktop/LungCapData.xls'\n\n\nCode\nLung_data\n\n\nError in eval(expr, envir, enclos): object 'Lung_data' not found\n\n\nGiven data consists of 725 rows and 6 columns\n\nWhat does the distribution of LungCap look like?\n\n\n\nCode\nLung_data %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram() +\n  geom_density(color = \"Red\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lung_data' not found\n\n\nBased on above histogram , we can say the distribution is very close to the normal distribution\nCompare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nLung_data %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lung_data' not found\n\n\nCompare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nMean_smokers <- Lung_data %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lung_data' not found\n\n\nCode\nMean_smokers\n\n\nError in eval(expr, envir, enclos): object 'Mean_smokers' not found\n\n\nThe mean of the lung capacity who smokes is greater than the people who doesnt smoke which doesnt make any sense in practical\nExamine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\nLung_data <- mutate(Lung_data, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lung_data, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lung_data' not found\n\n\nCode\nLung_data %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + coord_flip()\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lung_data' not found\n\n\nCode\n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n$y\n[1] \"Lung Capacity\"\n\n$x\n[1] \"Frequency\"\n\n$title\n[1] \"Relationship of LungCap and Smoke based on age categories\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nCompare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n\nCode\nLung_data %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lung_data' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke.\nCalculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\n\nCode\nCovariance_LA <- cov(Lung_data$LungCap, Lung_data$Age)\n\n\nError in is.data.frame(y): object 'Lung_data' not found\n\n\nCode\nCorrelation_LA <- cor(Lung_data$LungCap, Lung_data$Age)\n\n\nError in is.data.frame(y): object 'Lung_data' not found\n\n\nCode\nCovariance_LA\n\n\nError in eval(expr, envir, enclos): object 'Covariance_LA' not found\n\n\nCode\nCorrelation_LA\n\n\nError in eval(expr, envir, enclos): object 'Correlation_LA' not found\n\n\nFrom the above result we can say that both covariance and correlation is positive and which indicates direct relationship that means Lungcapacity increases as age increases"
  },
  {
    "objectID": "posts/HW_1_603.html#question-2",
    "href": "posts/HW_1_603.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nIP<- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nIP\n\n\n\n\n  \n\n\n\n\n\nCode\nIP <- mutate(IP, Probability = Inmate_count/sum(Inmate_count))\nIP\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has exactly 2 prior convictions?\n\n\nCode\nIP %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)\n\n\n\n\n  \n\n\n\nWhat is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\n\nCode\np_2 <- IP %>%\n  filter(Prior_convitions < 2)\nsum(p_2$Probability)\n\n\n[1] 0.6938272\n\n\nWhat is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\n\nCode\np <- IP %>%\n  filter(Prior_convitions <= 2)\nsum(p$Probability)\n\n\n[1] 0.891358\n\n\nWhat is the probability that a randomly selected inmate has more than 2 prior convictions?\n\n\nCode\nP_3 <- IP %>%\n  filter(Prior_convitions > 2)\nsum(P_3$Probability)\n\n\n[1] 0.108642\n\n\nWhat is the expected value for the number of prior convictions?\n\n\nCode\nIP <- mutate(IP, Wm = Prior_convitions*Probability)\nexpe<- sum(IP$Wm)\nexpe\n\n\n[1] 1.28642\n\n\nCalculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar_ <-sum(((IP$Prior_convitions-expe)^2)*IP$Probability)\nvar_\n\n\n[1] 0.8562353\n\n\nstandard deviation:\n\n\nCode\nsqrt(var_)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html",
    "href": "posts/HW2_ManiGogula.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-1",
    "href": "posts/HW2_ManiGogula.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-2",
    "href": "posts/HW2_ManiGogula.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-3",
    "href": "posts/HW2_ManiGogula.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-4",
    "href": "posts/HW2_ManiGogula.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a",
    "href": "posts/HW2_ManiGogula.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "href": "posts/HW2_ManiGogula.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-p-value",
    "href": "posts/HW2_ManiGogula.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b",
    "href": "posts/HW2_ManiGogula.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c",
    "href": "posts/HW2_ManiGogula.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-5",
    "href": "posts/HW2_ManiGogula.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#a-1",
    "href": "posts/HW2_ManiGogula.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiGogula.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#b-1",
    "href": "posts/HW2_ManiGogula.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#c-1",
    "href": "posts/HW2_ManiGogula.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiGogula.html#question-6",
    "href": "posts/HW2_ManiGogula.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html",
    "href": "posts/HW2_ManiShankerKamarapu.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "href": "posts/HW2_ManiShankerKamarapu.html#creating-the-table",
    "title": "Homework 2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\ns_size <- c(539, 847)\nmean_wait_time <- c(19, 18)\ns_sd <- c(10, 9)\n\nsurgery <- data.frame(procedure, s_size, mean_wait_time, s_sd)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstandard_error <- s_sd / sqrt(s_size)\nstandard_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = s_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nCI <- c(mean_wait_time - t_score * standard_error,\n        mean_wait_time + t_score * standard_error)\nCI\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nWe can be 90% confident that the population mean wait time for the bypass procedure is between 18.29029 and 19.70971 days.\nWe can be 90% confident that the population mean wait time for the angiography procedure is between 17.49078 and 18.50922 days.\nFrom the above results, we can be sure that confidence interval of angiography procedure is narrower."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success is 0.5499515 and confidence interval at 95% confidence level for p is [0.5189682, 0.5805580]."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary size for the sample is 278."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a",
    "href": "posts/HW2_ManiShankerKamarapu.html#a",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-test-statistic",
    "title": "Homework 2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-p-value",
    "title": "Homework 2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b",
    "href": "posts/HW2_ManiShankerKamarapu.html#b",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c",
    "href": "posts/HW2_ManiShankerKamarapu.html#c",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#a-1",
    "title": "Homework 2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/HW2_ManiShankerKamarapu.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "Homework 2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#b-1",
    "title": "Homework 2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW2_ManiShankerKamarapu.html#c-1",
    "title": "Homework 2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "href": "posts/HW2_ManiShankerKamarapu.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html",
    "href": "posts/EmmaRasmussenFinalPart1.html",
    "title": "Final Project Part 1",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(googlesheets4)"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "href": "posts/EmmaRasmussenFinalPart1.html#research-question",
    "title": "Final Project Part 1",
    "section": "Research Question:",
    "text": "Research Question:\nDoes political partisanship correlate with COVID-19 death rates?\nThe COVID-19 pandemic became a political matter. Behaviors associated with COVID-19 prevention were adopted on partisan lines (masking, social distancing, and vaccine uptake). Early in the pandemic, mask mandates were protested in some communities. My research question is have these behaviors affected COVID-19 death rates along partisan lines? If so, public health interventions could target communities that may be higher risk for COVID-19 deaths based on political partisanship.\nI am thinking death toll would make the most sense to measure than infection rates as infection rates are constantly changing (other studies have looked at infection rates over waves of the pandemic, see this study from the Pew Research Center (Jones 2022)). I also think that one way to measure partisanship will be the 2020 county-level election results (% voting for Trump). In other words, my research is looking to see if (county-level) Trump support correlates with COVID-19 death rates. Both these variables can be found in county-level data sets so I can join multiple dataset with county name (or FIPS code) as the “key”.\nOther variables to consider at the county-level (confounding variables): vaccine (and booster) uptake, average age of population"
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "href": "posts/EmmaRasmussenFinalPart1.html#hypothesis",
    "title": "Final Project Part 1",
    "section": "Hypothesis:",
    "text": "Hypothesis:\nWhile I came up with this research idea on my own, other organizations such as NPR (Wood and Brumfiel 2021) and the Pew Research Center ()have already tested this. For this project, I will use the most recent data I can find. I was hoping to consider the confounding variable of population density, for instance I am guessing more urban populations will tend to vote democratic but these more densely populated places may also have higher infection rates. However, I cannot find any county level population density data sets, so I may use the “Urban Rural Description” variable in one of my datasets.\nH0: B1 (and all beta values) is zero. There is no correlation Ha: B1 (or any beta value) is not zero. There is a correlation between partisanship and COVID-19 death rates."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "href": "posts/EmmaRasmussenFinalPart1.html#descriptive-statistics",
    "title": "Final Project Part 1",
    "section": "Descriptive Statistics:",
    "text": "Descriptive Statistics:\n\n#Reading in the data from google sheets\ngs4_deauth()\n\nvotedf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1fmxoA_bibvsxsvgRdVPCgMA7DkmJNZfxiWgLgCLcsOY/edit#gid=937778872\")\n\ncoviddf<-read_sheet(\"https://docs.google.com/spreadsheets/d/1Hy2O3HxhZGF_fhu6jgmoC2ibWwJTlI7pQOESBOd4hTU/edit#gid=787918384\")\n\n\n#Changing fips code to character format and adding in leading zeros\ncoviddf$\"FIPS Code\" <- as.character(coviddf$\"FIPS Code\")\ncoviddf<-mutate(coviddf, FIPSNEW=str_pad(coviddf$\"FIPS Code\", 5, pad = \"0\"))\nhead(coviddf, 12)\n\n# A tibble: 12 × 22\n   `Data as of`        `Start Date`        `End Date`          State County Na…¹\n   <dttm>              <dttm>              <dttm>              <chr> <chr>      \n 1 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 2 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 3 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Anchorage …\n 4 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 5 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 6 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Fairbanks …\n 7 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 8 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n 9 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AK    Matanuska-…\n10 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n11 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n12 2022-10-05 00:00:00 2020-01-01 00:00:00 2022-10-01 00:00:00 AL    Autauga Co…\n# … with 17 more variables: `Urban Rural Code` <dbl>, `FIPS State` <dbl>,\n#   `FIPS County` <dbl>, `FIPS Code` <chr>, Indicator <chr>,\n#   `Total deaths` <dbl>, `COVID-19 Deaths` <dbl>, `Non-Hispanic White` <dbl>,\n#   `Non-Hispanic Black` <dbl>,\n#   `Non-Hispanic American Indian or Alaska Native` <dbl>,\n#   `Non-Hispanic Asian` <dbl>,\n#   `Non-Hispanic Native Hawaiian or Other Pacific Islander` <dbl>, …\n# ℹ Use `colnames()` to see all variable names\n\nvotedf$county_fips <- as.character(votedf$county_fips)\nvotedf<-mutate(votedf, county_fipsNEW=str_pad(votedf$county_fips, 5, pad = \"0\"))\nhead(votedf, 12)\n\n# A tibble: 12 × 13\n    year state   state_po county_…¹ count…² office candi…³ party candi…⁴ total…⁵\n   <dbl> <chr>   <chr>    <chr>     <chr>   <chr>  <chr>   <chr>   <dbl>   <dbl>\n 1  2000 ALABAMA AL       AUTAUGA   1001    US PR… AL GORE DEMO…    4942   17208\n 2  2000 ALABAMA AL       AUTAUGA   1001    US PR… GEORGE… REPU…   11993   17208\n 3  2000 ALABAMA AL       AUTAUGA   1001    US PR… RALPH … GREEN     160   17208\n 4  2000 ALABAMA AL       AUTAUGA   1001    US PR… OTHER   OTHER     113   17208\n 5  2000 ALABAMA AL       BALDWIN   1003    US PR… AL GORE DEMO…   13997   56480\n 6  2000 ALABAMA AL       BALDWIN   1003    US PR… GEORGE… REPU…   40872   56480\n 7  2000 ALABAMA AL       BALDWIN   1003    US PR… RALPH … GREEN    1033   56480\n 8  2000 ALABAMA AL       BALDWIN   1003    US PR… OTHER   OTHER     578   56480\n 9  2000 ALABAMA AL       BARBOUR   1005    US PR… AL GORE DEMO…    5188   10395\n10  2000 ALABAMA AL       BARBOUR   1005    US PR… GEORGE… REPU…    5096   10395\n11  2000 ALABAMA AL       BARBOUR   1005    US PR… RALPH … GREEN      46   10395\n12  2000 ALABAMA AL       BARBOUR   1005    US PR… OTHER   OTHER      65   10395\n# … with 3 more variables: version <dbl>, mode <chr>, county_fipsNEW <chr>, and\n#   abbreviated variable names ¹​county_name, ²​county_fips, ³​candidate,\n#   ⁴​candidatevotes, ⁵​totalvotes\n# ℹ Use `colnames()` to see all variable names\n\n\n\nsummary(votedf)\n\n      year         state             state_po         county_name       \n Min.   :2000   Length:72617       Length:72617       Length:72617      \n 1st Qu.:2004   Class :character   Class :character   Class :character  \n Median :2012   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2011                                                           \n 3rd Qu.:2020                                                           \n Max.   :2020                                                           \n county_fips           office           candidate            party          \n Length:72617       Length:72617       Length:72617       Length:72617      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n candidatevotes      totalvotes         version             mode          \n Min.   :      0   Min.   :      0   Min.   :20220315   Length:72617      \n 1st Qu.:    115   1st Qu.:   5175   1st Qu.:20220315   Class :character  \n Median :   1278   Median :  11194   Median :20220315   Mode  :character  \n Mean   :  10782   Mean   :  42514   Mean   :20220315                     \n 3rd Qu.:   5848   3rd Qu.:  29855   3rd Qu.:20220315                     \n Max.   :3028885   Max.   :4264365   Max.   :20220315                     \n county_fipsNEW    \n Length:72617      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nsummary(coviddf)\n\n   Data as of           Start Date            End Date         \n Min.   :2022-10-05   Min.   :2020-01-01   Min.   :2022-10-01  \n 1st Qu.:2022-10-05   1st Qu.:2020-01-01   1st Qu.:2022-10-01  \n Median :2022-10-05   Median :2020-01-01   Median :2022-10-01  \n Mean   :2022-10-05   Mean   :2020-01-01   Mean   :2022-10-01  \n 3rd Qu.:2022-10-05   3rd Qu.:2020-01-01   3rd Qu.:2022-10-01  \n Max.   :2022-10-05   Max.   :2020-01-01   Max.   :2022-10-01  \n                                                               \n    State           County Name        Urban Rural Code   FIPS State   \n Length:3495        Length:3495        Min.   :1.000    Min.   : 1.00  \n Class :character   Class :character   1st Qu.:2.000    1st Qu.:18.00  \n Mode  :character   Mode  :character   Median :4.000    Median :33.00  \n                                       Mean   :3.645    Mean   :30.47  \n                                       3rd Qu.:5.000    3rd Qu.:42.00  \n                                       Max.   :6.000    Max.   :56.00  \n                                                                       \n  FIPS County      FIPS Code          Indicator          Total deaths   \n Min.   :  1.00   Length:3495        Length:3495        Min.   :   621  \n 1st Qu.: 31.00   Class :character   Class :character   1st Qu.:  1690  \n Median : 71.00   Mode  :character   Mode  :character   Median :  3284  \n Mean   : 99.37                                         Mean   :  7163  \n 3rd Qu.:121.00                                         3rd Qu.:  6990  \n Max.   :840.00                                         Max.   :220829  \n                                                                        \n COVID-19 Deaths   Non-Hispanic White Non-Hispanic Black\n Min.   :  101.0   Min.   :0.0270     Min.   :0.0010    \n 1st Qu.:  176.0   1st Qu.:0.6677     1st Qu.:0.0230    \n Median :  364.0   Median :0.8300     Median :0.0690    \n Mean   :  852.7   Mean   :0.7742     Mean   :0.1242    \n 3rd Qu.:  844.0   3rd Qu.:0.9290     3rd Qu.:0.1800    \n Max.   :31013.0   Max.   :1.0000     Max.   :0.7610    \n                   NA's   :3          NA's   :592       \n Non-Hispanic American Indian or Alaska Native Non-Hispanic Asian\n Min.   :0.0000                                Min.   :0.0010    \n 1st Qu.:0.0020                                1st Qu.:0.0070    \n Median :0.0040                                Median :0.0130    \n Mean   :0.0214                                Mean   :0.0261    \n 3rd Qu.:0.0100                                3rd Qu.:0.0280    \n Max.   :0.8610                                Max.   :0.5170    \n NA's   :1701                                  NA's   :1360      \n Non-Hispanic Native Hawaiian or Other Pacific Islander    Hispanic     \n Min.   :0.0000                                         Min.   :0.0030  \n 1st Qu.:0.0000                                         1st Qu.:0.0220  \n Median :0.0010                                         Median :0.0480  \n Mean   :0.0023                                         Mean   :0.0987  \n 3rd Qu.:0.0010                                         3rd Qu.:0.1090  \n Max.   :0.2000                                         Max.   :0.9870  \n NA's   :2183                                           NA's   :740     \n     Other        Urban Rural Description   Footnote           FIPSNEW         \n Min.   :0.0010   Length:3495             Length:3495        Length:3495       \n 1st Qu.:0.0090   Class :character        Class :character   Class :character  \n Median :0.0150   Mode  :character        Mode  :character   Mode  :character  \n Mean   :0.0174                                                                \n 3rd Qu.:0.0220                                                                \n Max.   :0.2410                                                                \n NA's   :1633                                                                  \n\n\nThis data is going to require some tidying before merging. In the coviddf, each county is listed 3 times, (once per indicator) so I will likely filter out just the indicator “Distribution of COVID-19 deaths (%)” so each county is listed only once. Similarly, the votedf contains extra years. For my research, I am only concerned with 2016 data so I will filter out % voting for Trump in 2016 as a measure of political affiliation/partisanship. Then I will merge the two dfs based on county names (will also require some data tidying).\nThe votedf was compiled by the MIT Election Data and Science Lab. It was first published in 2018 and has been updated with the 2020 election. It contains county-level presidential election data beginning in 2000 and going up to the 2020 election. The data has 12 columns, and 72,617 rows (many of which I will filter out before conducting analysis.) There are 1,892 distinct county names in the data set.\nThe coviddf only has 857 unique county names in the data frame. This may be because not all counties reported COVID-19 death counts. When I join the data sets, I will join so as to only include observations that we have information from both data frames. The coviddf is provisional, meaning that it is consistently updated (I believe on a weekly basis) with current COVID-19 death toll data. It is likely compiled by counties/towns reporting these numbers to the CDC. This data has limitations, not all counties report this, and not all report it accurately/ attribute COVID-19 as the true cause of death in all circumstances. Using the summary function, we can see the “mean” COVID-19 deaths by county is 852.7, however this isn’t super meaningful given each county has this reported 3 times in the data and the median is significantly lower. Statistics provided by the summary function will be more meaningful once the data is tidied."
  },
  {
    "objectID": "posts/EmmaRasmussenFinalPart1.html#references",
    "href": "posts/EmmaRasmussenFinalPart1.html#references",
    "title": "Final Project Part 1",
    "section": "References",
    "text": "References\nJones, B. (2022). The Changing Political Geography of COVID-19 Over the Last Two Years. Pew Research Center. March 3, 2022. https://www.pewresearch.org/politics/2022/03/03/the-changing-political-geography-of-covid-19-over-the-last-two-years/\nMIT Election Data and Science Lab. (2021) County Presidential Election Returns 2000-2020. Accessed from the Harvard Dataverse [October 11, 2022]. https://doi.org/10.7910/DVN/VOQCHQ\nNational Center for Health Statistics. (2022). Provisional COVID-19 Deaths by County, and Race and Hispanic Origin. Accessed from the Centers for Disease Control [October 11, 2022]. https://data.cdc.gov/d/k8wy-p9cg\nWood, D. and Brumfiel, G. (2021). Pro-Trump counties now have far higher COVID death rates. Misinformation is to blame. NPR. December 5, 2021. https://www.npr.org/sections/health-shots/2021/12/05/1059828993/data-vaccine-misinformation-trump-counties-covid-death-rate\n[Need to add italics to references]"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html",
    "href": "posts/NiyatiSharma_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\n\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-1",
    "href": "posts/NiyatiSharma_HW1.html#question-1",
    "title": "HW1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "href": "posts/NiyatiSharma_HW1.html#read-the-data-from-xls-file",
    "title": "HW1",
    "section": "Read the data from xls file",
    "text": "Read the data from xls file\n\n\nCode\nRE <- read_excel(\"_data/LungCapData.xls\")\nRE\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows\n\n\n##A\n\n\nCode\nRE %>% \n  ggplot(aes(LungCap))+\n  geom_histogram(bins=20)\n\n\n\n\n\nThe histogram looks close to normal distributed."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b",
    "href": "posts/NiyatiSharma_HW1.html#b",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\nRE %>%\n  ggplot(aes (LungCap, color=Gender)) +\n  geom_boxplot() +\n  theme_classic() \n\n\n\n\n\nThe probability density of the female is higher than the males."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c",
    "href": "posts/NiyatiSharma_HW1.html#c",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\nMean_Smoker <- RE %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_Smoker\n\n\n# A tibble: 2 × 2\n  Smoke  mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nggplot(RE, aes(LungCap,Smoke))+\n  geom_boxplot()\n\n\n\n\n\nFrom this sample, it appears that smokers have a higher mean lung capacity than non-smokers."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d",
    "href": "posts/NiyatiSharma_HW1.html#d",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\nRE<-RE %>% \n  mutate(Category = as.factor(case_when(Age <= 13 ~ \"13 and under\", \n                           Age == 14 |Age ==15 ~ \"14-15\", \n                           Age == 16 | Age==17 ~ \"16-17\",\n                           Age >= 18 ~ \"18 or over\"\n                           )))\nRE\n\n\n# A tibble: 725 × 7\n   LungCap   Age Height Smoke Gender Caesarean Category    \n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>       \n 1    6.48     6   62.1 no    male   no        13 and under\n 2   10.1     18   74.7 yes   female no        18 or over  \n 3    9.55    16   69.7 no    female yes       16-17       \n 4   11.1     14   71   no    male   no        14-15       \n 5    4.8      5   56.9 no    male   no        13 and under\n 6    6.22    11   58.7 no    female no        13 and under\n 7    4.95     8   63.3 no    male   yes       13 and under\n 8    7.32    11   70.4 no    male   no        13 and under\n 9    8.88    15   70.5 no    male   no        14-15       \n10    6.8     11   59.2 no    male   no        13 and under\n# … with 715 more rows\n\n\nCode\nRE %>%\n  ggplot(aes( LungCap, color = Smoke)) +\n  geom_histogram()+\n  facet_grid(Smoke ~ Category)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe people who smoke are few in age group of “less than or equal to 13”. From the result we can say age is inversely proportional to the lung capacity."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e",
    "href": "posts/NiyatiSharma_HW1.html#e",
    "title": "HW1",
    "section": "E",
    "text": "E\nForm the above data we can say the output are pretty similar that smokers have a lower lung capacity compared to non-smokers"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f",
    "href": "posts/NiyatiSharma_HW1.html#f",
    "title": "HW1",
    "section": "F",
    "text": "F\ncorrelation and covariance between lung capacity and age\n\n\nCode\ncov(RE$LungCap,RE$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(RE$LungCap,RE$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is positive and indicates that age and lung capacity are directly related. Correlation is also positive,from these results we can conclude that the lung capacity increases with age."
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#question-2",
    "href": "posts/NiyatiSharma_HW1.html#question-2",
    "title": "HW1",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nx <- c(0:4)\nfreq <- c(128, 434, 160, 64, 24)\nconvictions <- data_frame(x, freq)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nconvictions\n\n\n# A tibble: 5 × 2\n      x  freq\n  <int> <dbl>\n1     0   128\n2     1   434\n3     2   160\n4     3    64\n5     4    24\n\n\n\n\nCode\nconvictions <- convictions %>% mutate(probability = freq/sum(freq))\nconvictions\n\n\n# A tibble: 5 × 3\n      x  freq probability\n  <int> <dbl>       <dbl>\n1     0   128      0.158 \n2     1   434      0.536 \n3     2   160      0.198 \n4     3    64      0.0790\n5     4    24      0.0296"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#a",
    "href": "posts/NiyatiSharma_HW1.html#a",
    "title": "HW1",
    "section": "A",
    "text": "A\nProbability of exactly 2 is 19.75%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#b-1",
    "href": "posts/NiyatiSharma_HW1.html#b-1",
    "title": "HW1",
    "section": "B",
    "text": "B\n\n\nCode\na <-head(convictions,2)\nsum(a$probability)\n\n\n[1] 0.6938272\n\n\nProbability that a randomly selected inmate has fewer than 2 prior convictions : 69.38%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#c-1",
    "href": "posts/NiyatiSharma_HW1.html#c-1",
    "title": "HW1",
    "section": "C",
    "text": "C\n\n\nCode\na <-head(convictions,3)\nsum(a$probability)\n\n\n[1] 0.891358\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions : 89.13%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#d-1",
    "href": "posts/NiyatiSharma_HW1.html#d-1",
    "title": "HW1",
    "section": "D",
    "text": "D\n\n\nCode\na <-tail(convictions,2)\nsum(a$probability)\n\n\n[1] 0.108642\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions? : 10.86%"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#e-1",
    "href": "posts/NiyatiSharma_HW1.html#e-1",
    "title": "HW1",
    "section": "E",
    "text": "E\n\n\nCode\nWE <- weighted.mean(convictions$x,convictions$probability)\nWE\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions : 1.28"
  },
  {
    "objectID": "posts/NiyatiSharma_HW1.html#f-1",
    "href": "posts/NiyatiSharma_HW1.html#f-1",
    "title": "HW1",
    "section": "F",
    "text": "F\nThe variance is 0.857 and the standard deviation is 0.925\n\n\nCode\nAB <- (sum(freq*((x-WE)^2)))/(sum(freq)-1)\nAB\n\n\n[1] 0.8572937\n\n\nCode\nsqrt(AB)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/CalebHill_HW1.html",
    "href": "posts/CalebHill_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nNext, let’s compare the probability distribution of the LungCap with respect to Males and Females, using a boxplot.\n\n\nCode\nboxplot(LungCap ~ Gender, df)\n\n\n\n\n\nThe minimum and mean are very similar to each other, with the minimum around 1 and the mean around 8. The maximum does differ though by gender, at 13 to 14/15 respectively.\n\n\n\nFor the third question, we’re going to compare the mean lung capacities for smokers and non-smokers. To compare the mean, we’ll again use the box-plot.\n\n\nCode\nboxplot(LungCap ~ Smoke, df)\n\n\n\n\n\nWhile the mean is very similar, hovering between 8 and 9, the range is what is substantial. A smoker’s lung capacity has a much smaller range, 4 - 13, compared to non-smokers, at 1 - 15. This makes sense, as a smoker’s lungs would start to have less capacity through consistent substance abuse.\n\n\n\nFor question four, we need to create a new variable, Age Group, followed by comparing the relationship between Smoking and Lung Capacity, broken down by Age Group. First, we’ll create the new column, referencing the Age column to determine groups.\n\n\nCode\ndf_new <- df %>%\n  mutate(\n    Age_Group = dplyr::case_when(\n      Age <= 13 ~ \"Less than or equal to 13\",\n      Age == 14 | Age == 15 ~ \"14 or 15\",\n      Age == 16 | Age == 17 ~ \"16 or 17\",\n      Age >= 18 ~ \"Greater than or equal to 18\"\n    ),\n    Age_Group = factor(\n      Age_Group,\n      level = c(\"Less than or equal to 13\", \"14 or 15\", \"16 or 17\", \"Greater than or equal to 18\")\n    )\n  )\nhead(df_new)\n\n\n# A tibble: 6 × 7\n  LungCap   Age Height Smoke Gender Caesarean Age_Group                  \n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>     <fct>                      \n1    6.48     6   62.1 no    male   no        Less than or equal to 13   \n2   10.1     18   74.7 yes   female no        Greater than or equal to 18\n3    9.55    16   69.7 no    female yes       16 or 17                   \n4   11.1     14   71   no    male   no        14 or 15                   \n5    4.8      5   56.9 no    male   no        Less than or equal to 13   \n6    6.22    11   58.7 no    female no        Less than or equal to 13   \n\n\nGood. Now we can place a histogram to better understand the relationship between LungCap and Smoking status. To view it by age group, we’ll add a facet wrap to the visualization.\n\n\nCode\nggplot(df_new, aes(LungCap, color=Smoke)) +\n  geom_histogram() +\n  facet_wrap(~Age_Group)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThose that are smokers have a smaller sample size than non-smokers. Looking purely at the distribution of each, we can see that three of the four age groups follow a normal distribution, save the 14 or 15 group that has a somewhat “two hump” distribution.\nEven so, smoking status does seem to mirror the non-smoker distribution, when it comes to the overall sample count and LungCap.\n\n\n\nFor the fifth question, we’ll compare the lung capacities for smokers and non-smokers within each age group. We’ll use a box-plot and facet wrap this visualization again by Age Group.\n\n\nCode\nggplot(df_new, aes(LungCap, Smoke)) +\n  geom_boxplot() +\n  facet_wrap(~Age_Group)\n\n\n\n\n\nWe can readily see that smokers, irrespective of age, have a substantially smaller lung capacity range compared to non-smokers. While the mean might be similar, sometimes even smaller for “13 years old or less”, the length of each capacity varies for non-smokers where it doesn’t for smokers.\n\n\n\nFor the sixth question, we shall calculate the covariance and correlation between LungCap and Age.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nCovariance is the relationship between a pair of random variables where change in one variable causes change in another variable. With a covariance of 8.73, that means that there is a positive relationship between the two variables and that, by every 1 point change of Age, that can result in an average of 8.73 point change in LungCap.\nCorrelations show whether and how strongly pairs or variables are related to one another. Correlation can range from 0.0 to 1.0. With a result of 0.81, that means there is a high correlation between LungCap and Age."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#a-1",
    "href": "posts/CalebHill_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nLet’s calculate the probability that a randomly selected inmate has EXACTLY 2 prior convictions.\n\n\nCode\ndbinom(2, 810, 0.1975)\n\n\n[1] 7.90917e-74\n\n\nSo 7.9%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#b-1",
    "href": "posts/CalebHill_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nLet’s calculate the probability that a randomly selected inmate has FEWER THAN 2 prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975, lower.tail=FALSE)\n\n\n[1] 1\n\n\nNot sure why it’s pulling 1."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#c-1",
    "href": "posts/CalebHill_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLet’s calculate the probability that a randomly selected inmate has 2 OR FEWER prior convictions.\n\n\nCode\npbinom(2, 810, 0.1975)\n\n\n[1] 7.989018e-74\n\n\nSo 7.98%."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#d-1",
    "href": "posts/CalebHill_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nLet’s calculate the probability that a randomly selected inmate has MORE THAN 2 prior convictions."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#e-1",
    "href": "posts/CalebHill_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nLet’s calculate the expected value for the number of prior convictions. As I am unable to calculate sections B and D, I’m unable to determine the expected value."
  },
  {
    "objectID": "posts/CalebHill_HW1.html#f-1",
    "href": "posts/CalebHill_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nLet’s calculate the variance and the standard deviation for the Prior Convictions.As I am unable to determine the expected value, I cannot calculate the variance and standard deviation either."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html",
    "href": "posts/HW1_ManiShankerKamarapu.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"_data/LungCapData.xls\")\nLc\n\n\n\n\n  \n\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a",
    "href": "posts/HW1_ManiShankerKamarapu.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\n\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b",
    "href": "posts/HW1_ManiShankerKamarapu.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\n\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c",
    "href": "posts/HW1_ManiShankerKamarapu.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\nMean_smoke\n\n\n\n\n  \n\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d",
    "href": "posts/HW1_ManiShankerKamarapu.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\n\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e",
    "href": "posts/HW1_ManiShankerKamarapu.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f",
    "href": "posts/HW1_ManiShankerKamarapu.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\nCorrelation <- cor(Lc$LungCap, Lc$Age)\nCovariance\n\n\n[1] 8.738289\n\n\nCode\nCorrelation\n\n\n[1] 0.8196749\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "href": "posts/HW1_ManiShankerKamarapu.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "href": "posts/HW1_ManiShankerKamarapu.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "href": "posts/HW1_ManiShankerKamarapu.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/HW 2 - Eris Dodds.html",
    "href": "posts/HW 2 - Eris Dodds.html",
    "title": "",
    "section": "",
    "text": "Question 1\nSee Code\n\nbypass_mean<-19\nbypass_sd<-10\nbypass_size<-539\nstandard_error_bypass<-bypass_sd/sqrt(bypass_size)\nconfidence_level<-0.9\ntail_area<-(1-confidence_level)/2\nt_score<-qt(p = 1-tail_area, df = bypass_size-1)\nCI_bypass<-c(bypass_mean - t_score * standard_error_bypass, bypass_mean + t_score * standard_error_bypass)\nprint(CI_bypass)\n\n[1] 18.29029 19.70971\n\nanio_mean<-18\nanio_sd<-9\nanio_size<-847\nstandard_error_anio<-anio_sd/sqrt(anio_size)\ntail_area_anio<-(1-confidence_level)/2\nt_score_anio<-qt(p = 1-tail_area_anio, df = anio_size)\nCI_anio<-c(anio_mean - t_score_anio * standard_error_anio, anio_mean + t_score_anio * standard_error_anio)\nprint(CI_anio)\n\n[1] 17.49078 18.50922\n\n\n\n\nQuestion 2\nPE<-.55 Lower Limit = .52 Upper Limit = .58\n\npop<-567\nsize<-1031\nPE<-pop/size\nprop.test(pop,size)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  pop out of size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n\n\nQuestion 3\n\n(1.96)^2 *(42.5)^2/ (5)^2\n\n[1] 277.5556\n\n\n\n\nQuestion 4\nThe p value is .049, showing it is statisitcally significant and we can reject the null. Other aspects to this question are verified within the code\n\nfem_size<-9\nfem_mean<-410\nnull<-500\nsd<-90\n\nSE<-sd/sqrt(fem_size)\nt_score<-(fem_mean-null)/SE\np_value<-(pt(t_score, df=9))*2\nupper_p<-(pt(t_score, df=8, lower.tail = FALSE))\nupper_p\n\n[1] 0.9914642\n\nlower_p<-(pt(t_score, df=8, lower.tail = FALSE))\nlower_p\n\n[1] 0.9914642\n\n\n\n\nQuestion 5\n\nsee code\nJones = .051 not significant, cannot reject null. Smith = .049 significant, reject null.\nBeing broad about the direction of the p value, in this case, would overshadow how marginally significant and insignificant the p values actually came out to in this case.\n\n\njones_mean<-519.5\nsmith_mean<-519.7\njones_se<-10\nsmith_se<-10\nnull<-500\njones_t<-(jones_mean-null)/jones_se\njones_t\n\n[1] 1.95\n\nsmith_t<-(smith_mean-null)/smith_se\nsmith_t\n\n[1] 1.97\n\njones_p<-pt(jones_t, df=999, lower.tail = FALSE)*2\njones_p\n\n[1] 0.05145555\n\nsmith_p<-pt(smith_t, df=999, lower.tail = FALSE)*2\nsmith_p\n\n[1] 0.04911426\n\n\n\n\nQuestion 6\nThe results of a t test show that the mean is less that 45, with a relatively small p value. We can have more confidence, then, that the average gas tax per gallon was less than .45 cents. ::: {.cell}\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n:::"
  },
  {
    "objectID": "posts/HW_1_QH.html",
    "href": "posts/HW_1_QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a",
    "href": "posts/HW_1_QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/HW_1_QH.html#d",
    "href": "posts/HW_1_QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/HW_1_QH.html#e",
    "href": "posts/HW_1_QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/HW_1_QH.html#f",
    "href": "posts/HW_1_QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/HW_1_QH.html#a-1",
    "href": "posts/HW_1_QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/HW_1_QH.html#b",
    "href": "posts/HW_1_QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/HW_1_QH.html#c",
    "href": "posts/HW_1_QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/HW_1_QH.html#d-1",
    "href": "posts/HW_1_QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/HW_1_QH.html#e-1",
    "href": "posts/HW_1_QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/HW_1_QH.html#f-1",
    "href": "posts/HW_1_QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html",
    "href": "posts/HW2_ToryBartelloni.html",
    "title": "DACSS 603: Homework 2",
    "section": "",
    "text": "Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\nlibrary(dplyr)\nsurgery <- data.frame(Procedure=c(\"Bypass\",\"Angiography\"), \n                      Sample_Size=c(539,847),\n                      Mean_Wait_Time=c(19,18),\n                      Standard_Deviation=c(10,9))\n\nbypass <- surgery %>% filter(Procedure == \"Bypass\")\n\nt_score_bypass <- qt(p=1-0.05, df=bypass$Sample_Size-1)\n\nse_bypass <- bypass$Standard_Deviation / sqrt(bypass$Sample_Size)\n\nCI_bypass <- c(bypass$Mean_Wait_Time - t_score_bypass * se_bypass,\n    bypass$Mean_Wait_Time + t_score_bypass * se_bypass)\n\nangio <- surgery %>% filter(Procedure == \"Angiography\")\n\nt_score_angio <- qt(p=1-0.05, df=angio$Sample_Size-1)\n\nse_angio <- angio$Standard_Deviation / sqrt(angio$Sample_Size)\n\nCI_angio <- c(angio$Mean_Wait_Time - t_score_angio * se_angio,\n    angio$Mean_Wait_Time + t_score_angio * se_angio)\n\nCI <- data.frame(Procedure = c(\"Bypass\", \"Angiography\"),\n                 Lower_Limit = c(CI_bypass[1],CI_angio[1]),\n                 Upper_Limit = c(CI_bypass[2],CI_angio[2]))\n\nknitr::kable(CI, caption = \"90% Confidence Levels for Cardiac Procedures\")\n\n\n\n90% Confidence Levels for Cardiac Procedures\n\n\nProcedure\nLower_Limit\nUpper_Limit\n\n\n\n\nBypass\n18.29029\n19.70971\n\n\nAngiography\n17.49078\n18.50922\n\n\n\n\n\nThe confidence interval is more narrow for angiogrphy because the larger sample size reduces t-score, and the larger sample and lower standard deviation together reduce the standard error."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#a.",
    "href": "posts/HW2_ToryBartelloni.html#a.",
    "title": "DACSS 603: Homework 2",
    "section": "A.",
    "text": "A.\nAbove we have shown that the difference in the results leads to the t-statistics and p-values provided."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#b.",
    "href": "posts/HW2_ToryBartelloni.html#b.",
    "title": "DACSS 603: Homework 2",
    "section": "B.",
    "text": "B.\nThe p-values observed show that at a 95% confidence level Jones’ results are non-significant while Smith’s results are significant."
  },
  {
    "objectID": "posts/HW2_ToryBartelloni.html#c.",
    "href": "posts/HW2_ToryBartelloni.html#c.",
    "title": "DACSS 603: Homework 2",
    "section": "C.",
    "text": "C.\nThis is a good example of why not reporting p-values can be insufficient or misleading because the results we observed are extremely close, but the arbitrary boundary we agreed upon prior to the test distinguishes them into different categories. This would not be so important if the difference between those categories were not important. Without the context of the specific results we could see the two extremely similar results treated and acted upon in starkly different ways.\nI would argue this is one good reason why we should be reporting p-values up until .001 so researchers and users of the data can fully understand the context they would be applying the result within. Good to note that reporting extremely small p-values (<.001) has it’s drawbacks as well and we do not want to overemphasize results that may be the result of methodology rather than a real and important distinction for instance."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html",
    "href": "posts/Homework2_Kaushika Potluri.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "href": "posts/Homework2_Kaushika Potluri.html#loading-in-packages",
    "title": "Homework 2",
    "section": "Loading in packages:",
    "text": "Loading in packages:\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stats)\n\n\n##Question 1\nConstruct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures. Is the confidence interval narrower for Angiography or Bypass surgery?\n\nAngiography\n\n\nCode\nang_mean <- 18\nang_sd <- 9\nang_ss <- 847\n\nang_se <- ang_sd/sqrt(ang_ss)\n\nang_cl <- 0.90  \nang_tail <- (1-ang_cl)/2\nang_tscore <- qt(p = 1-ang_tail, df = ang_ss-1)\n\nang_ci <- c(ang_mean - ang_tscore * ang_se,\n        ang_mean + ang_tscore * ang_se)\nprint(ang_ci)\n\n\n[1] 17.49078 18.50922\n\n\n\n\nCode\n#assessing Confidence interval\n18.50922 - 17.49078\n\n\n[1] 1.01844\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_ang <- ang_tscore * ang_se\nMargin_of_error_ang * 1.01\n\n\n[1] 0.5143103\n\n\nWe can be 90% confident that the population mean wait time for the Angiography procedure is between 17.49078 and 18.50922 days with margin of error +/-0.51\n\n\nBypass\n\n\nCode\nbypass_mean <- 19\nbypass_sd <- 10\nbypass_ss <- 539\n\nbypass_se <- bypass_sd/sqrt(bypass_ss)\n\nbypass_cl <- 0.90  \nbypass_tail <- (1-bypass_cl)/2\nbypass_tscore <- qt(p = 1-bypass_tail, df = bypass_ss-1)\n\nbypass_ci <- c(bypass_mean - bypass_tscore * bypass_se,\n        bypass_mean + bypass_tscore * bypass_se)\nprint(bypass_ci)\n\n\n[1] 18.29029 19.70971\n\n\nWe can be 90% confident that the population mean wait time for the Bypass procedure is between 18.29029 and 19.70971 days.\n\n\nCode\n#assessing Confidence interval\n19.70971 - 18.29029\n\n\n[1] 1.41942\n\n\n####Margin of error\n\n\nCode\nMargin_of_error_bypass <- bypass_tscore * bypass_se\nMargin_of_error_bypass * 1.41\n\n\n[1] 1.000692\n\n\nTherefore, the confidence interval is more narrow for Angiographies."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-2",
    "href": "posts/Homework2_Kaushika Potluri.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nA survey of 1031 adult Americans was carried out by the National Center for Public Policy. Assume that the sample is representative of adult Americans. Among those surveyed, 567 believed that college education is essential for success. Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success. Construct and interpret a 95% confidence interval for p.\n\n\nCode\n#n = Number of American adults (population), x = sample (surveyed)\nn = 1031\nx = 567 #(believed that college education is essential for success)\n\n#Using prop.test to find p (The CI is 95% by default)\n#This  function will return the range for the point estimate at 95% CI.\nprop.test(x, n)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe percentage of adult Americans who think a college education is necessary for success is p, which is 0.5499515. We have a confidence interval of 95 percent confidence interval that equals, [0.5189682, 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-3",
    "href": "posts/Homework2_Kaushika Potluri.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nSuppose that the financial aid office of UMass Amherst seeks to estimate the mean cost of textbooks per semester for students. The estimate will be useful if it is within $5 of the true population mean (i.e. they want the confidence interval to have a length of $10 or less). The financial aid office is pretty sure that the amount spent on books varies widely, with most values between $30 and $200. They think that the population standard deviation is about a quarter of this range (in other words, you can assume they know the population standard deviation). Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#Evaluating standard deviation using the given values\nUMassSD <- (200-30)/4\nUMassSD\n\n\n[1] 42.5\n\n\nSince the significance level is at 5% our Confidence level is 95%. A 95% confidence level has a z-score of 1.96. With this ideal sample size can be calculated.\n\n\nCode\n#samplesize = ((UMassSD * zscore)/5)^2\nsamplesize <- ((UMassSD * 1.96)/5)^2\nprint(samplesize)\n\n\n[1] 277.5556\n\n\nThe size necessary for the sample is 278."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-4",
    "href": "posts/Homework2_Kaushika Potluri.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nAccording to a union agreement, the mean income for all senior-level workers in a large service company equals $500 per week. A representative of a women’s group decides to analyze whether the mean income μ for female employees matches this norm. For a random sample of nine female employees, ȳ = $410 and s = 90.\n\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\np_mean <- 500\ns_meanfemale <- 410\ns_sizefemale = 9\nsd = 90\n\n#find standard error\nstandarderrorfemale<- sd/sqrt(s_sizefemale)\nstandarderrorfemale\n\n\n[1] 30\n\n\n\n\nCode\n#calculating t-score\nt_stat<- (s_meanfemale-p_mean)/standarderrorfemale\nt_stat\n\n\n[1] -3\n\n\n\n\nCode\n#calculating p value\ndf <- 9-1\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nSince the p value is less than .05 we can reject the null hypothesis\n\nReport the P-value for Ha : μ < 500. Interpret.\n\nAssuming that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\npvalue_lower <- pt(-t_stat, df, lower.tail = FALSE)\npvalue_lower\n\n\n[1] 0.008535841\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500.\n\nReport and interpret the P-value for H a: μ > 500.\n\n\n\nCode\npvalue_upper <- pt(t_stat, df, lower.tail = FALSE)\npvalue_upper\n\n\n[1] 0.9914642\n\n\nAs p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500.\n\n\nCode\n#checking if sum = 1\npvalue_upper + pvalue_lower\n\n\n[1] 1"
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-5",
    "href": "posts/Homework2_Kaushika Potluri.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nJones and Smith separately conduct studies to test H0: μ = 500 against Ha : μ ≠ 500, each with n = 1000. Jones gets ȳ = 519.5, with se = 10.0. Smith gets ȳ = 519.7, with se = 10.0.\n\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nJones\n\n\nCode\n#first calculating t-value for Jones\nt_stat_Jones <- (519.5 - 500)/(10)\nt_stat_Jones\n\n\n[1] 1.95\n\n\nCode\ndf <- 1000-1\n#now we calculate p value for Jones\n\n\np_value_Jones <- 2*pt(t_stat_Jones,df, lower.tail = FALSE)\np_value_Jones\n\n\n[1] 0.05145555\n\n\n\n\nSmith\n\n\nCode\n#first calculating t-value for Smith\nt_stat_Smith <- (519.7 - 500)/(10)\nt_stat_Smith\n\n\n[1] 1.97\n\n\nCode\ndf <- 1000-1\n\n#now we calculate p value for Smith\np_value_Smith <- 2*pt(t_stat_Smith,df, lower.tail = FALSE)\np_value_Smith\n\n\n[1] 0.04911426\n\n\nb)Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAnswer : When they say ‘statistically significant’ it means the p-value is smaller than the 0.05. For Jones, the p-value is 0.051 which is greater than the 0.05 significance level. This means that it is not statistically significant and we cannot reject the null hypothesis.\nFor Smith, the p-value is 0.049 which is smaller than the significance level. This means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\nUsing this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” without reporting the actual P-value.\n\nAnswer : One cannot assess the validity of the result if we do not provide the P-value and you cannot tell how close the p-value is to being significant. Since the values of Jones and Smith’s is barely greater and lesser than 0.05 respectively, it is important to report the p-value because studies with very similar samples could report that the null should or should not be rejected. This could draw very different conclusions."
  },
  {
    "objectID": "posts/Homework2_Kaushika Potluri.html#question-6",
    "href": "posts/Homework2_Kaushika Potluri.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\nAre the taxes on gasoline very high in the United States? According to the American Petroleum Institute, the per gallon federal tax that was levied on gasoline was 18.4 cents per gallon. However, state and local taxes vary over the same period. The sample data of gasoline taxes for 18 large cities is given below in the variable called gas_taxes.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\nAnswer :\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\n#Mean of taxes\nMean_gastaxes <- mean(gas_taxes)\nMean_gastaxes\n\n\n[1] 40.86278\n\n\n\n\nCode\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe p-value is 0.03 at 95% confidence level. This is lesser than the 5% significance level. Therefore, this proves that we can reject the null hypothesis that the average tax per gallon was greater than or equal to 45 cents. We can say that the average tax per gallon of gas in the US in 2005 was less than 45 cents with 95% confidence."
  },
  {
    "objectID": "posts/HW2_Saaradhaa.html",
    "href": "posts/HW2_Saaradhaa.html",
    "title": "Homework 2",
    "section": "",
    "text": "Qn 2\n\nset.seed(0)\nprop <- prop.test(x=567, n=1031)\nprop\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% CI is [0.5189682, 0.580558], which includes the point estimate 0.5499515 and excludes 0.5. Hence, we can reject the null hypothesis that the true probability is 0.5 at the 5% significance level, p = 0.0014898.\n\n\nQn 3\n\n# calculate population SD.\nPSD <- (200-30)/4\n\n# calculate sample size.\nn <- round(((1.96*PSD)/5)^2)\n\nThe minimum required sample size is 278.\n\n\nQn 4a\nAssumptions: H0 is true, observations are independent of one another, y is continuous and sample is approximately normally distributed. H0: μ = 500 Ha: μ ≠ 500\n\n# calculate t-statistic.\nt <- (410-500)/(90/sqrt(9))\n\n# calculate p-value.\np <- 2*pt(q=abs(t), df=8, lower.tail=FALSE)\np\n\n[1] 0.01707168\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8) = 3, p = 0.0170717. Female employees’ mean income significantly differs from $500 per week.\n[I have a question - I am confused on whether I was right to use the absolute value here, and when we should use absolute values.]\n\n\nQn 4b\n\n# calculate p-value.\np2 <- pt(q=t, df=8, lower.tail=TRUE)\np2\n\n[1] 0.008535841\n\n\nWe can reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.0085358. Female employees’ mean income is significantly less than $500 per week.\n\n\nQn 4c\n\n# calculate p-value.\np3 <- pt(q=t, df=8, lower.tail=FALSE)\np3\n\n[1] 0.9914642\n\n\nWe fail to reject the null hypothesis at the 5% significance level, t(8)= -3, p = 0.9914642. Female employees’ mean income is not significantly more than $500 per week.\n\n\nQn 5a\n\n# calculate SD for Jones and Smith.\nSD <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_j <- ((519.5-500)/SD) * sqrt(1000)\nt_j\n\n[1] 1.95\n\n# calculate p-value for Jones.\np_j <- 2*(pt(q=t_j, df=999, lower.tail=FALSE))\np_j\n\n[1] 0.05145555\n\n# calculate t for Smith.\nt_s <- ((519.7-500)/SD) * sqrt(1000)\nt_s\n\n[1] 1.97\n\n# calculate p-value for Smith.\np_s <- 2*(pt(q=t_s, df=999, lower.tail=FALSE))\np_s\n\n[1] 0.04911426\n\n\n\n\nQn 5b\nThe result is statistically significant for Smith, but not Jones.\n\n\nQn 5c\nIt is useful to report the exact p-value in cases like this, when the p-value is very close to alpha. It helps the reader to understand (1) why it was/was not rejected, and (2) how much evidence there is against the null hypothesis.\n\n\nQn 6\n\n#create variable.\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n# do t-test.\ntax <- t.test(gas_taxes, alternative=\"less\",mu=45)\ntax\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% CI is [-, 44.6794598], which includes the estimated mean 40.8627778 and excludes 45. Hence, we can reject the null hypothesis at the 5% significance level, t(17)= -1.8857058, p = 0.0382708. The average tax per gallon in the US in 2005 was significantly less than 45 cents."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html",
    "href": "posts/MeghaJoseph_hw1.html",
    "title": "HOME WORK1 603",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1",
    "href": "posts/MeghaJoseph_hw1.html#answer-1",
    "title": "HOME WORK1 603",
    "section": "Answer 1",
    "text": "Answer 1\n\n\nCode\nreadD <- read_excel(\"_data/LungCapData.xls\")\nreadD\n\n\n# A tibble: 725 × 6\n   LungCap   Age Height Smoke Gender Caesarean\n     <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n 1    6.48     6   62.1 no    male   no       \n 2   10.1     18   74.7 yes   female no       \n 3    9.55    16   69.7 no    female yes      \n 4   11.1     14   71   no    male   no       \n 5    4.8      5   56.9 no    male   no       \n 6    6.22    11   58.7 no    female no       \n 7    4.95     8   63.3 no    male   yes      \n 8    7.32    11   70.4 no    male   no       \n 9    8.88    15   70.5 no    male   no       \n10    6.8     11   59.2 no    male   no       \n# … with 715 more rows"
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-a",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (a)",
    "text": "Answer 1 (a)\nDistribution of LungCap:\n\n\nCode\nhist(readD$LungCap)\n\n\n\n\n\nThe distribution is a normal distribution. ## Answer 1 (b)\n\n\nCode\nboxplot(readD$LungCap ~ readD$Gender)\n\n\n\n\n\nThe mean of males appear higher than females."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-c",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (c)",
    "text": "Answer 1 (c)\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(Mean=mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke  Mean\n  <chr> <dbl>\n1 no     7.77\n2 yes    8.65\n\n\nCode\nreadD%>%\n  group_by(Smoke) %>% \n  summarize(stdev=sd(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke stdev\n  <chr> <dbl>\n1 no     2.73\n2 yes    1.88\n\n\nCode\nggplot(readD, aes(x=LungCap, y=Smoke))+geom_boxplot()\n\n\n\n\n\nThe mean of smokers is higher than the mean of non smokers and therefore it is not sensible."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-d",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (d)",
    "text": "Answer 1 (d)\n\n\nCode\nclass(readD$Age)\n\n\n[1] \"numeric\"\n\n\nCode\nreadD <- mutate(readD, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", \n                                            Age == 14 | Age == 15 ~ \"14 to 15\", \n                                            Age == 16 | Age == 17 ~ \"16 to 17\", \n                                            Age >= 18 ~ \"18 and above\"))\nggplot(readD, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\nreadD %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  facet_wrap(vars(Smoke)) +\n  labs(y = \"Lung Capacity\", x = \"Age\")\n\n\n\n\n\nFrom the above results we can say that people from age group 10 and above smoke."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "href": "posts/MeghaJoseph_hw1.html#answer-1-f",
    "title": "HOME WORK1 603",
    "section": "Answer 1 (f)",
    "text": "Answer 1 (f)\n\n\nCode\ncor(readD$LungCap,readD$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(readD$LungCap,readD$Age)\n\n\n[1] 8.738289\n\n\nFrom the data we can see that the covariance is positive and it shows that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. Therefore as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/MeghaJoseph_hw1.html#answer-2",
    "href": "posts/MeghaJoseph_hw1.html#answer-2",
    "title": "HOME WORK1 603",
    "section": "Answer 2",
    "text": "Answer 2\n\n\nCode\nX<-c(0, 1, 2, 3, 4)\nFrequency<-c(128, 434, 160, 64, 24)\nC<- data.frame(X, Frequency)\nC\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nCode\nC<-rename(C, PriorConvictions=X)\nC\n\n\n  PriorConvictions Frequency\n1                0       128\n2                1       434\n3                2       160\n4                3        64\n5                4        24\n\n\nCode\n#visualizing df using bar chart\nggplot(C, aes(x=PriorConvictions, y=Frequency))+geom_bar(stat=\"identity\")+geom_text(aes(label = Frequency), vjust = -.3)\n\n\n\n\n\nCode\n#There are 810 obs in df\nsum(Frequency)\n\n\n[1] 810\n\n\n\n\nCode\nPO<-Frequency/810\nPO\n\n\n[1] 0.15802469 0.53580247 0.19753086 0.07901235 0.02962963\n\n\nCode\n#A\n# P(x=2)=160/810\n160/810\n\n\n[1] 0.1975309\n\n\nCode\n#B\n#P(x<2)=P(0)+P(1)\n(128+434)/810\n\n\n[1] 0.6938272\n\n\nCode\n#C\n#P(x<=2)=P(0)+P(1)+P(2)\n(128+434+160)/810\n\n\n[1] 0.891358\n\n\nCode\n#D\n#1-P(above)\n1-((128+434+160)/810)\n\n\n[1] 0.108642\n\n\nCode\n#E\n#Expected value=sum of probabilities*each value (0, 1, 2, 3 or 4)\nweighted.mean(X, PO)\n\n\n[1] 1.28642\n\n\nCode\n#F\n#Calculating the Variance using the formula for variance\n(sum(Frequency*((X-1.28642)^2)))/(sum(Frequency)-1)\n\n\n[1] 0.8572937\n\n\nCode\n#Calculating the sample standard deviation from the variance\nsqrt(0.8572937)\n\n\n[1] 0.9259016\n\n\nAnswer\na: 19.75% b :9.38% c :89.14% d :10.86% e :1.28642 f: variance: 0.8572937 standard deviation: 0.9259016"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html",
    "href": "posts/DACSS 603 HW 1.html",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "",
    "text": "Code\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.5\n✔ tibble  3.1.8     ✔ stringr 1.4.1\n✔ tidyr   1.2.1     ✔ forcats 0.5.2\n✔ readr   2.1.3     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n\n# Reading in File\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\n\n\n\n\n\n\n\nCode\nhist(LungCapData$LungCap)\n\n\n\n\n\nThe histogram above shows that the Lung Cap data is roughly normally distributed because a majority of the observations are centered around the mean. There are fewer observations at the tail ends of the histogram.\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = LungCapData, main = \"Lung Capacity by Gender\",\n        xlab = \"Gender\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the box-plots above, it appears that males in this study had slightly higher lung capacities than females, with the median for males at 9 and the median for females at 8. However, both genders had large ranges, but these ranges reflected the overall pattern of males having slightly higher lung capacities.\n\n\n\n\n\nCode\nsmokers <- filter(LungCapData, Smoke == \"yes\")\nmean(smokers$LungCap)\n\n\n[1] 8.645455\n\n\nCode\nnonsmokers <- filter(LungCapData, Smoke == \"no\")\nmean(nonsmokers$LungCap)\n\n\n[1] 7.770188\n\n\nThe mean lung capacity for smokers (8.65) is higher than the mean lung capacity for non-smokers (7.77). Based on what we now know about how smoking affects the lungs, these results don’t seem to make sense. However, there is the possibility that smokers may be more used to deep inhales/exhales and therefore could have better lung capacity until the substance has more of an effect on their lungs. There may also be external factors that led to these results that aren’t clear from the data right now.\n\n\n\n\n\nCode\nLungCapData <- within(LungCapData, {\n  Age.group <- NA\n  Age.group[Age <= 13] <- \"13 and Under\"\n  Age.group[Age >= 14 & Age <= 15] <- \"14-15\"\n  Age.group[Age >= 16 & Age <= 17] <- \"16-17\"\n  Age.group[Age >= 18] <- \"18 and Over\"\n} )\n\n\n\n\n\n\nCode\n# Boxplots\n\nsmoking_age <- filter(LungCapData, Smoke == \"yes\")\n\nboxplot(LungCap ~ Age.group, data = smoking_age,\n        main = \"Lung Capacity of Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nFrom the boxplot above, we can see that smokers’ lung capacities reach about a maximum of 12 as age increases, but there is not very much improvement in the maximums. The medians move a bit more as age increases, but still not very dramatically after ages 14 and 15. Smokers that are 18 and over have higher lung capacities overall, but this may just be because of natural aging processes and development.\n\n\nCode\n# Means\n\nsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  7.20\n2 14-15         8.39\n3 16-17         9.38\n4 18 and Over  10.5 \n\n\nWe see the same trend in means as in the medians: mean lung capacity to increases as the age increases.\n\n\n\n\n\nCode\n# Boxplot\n\nnonsmoking_age <- filter(LungCapData, Smoke == \"no\")\n\nboxplot(LungCap ~ Age.group, data = nonsmoking_age,\n        main = \"Lung Capacity of Non-Smokers by Age Group\",\n        xlab = \"Age Group\", ylab = \"Lung Capacity\")\n\n\n\n\n\nIn non-smokers, we see the same trend of increasing lung capacities as age increases, but the median lung capacities in the two older age groups in the non-smoking group are higher than those in the smoking group. There are also more outliers for non-smokers, especially in the 14-15 category.\n\n\nCode\n# Means\n\nnonsmoking_age %>%\n  group_by(Age.group) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 4 × 2\n  Age.group     name\n  <chr>        <dbl>\n1 13 and Under  6.36\n2 14-15         9.14\n3 16-17        10.5 \n4 18 and Over  11.1 \n\n\nThe means of the non-smoking group by age follow the same trend as the medians, as well as in the smoking group. However, the mean lung capacity for the oldest two age groups in the non-smoking category are higher than the means for those groups in the smoking category.\n\n\n\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"13 and Under\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     6.36\n2 yes    7.20\n\n\nThe mean lung capacity for smokers is higher than the mean lung capacity for non-smokers in the age group 13 and under, which mirrors the general means we found earlier. However, from the boxplot of Smokers by Age Group, we can see that there is a very low outlier in this age group, which might be affecting the mean for this group as well as overall smokers.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"14-15\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     9.14\n2 yes    8.39\n\n\nIn this age group, the mean lung capacity for non-smokers is higher than the mean lung capacity for smokers–unlike the younger group.\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"16-17\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no    10.5 \n2 yes    9.38\n\n\nThe same trend continues in this age group, with the mean lung capacity in non-smokers ages 16 and 17 higher than the mean lung capacity of smokers in this group. Yet as the ages increase, the mean lung capacities for non-smokers and smokers increase about the same amount (by 1).\n\n\n\n\n\nCode\nLungCapData %>%\n  filter(Age.group == \"18 and Over\") %>%\n  group_by(Smoke) %>%\n  summarise_at(vars(LungCap), list(name = mean))\n\n\n# A tibble: 2 × 2\n  Smoke  name\n  <chr> <dbl>\n1 no     11.1\n2 yes    10.5\n\n\nIn this oldest age group, the same trend continues: the mean lung capacity for non-smokers is higher than that of smokers. This pattern in the groups 18+, 16-17, and 14-15 are not found in the overall means for smokers and nonsmokers, suggesting that the outlier in the 13 and Under group might have brought down the overall mean for smokers.\n\n\n\n\n\n\nCode\n# Correlation\n\ncor(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 0.8196749\n\n\nThe correlation between lung capacity and age is positive and strong. As age increases, lung capacity also increases. The value of 0.8 is close to 1, meaning there is a somewhat strong relationship between the two variables.\n\n\nCode\n# Covariance\n\ncov(LungCapData$Age, LungCapData$LungCap, use = \"everything\")\n\n\n[1] 8.738289\n\n\nThe covariance is positive, meaning that there is a positive relationship between the varaibles, which is also clear from the correlation (since the correlation coefficient is a function of the covariance). Age and lung capacity have an overall positive relationship: as age increases, so does lung capacity."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-a",
    "href": "posts/DACSS 603 HW 1.html#part-a",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part A",
    "text": "Part A\n\n\nCode\n160/810\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-b",
    "href": "posts/DACSS 603 HW 1.html#part-b",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part B",
    "text": "Part B\n\n\nCode\n(434 + 128)/810\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-c",
    "href": "posts/DACSS 603 HW 1.html#part-c",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part C",
    "text": "Part C\n\n\nCode\n(160 + 434 + 128)/810\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-d",
    "href": "posts/DACSS 603 HW 1.html#part-d",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part D",
    "text": "Part D\n\n\nCode\n(64 + 24)/810\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-e",
    "href": "posts/DACSS 603 HW 1.html#part-e",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part E",
    "text": "Part E\n\n\nCode\n# Creating vector\nconvict <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\n\nweighted.mean(convict)\n\n\n[1] 1.28642\n\n\nThe expected value for the number of prior convictions is 1.27–but since prior convictions have to be a whole number, that would be rounded to 1."
  },
  {
    "objectID": "posts/DACSS 603 HW 1.html#part-f",
    "href": "posts/DACSS 603 HW 1.html#part-f",
    "title": "DACSS 603 HW 1 Kimble",
    "section": "Part F",
    "text": "Part F\n\n\nCode\nvar(convict)\n\n\n[1] 0.8572937\n\n\nCode\nsd(convict)\n\n\n[1] 0.9259016"
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-1",
    "href": "posts/Niharika_HW1.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-data",
    "href": "posts/Niharika_HW1.html#reading-data",
    "title": "Homework 1",
    "section": "Reading data",
    "text": "Reading data\n\n\nCode\nLc <- read_excel(\"LungCapData.xls\")\n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nLc\n\n\nError in eval(expr, envir, enclos): object 'Lc' not found\n\n\nThe data consists of 725 rows and 6 columns. It determines the lung capacity of the based on their age, height and different characteristics. The main key classification that I can see is if they smoke or not."
  },
  {
    "objectID": "posts/Niharika_HW1.html#a",
    "href": "posts/Niharika_HW1.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\nThe distribution of LungCap looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(LungCap, ..density..)) +\n  geom_histogram(bins= 25, color = \"orange\") +\n  geom_density(color = \"darkblue\") +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap\", x = \"Lung Capcity\", y = \"Probability density\")\n\n\nError in ggplot(., aes(LungCap, ..density..)): object 'Lc' not found\n\n\nThe histogram and density plots show that it is pretty close to a normal distribution. Most of the observations are close to the mean."
  },
  {
    "objectID": "posts/Niharika_HW1.html#b",
    "href": "posts/Niharika_HW1.html#b",
    "title": "Homework 1",
    "section": "1b",
    "text": "1b\nThe distribution of LungCap on basis of gender looks as follows:\n\n\nCode\nLc %>%\n  ggplot(aes(y = dnorm(LungCap), color = Gender)) +\n  geom_boxplot() +\n  theme_classic() + \n  labs(title = \"Probability distribution of LungCap based on gender\", y = \"Probability density\")\n\n\nError in ggplot(., aes(y = dnorm(LungCap), color = Gender)): object 'Lc' not found\n\n\nThe box plot shows that the probability density of the male is lesser than the female."
  },
  {
    "objectID": "posts/Niharika_HW1.html#c",
    "href": "posts/Niharika_HW1.html#c",
    "title": "Homework 1",
    "section": "1c",
    "text": "1c\nComparison of mean lung capacities between smokers and non-smokers:\n\n\nCode\nMean_smoke <- Lc %>%\n  group_by(Smoke) %>%\n  summarise(mean = mean(LungCap))\n\n\nError in group_by(., Smoke): object 'Lc' not found\n\n\nCode\nMean_smoke\n\n\nError in eval(expr, envir, enclos): object 'Mean_smoke' not found\n\n\nFrom the above table, we see that the mean lung capacity of those who smoke is greater than those who don’t smoke, but it doesn’t make sense. It also depends on the biological factors of the person who smoke, so we can’t conclude it."
  },
  {
    "objectID": "posts/Niharika_HW1.html#d",
    "href": "posts/Niharika_HW1.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nRelationship between Smoke and Lung capacity on basis of given age categories:\n\n\nCode\nLc <- mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\",\n                                    Age == 14 | Age == 15 ~ \"14 to 15\",\n                                    Age == 16 | Age == 17 ~ \"16 to 17\",\n                                    Age >= 18 ~ \"greater than or equal to 18\"))\n\n\nError in mutate(Lc, AgeGrp = case_when(Age <= 13 ~ \"less than or equal to 13\", : object 'Lc' not found\n\n\nCode\nLc %>%\n  ggplot(aes(y = LungCap, color = Smoke)) +\n  geom_histogram(bins = 25) +\n  facet_wrap(vars(AgeGrp)) +\n  theme_classic() + \n  labs(title = \"Relationship of LungCap and Smoke based on age categories\", y = \"Lung Capacity\", x = \"Frequency\")\n\n\nError in ggplot(., aes(y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nFrom the above plot, we can derive two important observations: 1. The lung capacity of non smokers is more than smokers. 2. The people who smoke are less in age group of “less than or equal to 13”. So as the result as age increases the lung capacity decreases."
  },
  {
    "objectID": "posts/Niharika_HW1.html#e",
    "href": "posts/Niharika_HW1.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nRelationship between Smoke and Lung capacity on basis of age:\n\n\nCode\nLc %>%\n  ggplot(aes(x = Age, y = LungCap, color = Smoke)) +\n  geom_line() +\n  theme_classic() + \n  facet_wrap(vars(Smoke)) +\n  labs(title = \"Relationship of LungCap and Smoke based on age\", y = \"Lung Capacity\", x = \"Age\")\n\n\nError in ggplot(., aes(x = Age, y = LungCap, color = Smoke)): object 'Lc' not found\n\n\nForm the above data we can compare 1d and 1e and can say the results are pretty similar. Only 10 and above age group smoke."
  },
  {
    "objectID": "posts/Niharika_HW1.html#f",
    "href": "posts/Niharika_HW1.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\nCalculating the correlation and covariance between Lung Capacity and Age:\n\n\nCode\nCovariance <- cov(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCorrelation <- cor(Lc$LungCap, Lc$Age)\n\n\nError in is.data.frame(y): object 'Lc' not found\n\n\nCode\nCovariance\n\n\nError in eval(expr, envir, enclos): object 'Covariance' not found\n\n\nCode\nCorrelation\n\n\nError in eval(expr, envir, enclos): object 'Correlation' not found\n\n\nWe can observe from the comparison that the covariance is positive and it indicates that there is a direct relationship between age and lung capacity. And the correlation is also positive, so they move in same direction. We can say from these results that as the age increases, the lung capacity also increases that is they are directly proportional to each other."
  },
  {
    "objectID": "posts/Niharika_HW1.html#question-2",
    "href": "posts/Niharika_HW1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2"
  },
  {
    "objectID": "posts/Niharika_HW1.html#reading-the-table",
    "href": "posts/Niharika_HW1.html#reading-the-table",
    "title": "Homework 1",
    "section": "Reading the table",
    "text": "Reading the table\n\n\nCode\nPrior_convitions <- c(0:4)\nInmate_count <- c(128, 434, 160, 64, 24)\nPc <- data_frame(Prior_convitions, Inmate_count)\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\nCode\nPc\n\n\n\n\n  \n\n\n\n\n\nCode\nPc <- mutate(Pc, Probability = Inmate_count/sum(Inmate_count))\nPc"
  },
  {
    "objectID": "posts/Niharika_HW1.html#a-1",
    "href": "posts/Niharika_HW1.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\nProbability that a randomly selected inmate has exactly 2 prior convictions:\n\n\nCode\nPc %>%\n  filter(Prior_convitions == 2) %>%\n  select(Probability)"
  },
  {
    "objectID": "posts/Niharika_HW1.html#b-1",
    "href": "posts/Niharika_HW1.html#b-1",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\nProbability that a randomly selected inmate has fewer than 2 convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions < 2)\nsum(temp$Probability)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/Niharika_HW1.html#c-1",
    "href": "posts/Niharika_HW1.html#c-1",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\nProbability that a randomly selected inmate has 2 or fewer prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions <= 2)\nsum(temp$Probability)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/Niharika_HW1.html#d-1",
    "href": "posts/Niharika_HW1.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\nProbability that a randomly selected inmate has more than 2 prior convictions:\n\n\nCode\ntemp <- Pc %>%\n  filter(Prior_convitions > 2)\nsum(temp$Probability)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#e-1",
    "href": "posts/Niharika_HW1.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nExpected value for the number of prior convictions:\n\n\nCode\nPc <- mutate(Pc, Wm = Prior_convitions*Probability)\ne <- sum(Pc$Wm)\ne\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/Niharika_HW1.html#f-1",
    "href": "posts/Niharika_HW1.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nVariance for the Prior Convictions:\n\n\nCode\nv <-sum(((Pc$Prior_convitions-e)^2)*Pc$Probability)\nv\n\n\n[1] 0.8562353\n\n\nstandard deviation for the Prior Convictions:\n\n\nCode\nsqrt(v)\n\n\n[1] 0.9253298"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html",
    "href": "posts/KenDocekal_HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a",
    "href": "posts/KenDocekal_HW1.html#a",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nRead in the data from the Excel file:\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\nLungCapData <- read_excel(\"_data/LungCapData.xls\")\nView(LungCapData)\n\n\nWarning in View(LungCapData): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(LungCapData$LungCap)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b",
    "href": "posts/KenDocekal_HW1.html#b",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability distribution of the LungCap, Males and Females, in a box plot:\n\n\nCode\nboxplot(LungCapData$LungCap ~ LungCapData$Gender)"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c",
    "href": "posts/KenDocekal_HW1.html#c",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nLung capacities for smokers and non-smokers, mean and standard deviation:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(mean = mean(LungCap, na.rm = TRUE), sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     7.77  2.73\n2 yes    8.65  1.88\n\n\nResults seem to point to smokers having greater lung capacity which is odd and could indicate factors other than age are influencing lung capacity"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d",
    "href": "posts/KenDocekal_HW1.html#d",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nThe relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”:\nage 13 and lower:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age <=13)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     6.36  2.21\n2 yes    7.20  1.58\n\n\nage 14 to 15:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 14:15)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     8.84 1.36 \n2 yes    8.91 0.865\n\n\nage 16 to 17:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age == 16:17)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no    10.4   1.73\n2 yes    9.60  1.41\n\n\nage 18 and over:\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n dplyr::filter(Age >=18)%>% \n  summarise(mean = mean(LungCap, na.rm = TRUE),sd = sd(LungCap, na.rm = TRUE))\n\n\n# A tibble: 2 × 3\n  Smoke  mean    sd\n  <chr> <dbl> <dbl>\n1 no     11.1  1.56\n2 yes    10.5  1.25"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e",
    "href": "posts/KenDocekal_HW1.html#e",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nWhen looking at mean lung capacity of smokers versus non-smokers by age groups we can see lung capacity increasing consistently as age increases. For the two lowest age groups mean capacity is lower for non-smokers although the difference decreases as age increases; this trend is reversed from age 16 onwards as non-smokers overtake smokers in lung capacity. Across all age groups non-smokers also have a greater standard deviation in lung capacity compared to smokers with the age 13 and under non-smoker group having the greatest standard deviation. It is likely that the greater number of age 13 and under respondents is the reason why overall results mirror the distribution seen in the youngest age group."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f",
    "href": "posts/KenDocekal_HW1.html#f",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nCovariance between lung capacity and age:\n\n\nCode\ncov(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 8.738289\n\n\nA positive covariance is shown which lets us know that as age increases lung capacity also increases.\nCorrelation between lung capacity and age:\n\n\nCode\ncor(LungCapData$Age,LungCapData$LungCap)\n\n\n[1] 0.8196749\n\n\nThe correlation coefficient is also positive; similar to the covariance this lets us know that there is a positive relationship between age and lung capacity. Additionally, since .819 is a relatively high score, as a score of 1 would indicate a perfect positive relationship, we know there is a strong relationship where a older respondent would be highly likely to have higher lung capacity and a younger respondent would likely have lower lung capacity."
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#a-1",
    "href": "posts/KenDocekal_HW1.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\nThe probability that a randomly selected inmate has exactly 2 prior convictions:\nCreate data frame:\n\n\nCode\nconvictions<- c(0,1,2,3,4)\nprisoners<- c(128, 434, 160, 64, 24)\n\ndf <- data.frame(convictions, prisoners)\n\ntibble(df)\n\n\n# A tibble: 5 × 2\n  convictions prisoners\n        <dbl>     <dbl>\n1           0       128\n2           1       434\n3           2       160\n4           3        64\n5           4        24\n\n\nProbability of exactly 2 prior convictions:\n\n\nCode\n160/sum(prisoners)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#b-1",
    "href": "posts/KenDocekal_HW1.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\nProbability of fewer than 2 prior convictions (total # of prisoners with less than 2 prior convictions = 562):\n\n\nCode\n562/sum(prisoners)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#c-1",
    "href": "posts/KenDocekal_HW1.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\nProbability of 2 or fewer prior convictions (total # of prisoners with 2 or fewer prior convictions = 722):\n\n\nCode\n722/sum(prisoners)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#d-1",
    "href": "posts/KenDocekal_HW1.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\nProbability of more than 2 prior convictions (total # of prisoners with more than 2 prior convictions = 88):\n\n\nCode\n88/sum(prisoners)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#e-1",
    "href": "posts/KenDocekal_HW1.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nThe expected value for the number of prior convictions (using the probability of observing each prisoner prior conviction group):\n\n\nCode\ncon1<- c(0,1,2,3,4)\npprob<- c(.158,.536,.198,.079,.028)\n\n\nsum(con1*pprob)\n\n\n[1] 1.281"
  },
  {
    "objectID": "posts/KenDocekal_HW1.html#f-1",
    "href": "posts/KenDocekal_HW1.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\nVariance and standard deviation for prior convictions:\n\n\nCode\nvar(prisoners)\n\n\n[1] 25948\n\n\nCode\nsd(prisoners)\n\n\n[1] 161.0838"
  },
  {
    "objectID": "posts/hw2_boonstra.html",
    "href": "posts/hw2_boonstra.html",
    "title": "Homework 2",
    "section": "",
    "text": "This question involves calculating 90% confidence intervals for data on mean wait time between heart surgery procedures being scheduled and the procedures being conducted for individuals in Ontario, Canada.\nThe equation for calculating confidence intervals using the Student’s t-distribution is as follows:\n\\(CI = \\overline{x} \\pm (t \\times \\frac{\\sigma}{\\sqrt{n}})\\)\n\n\nStarting with the Bypass subset, we can fill in some of these values:\n\\(CI_{bypass} = 19 \\pm (t \\times \\frac{10}{\\sqrt{539}})\\)\nThe t-quantile for the 90% confidence interval at 538 degrees of freedom is equal to 1.6476908, leaving us with the equation:\n\\(CI_{bypass} = 19 \\pm (1.648 \\times \\frac{10}{\\sqrt{539}})\\)\nThis maths out to 18.2902893 and 19.7097107.\n\n\n\nSimilarly, for the Angiography subset:\n\\(CI_{angiography} = 18 \\pm (t \\times \\frac{9}{847} )\\)\nThe t-quantile for the 90% confidence interval at 84 degrees of freedom is equal to 1.6466568, leaving us with the equation:\n\\(CI_{angiography} = 18 \\pm (1.647 \\times \\frac{9}{847} )\\)\nThis maths out to 17.4907818 and 18.5092182.\n\n\n\nBetween these two subsets, the 90% confidence interval is narrower for the Angiography subset:\n\\(CI_{bypass\\_range} = CI_{bypass\\_high} - CI_{bypass\\_low}=\\) 1.4194214\n\\(CI_{angiography\\_range}=CI_{angiography\\_high} - CI_{angiography\\_low}=\\) 1.0184363"
  },
  {
    "objectID": "posts/hw2_boonstra.html#t-tests",
    "href": "posts/hw2_boonstra.html#t-tests",
    "title": "Homework 2",
    "section": "t-tests",
    "text": "t-tests\nThese tests operate under the assumption that female employees’ pay data are randomly sampled and normally distributed.\n\nt.test(fem_pay,mu=500) # two-sided\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.01707\nalternative hypothesis: true mean is not equal to 500\n95 percent confidence interval:\n 340.8199 479.1801\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"less\") # one-sided, H_0 mean is not less\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.008536\nalternative hypothesis: true mean is less than 500\n95 percent confidence interval:\n     -Inf 465.7864\nsample estimates:\nmean of x \n      410 \n\nt.test(fem_pay,mu=500,alternative=\"greater\") # one-sided, H_0 mean is not greater\n\n\n    One Sample t-test\n\ndata:  fem_pay\nt = -3, df = 8, p-value = 0.9915\nalternative hypothesis: true mean is greater than 500\n95 percent confidence interval:\n 354.2136      Inf\nsample estimates:\nmean of x \n      410 \n\n\n\nPart A\nFrom the first test, we can reject the null hypothesis that mean pay for female employees is equal to $500 per week. This holds at the 5% significance level, with a p-value of less than 0.02, and a t-statistic of -3.\n\n\nPart B\nFrom the second test, we get a p-value of less than 0.009, which enables us at the 5% significance level to reject the null hypothesis that mean pay for female employees is not less than $500, and accept the alternative hypothesis that mean pay is less than $500.\n\n\nPart C\nFrom the third test, we get a p-value of greater than 0.99, which mean that we fail to reject the null hypothesis that mean pay for female employees is not greater than $500."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html",
    "href": "posts/nboonstra_final_603_proposal.html",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "",
    "text": "Code\nrm(list=ls())\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#theory",
    "href": "posts/nboonstra_final_603_proposal.html#theory",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Theory",
    "text": "Theory\nA review of even such a small sample of the literature as the works mentioned above will clearly demonstrate that, beyond disagreement over the presence of partisan turnout bias, there is little consensus on the theoretical aspect of such a phenomenon. Before offering my hypothesis, therefore, I would like to briefly address this theoretical side of the argument.\nShaw and Petrocik (2020) take issue with a notion found in turnout bias literature, the notion being “that turnout is endogenous to candidate preference” (p. 53). They cite Downs’ (1957) famous equation, \\(V=(P*B)-C\\), as evidence that it is the intensity of one’s political beliefs, and not their direction, that determines the decision to vote or not, and that therefore turnout is not endogenous to candidate preference.\nI believe this argument misses a subtle nuance that is key to the turnout bias debate. Suppose that not all individuals in a given polity face the same costs to voting; assume, in other words, that a more accurate rendition of Downs’ equation would be \\(V_i=(P*B_i)-C_i\\), in which both cost of voting and the perceived benefit of a preferred candidate’s victory are unique to the individual. For the sake of this argument, the manner in which these costs are distributed is not important; only the fact that there are unequal costs matters. Suppose further that one of the parties in this polity has established itself as being the party that lobbies for a reduction in the cost of voting, particularly for those who face disproportionately high barriers. In a world of rational actors and perfect information, it would follow, ceteris paribus, that an individual who faced disproportionately high costs to voting would support this party, since this party would lobby to improve opportunities for this group. However, the very higher cost of voting that would motivate this individual to support this party could also prevent them from ultimately voting for that candidate in an election. Thus, it could be said that turnout is endogenous to candidate preference – or, more accurately, that the cost of voting is endogenous to both candidate preference and turnout.\nWe can apply this theoretical model to the American case. Certain individuals do face higher barriers to voting; unfortunately, unlike in the model, these barriers do tend to be distributed in a certain manner, often inequitably by race and socioeconomic status. Additionally, it would not be difficult to argue that, of the two major parties, the Democrats have placed themselves in the position of the party lobbying for expanded voting access and reduction of barriers to the ballot box, starting with their role in the Civil Rights movement and corresponding legislation, and continuing to the start of the present Congress and the introduction of H.R. 1, a bill explicitly aimed at expanding voting rights. Thus, while our world is not one of completely perfect information or completely rational actors, and while a number of factors contribute to partisan identity and vote choice, there is a reasonable case to be made that individuals who face barriers to voting, ceteris paribus, would be more likely to support the Democratic Party. Once again, these very barriers to voting that would push individuals toward the Democrats also can restrict them from expressing that preference at the ballot box. Thus, we have our situation of endogeneity between partisan preference and turnout."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "href": "posts/nboonstra_final_603_proposal.html#hypotheses",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Hypotheses",
    "text": "Hypotheses\nWith the theoretical argument out of the way, I can proceed to out line some of the hypotheses I would like to test with this project.\n\\(H_1\\): Higher turnout will benefit Democrats in state-level Presidential elections.\n\\(H_2\\): Democrats will perform better in state-level Presidential elections as turnout increases relative to the previous election in that state.\nThe distinction of state-level elections is an important one; Shaw and Petrocik (2020) tend to aggregate their data, either by assessing elections on the national level or by aggregating county-level data. In the United States, Presidential elections are conducted at the state level, and I believe that this is the appropriate level of analysis for this analysis."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "href": "posts/nboonstra_final_603_proposal.html#election-data-1976-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Election Data, 1976-2020",
    "text": "Election Data, 1976-2020\nObtained from the MIT Election Project on 10/10/2022.\n\n\nCode\nelection_full <- read_csv(\"./_data/mit_election_1976_2020.csv\")\n\nelection_full <- election_full %>% \n  mutate(party_simplified2 = case_when(\n    party_detailed == \"DEMOCRAT\" ~ \"DEMOCRAT\",\n    party_detailed == \"REPUBLICAN\" ~ \"REPUBLICAN\",\n    party_detailed == \"LIBERTARIAN\" ~ \"LIBERTARIAN\",\n    party_detailed == \"GREEN\" ~ \"GREEN\",\n    party_detailed == \"INDEPENDENT\" ~ \"INDEPENDENT\",\n    TRUE ~ \"OTHER\"\n  )) %>% \n  mutate(party_dem = case_when(\n    party_detailed == \"DEMOCRAT\" ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(election_full, n=20)\n\n\n# A tibble: 20 × 17\n    year state    state…¹ state…² state…³ state…⁴ office candi…⁵ party…⁶ writein\n   <dbl> <chr>    <chr>     <dbl>   <dbl>   <dbl> <chr>  <chr>   <chr>   <lgl>  \n 1  1976 ALABAMA  AL            1      63      41 US PR… \"CARTE… DEMOCR… FALSE  \n 2  1976 ALABAMA  AL            1      63      41 US PR… \"FORD,… REPUBL… FALSE  \n 3  1976 ALABAMA  AL            1      63      41 US PR… \"MADDO… AMERIC… FALSE  \n 4  1976 ALABAMA  AL            1      63      41 US PR… \"BUBAR… PROHIB… FALSE  \n 5  1976 ALABAMA  AL            1      63      41 US PR… \"HALL,… COMMUN… FALSE  \n 6  1976 ALABAMA  AL            1      63      41 US PR… \"MACBR… LIBERT… FALSE  \n 7  1976 ALABAMA  AL            1      63      41 US PR…  <NA>   <NA>    TRUE   \n 8  1976 ALASKA   AK            2      94      81 US PR… \"FORD,… REPUBL… FALSE  \n 9  1976 ALASKA   AK            2      94      81 US PR… \"CARTE… DEMOCR… FALSE  \n10  1976 ALASKA   AK            2      94      81 US PR… \"MACBR… LIBERT… FALSE  \n11  1976 ALASKA   AK            2      94      81 US PR…  <NA>   <NA>    TRUE   \n12  1976 ARIZONA  AZ            4      86      61 US PR… \"FORD,… REPUBL… FALSE  \n13  1976 ARIZONA  AZ            4      86      61 US PR… \"CARTE… DEMOCR… FALSE  \n14  1976 ARIZONA  AZ            4      86      61 US PR… \"MCCAR… INDEPE… FALSE  \n15  1976 ARIZONA  AZ            4      86      61 US PR… \"MACBR… LIBERT… FALSE  \n16  1976 ARIZONA  AZ            4      86      61 US PR… \"CAMEJ… SOCIAL… FALSE  \n17  1976 ARIZONA  AZ            4      86      61 US PR… \"ANDER… AMERIC… FALSE  \n18  1976 ARIZONA  AZ            4      86      61 US PR… \"MADDO… AMERIC… FALSE  \n19  1976 ARIZONA  AZ            4      86      61 US PR…  <NA>   <NA>    TRUE   \n20  1976 ARKANSAS AR            5      71      42 US PR… \"CARTE… DEMOCR… FALSE  \n# … with 7 more variables: candidatevotes <dbl>, totalvotes <dbl>,\n#   version <dbl>, notes <lgl>, party_simplified <chr>,\n#   party_simplified2 <chr>, party_dem <dbl>, and abbreviated variable names\n#   ¹​state_po, ²​state_fips, ³​state_cen, ⁴​state_ic, ⁵​candidate, ⁶​party_detailed\n\n\nCode\ncolnames(election_full)\n\n\n [1] \"year\"              \"state\"             \"state_po\"         \n [4] \"state_fips\"        \"state_cen\"         \"state_ic\"         \n [7] \"office\"            \"candidate\"         \"party_detailed\"   \n[10] \"writein\"           \"candidatevotes\"    \"totalvotes\"       \n[13] \"version\"           \"notes\"             \"party_simplified\" \n[16] \"party_simplified2\" \"party_dem\"        \n\n\nCode\nsummary(election_full)\n\n\n      year         state             state_po           state_fips   \n Min.   :1976   Length:4287        Length:4287        Min.   : 1.00  \n 1st Qu.:1988   Class :character   Class :character   1st Qu.:16.00  \n Median :2000   Mode  :character   Mode  :character   Median :28.00  \n Mean   :1999                                         Mean   :28.62  \n 3rd Qu.:2012                                         3rd Qu.:41.00  \n Max.   :2020                                         Max.   :56.00  \n   state_cen        state_ic        office           candidate        \n Min.   :11.00   Min.   : 1.00   Length:4287        Length:4287       \n 1st Qu.:33.00   1st Qu.:22.00   Class :character   Class :character  \n Median :53.00   Median :42.00   Mode  :character   Mode  :character  \n Mean   :53.67   Mean   :39.75                                        \n 3rd Qu.:81.00   3rd Qu.:61.00                                        \n Max.   :95.00   Max.   :82.00                                        \n party_detailed      writein        candidatevotes       totalvotes      \n Length:4287        Mode :logical   Min.   :       0   Min.   :  123574  \n Class :character   FALSE:3807      1st Qu.:    1177   1st Qu.:  652274  \n Mode  :character   TRUE :477       Median :    7499   Median : 1569180  \n                    NA's :3         Mean   :  311908   Mean   : 2366924  \n                                    3rd Qu.:  199242   3rd Qu.: 3033118  \n                                    Max.   :11110250   Max.   :17500881  \n    version          notes         party_simplified   party_simplified2 \n Min.   :20210113   Mode:logical   Length:4287        Length:4287       \n 1st Qu.:20210113   NA's:4287      Class :character   Class :character  \n Median :20210113                  Mode  :character   Mode  :character  \n Mean   :20210113                                                       \n 3rd Qu.:20210113                                                       \n Max.   :20210113                                                       \n   party_dem     \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.1428  \n 3rd Qu.:0.0000  \n Max.   :1.0000  \n\n\nThis dataframe contains state-level election results for all 50 states and the District of Columbia for the six Presidential elections from 1976 to 2020. (I am currently not sure that I will use that entire date range, particularly because it does not exactly coincide with the turnout data available, but for now I am including the full data set.) Included in the dataframe are candidate vote totals and party affiliations, which I have used to add an extra column, party_dem, which is a dummy variable recording whether or not a given candidate is a Democrat. The data already come in tidy, which is a nice touch; a “case” or row is a given candidate’s performance in a given state’s Presidential election in a given year."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "href": "posts/nboonstra_final_603_proposal.html#turnout-data-1980-2014",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Turnout data, 1980-2014",
    "text": "Turnout data, 1980-2014\nObtained from the US Elections Project on 10/11/2022.\n\n\nCode\nturnout <- read_excel(\"./_data/1980-2014 November General Election.xlsx\",\n                      skip=2,\n                      col_types=c(\n                        \"numeric\",\"skip\",\"skip\",\"text\",\n                        \"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\n                        \"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\",\"numeric\"\n                      ),\n                      col_names=c(\n                        \"year\",\"state\",\n                        \"totballots_vep_rate\",\"highestoff_vep_rate\",\"highestoff_vap_rate\",\n                        \"totalballots_count\",\"highestoff_count\",\"vep_count\",\"vap_count\",\n                        \"noncitizen_percent\",\"prison_count\",\"probation_count\",\n                        \"parole_count\",\"totineligible_count\",\"overseas_count\"\n                      ))\n\nhead(turnout,n=20)\n\n\n# A tibble: 20 × 15\n    year state   totba…¹ highe…² highe…³ total…⁴ highe…⁵ vep_c…⁶ vap_c…⁷ nonci…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  2014 United…   0.367   0.36    0.332  8.33e7  8.17e7  2.27e8  2.46e8   0.084\n 2  2014 Alabama   0.332   0.329   0.315  1.19e6  1.18e6  3.59e6  3.75e6   0.025\n 3  2014 Alaska    0.548   0.542   0.51   2.85e5  2.82e5  5.21e5  5.53e5   0.039\n 4  2014 Arizona   0.341   0.334   0.295  1.54e6  1.51e6  4.51e6  5.11e6   0.101\n 5  2014 Arkans…   0.403   0.401   0.375  8.53e5  8.49e5  2.12e6  2.26e6   0.04 \n 6  2014 Califo…   0.307   0.299   0.247  7.51e6  7.32e6  2.44e7  2.96e7   0.168\n 7  2014 Colora…   0.547   0.537   0.494  2.08e6  2.04e6  3.80e6  4.13e6   0.072\n 8  2014 Connec…   0.425   0.423   0.385  1.10e6  1.09e6  2.58e6  2.83e6   0.082\n 9  2014 Delawa…   0.349   0.343   0.318  2.38e5  2.34e5  6.82e5  7.35e5   0.051\n10  2014 Distri…   0.357   0.353   0.32   1.77e5  1.75e5  4.96e5  5.47e5   0.094\n11  2014 Florida   0.433   0.428   0.376  6.03e6  5.95e6  1.39e7  1.58e7   0.106\n12  2014 Georgia   0.386   0.382   0.338  2.60e6  2.57e6  6.73e6  7.60e6   0.071\n13  2014 Hawaii    0.365   0.362   0.329  3.70e5  3.66e5  1.01e6  1.11e6   0.086\n14  2014 Idaho     0.398   0.393   0.365  4.45e5  4.40e5  1.12e6  1.21e6   0.046\n15  2014 Illino…   0.408   0.402   0.366  3.68e6  3.63e6  9.03e6  9.92e6   0.085\n16  2014 Indiana   0.287   0.278   0.267  1.39e6  1.34e6  4.83e6  5.03e6   0.035\n17  2014 Iowa      0.503   0.498   0.473  1.14e6  1.13e6  2.27e6  2.39e6   0.036\n18  2014 Kansas    0.433   0.425   0.398  8.87e5  8.70e5  2.05e6  2.18e6   0.052\n19  2014 Kentuc…   0.449   0.442   0.422  1.46e6  1.44e6  3.25e6  3.41e6   0.027\n20  2014 Louisi…   0.449   0.439   0.415  1.50e6  1.47e6  3.35e6  3.55e6   0.03 \n# … with 5 more variables: prison_count <dbl>, probation_count <dbl>,\n#   parole_count <dbl>, totineligible_count <dbl>, overseas_count <dbl>, and\n#   abbreviated variable names ¹​totballots_vep_rate, ²​highestoff_vep_rate,\n#   ³​highestoff_vap_rate, ⁴​totalballots_count, ⁵​highestoff_count, ⁶​vep_count,\n#   ⁷​vap_count, ⁸​noncitizen_percent\n\n\nCode\ncolnames(turnout)\n\n\n [1] \"year\"                \"state\"               \"totballots_vep_rate\"\n [4] \"highestoff_vep_rate\" \"highestoff_vap_rate\" \"totalballots_count\" \n [7] \"highestoff_count\"    \"vep_count\"           \"vap_count\"          \n[10] \"noncitizen_percent\"  \"prison_count\"        \"probation_count\"    \n[13] \"parole_count\"        \"totineligible_count\" \"overseas_count\"     \n\n\nCode\nsummary(turnout)\n\n\n      year         state           totballots_vep_rate highestoff_vep_rate\n Min.   :1980   Length:936         Min.   :0.0000      Min.   :0.2020     \n 1st Qu.:1988   Class :character   1st Qu.:0.4310      1st Qu.:0.4140     \n Median :1997   Mode  :character   Median :0.5200      Median :0.5010     \n Mean   :1997                      Mean   :0.5125      Mean   :0.4993     \n 3rd Qu.:2006                      3rd Qu.:0.6040      3rd Qu.:0.5840     \n Max.   :2014                      Max.   :0.7880      Max.   :0.7840     \n                                   NA's   :215         NA's   :1          \n highestoff_vap_rate totalballots_count  highestoff_count   \n Min.   :0.1990      Min.   :   122356   Min.   :   117623  \n 1st Qu.:0.3895      1st Qu.:   422851   1st Qu.:   488820  \n Median :0.4770      Median :  1170867   Median :  1236230  \n Mean   :0.4733      Mean   :  3074280   Mean   :  3509231  \n 3rd Qu.:0.5560      3rd Qu.:  2395791   3rd Qu.:  2336586  \n Max.   :0.7390      Max.   :132609063   Max.   :131304731  \n NA's   :1           NA's   :223         NA's   :1          \n   vep_count           vap_count         noncitizen_percent  prison_count    \n Min.   :   270122   Min.   :   277261   Min.   :0.00400    Min.   :      0  \n 1st Qu.:   999644   1st Qu.:  1044366   1st Qu.:0.01500    1st Qu.:   3464  \n Median :  2662524   Median :  2778086   Median :0.03100    Median :  10018  \n Mean   :  7277622   Mean   :  7840064   Mean   :0.04344    Mean   :  39257  \n 3rd Qu.:  4569632   3rd Qu.:  4898253   3rd Qu.:0.06600    3rd Qu.:  24819  \n Max.   :227157964   Max.   :245712915   Max.   :0.18900    Max.   :1605448  \n                                                                             \n probation_count    parole_count    totineligible_count overseas_count   \n Min.   :      0   Min.   :     0   Min.   :      0     Min.   :   6916  \n 1st Qu.:      0   1st Qu.:     0   1st Qu.:   6210     1st Qu.:  43108  \n Median :   7982   Median :  1870   Median :  21329     Median :  89605  \n Mean   :  67542   Mean   : 16227   Mean   :  90039     Mean   : 920963  \n 3rd Qu.:  38902   3rd Qu.:  6592   3rd Qu.:  52525     3rd Qu.:1803021  \n Max.   :2451708   Max.   :637410   Max.   :3363118     Max.   :5345814  \n                                                        NA's   :867      \n\n\nAdditional turnout data are available from the USEP by election from 2000-2020, albeit in their own individual spreadsheets; I may end up merging the 2016 and 2020 spreadsheets into this 1980-2014 set. It is important to note that this dataset includes observations for both Presidential and midterm election years, while I only intend to analyze Presidential elections.\nThis dataset makes distinctions between turnout based on voting-age population (VAP) and voting-eligible population (VEP). The literature generally agrees that VEP is the most reliable and consistent measure. However, given that one of the main differences between the two is the barrier of felony disenfranchisement, a barrier that is often inequitably distributed by race, I may end up using VAP turnout in my analysis; I have not yet decided as of the time of this submission."
  },
  {
    "objectID": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "href": "posts/nboonstra_final_603_proposal.html#voter-id-data-2000-2020",
    "title": "Voter Turnout and Partisan Bias in U.S. Presidential Elections",
    "section": "Voter ID data, 2000-2020",
    "text": "Voter ID data, 2000-2020\nObtained from the National Conference of State Legislatures, who kindly provided via email a spreadsheet version of the data on this webpage on 10/11/2022.\n\n\nCode\nvoter_id <- read_excel(\"./_data/voter_id_chronology.xlsx\",\n                      skip = 2,\n                      col_types = c(\"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"text\",\"skip\",\"text\",\"skip\",\n                                    \"text\",\"skip\",\"skip\"))\n\nvoter_id <- voter_id %>% \n  pivot_longer(cols=c(2:7),\n               names_to=\"year\",\n               values_to=\"id_text\") %>% \n  mutate(id_req = case_when(\n    grepl(\"no id\", id_text, ignore.case = TRUE) ~ 0,\n    TRUE ~ 1\n  )) %>% \n  mutate(id_strict = case_when(\n    grepl(\"Strict\", id_text) ~ 1,\n    TRUE ~ 0\n  )) %>% \n  mutate(id_photo = case_when(\n    grepl(\" photo\", id_text, ignore.case = TRUE) ~ 1,\n    TRUE ~ 0\n  ))\n\nhead(voter_id,n=20)\n\n\n# A tibble: 20 × 6\n   State    year  id_text                 id_req id_strict id_photo\n   <chr>    <chr> <chr>                    <dbl>     <dbl>    <dbl>\n 1 Alabama  2000  No ID required at polls      0         0        0\n 2 Alabama  2004  Non-strict, non-photo        1         0        0\n 3 Alabama  2008  Non-strict, non-photo        1         0        0\n 4 Alabama  2012  Non-strict, non-photo        1         0        0\n 5 Alabama  2016  Non-strict, photo            1         0        1\n 6 Alabama  2020  Non-strict, photo            1         0        1\n 7 Alaska   2000  Non-strict, non-photo        1         0        0\n 8 Alaska   2004  Non-strict, non-photo        1         0        0\n 9 Alaska   2008  Non-strict, non-photo        1         0        0\n10 Alaska   2012  Non-strict, non-photo        1         0        0\n11 Alaska   2016  Non-strict, non-photo        1         0        0\n12 Alaska   2020  Non-strict, non-photo        1         0        0\n13 Arizona  2000  No ID required at polls      0         0        0\n14 Arizona  2004  No ID required at polls      0         0        0\n15 Arizona  2008  Strict non-photo             1         1        0\n16 Arizona  2012  Strict non-photo             1         1        0\n17 Arizona  2016  Strict non-photo             1         1        0\n18 Arizona  2020  Strict non-photo             1         1        0\n19 Arkansas 2000  Non-strict, non-photo        1         0        0\n20 Arkansas 2004  Non-strict, non-photo        1         0        0\n\n\nCode\ncolnames(voter_id)\n\n\n[1] \"State\"     \"year\"      \"id_text\"   \"id_req\"    \"id_strict\" \"id_photo\" \n\n\nCode\nsummary(voter_id)\n\n\n    State               year             id_text              id_req      \n Length:306         Length:306         Length:306         Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :1.0000  \n                                                          Mean   :0.5033  \n                                                          3rd Qu.:1.0000  \n                                                          Max.   :1.0000  \n   id_strict          id_photo     \n Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.0000  \n Mean   :0.09804   Mean   :0.1928  \n 3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.0000  \n\n\nGiven that barriers to voting factor into the argument behind my research, I wanted to include data on voter ID laws in my analysis, as a controlling (or other type of) variable. The data here track voter ID laws across all 50 U.S. states and the District of Columbia from 2000 to 2020.\nThese data are surprisingly well balanced when it comes to the occurrence of voter ID laws; 50.33 percent of elections were held under voter-ID laws of some sort. Cases are also specified by whether or not a voter ID law was strict (i.e. required the voter to cast a provisional ballot and verify their identity after Election Day), and whether or not the state required a photo on the identification. Strict voter ID laws are the most rare, occurring in only 9.8 percent of elections in the data set; photo requirements are slightly more common, occurring in 19.28 percent of elections."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html",
    "href": "posts/NiyatiSharma_blog1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\n\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#introduction",
    "href": "posts/NiyatiSharma_blog1.html#introduction",
    "title": "Final Project Proposal",
    "section": "Introduction",
    "text": "Introduction\nCredit risk is defined as the risk of loss resulting from the failure by a borrower to repay the principal and interest owed to the leader.So the purpose of credit analysis is to determine the creditworthiness of borrowers by measuring the risk of loss that the lender is exposed to.When calculating the credit risk of a particular borrower, lenders consider various factors like analyze different documents, such as the borrower’s income statement, balance sheet, credit reports, and other documents that reveal the financial situation of the borrower. to evaluate the characteristics of the borrower and conditions of the loan to estimate the probability of default and the subsequent risk of financial loss."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#research-question",
    "href": "posts/NiyatiSharma_blog1.html#research-question",
    "title": "Final Project Proposal",
    "section": "Research Question",
    "text": "Research Question\nQ1. How credit risk depends on the age of the person. Q2. Dominating factor on which credit risk depends. Q3. Is credit risk depends on loan_intent?"
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#hypothesis",
    "href": "posts/NiyatiSharma_blog1.html#hypothesis",
    "title": "Final Project Proposal",
    "section": "Hypothesis",
    "text": "Hypothesis\nAccording to research credit risk of a particular borrower, lenders consider various factors include the borrower’s capacity to repay are income, character, house ownership, and credit history. Check the relationship between the age, income with credit risk with new dataset."
  },
  {
    "objectID": "posts/NiyatiSharma_blog1.html#dataset",
    "href": "posts/NiyatiSharma_blog1.html#dataset",
    "title": "Final Project Proposal",
    "section": "Dataset",
    "text": "Dataset\nThis dataset contains columns simulating credit bureau data, factors on which credit risk depends. The variables of interest for me are income, age, employment length and home ownership.\n\n\nCode\nlibrary(readr)\ndf <- read_csv(\"C:/Users/Lenovo/Downloads/credit_risk_dataset_1.csv\")\n\n\nError: 'C:/Users/Lenovo/Downloads/credit_risk_dataset_1.csv' does not exist.\n\n\nCode\nsummary(df)\n\n\nError in object[[i]]: object of type 'closure' is not subsettable"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html",
    "href": "posts/HW2_ShoshanaBuck.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#tail-area-and-standard-error-for-bypass",
    "title": "Homework 2",
    "section": "Tail area and standard error for Bypass",
    "text": "Tail area and standard error for Bypass\n\n\nCode\ntail_area<- (1-.90)/2\ntail_area\n\n\n[1] 0.05\n\n\nCode\nstandard_error<- 10/sqrt(539)\nstandard_error\n\n\n[1] 0.4307305"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#t-value-for-bypass",
    "title": "Homework 2",
    "section": "t-value for Bypass",
    "text": "t-value for Bypass\n\n\nCode\nt_score<- qt(p= 1-tail_area, df= 538)\nt_score\n\n\n[1] 1.647691"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-bypass",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for Bypass",
    "text": "Confidence interval and margin of error for Bypass\n\n\nCode\nCI<- c(19 - t_score * standard_error, 19 + t_score * standard_error)\nCI\n\n\n[1] 18.29029 19.70971\n\n\nCode\nMOE<- t_score *standard_error\nMOE *1.41\n\n\n[1] 1.000692"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error-of-angiography",
    "title": "Homework 2",
    "section": "Standard error of angiography",
    "text": "Standard error of angiography\n\n\nCode\nstandard_error2<- 9/sqrt(847)\nstandard_error2\n\n\n[1] 0.3092437"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#t-score-for-angiography",
    "title": "Homework 2",
    "section": "t-score for angiography",
    "text": "t-score for angiography\n\n\nCode\nt_score2<- qt(p= 1-.05, df= 846)\nt_score2\n\n\n[1] 1.646657"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-and-margin-of-error-for-angiography",
    "title": "Homework 2",
    "section": "Confidence interval and margin of error for angiography",
    "text": "Confidence interval and margin of error for angiography\n\n\nCode\nCI<- c(18 - t_score2 * standard_error2, 18 + t_score2 * standard_error2)\nCI\n\n\n[1] 17.49078 18.50922\n\n\nCode\nMOE2<- t_score2 *standard_error2\nMOE2 *1.01\n\n\n[1] 0.5143103\n\n\nThe Bypass points are [18.29029 & 19.70971] days and has a margin of error of +/- 0.7. Whereas the angiography is [17.49 & 18.50] days with a margin of error of +/- 0.5. Angigography is more narrower because it has a larger sample size and the range between the high and low end of the confidence interval is smaller."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "href": "posts/HW2_ShoshanaBuck.html#point-estimate-p",
    "title": "Homework 2",
    "section": "Point estimate P",
    "text": "Point estimate P\n\n\nCode\ns_size<- 1031\nb<- 567\n\npoint_estimate<- b/s_size\npoint_estimate\n\n\n[1] 0.5499515"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "href": "posts/HW2_ShoshanaBuck.html#confidence-interval-for-p",
    "title": "Homework 2",
    "section": "95% confidence interval for P",
    "text": "95% confidence interval for P\n\n\nCode\nprop.test(b,s_size)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  b out of s_size, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nBased off the point estimate, 54% of the adult Americans that were surveyed by the National Center for Public Policy believe that college education is essential for success. 95% confidence interval of adult Americans who believe that college education is essential for success is [0.5189682 0.5805580] which contains the true population mean."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "href": "posts/HW2_ShoshanaBuck.html#standard-deviation",
    "title": "Homework 2",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nCode\nsd<- (200-30)/4\nsd\n\n\n[1] 42.5"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "href": "posts/HW2_ShoshanaBuck.html#solving-for-n",
    "title": "Homework 2",
    "section": "Solving for n",
    "text": "Solving for n\n\n\nCode\n#steps for the equation\n#1. 5 = 1.96 * (42.5/sqrt(n))\n\n#2. 5 = 8.3/sqrt(n)\n\n#3. 5*sqrt(n)= 83.3\n\n#4. sqrt(n) = 83.3/5\n\n#5. n= (83.3/5)^2\n\n#6. n= 278.89\n\n\nThe standard deviation from the data is 42.5. Since we have solved for the standard deviation we can plug it into the CI equation and solve for n."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-4",
    "href": "posts/HW2_ShoshanaBuck.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\n\nA\nTest whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result. ## Assumptions\nWe are assuming there is normal distribution, the null hypothesis is: μ= 500 and the alternative hypothesis is 500> μ <500."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#standard-error",
    "href": "posts/HW2_ShoshanaBuck.html#standard-error",
    "title": "Homework 2",
    "section": "Standard error",
    "text": "Standard error\n\n\nCode\ns_sizef<- 9\nsd<-90\ns_meanf<- 410\nnull_hypo_mean<- 500\n\nstandard_errorf<- sd/sqrt(s_sizef)\nstandard_errorf\n\n\n[1] 30"
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#t-score",
    "href": "posts/HW2_ShoshanaBuck.html#t-score",
    "title": "Homework 2",
    "section": "t-score",
    "text": "t-score\n\n\nCode\nt_stat<- (s_meanf-null_hypo_mean)/standard_errorf\nt_stat\n\n\n[1] -3\n\n\nI took the sample mean of 410 subtracted that from the mu = 500 and then divided it by the standard error = 30."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#p-value",
    "href": "posts/HW2_ShoshanaBuck.html#p-value",
    "title": "Homework 2",
    "section": "p-value",
    "text": "p-value\n\n\nCode\np_value<- (pt(t_stat, df=8)) *2\np_value\n\n\n[1] 0.01707168\n\n\nThe p-value than the 5% significance level so we can reject the null hypothesis in favor of the alternative hypothesis."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#b-c",
    "href": "posts/HW2_ShoshanaBuck.html#b-c",
    "title": "Homework 2",
    "section": "B +C",
    "text": "B +C\nReport the P-value for Ha : μ < 500. Interpret. Report and interpret the P-value for H a: μ > 500.\n\n\nCode\nupper_p_value<- (pt(t_stat, df=8, lower.tail = FALSE))\nupper_p_value\n\n\n[1] 0.9914642\n\n\nCode\nlower_p_value<- (pt(t_stat, df=8, lower.tail = TRUE))\nlower_p_value\n\n\n[1] 0.008535841\n\n\nThe upper-tailed p-value is 0.99 and the lower-tailed p-value is 0.008. If you add the two tails together they will equal 1."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-5",
    "href": "posts/HW2_ShoshanaBuck.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nShow that t = 1.95 and P-value = 0.051 for Jones. Show that t = 1.97 and P-value = 0.049 for Smith.\n\n\nCode\njones_sample_mean<- 519.5\nsmith_sample_mean<-519.7\nnull_hyp<- 500\njones_se<- 10\nsmith_se<- 10\n\n\n\nA: Jones t-score and p-value\n\n\nCode\njones_t_stat<- (jones_sample_mean-null_hyp)/jones_se\njones_t_stat\n\n\n[1] 1.95\n\n\nCode\njones_p_value<- pt(jones_t_stat, df=999, lower.tail = FALSE) *2\njones_p_value\n\n\n[1] 0.05145555\n\n\n\n\nA: Smith t-score and p-value\n\n\nCode\nsmith_t_stat<-(smith_sample_mean-null_hyp)/smith_se\nsmith_t_stat\n\n\n[1] 1.97\n\n\nCode\nsmith_p_value<- pt(smith_t_stat, df=999, lower.tail = FALSE)*2\nsmith_p_value\n\n\n[1] 0.04911426\n\n\n\n\nB\nUsing α = 0.05, for each study indicate whether the result is “statistically significant.”\nThe results are “statistically significant when the p-value is smaller than the 0.05. Jones p-value is 0.051 which is greater than the 0.05 significance level which means it is not statistically significant and we cannot reject the null hypothesis. Smith’s p-value is 0.49 which is smaller than the significance level which means it is statistically significant and that we can reject the null hypothesis in favor of the alternative hypothesis.\n\n\nC\n“P ≤ 0.05” versus “P > 0.05,” or as “reject H0” versus “Do not reject H0 ,” is a misleading statement without providing the p-values because it makes it seem that there is a drastic difference between Jones and Smith that caused one hypothesis to be statistically significant and the other one not to be. However, when looking at the actual p-value it can be noted that there is a very small difference between the values."
  },
  {
    "objectID": "posts/HW2_ShoshanaBuck.html#question-6",
    "href": "posts/HW2_ShoshanaBuck.html#question-6",
    "title": "Homework 2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\n\n\n\nCode\nt.test(gas_taxes, mu=45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents?\nAt the 95% confidence level the p-value is 0.03 which is less than the 5% significance level. This proves that we can reject the null hypothesis and that the average tax per gallon of gas in the US in 2005 was less than 45 cents."
  },
  {
    "objectID": "posts/KenDocekal_finalproject1.html",
    "href": "posts/KenDocekal_finalproject1.html",
    "title": "Final Project 1",
    "section": "",
    "text": "Research Question\nHow much does state policy intervention impact future social and economic value preferences in residents?\nWhile political values often explicitly inform social and economic policy actions taken by governments, policy actions themselves can also affect the development of the values of both program recipients and the greater public. Low-income recipients are assumed to benefit from, and therefore favor, state intervention and redistributive policies while upper income groups are assumed to be against but this is not always true, especially at the program level (Bueno et al.). Authors like Holland note that “the poor only have an economic interest in supporting social expenditures in contexts where they expect policies to redistribute resources or risks in their favor”.\nThis study seeks to better understand the relationship between policy action and value formation at the sub-national level by looking at the effect of US state policy interventions on residents’ subsequent policy preferences. By looking at how differences in US states’ social and economic policy intervention from 1936 to 2000 we can see how these factors may shape the subsequent policy values of residents. The dataset “Correlates of State Policy” includes variables which also allow us to better understand the role of differences in policy design and implementation by controlling for variables that may moderate impact, such as the length of policy implementation (Soss) and differences in economic interest (Ansell).\nSources:\nAnsell, Ben. 2014. “The Political Economy of Ownership.” American Political Science Review 108(02):383{402.\nBoehmke, Frederick J., and Paul Skinner. 2012. “State Policy Innovativeness Revisited.” State Politics and Policy Quarterly, 12(3):303-29.\nBueno, Natalia and Nunes, Felipe and Zucco, Cesar, Making the bourgeoisie? Values, voice, and state-provided homeownership (January 7, 2022). SSRN.\nCaughey, Devin, and Christopher Warshaw. 2015. “The Dynamics of State Policy Liberalism, 1936–2014.” American Journal of Political Science, September. doi: 10.1111/ajps.12219.\nHolland, Alisha C. 2018. “Diminished Expectations: Redistributive preferences in truncated welfare states.” World Politics 70(4):555{594\nJacoby, William G., and Saundra K. Schneider. 2008. “A New Measure of Policy Spending Priorities in the American States.”\nJordan, Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing, MI: Institute for Public Policy and Social Research (IPPSR).\nRigby, Elizabeth and Gerald C. Wright. 2013. “Political Parties and Representation of the Poor in the American States.” American Journal of Political Science 57(3): 552-565.\nSoss, Joe. 1999. “Lessons of Welfare: Policy Design, Political Learning, and Political Action.” The American Political Science Review 93(2):363{380.\n\n\nHypothesis\nIncreased state intervention increases US state residents’ preference for future interventions in social and economic policy.\nThis study proposes to build on Bueno et al.’s exploration of the effects of state-provided home ownership on political values and policy preferences by exploring that relationship at the level of US states. Additionally, instead of focusing on a single social program, we will examine the cumulative effects of multiple policy interventions across 65 years in 50 US states. This will provide insights into the effect of public policy on value differences at the sub-national level and on different subgroups including program non-participants. We will be able to see how this relationship may vary according to state and population characteristics despite differences in policy design and implementation.\n\n\nDescriptive Statistics\nThis dataset is from the Correlates of State Policy Project by the Institute for Public Policy and Social Research at Michigan State University. The full dataset, which contains 928 variables and covers data from 1900 to 2016, draws from multiple sources including government agencies and peer-reviewed articles listed in the Sources section. Due to limited data coverage across all years however, this study will focus on the period from 1935 to 2000. We will examining the following 25 variables (listed with description and years available):\nIndependent-\nYear 1935 - 2000\nState 1935 - 2000\nEcondev - Did State adopt Strategic Planning for Economic Development? 1981 – 1992\nPldvpag - Did State adopt Planning/Development Agency? 1935 – 1978\nUrbrenen - Did State adopt Urban Renewal ? 1941 – 1952\nPollib_median - State Policy Liberalism Score – Median 1936 – 2014\nPolicypriorityscore - State Policy Priority Score - collective goods (e.g., education and highways) v particularized benefits (e.g., health care and welfare) 1982-2005\nPoptotal - Population Total 1900 – 2008\nPopfemale - Female Population 1994 – 2010\nNonwhite - Proportion of the population that is nonwhite 1974 - 2011\nSoc_capital_ma - Hawes et al. Weighted Moving Average Measure of Social Capital 1984 - 2011\nEvangelical_pop - Evangelical Population 1975 - 2013\nNewimmig - New Immigrant Green Card Holders 1988 – 2011\nPopdensity - Population Density 1975 – 1999\nGsp_q - Gross State Product Combined in Millions of 2016 Dollars 1963 – 2010\nGini_coef - Gini Coefficient 1917 - 2013\nHsdiploma - High School Diploma 1975 – 2006\nEducspend - State Education Spending 1975 – 2001\nNofelons - Number of Felons Ineligible to Vote 1980 – 2010\nCo2emissions - Total CO2 emissions from fossil-fuels (metric tons) 1960 – 2001\nIdeo - State Ideology Score 1976 – 2011\nDependent-\nVst_ec - Mean Economic Liberalism- All Voters 2000\nVst_soc - Mean Social Liberalism- All Voters 2000\nVavgec_low - Mean Economic Liberalism Score for Low Income Voting Citizens 2000\nVavgsoc_low - Mean Social Liberalism Score for Low Income Voting Citizens 2000\nReading in dataset\n\n\nCode\nlibrary(readr)\nlibrary(readxl)\n\n\nstatedata <- read.csv(\"_data/correlatesofstatepolicyprojectv1_10.csv\")\n\n\nSpecifying variables\n\n\nCode\nstatedata1 = subset(statedata, select = c(policypriorityscore, econdev, pldvpag, urbrenen, year, state, poptotal, popfemale, nonwhite, soc_capital_ma, evangelical_pop, newimmig, popdensity, gsp_q, gini_coef, hsdiploma, educspend, nofelons, co2emissions, ideo, pollib_median,vst_ec, vst_soc, vavgec_low, vavgsoc_low))\n\n\nSpecifying date range\n\n\nCode\nsd <- subset(statedata1, year>1934 & year<2001, na.rm = TRUE ) \n\n\nDescriptive statistics\n\n\nCode\nstr(sd)\n\n\n'data.frame':   3366 obs. of  25 variables:\n $ policypriorityscore: num  NA NA NA NA NA NA NA NA NA NA ...\n $ econdev            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ pldvpag            : int  0 0 0 0 0 0 0 0 0 0 ...\n $ urbrenen           : int  0 0 0 0 0 0 0 0 0 0 ...\n $ year               : int  1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 ...\n $ state              : chr  \"Alaska\" \"Alaska\" \"Alaska\" \"Alaska\" ...\n $ poptotal           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popfemale          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nonwhite           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ soc_capital_ma     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ evangelical_pop    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ newimmig           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ popdensity         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ gsp_q              : int  NA NA NA NA NA NA NA NA NA NA ...\n $ gini_coef          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ hsdiploma          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ educspend          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nofelons           : int  NA NA NA NA NA NA NA NA NA NA ...\n $ co2emissions       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ideo               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ pollib_median      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_ec             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vst_soc            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgec_low         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ vavgsoc_low        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nCode\nglimpse(sd)\n\n\nRows: 3,366\nColumns: 25\n$ policypriorityscore <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ econdev             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ pldvpag             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ urbrenen            <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ year                <int> 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 19…\n$ state               <chr> \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", \"Alaska\", …\n$ poptotal            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popfemale           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nonwhite            <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ soc_capital_ma      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ evangelical_pop     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ newimmig            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ popdensity          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gsp_q               <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ gini_coef           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hsdiploma           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ educspend           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ nofelons            <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ co2emissions        <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ideo                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pollib_median       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_ec              <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vst_soc             <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgec_low          <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ vavgsoc_low         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nsummary(sd)\n\n\n policypriorityscore    econdev           pldvpag          urbrenen     \n Min.   :-0.2296     Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:-0.0372     1st Qu.:0.00000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median : 0.0144     Median :0.00000   Median :1.0000   Median :1.0000  \n Mean   : 0.0093     Mean   :0.09364   Mean   :0.7703   Mean   :0.5327  \n 3rd Qu.: 0.0638     3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   : 0.1987     Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :2416        NA's   :66        NA's   :66       NA's   :66      \n      year         state              poptotal          popfemale       \n Min.   :1935   Length:3366        Min.   :  100000   Min.   :  236763  \n 1st Qu.:1951   Class :character   1st Qu.:  960954   1st Qu.:  645293  \n Median :1968   Mode  :character   Median : 2600000   Median : 1900000  \n Mean   :1968                      Mean   : 3892303   Mean   : 2692111  \n 3rd Qu.:1984                      3rd Qu.: 4700000   3rd Qu.: 3100000  \n Max.   :2000                      Max.   :34000000   Max.   :17000000  \n                                   NA's   :30         NA's   :3009      \n    nonwhite      soc_capital_ma    evangelical_pop    newimmig     \n Min.   :0.0048   Min.   :-2.9133   Min.   : 1.10   Min.   :   159  \n 1st Qu.:0.0785   1st Qu.:-0.4193   1st Qu.: 9.60   1st Qu.:  1518  \n Median :0.1360   Median : 0.2357   Median :14.10   Median :  3973  \n Mean   :0.1752   Mean   : 0.3108   Mean   :18.83   Mean   : 18447  \n 3rd Qu.:0.2586   3rd Qu.: 1.0615   3rd Qu.:26.00   3rd Qu.: 11424  \n Max.   :0.7130   Max.   : 3.0868   Max.   :74.00   Max.   :732735  \n NA's   :2016     NA's   :2550      NA's   :2066    NA's   :2703    \n   popdensity            gsp_q           gini_coef        hsdiploma    \n Min.   :   0.6496   Min.   :    993   Min.   :0.3215   Min.   : 0.00  \n 1st Qu.:  31.2611   1st Qu.:  12325   1st Qu.:0.4324   1st Qu.:73.90  \n Median :  85.3188   Median :  31568   Median :0.4667   Median :76.80  \n Mean   : 163.7982   Mean   :  74118   Mean   :0.4766   Mean   :75.94  \n 3rd Qu.: 165.7868   3rd Qu.:  83769   3rd Qu.:0.5147   3rd Qu.:80.80  \n Max.   :1082.7000   Max.   :1300000   Max.   :0.7172   Max.   :91.80  \n NA's   :2116        NA's   :1428      NA's   :48       NA's   :2054   \n   educspend          nofelons       co2emissions         ideo        \n Min.   :    0.0   Min.   :     0   Min.   :  4.00   Min.   :-0.5806  \n 1st Qu.:  816.2   1st Qu.:  4668   1st Qu.: 24.00   1st Qu.:-0.2157  \n Median : 1809.5   Median : 15733   Median : 60.00   Median :-0.1392  \n Mean   : 3421.9   Mean   : 34844   Mean   : 88.28   Mean   :-0.1364  \n 3rd Qu.: 4058.0   3rd Qu.: 41280   3rd Qu.:107.50   3rd Qu.:-0.0625  \n Max.   :35482.0   Max.   :499362   Max.   :669.00   Max.   : 0.4545  \n NA's   :2054      NA's   :2805     NA's   :1275     NA's   :2129     \n pollib_median          vst_ec          vst_soc         vavgec_low    \n Min.   :-2.32065   Min.   :-0.367   Min.   :-0.379   Min.   :-0.387  \n 1st Qu.:-0.66509   1st Qu.:-0.171   1st Qu.:-0.163   1st Qu.: 0.021  \n Median :-0.07600   Median :-0.094   Median :-0.001   Median : 0.108  \n Mean   :-0.01096   Mean   :-0.094   Mean   :-0.024   Mean   : 0.079  \n 3rd Qu.: 0.68865   3rd Qu.:-0.047   3rd Qu.: 0.106   3rd Qu.: 0.174  \n Max.   : 2.57199   Max.   : 0.147   Max.   : 0.357   Max.   : 0.290  \n NA's   :114        NA's   :3319     NA's   :3319     NA's   :3319    \n  vavgsoc_low    \n Min.   :-0.466  \n 1st Qu.:-0.200  \n Median :-0.093  \n Mean   :-0.073  \n 3rd Qu.: 0.052  \n Max.   : 0.377  \n NA's   :3319"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html",
    "href": "posts/HW2_PrahithaMovva.html",
    "title": "Homework 2 - Prahitha Movva",
    "section": "",
    "text": "Code\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(stats)\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#angiography",
    "href": "posts/HW2_PrahithaMovva.html#angiography",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Angiography",
    "text": "Angiography\n\n\nCode\nsample.mean <- 18\nsample.n <- 847\nsample.sd <- 9\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 17.49078 18.50922\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.018436"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#bypass",
    "href": "posts/HW2_PrahithaMovva.html#bypass",
    "title": "Homework 2 - Prahitha Movva",
    "section": "Bypass",
    "text": "Bypass\n\n\nCode\nsample.mean <- 19\nsample.n <- 539\nsample.sd <- 10\nsample.se <- sample.sd/sqrt(sample.n)\n\nalpha <- 0.10\ndegrees.freedom <- sample.n - 1\nt.score <- qt(p=alpha/2, df=degrees.freedom,lower.tail=F)\n\nmargin.error <- t.score * sample.se\nlower.bound <- sample.mean - margin.error\nupper.bound <- sample.mean + margin.error\nprint(c(lower.bound,upper.bound))\n\n\n[1] 18.29029 19.70971\n\n\nCode\nprint(upper.bound - lower.bound)\n\n\n[1] 1.419421\n\n\nThe 90% confidence interval for angiography is [17.49, 18.51] wait days (1.02) and for bypass is [18.29, 19.71] wait days (1.42). The confidence interval for angiography is slightly narrower (by 0.4)."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a",
    "href": "posts/HW2_PrahithaMovva.html#a",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\nHo: The true mean income of female employees is $500/week\nHa: The true mean income of female employees is not $500/week\nAssumptions:\n\nThe data is normally distributed\nHo is true\n95% CI\n\n\n\nCode\nt.numerator <- sample.mean - population.mean\nt.denominator <- sample.s/sqrt(sample.n)\nt.statistic <- t.numerator/t.denominator\n\np.value <- pt(q=abs(t.statistic), df=sample.n-1, lower.tail=F)*2\nprint(t.statistic)\n\n\n[1] -3\n\n\nCode\nprint(p.value)\n\n\n[1] 0.01707168\n\n\nThe t statistic is -3 and the p-value at 5% significance level is 0.017. Since the p-value is less than 0.05, it is evidence against Ho, i.e., the mean income of female employees differ significantly from $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b",
    "href": "posts/HW2_PrahithaMovva.html#b",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\np.value_less <- pt(q=t.statistic, df=sample.n-1, lower.tail=T)\nprint(p.value_less)\n\n\n[1] 0.008535841\n\n\nHere too, the p-value at 5% significance level is less than 0.05. So we reject Ho and can say that the mean income of female employees is significantly less than $500 per week."
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c",
    "href": "posts/HW2_PrahithaMovva.html#c",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\np.value_greater <- pt(q=t.statistic, df=sample.n-1, lower.tail=F)\nprint(p.value_greater)\n\n\n[1] 0.9914642\n\n\nHere, the p-value at 5% significance level is higher than 0.05 and we fail to reject Ho. This means, we do not have evidence that the mean income of female employees is more than $500 per week.\n\n\nCode\np.value_greater + p.value_less\n\n\n[1] 1"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#a-1",
    "href": "posts/HW2_PrahithaMovva.html#a-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\njones.t <- ((jones.mean-population.mean)/jones.se)\njones.t\n\n\n[1] 1.95\n\n\nCode\njones.p <- pt(q=abs(jones.t), df=sample.n-1, lower.tail=F)*2\njones.p\n\n\n[1] 0.05145555\n\n\nCode\nsmith.t <- ((smith.mean-population.mean)/smith.se)\nsmith.t\n\n\n[1] 1.97\n\n\nCode\nsmith.p <- pt(q=abs(smith.t), df=sample.n-1, lower.tail=F)*2\nsmith.p\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#b-1",
    "href": "posts/HW2_PrahithaMovva.html#b-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "b",
    "text": "b\nAt 5% significance level, the result for Smith is statistically significant but that of Jones is not"
  },
  {
    "objectID": "posts/HW2_PrahithaMovva.html#c-1",
    "href": "posts/HW2_PrahithaMovva.html#c-1",
    "title": "Homework 2 - Prahitha Movva",
    "section": "c",
    "text": "c\nThis example shows using P > 0.05 or P <= 0.05 to see if we can the reject the null hypothesis or not is misleading, if the actual p-value is not reported. Both the p-values are only 0.1 significance level away from 0.05 but only one is significant, so the experiment might not be meaningful without the actual p-values."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html",
    "href": "posts/Final Project - Working Draft.html",
    "title": "",
    "section": "",
    "text": "library(ggplot2)\nlibrary(markdown)\n\nError in library(markdown): there is no package called 'markdown'\n\nlibrary(rmarkdown)\nlibrary(tidyr)\nlibrary(tidyselect)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ readr   2.1.3      ✔ stringr 1.4.1 \n✔ purrr   0.3.5      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(readxl)"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#research-question",
    "href": "posts/Final Project - Working Draft.html#research-question",
    "title": "",
    "section": "Research Question",
    "text": "Research Question\nI am interested in examining the relationship between exports from China to the US, and the increase in Co2 emissions over the years. China is marked to be the highest Co2 emitting country followed by the US. However, it is clear that China pulls some of the carbon weight for the United States by manufacturing a wealth of goods. I would like to explore the connection between exports to the US and increasing carbon emissions. This could be helpful in guiding policy in international trade and climate change moving forward. I would also like to specify the industries and goods more closely related to these emissions, if possible."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#hypothesis",
    "href": "posts/Final Project - Working Draft.html#hypothesis",
    "title": "",
    "section": "Hypothesis",
    "text": "Hypothesis\nI am starting with the hypothesis that there exports from China to the US is positively related with its Co2 emissions. However, taken together, its also significant to ask whether or not export process alone accounts for the greater sum of Co2 emission increases. The manufacturing process likely plays a role here, and may be considered for further analyses. Exports may be used in an inferential manner, suggesting that increased exports indicate higher rates of manufacturing that could thereby increase Co2 emissions. As such, exports would be an indirect measure of domestic activity, the correlation of which could lead to further insights. A search of datasets with more direct measures of carbon emissions in China relating to trade and supply of goods to the US may also be considered."
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#export-data",
    "href": "posts/Final Project - Working Draft.html#export-data",
    "title": "",
    "section": "Export Data",
    "text": "Export Data\nTwo initial datasets have been pulled for the purpose of this study. The first set includes data on exports from China to the US. The set has 2 columns of interest representing the date, value (in US dollars”). The overall dataset has 3 columns, each containing 30 rows. This dataset was chosen because it covers a relatively adequate sample range from 1992-2020. This data was pulled from the United Nations COMTRADE database on comerce and trade.\n\ncomtrade_historical_CHNUSA00002 <- read.csv(\"~/Downloads/comtrade_historical_CHNUSA00002.csv\")\n\nWarning in file(file, \"rt\"): cannot open file '/home/runner/Downloads/\ncomtrade_historical_CHNUSA00002.csv': No such file or directory\n\n\nError in file(file, \"rt\"): cannot open the connection\n\nexports<-comtrade_historical_CHNUSA00002\n\nError in eval(expr, envir, enclos): object 'comtrade_historical_CHNUSA00002' not found\n\nglimpse(exports)\n\nError in glimpse(exports): object 'exports' not found\n\nsummary(exports)\n\nError in summary(exports): object 'exports' not found"
  },
  {
    "objectID": "posts/Final Project - Working Draft.html#carbon-data",
    "href": "posts/Final Project - Working Draft.html#carbon-data",
    "title": "",
    "section": "Carbon Data",
    "text": "Carbon Data\nCarbon data was pulled to show the difference in carbon emissions from china between the years 2010-2020. It is unclear, yet, if this dataset will be used for final drafts, as it exludes a number of years reviewed in the export data. As a result, the gaps in years may lead to weaker analyses. For now, this data will be considered.\n\nstatistic_id270499_global_co2_emissions_by_select_country_2010_2020<-read_excel(\"Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx\")\n\nError: `path` does not exist: 'Downloads/statistic_id270499_global-co2-emissions-by-select-country-2010-2020.xlsx'\n\n\n\ncarbon<-statistic_id270499_global_co2_emissions_by_select_country_2010_2020\n\nError in eval(expr, envir, enclos): object 'statistic_id270499_global_co2_emissions_by_select_country_2010_2020' not found\n\nsummarize(carbon)\n\nError in summarize(carbon): object 'carbon' not found\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html",
    "href": "posts/NiyatiSharma_HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\n#| label: setup\n#| warning: false\n#| message: false\n \nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section",
    "href": "posts/NiyatiSharma_HW2.html#section",
    "title": "Homework 2",
    "section": "1",
    "text": "1\nCreating the table with the given data.\n\n\nCode\nSP <- c('Bypass', 'Angiography')\nSS <- c(539, 847)\nMW <- c(19, 18)\nSD <- c(10, 9)\n\nServeyData <- data.frame(SP, SS, MW, SD)\nServeyData\n\n\n           SP  SS MW SD\n1      Bypass 539 19 10\n2 Angiography 847 18  9\n\n\nCalculate Standard error\n\n\nCode\nSE <- SD / sqrt(SS)\nSE\n\n\n[1] 0.4307305 0.3092437\n\n\ncalculate the area of the two tails\n\n\nCode\nCL <- 0.90  \n#area in each tail of the distribution for 90%\ntail_area <- (1-CL)/2\ntail_area\n\n\n[1] 0.05\n\n\ncalculate t-values by using the qt() function\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = SS-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\ncalculate the confidence interval\n\n\nCode\nCI <- c(MW - t_score * SE,\n        MW + t_score * SE)\nprint(CI)\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days and for angiography it is [17.49, 18.51] days. The confidence interval for angiography is narrower."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-1",
    "href": "posts/NiyatiSharma_HW2.html#section-1",
    "title": "Homework 2",
    "section": "2",
    "text": "2\nUsing prop.test() to calculate p and the 95% confidence interval.\n\n\nCode\nset.seed(0)\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\nThe 95% confidence interval for the point estimate is 0.5195839 - 0.5803191.The point estimate for the proportion of all adult Americans who believe that a college education is essential for success is 0.55."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-2",
    "href": "posts/NiyatiSharma_HW2.html#section-2",
    "title": "Homework 2",
    "section": "3",
    "text": "3\nCalculate the min sample size\n\n\nCode\n# calculate population SD.\nSD <- (200-30)/4\n#margin of error\nME <- (10/2)\n# calculate sample size.\nsamplesize <- ((1.96*SD)/ME)^2\nsamplesize\n\n\n[1] 277.5556\n\n\nthe size of the sample should be 278"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-3",
    "href": "posts/NiyatiSharma_HW2.html#section-3",
    "title": "Homework 2",
    "section": "4",
    "text": "4"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a",
    "href": "posts/NiyatiSharma_HW2.html#a",
    "title": "Homework 2",
    "section": "a",
    "text": "a\ncalculate t statistic since it will show us the difference in two means\nNull hypothesis mean = 500\n\n\nCode\nt_stats <- (410-500)/(90/sqrt(9))\nt_stats\n\n\n[1] -3\n\n\nCalculate P value\n\n\nCode\np_value <- 2* pt(t_stats, df=8)\np_value\n\n\n[1] 0.01707168\n\n\nThe test statistic is -3 and the p-value is 0.01707168.The p-value is substantially less than .05 is the evidence that we can reject the null hypothesis. There is strong evidence that the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b",
    "href": "posts/NiyatiSharma_HW2.html#b",
    "title": "Homework 2",
    "section": "b",
    "text": "b\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = TRUE)\nPL\n\n\n[1] 0.008535841\n\n\nSince p-value is 0.0085 is less than the alpha level of 0.05, we can reject the null hypothesis. There is evidence that the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c",
    "href": "posts/NiyatiSharma_HW2.html#c",
    "title": "Homework 2",
    "section": "c",
    "text": "c\n\n\nCode\nPL <- pt(t_stats, df = 8, lower.tail = FALSE)\nPL\n\n\n[1] 0.9914642\n\n\nSince p-value is 0.991 is more than the alpha level of 0.05, we cannot reject the null hypothesis. There is evidence that the mean income of female employees is more than $500."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-4",
    "href": "posts/NiyatiSharma_HW2.html#section-4",
    "title": "Homework 2",
    "section": "5",
    "text": "5"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#a-1",
    "href": "posts/NiyatiSharma_HW2.html#a-1",
    "title": "Homework 2",
    "section": "a",
    "text": "a\n\n\nCode\n# calculate standard deviation \nStd_Dev <- 10*sqrt(1000)\n\n# calculate t for Jones.\nt_jones <- ((519.5-500)/Std_Dev) * sqrt(1000)\nt_jones\n\n\n[1] 1.95\n\n\nCode\n# calculate p-value for Jones.\np_jones <- 2*(pt(q=t_jones, df=999, lower.tail=FALSE))\np_jones\n\n\n[1] 0.05145555\n\n\nCode\n# calculate t for Smith.\nt_smith <- ((519.7-500)/Std_Dev) * sqrt(1000)\nt_smith\n\n\n[1] 1.97\n\n\nCode\n# calculate p-value for Smith.\np_smith <- 2*(pt(q=t_smith, df=999, lower.tail=FALSE))\np_smith\n\n\n[1] 0.04911426"
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#b-1",
    "href": "posts/NiyatiSharma_HW2.html#b-1",
    "title": "Homework 2",
    "section": "b",
    "text": "b\nAt the .05 significance level, we could say that Jones would be unable to reject the null hypothesis since his exceeds .05. Smith on the other hand would barley be able to reject the null hypothesis with his equalling .049."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#c-1",
    "href": "posts/NiyatiSharma_HW2.html#c-1",
    "title": "Homework 2",
    "section": "c",
    "text": "c\nBoth of these p values were extremely close to the actual cut off point which shows including them is important. If I would have saw these p scores I would have had doubts or questions regarding the data and would have ran my own test to validate the claims. I think that is reason it would be important to include them to allow other people to see how close the study was."
  },
  {
    "objectID": "posts/NiyatiSharma_HW2.html#section-5",
    "href": "posts/NiyatiSharma_HW2.html#section-5",
    "title": "Homework 2",
    "section": "6",
    "text": "6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nHere we can see that the p value for this is .038 which means we can reject the null hypothesis that gas prices are equal to or greater than 45 cents. The mean sample that came up was also within the range of the confidence interval."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html",
    "href": "posts/FinalPt1_KarenKimble.html",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "",
    "text": "Code\n# Setup\n\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(readr)\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\n# Importing datasets\n\nNYC_2019 <- read_csv(\"_data/2018-2019_School_Demographic_Snapshot.csv\", col_types = cols(`Grade PK (Half Day & Full Day)` = col_skip(), `# Multiple Race Categories Not Represented` = col_skip(), `% Multiple Race Categories Not Represented` = col_skip()))\n\nNYC_2019$`% Poverty` <- percent(NYC_2019$`% Poverty`, accuracy=0.1)\n\nNYC_2021 <- read_csv(\"_data/2020-2021_Demographic_Snapshot_School.csv\", col_types = cols(`Grade 3K+PK (Half Day & Full Day)` = col_skip(), `# Multi-Racial` = col_skip(), `% Multi-Racial` = col_skip(), `# Native American` = col_skip(), `% Native American` = col_skip(), `# Missing Race/Ethnicity Data` = col_skip(), `% Missing Race/Ethnicity Data` = col_skip()))\n\n# In order to bind the data, I had to remove columns that were not present in the other spreadsheet: Grade PK or 3K, Native American, the different multi-racial categories, and Missing Data\n\nschool_data <- rbind(NYC_2019, NYC_2021)\n\n# Making values coded as \"above 95%\" to equal 95% and \"below 5%\" to equal 5% for the purposes of this analysis\n\nschool_data$`% Poverty` <- recode(school_data$`% Poverty`, \"Above 95%\" = \"95%\", \"Below 5%\" = \"5%\")\n\n# Re-coding variables as numeric\n\nschool_data$`% Poverty` <- sapply(school_data$`% Poverty`, function(x) gsub(\"%\", \"\", x))\n\nschool_data$`% Poverty` <- as.numeric(school_data$`% Poverty`)\n\nschool_data$`Economic Need Index` <- as.numeric(school_data$`Economic Need Index`)\n\n\nWarning: NAs introduced by coercion"
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#research-question",
    "href": "posts/FinalPt1_KarenKimble.html#research-question",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Research Question",
    "text": "Research Question\nThe research question I want to explore is whether child poverty has increased in schools that are predominantly made up of non-white students from the 2014-2015 school year to the 2020-2021 school year. I think this is extremely important to look at because of the pandemic’s impact on not only child learning but also families’ economic resources. According to the Columbia University Center on Poverty and Social Policy, “nearly a quarter of children ages 0-3 live in poverty and nearly half of the city’s young children live in lower-opportunity neighborhoods where the poverty rate is at least 20 percent” (“Poverty”). Unfortunately, research shows that poverty is disproportionately felt according to one’s race or ethnicity. In New York State, as of 2021, child poverty among children of color is almost 30%, with Black or African American children more than twice as likely to live in poverty than White, Non-Hispanic children (“New York State”, 2021). With this disproportionate level of economic need in children of color, it seems important to investigate if the poverty level within New York City schools that are predominately non-White has increased significantly compared to schools that are predominantly White. When searching the UMass Libraries databases and other sources, it was hard to find studies that used this data in this way. It is important to understand if there is increasing poverty levels within an already vulnerable group."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "href": "posts/FinalPt1_KarenKimble.html#hypothesis",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Hypothesis",
    "text": "Hypothesis\nI hypothesize that the poverty rate in NYC schools that are predominantly children of color will have increased more between the 2014-2015 and the 2020-2021 school years than the poverty rate in schools that are predominantly White. Since I have not found many previous studies on this, it is hard to know if this hypothesis was tested before. However, this data is fairly recent and also relates to the pandemic’s effects on economics, so I think it is still a significant contribution to test this hypothesis."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "href": "posts/FinalPt1_KarenKimble.html#descriptive-statistics",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nA description and summary of your data. How was your data collected by its original collectors? What are the important variables of interest for your research question? Use functions like glimpse() and summary() to present your data.\nThe data was collected by New York City and put on its Open Data source. The data covers NYC schools in the academic years 2014-2015 to 2020-2021. The important variables of interest included in the data are:\n\nAcademic year\nNumber and percentage of Asisan, Black, Hispanic, and White students\nNumber and percentage of students in poverty\nEconomic need index, which is the average of students’ “Economic Need Values”\n\nThe Economic Need Index (ENI) estimates the percentage of students facing economic hardship\n\n\nThe other variables included are: DBN (district, borough, school number), school name, total enrollment, enrollment numbers for K-12, number and percentage of female and male students, number and percentage of students with disabilities, and number and percentage of English-Language Learner (ELL) students.\n\n\nCode\nglimpse(school_data)\n\n\nRows: 18,142\nColumns: 36\n$ DBN                            <chr> \"01M015\", \"01M015\", \"01M015\", \"01M015\",…\n$ `School Name`                  <chr> \"P.S. 015 Roberto Clemente\", \"P.S. 015 …\n$ Year                           <chr> \"2014-15\", \"2015-16\", \"2016-17\", \"2017-…\n$ `Total Enrollment`             <dbl> 183, 176, 178, 190, 174, 270, 270, 271,…\n$ `Grade K`                      <dbl> 27, 32, 28, 28, 20, 44, 47, 37, 34, 30,…\n$ `Grade 1`                      <dbl> 47, 33, 33, 32, 33, 40, 43, 46, 38, 39,…\n$ `Grade 2`                      <dbl> 31, 39, 27, 33, 30, 39, 41, 47, 42, 43,…\n$ `Grade 3`                      <dbl> 19, 23, 31, 23, 30, 35, 43, 40, 46, 41,…\n$ `Grade 4`                      <dbl> 17, 17, 24, 31, 20, 40, 35, 43, 42, 44,…\n$ `Grade 5`                      <dbl> 24, 18, 18, 26, 28, 42, 40, 34, 42, 42,…\n$ `Grade 6`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 7`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 8`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 9`                      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 10`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 11`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `Grade 12`                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `# Female`                     <dbl> 84, 83, 83, 99, 85, 132, 125, 127, 114,…\n$ `% Female`                     <dbl> 0.459, 0.472, 0.466, 0.521, 0.489, 0.48…\n$ `# Male`                       <dbl> 99, 93, 95, 91, 89, 138, 145, 144, 143,…\n$ `% Male`                       <dbl> 0.541, 0.528, 0.534, 0.479, 0.511, 0.51…\n$ `# Asian`                      <dbl> 8, 9, 14, 20, 24, 30, 27, 24, 23, 14, 2…\n$ `% Asian`                      <dbl> 0.044, 0.051, 0.079, 0.105, 0.138, 0.11…\n$ `# Black`                      <dbl> 65, 57, 51, 52, 48, 47, 55, 51, 49, 52,…\n$ `% Black`                      <dbl> 0.355, 0.324, 0.287, 0.274, 0.276, 0.17…\n$ `# Hispanic`                   <dbl> 107, 105, 105, 110, 95, 158, 169, 180, …\n$ `% Hispanic`                   <dbl> 0.585, 0.597, 0.590, 0.579, 0.546, 0.58…\n$ `# White`                      <dbl> 2, 2, 4, 6, 6, 27, 16, 15, 16, 18, 25, …\n$ `% White`                      <dbl> 0.011, 0.011, 0.022, 0.032, 0.034, 0.10…\n$ `# Students with Disabilities` <dbl> 64, 60, 51, 49, 38, 82, 82, 88, 90, 92,…\n$ `% Students with Disabilities` <dbl> 0.350, 0.341, 0.287, 0.258, 0.218, 0.30…\n$ `# English Language Learners`  <dbl> 17, 16, 12, 8, 8, 18, 13, 9, 8, 8, 120,…\n$ `% English Language Learners`  <dbl> 0.093, 0.091, 0.067, 0.042, 0.046, 0.06…\n$ `# Poverty`                    <chr> \"169\", \"149\", \"152\", \"161\", \"145\", \"200…\n$ `% Poverty`                    <dbl> 92.3, 84.7, 85.4, 84.7, 83.3, 74.1, 80.…\n$ `Economic Need Index`          <dbl> 0.930, 0.889, 0.882, 0.890, 0.880, 0.60…\n\n\n\n\nCode\nsummary(school_data)\n\n\n     DBN            School Name            Year           Total Enrollment\n Length:18142       Length:18142       Length:18142       Min.   :   7.0  \n Class :character   Class :character   Class :character   1st Qu.: 323.0  \n Mode  :character   Mode  :character   Mode  :character   Median : 477.0  \n                                                          Mean   : 592.3  \n                                                          3rd Qu.: 695.0  \n                                                          Max.   :6040.0  \n                                                                          \n    Grade K          Grade 1          Grade 2          Grade 3      \n Min.   :  0.00   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 32.00   Median : 33.00   Median : 32.00   Median : 28.00  \n Mean   : 44.25   Mean   : 45.79   Mean   : 45.73   Mean   : 45.33  \n 3rd Qu.: 78.00   3rd Qu.: 81.00   3rd Qu.: 82.00   3rd Qu.: 81.00  \n Max.   :393.00   Max.   :383.00   Max.   :349.00   Max.   :369.00  \n                                                                    \n    Grade 4         Grade 5          Grade 6          Grade 7      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  0.0   1st Qu.:  0.00   1st Qu.:  0.00   1st Qu.:  0.00  \n Median : 22.0   Median : 19.00   Median :  0.00   Median :  0.00  \n Mean   : 44.8   Mean   : 44.18   Mean   : 43.15   Mean   : 42.37  \n 3rd Qu.: 80.0   3rd Qu.: 80.00   3rd Qu.: 64.00   3rd Qu.: 62.00  \n Max.   :376.0   Max.   :351.00   Max.   :771.00   Max.   :796.00  \n                                                                   \n    Grade 8          Grade 9           Grade 10         Grade 11      \n Min.   :  0.00   Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n 1st Qu.:  0.00   1st Qu.:   0.00   1st Qu.:   0.0   1st Qu.:   0.00  \n Median :  0.00   Median :   0.00   Median :   0.0   Median :   0.00  \n Mean   : 41.88   Mean   :  49.34   Mean   :  48.7   Mean   :  39.85  \n 3rd Qu.: 60.00   3rd Qu.:  68.00   3rd Qu.:  69.0   3rd Qu.:  54.00  \n Max.   :784.00   Max.   :1555.00   Max.   :3832.0   Max.   :1529.00  \n                                                                      \n    Grade 12          # Female         % Female          # Male      \n Min.   :   0.00   Min.   :   0.0   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:   0.00   1st Qu.: 146.0   1st Qu.:0.4620   1st Qu.: 163.0  \n Median :   0.00   Median : 232.0   Median :0.4880   Median : 248.0  \n Mean   :  39.58   Mean   : 287.4   Mean   :0.4827   Mean   : 304.9  \n 3rd Qu.:  53.00   3rd Qu.: 347.0   3rd Qu.:0.5130   3rd Qu.: 364.0  \n Max.   :1566.00   Max.   :2405.0   Max.   :1.0000   Max.   :3635.0  \n                                                                     \n     % Male          # Asian           % Asian          # Black      \n Min.   :0.0000   Min.   :   0.00   Min.   :0.0000   Min.   :   0.0  \n 1st Qu.:0.4870   1st Qu.:   5.00   1st Qu.:0.0130   1st Qu.:  42.0  \n Median :0.5120   Median :  17.00   Median :0.0400   Median : 105.0  \n Mean   :0.5173   Mean   :  95.38   Mean   :0.1136   Mean   : 154.1  \n 3rd Qu.:0.5380   3rd Qu.:  79.00   3rd Qu.:0.1400   3rd Qu.: 198.0  \n Max.   :1.0000   Max.   :3671.00   Max.   :0.9470   Max.   :1493.0  \n                                                                     \n    % Black        # Hispanic     % Hispanic        # White       \n Min.   :0.000   Min.   :   1   Min.   :0.0060   Min.   :   0.00  \n 1st Qu.:0.083   1st Qu.:  89   1st Qu.:0.1980   1st Qu.:   6.00  \n Median :0.251   Median : 180   Median :0.3990   Median :  15.00  \n Mean   :0.316   Mean   : 241   Mean   :0.4251   Mean   :  87.24  \n 3rd Qu.:0.502   3rd Qu.: 313   3rd Qu.:0.6323   3rd Qu.:  78.00  \n Max.   :0.987   Max.   :2056   Max.   :1.0000   Max.   :3190.00  \n                                                                  \n    % White       # Students with Disabilities % Students with Disabilities\n Min.   :0.0000   Min.   :  0.0                Min.   :0.0000              \n 1st Qu.:0.0140   1st Qu.: 66.0                1st Qu.:0.1570              \n Median :0.0330   Median : 98.0                Median :0.2030              \n Mean   :0.1205   Mean   :121.6                Mean   :0.2295              \n 3rd Qu.:0.1440   3rd Qu.:146.0                3rd Qu.:0.2540              \n Max.   :0.9450   Max.   :925.0                Max.   :1.0000              \n                                                                           \n # English Language Learners % English Language Learners  # Poverty        \n Min.   :   0.0              Min.   :0.0000              Length:18142      \n 1st Qu.:  18.0              1st Qu.:0.0430              Class :character  \n Median :  43.0              Median :0.0950              Mode  :character  \n Mean   :  81.1              Mean   :0.1363                                \n 3rd Qu.: 100.0              3rd Qu.:0.1800                                \n Max.   :1219.0              Max.   :1.0000                                \n                                                                           \n   % Poverty      Economic Need Index\n Min.   :  2.90   Min.   :0.030      \n 1st Qu.: 69.30   1st Qu.:0.579      \n Median : 81.40   Median :0.743      \n Mean   : 75.89   Mean   :0.691      \n 3rd Qu.: 89.90   3rd Qu.:0.846      \n Max.   :100.00   Max.   :0.998      \n                  NA's   :9169       \n\n\nCode\n# Note: the summary data for the enrollment numbers split by grade is somewhat off (especially minimums) because there is no variable listed for type of school (i.e., middle versus high school). So, for example, an elementary school would have an enrollment total of 0 for grade 12, which would show up as the minimum.\n\n\nAs we can see from this summary, the median percent of poverty in NYC schools (81.4%) is higher than the mean percent (75.89%), indicating that there may be low outliers with very low percentages of poverty. The same holds true for the Economic Need Index, with the mean (0.691) lower than the median (0.743). It is troubling, however, that both the mean and median percentages of poverty in NYC schools overall is more than three-fourths of the population."
  },
  {
    "objectID": "posts/FinalPt1_KarenKimble.html#references",
    "href": "posts/FinalPt1_KarenKimble.html#references",
    "title": "DACSS 603 Final Project Pt 1",
    "section": "References",
    "text": "References\nNew York State Child Poverty Facts. Schuyler Center for Analysis and Advocacy. (2021, February 18). Retrieved from https://scaany.org/wp-content/uploads/2021/02/NYS-Child-Poverty-Facts_Feb2021.pdf\nPoverty in New York City. Columbia University Center on Poverty and Social Policy. (n.d.). Retrieved from https://www.povertycenter.columbia.edu/poverty-in-new-york-city#:~:text=Children%20and%20Families%20in%20New%20York%20City&text=Through%20surveys%2C%20we%20find%20that,is%20at%20least%2020%20percent."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html",
    "href": "posts/KalimahMuhammad_hw1.html",
    "title": "Resubmission: Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot2)\nlungcap<- read_excel(\"_data/LungCapData.xls\")\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "href": "posts/KalimahMuhammad_hw1.html#lungcapdata",
    "title": "Resubmission: Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n\n\n1b. Compare the probability distribution of the LungCap with respect to Males and Females?\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Gender `mean(LungCap)`\n  <chr>            <dbl>\n1 female            7.41\n2 male              8.31\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               7.77\n2 yes              8.65\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\nCode\n#Lung capacity for those age 13 and under\nlungcap %>%\nfilter(Age <= 13)%>%\ngroup_by(Smoke)%>%\nsummarise(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\nCode\n#Lung capacity for those between the age of 14 to 15\nlungcap%>%\nfilter(Age== 14 | Age ==15)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               9.14\n2 yes              8.39\n\n\nCode\n#Lung capacity for those between the age of 16 to 17\nlungcap%>%\nfilter(Age==16 |Age==16)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no              10.1 \n2 yes              8.90\n\n\nCode\n#Lung capacity for those 18 and older\nlungcap%>%\nfilter(Age>=18)%>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke `mean(LungCap)`\n  <chr>           <dbl>\n1 no               11.1\n2 yes              10.5\n\n\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nWith the exception of those age 13 years old and under, all non-smokers had a greater lung capacity than smokers. For those over the age of 18, the difference of the average in lung capacity for non-smokers to smokers was 0.55. For 16-17 year olds, the difference was the greatest at 1.16. The difference for 14-15 year olds was 0.74 and for those 13 years old and under, the difference was -0.843.\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\nHere the average lung capacity for non-smokers is higher than for smokers. This differs from the results of question 1c. Overall, we see lung capacity increase with age irrespective of smoking so this may contribute to the change in results.\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\n[1] 0.8196749\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "href": "posts/KalimahMuhammad_hw1.html#inmate-data",
    "title": "Resubmission: Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\npriors<- c(0, 1, 2, 3, 4)\nfrequency<- c(128, 434, 160, 64, 24)\nprison <-data.frame(priors,frequency)\nView(prison)\n\n\nWarning in View(prison): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20%\n2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69%\n2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89%\n2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11%\n2e. What is the expected value for the number of prior convictions?\n\n\nCode\n((128*0)+(434*1)+(160*2)+(64*3)+(24*4))/sum(frequency)\n\n\n[1] 1.28642\n\n\n2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison$priors)*((810-1)/810)#calculate population variance\n\n\n[1] 2.496914\n\n\nCode\nsd(prison$priors)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/KarenDetter_FinalPt1.html",
    "href": "posts/KarenDetter_FinalPt1.html",
    "title": "Final Project Proposal",
    "section": "",
    "text": "Background / Research Question\nWhat predicts support for government regulation of ‘Big Tech’?\nIn 2001, Google piloted a program to boost profits, which were sinking as the “dot-com bubble” burst, by collecting data generated from users’ search queries and using it to sell precisely targeted advertising. The company’s ad revenues grew so quickly that they expanded their data collection tools with tracking “cookies” and predictive algorithms. Other technology firms took notice of Google’s soaring profits, and the sale of passively-collected data from people’s online activities soon became the predominant business model of the internet economy (Zuboff, 2015).\nAs the data-collection practices of ‘Big Tech’ firms, including Google, Amazon, Facebook (Meta), Apple, and Microsoft, have gradually been exposed, the public is now aware that the ‘free’ platforms that have become essential to daily life are actually harvesting personal information as payment. Despite consumers being essentially extorted into accepting this arrangement, regulatory intervention of ‘surveillance capitalism’ has remained limited.\nOver the two decades since passive data collection began commercializing the internet, survey research has shown the American public’s increasing concern about the dominance Big Tech has been allowed to exert. A 2019 study conducted by Pew Research Center found that 81% of Democrats and 70% of Republicans think there should be more government regulation of corporate data-use practices (Pew Research Center, 2019). It is very unusual to find majorities of both Republicans and Democrats agreeing on any policy position, since party affiliation is known to be a main predictor of any political stance, especially in the current polarized climate. The natural question that arises, then, is what other factors predict support for increased regulation of data-collection practices?\n\n\nHypothesis\nAlthough few studies have directly examined the mechanisms behind public support for regulation of passive data collection, a good amount of research has been done on factors influencing individual adoption of privacy protection measures (Barth et al., 2019; Boerman et al., 2021; Turow et al., 2015). It seems a reasonable extrapolation that these factors would similarly influence support for additional data privacy regulation, leading to these hypotheses:\n\nA higher level of awareness of data collection issues predicts support for increased ‘Big Tech’ regulation.\nGreater understanding of how companies use passively collected data predicts support for increased regulation.\nThe feeling of having no personal control over online tracking ‘digital resignation’ predicts support for increased regulation.\nCertain demographic traits (age group, education level, and political ideology) have some kind of effect on attitudes toward ‘Big Tech’ regulation.\n\nSince there are currently dozens of data privacy bills pending in Congress, pinpointing the forces driving support for this type of legislation can help with both shaping the regulatory framework needed and appealing for broader support from voters.\n\n\nDescriptive Statistics\nPew Research Center’s American Trends Panel (Wave 49) data set can provide insight into which of these factors are predictive of support for greater regulation of technology company data practices. In June 2019, an online survey covering a wide variety of topics was conducted and 4,272 separate observations for 144 variables were collected from adults age 18 and over. The margin of error (at the 95% confidence level) is given as +/- 1.87 percentage points.\nThe data set was compiled in SPSS and all pertinent variables are categorical.\n\n\nCode\n#read in data from SPSS file\nwav49 <- read_sav(\"_data/ATPW49.sav\")\nwav49\n\n\n# A tibble: 4,272 × 144\n     QKEY DEVICE_TYPE_…¹ LANG_…² FORM_…³ SOCME…⁴ SOCME…⁵ SOCME…⁶ SOCME…⁷ SNSUS…⁸\n    <dbl> <dbl+lbl>      <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l>\n 1 100260 2 [Tablet]     9 [Eng… 2 [For… 2 [No,… 2 [No,… 2 [No,… 2 [No,… 0 [Doe…\n 2 100588 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 2 [No,… 1 [Soc…\n 3 100637 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 4 101224 1 [Mobile pho… 9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 5 101322 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 6 101437 3 [Desktop]    9 [Eng… 2 [For… 1 [Yes… 2 [No,… 2 [No,… 2 [No,… 1 [Soc…\n 7 101472 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 2 [No,… 1 [Yes… 2 [No,… 1 [Soc…\n 8 101493 3 [Desktop]    9 [Eng… 1 [For… 1 [Yes… 2 [No,… 2 [No,… 1 [Yes… 1 [Soc…\n 9 102198 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 2 [No,… 1 [Yes… 1 [Soc…\n10 103094 1 [Mobile pho… 9 [Eng… 1 [For… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Yes… 1 [Soc…\n# … with 4,262 more rows, 135 more variables: ELECTFTGSNSINT_W49 <dbl+lbl>,\n#   TALKDISASNSINT_W49 <dbl+lbl>, TALKCMNSNSINT_W49 <dbl+lbl>,\n#   SECUR1_W49 <dbl+lbl>, PRIVACYNEWS1_W49 <dbl+lbl>,\n#   HOMEASSIST1_W49 <dbl+lbl>, HOMEASSIST2_W49 <dbl+lbl>,\n#   HOMEASSIST3_W49 <dbl+lbl>, HOMEASSIST4_W49 <dbl+lbl>,\n#   HOMEASSIST5a_W49 <dbl+lbl>, HOMEASSIST5b_W49 <dbl+lbl>,\n#   HOMEIOT_W49 <dbl+lbl>, FITTRACK_W49 <dbl+lbl>, LOYALTY_W49 <dbl+lbl>, …\n\n\nSince there are so many variables in the data set, selecting the variables of interest into a new data frame will make it easier to manage:\n\n\nCode\nsel_vars <- c('PRIVACYNEWS1_W49', 'TRACKCO1a_W49', 'CONTROLCO_W49', 'UNDERSTANDCO_W49', 'ANONYMOUS1CO_W49', 'PP4_W49', 'PRIVACYREG_W49', 'GOVREGV1_W49', 'PROFILE4_W49', 'F_AGECAT', 'F_EDUCCAT', 'F_PARTYSUM_FINAL', 'F_IDEO')\nwav49_selected <- wav49[sel_vars]\nwav49_selected\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS1_…¹ TRACKC…² CONTRO…³ UNDERS…⁴ ANONYM…⁵ PP4_W49  PRIVA…⁶ GOVREG…⁷\n   <dbl+lbl>       <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+lb> <dbl+l> <dbl+lb>\n 1 4 [Not at all … NA       NA       NA       NA       NA       3 [Ver… NA      \n 2 3 [Not too clo…  3 [Som…  2 [Som…  3 [Ver…  1 [Yes…  3 [Ver… 3 [Ver…  1 [Mor…\n 3 3 [Not too clo…  3 [Som…  3 [Ver…  3 [Ver…  1 [Yes…  2 [Som… 3 [Ver…  1 [Mor…\n 4 4 [Not at all … NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 5 4 [Not at all …  1 [All…  4 [No …  4 [Not…  2 [No,… NA       4 [Not…  1 [Mor…\n 6 2 [Somewhat cl… NA       NA       NA       NA        3 [Ver… 3 [Ver… NA      \n 7 2 [Somewhat cl…  2 [Mos…  3 [Ver…  3 [Ver…  2 [No,…  2 [Som… 2 [Som…  3 [Abo…\n 8 1 [Very closel…  1 [All…  4 [No …  4 [Not…  2 [No,… NA       3 [Ver…  3 [Abo…\n 9 3 [Not too clo…  1 [All…  3 [Ver…  2 [Som…  2 [No,…  3 [Ver… 3 [Ver…  1 [Mor…\n10 3 [Not too clo…  3 [Som…  2 [Som…  1 [A g…  1 [Yes…  2 [Som… 2 [Som…  2 [Les…\n# … with 4,262 more rows, 5 more variables: PROFILE4_W49 <dbl+lbl>,\n#   F_AGECAT <dbl+lbl>, F_EDUCCAT <dbl+lbl>, F_PARTYSUM_FINAL <dbl+lbl>,\n#   F_IDEO <dbl+lbl>, and abbreviated variable names ¹​PRIVACYNEWS1_W49,\n#   ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49, ⁵​ANONYMOUS1CO_W49,\n#   ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49\n\n\nThe variable labels contain the survey questions asked:\n\n\nCode\n#summary of $variable names and their [labels]\nvar_label(wav49_selected)\n\n\nError in var_label(wav49_selected): could not find function \"var_label\"\n\n\nBecause the data set is made up of categorical variables, transformation is required before computing any statistics:\n\n\nCode\n#convert all variables to factors\nwav49_factored <- wav49_selected %>%\n  mutate_all(as_factor)\n#convert user-defined missing values to regular missing values\nzap_missing(wav49_factored)\n\n\n# A tibble: 4,272 × 13\n   PRIVACYNEWS…¹ TRACK…² CONTR…³ UNDER…⁴ ANONY…⁵ PP4_W49 PRIVA…⁶ GOVRE…⁷ PROFI…⁸\n   <fct>         <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>   <fct>  \n 1 Not at all c… <NA>    <NA>    <NA>    <NA>    <NA>    Very l… <NA>    <NA>   \n 2 Not too clos… Some o… Some c… Very l… Yes, i… Very l… Very l… More r… <NA>   \n 3 Not too clos… Some o… Very l… Very l… Yes, i… Some    Very l… More r… Somewh…\n 4 Not at all c… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    <NA>   \n 5 Not at all c… All or… No con… Nothing No, it… <NA>    Not at… More r… Not to…\n 6 Somewhat clo… <NA>    <NA>    <NA>    <NA>    Very l… Very l… <NA>    Not to…\n 7 Somewhat clo… Most o… Very l… Very l… No, it… Some    Some    About … Somewh…\n 8 Very closely  All or… No con… Nothing No, it… <NA>    Very l… About … Somewh…\n 9 Not too clos… All or… Very l… Some    No, it… Very l… Very l… More r… Somewh…\n10 Not too clos… Some o… Some c… A grea… Yes, i… Some    Some    Less r… Somewh…\n# … with 4,262 more rows, 4 more variables: F_AGECAT <fct>, F_EDUCCAT <fct>,\n#   F_PARTYSUM_FINAL <fct>, F_IDEO <fct>, and abbreviated variable names\n#   ¹​PRIVACYNEWS1_W49, ²​TRACKCO1a_W49, ³​CONTROLCO_W49, ⁴​UNDERSTANDCO_W49,\n#   ⁵​ANONYMOUS1CO_W49, ⁶​PRIVACYREG_W49, ⁷​GOVREGV1_W49, ⁸​PROFILE4_W49\n\n\nAfter the variables are converted to meaningful factors, a summary of response frequencies can be generated:\n\n\nCode\nsummary(wav49_factored)\n\n\n           PRIVACYNEWS1_W49                 TRACKCO1a_W49 \n Very closely      : 461    All or almost all of it: 881  \n Somewhat closely  :2046    Most of it             : 703  \n Not too closely   :1397    Some of it             : 381  \n Not at all closely: 359    Very little of it      :  88  \n Refused           :   9    None of it             :  76  \n                            Refused                :  11  \n                            NA's                   :2132  \n                 CONTROLCO_W49      UNDERSTANDCO_W49\n A great deal of control:  68   A great deal: 132   \n Some control           : 313   Some        : 716   \n Very little control    :1134   Very little :1040   \n No control             : 621   Nothing     : 242   \n Refused                :   4   Refused     :  10   \n NA's                   :2132   NA's        :2132   \n                                                    \n               ANONYMOUS1CO_W49         PP4_W49          PRIVACYREG_W49\n Yes, it is possible   : 772    A great deal: 328   A great deal: 136  \n No, it is not possible:1357    Some        :1405   Some        :1380  \n Refused               :  11    Very little : 751   Very little :2153  \n NA's                  :2132    Not at all  :  82   Not at all  : 593  \n                                Refused     :   5   Refused     :  10  \n                                NA's        :1701                      \n                                                                       \n                GOVREGV1_W49        PROFILE4_W49    F_AGECAT   \n More regulation      :1631   A great deal: 384   18-29 : 671  \n Less regulation      : 145   Somewhat    :1410   30-49 :1314  \n About the same amount: 331   Not too much: 900   50-64 :1308  \n Refused              :  33   Not at all  : 113   65+   : 977  \n NA's                 :2132   Refused     :   9   DK/REF:   2  \n                              NA's        :1456                \n                                                               \n                 F_EDUCCAT              F_PARTYSUM_FINAL\n College graduate+    :1600   Rep/Lean Rep      :1823   \n Some College         :1182   Dem/Lean Dem      :2296   \n H.S. graduate or less:1483   DK/Refused/No lean: 153   \n Don't know/Refused   :   7                             \n                                                        \n                                                        \n                                                        \n               F_IDEO    \n Very conservative: 353  \n Conservative     : 977  \n Moderate         :1615  \n Liberal          : 828  \n Very liberal     : 386  \n Refused          : 113  \n                         \n\n\n*High NA value indicates that the question was not presented to all respondents\nThe data set is now primed for examining correlations and testing hypotheses.\n\n\nReferences\nBarth, S., de Jong, M. D. T., Junger, M., Hartel, P. H. & Roppelt, J. C. (2019). Putting the privacy paradox to the test: Online privacy and security behaviors among users with technical knowledge, privacy awareness, and financial resources. Telematics and Informatics, 41, 55–69. doi:10.1016/j.tele.2019.03.003\nBoerman, S. C., Kruikemeier, S., & Zuiderveen Borgesius, F. J. (2021). Exploring Motivations for Online Privacy Protection Behavior: Insights From Panel Data. Communication Research, 48(7), 953–977. https://doi.org/10.1177/0093650218800915\nPew Research Center. (2019). Americans and privacy: Concerned, confused and feeling lack of control over their personal information. https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-concerned-confused-and- feeling-lack-of-control-over-their-personal-information/\nPew Research Center. (2020). Wave 49 American trends panel [Data set]. https://www.pewresearch.org/internet/dataset/american-trends-panel-wave-49/\nTurow, J., Hennessy, M. & Draper, N. (2015). The tradeoff fallacy – How marketers are misrepresenting American consumers and opening them up to exploitation. Annenberg School for Communication.\nZuboff, S. (2015). Big other: Surveillance capitalism and the prospects of an information civilization. Journal of Information Technology, 30(1), 75–89. doi:10.1057/jit.2015.5"
  },
  {
    "objectID": "posts/HW1_Yakub Rabiutheen.html",
    "href": "posts/HW1_Yakub Rabiutheen.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap,freq = FALSE)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\nComparison of the Genders for both Men and Women using a Boxplot.\n\n\nCode\nboxplot(df$LungCap ~ df$Gender)\n\n\n\n\n\n\n\n\nHere is the capacity of Smokers vs Non-Smokers\n\n\nCode\nboxplot(df$LungCap~df$Smoke,\n        ylab = \"Capacity\", \n        main = \"Lung Capacity of Smokers Vs Non-Smokers\",\n        las = 1)\n\n\n\n\n\n\n\n\nLet’s break it down even further, this is the Lung Capacity by Age Group\n\n\nCode\ndf$Agegroups<-cut(df$Age,breaks=c(-Inf, 13, 15, 17, 20), labels=c(\"0-13 years\", \"14-15 years\", \"16-17 years\", \"18+ years\"))\n\n\nBelow is the overall Lung Capacity of Age Groups without including Smokers.\n\n\nCode\nlibrary(ggplot2)\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Gender)) +\n          geom_bar(stat = \"identity\") +\n          coord_flip() +\n          theme_classic()\n\n\n\n\n\n#e\nHere is a comparision of AgeGroup Lung Capacity in comparison with Smoker vs Non-Smoker.\n\n\nCode\nggplot(df, aes(x = LungCap, y = Agegroups, fill = Smoke)) +\n    geom_bar(stat = \"identity\") +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\n\nBased on the comparison of lung capacities between Smoker and Non-Smoker the results are pretty similar.\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nQuestion 2\n\n\nCode\nX <- c(0:4)\nFrequency <- c(128, 434, 160, 64, 24)\ndf <- data.frame(X, Frequency)\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nAs shown below, the most common Prior Convictions is 1.\n\n\nCode\ndf\n\n\n  X Frequency\n1 0       128\n2 1       434\n3 2       160\n4 3        64\n5 4        24\n\n\nDividing by the total among 810 we can determine the probability for each. 810 is the Sum of the Frequency which I checked manually.\n\n\nCode\ndf2 <- mutate(df, Probability = Frequency/sum(Frequency))\n\n\nError in mutate(df, Probability = Frequency/sum(Frequency)): could not find function \"mutate\"\n\n\nCode\ndf2\n\n\nError in eval(expr, envir, enclos): object 'df2' not found\n\n\n\nFilter for Probability of 2 Convictions\n\n\n\nCode\nb2 <- df2 %>% \n  filter(X < 2)\n\n\nError in df2 %>% filter(X < 2): could not find function \"%>%\"\n\n\nCode\nsum(b2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'b2' not found\n\n\n\nFilter for Probability of Less than 2 Convictions\n\n\n\nCode\nc2 <- df2 %>% \n  filter(X <= 2)\n\n\nError in df2 %>% filter(X <= 2): could not find function \"%>%\"\n\n\nCode\nsum(c2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'c2' not found\n\n\n\n\n\nFilter for Probability of greater than 2 convictions.\n\n\nCode\nd2 <- df2 %>% \n  filter(X > 2)\n\n\nError in df2 %>% filter(X > 2): could not find function \"%>%\"\n\n\nCode\nsum(d2$Probability)\n\n\nError in eval(expr, envir, enclos): object 'd2' not found\n\n\nWhat is the expected value of the number of prior convictions?\n\n\nCode\ne <- weighted.mean(df2$X, df2$Probability)\n\n\nError in weighted.mean(df2$X, df2$Probability): object 'df2' not found\n\n\nCode\ne\n\n\nError in eval(expr, envir, enclos): object 'e' not found\n\n\n\n\n\nVariance and Standard Deviation for Question.\n\n\nCode\nvar(df$X)\n\n\n[1] 2.5\n\n\n\n\nCode\nsd(df$X)\n\n\n[1] 1.581139"
  },
  {
    "objectID": "posts/Final_SteveONeill.html#introduction",
    "href": "posts/Final_SteveONeill.html#introduction",
    "title": "Final Part 1",
    "section": "Introduction",
    "text": "Introduction\nBy analyzing public Paycheck Protection Program data, I have the ability to examine correlations between PPP loan size, forgiveness status, business status, geographical location, and - most optimistically - political affiliation. In the United States, the PPP was introduced as an emergency economic measure in 2020 during the Covid-19 pandemic. As a massive direct-loan program, some have criticized it for lax oversight.\nPPP fraud is an important issue, but finding lawbreakers is probably better left to the Feds. Instead, I think there is room in the academic body of work to study political affiliation for loan recipients.\n\nExisting Work\nThere have already been many studies about the PPP focusing on overall economic impact and the emergence of non-traditional, non-bank lenders.\nSeparately, two interesting studies have been done about racial disparities re: access to PPP loans, with one finding that Black-owned businesses in Florida were 25% less likely to receive PPP funds (Chernenko & Scharfstein, 2022), and another finding even higher of a disparity (50%) but taking note of mitigating factors (e.g. access to “fintech” lending instead of traditional banks) (Atkins, Cook & Seamans)\nI am not so much interested in the efficacy of the PPP or the racial make-up of PPP borrowers. But I am interested in their political affiliation, i.e. if they registered are Democrat or Republican. So, similarly to Chernenko & Scharfstein, I intend to use public-access data to cross-reference federal PPP data with state-level voter registration and corporation search data.\nA relevant paper I found after I started looking at this data - “Buying the Vote? The Economics of Electoral Politics and Small-Business Loans” - does look at data from SBA and PPP loans and pit them against the hypothesis ====that “electoral considerations may have tilted the allocation of PPP funds toward firms in areas or industries that could have a significant impact on the results of the 2020 election”.(Duchin & Hackney, 2021)\nThat particular study measured political ad spending and FEC filings as indicators of electoral considerations and used the Partisan Voting Index (PVI) from the Cook Political National Report to find which states were “battleground” or solid-R states. PPP lending outcomes were compared with electoral considerations to find if the SBA (under Trump) preferred to give PPP loans to swing voters or members of the party “base” in anticipation of an upcoming general election.\nRather than aggregate-level state data, my analysis will compare individuals’ personal voter affiliation - if they are registered, and to which party - with their loan amount, forgiveness status, and other metrics."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#research-question",
    "href": "posts/Final_SteveONeill.html#research-question",
    "title": "Final Part 1",
    "section": "Research Question",
    "text": "Research Question\nI am still developing my research question, but a simple one could be:\nHow does political affiliation affect PPP loan forgiveness status?\nThe data for PPP loans is current available on ProPublica, but not in a queryable way.\nFortunately, the underlying data has been made public at https://data.sba.gov/dataset/ppp-foia.\nMy hypotheses could be that:\nHypothesis 1 (H1): PPP loan forgiveness was given to registered Republicans more often than registered Democrats\nNull Hypothesis (H0): There is no difference in loan forgiveness given to registered Democrats vs registered Republicans.\nIn some ways this is similar to Duchin & Hackney, but this benefits from a narrower look at who actually received PPP stimulus rather than eventual outcomes on the state level.\nClearly a few effects that would need to be controlled for. For example, Republicans could be more likely to be in any kind of business in the first place, confounding my results.\nIdeally, I would have focused on my home state of Massachusetts. However, although voter registration data is personally available in MA, you can’t download it all in one .csv, and scraping the Commonwealth’s website is not allowed. The National Conference of State Legislatures keeps track of which states have full voter registration data for download. For now, I will use Ohio as an example (although later on I will explain why I could have chosen better):\n\nPPP Data Dictionary\nTo help understand which fields mean what, the PPP data comes with a “Data Dictionary” with an explanation of columns [scroll right to see explanation]:\n\n\nCode\nppp_data_dictionary <- read_excel(\"_data/ppp-data-dictionary.xlsx\")\nppp_data_dictionary\n\n\n\n\n  \n\n\n\n\n\nPPP Data (Ohio)\nImporting the data of the actual PPP loans is pretty easy. This .csv contains just the PPP loans below $150k - others are contained in a smaller spreadsheet that covers all 50 states, and will be part of my final project.\n\n\nCode\n#ppp_all <- read_csv(\"_data/public_up_to_150k_9_220930.csv\")\n#ppp_all\n\n\nIt still seems to capture all of Ohio, because Ohio doesn’t come last alphabetically. But to manage it better I need to sample it to 200k rows. The code below is commented out because this is a one-time process. Henceforth, I will be working with the .csv I generated in this step.\n\n\nCode\n#ppp_ohio <- ppp_all %>% filter(BorrowerState == \"OH\")\n#ppp_ohio_sampled_10k <- ppp_ohio %>% sample_n(10000)\n#write_csv(ppp_ohio_sampled_10k, \"_data/ppp_ohio_sampled_10k.csv\")\n\nppp_ohio <- read_csv(\"_data/ppp_ohio_sampled_10k.csv\")\n\n\nRows: 10000 Columns: 53\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (35): DateApproved, SBAOfficeCode, ProcessingMethod, BorrowerName, Borro...\ndbl (18): LoanNumber, Term, SBAGuarantyPercentage, InitialApprovalAmount, Cu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nLLCs, Corporations\nVisually, you may notice that businesses and LLCs make up a majority of the loan recipients.\nWhen I was preparing to do this project for Massachusetts - before I knew about the limitation of voter registration data in that state - I contacted the Secretary of the Commonwealth and they sent me a document with the entire “Corporation Search” data in tabular format, with the name of the founder, every board member, business type, year established, etc. I anticipated I would be able to match those individuals with the voter registration data, but that data was unfortunately available.\nThe Corporation Search data would be essential to the successful completion of the project.\nI have not inquired yet, but assume Ohio will provide the same Corporation Search data. And if they do not, I will just pick another state that 1.) has public voter registration information, and 2.) can supply the Corporation Search data in .csv or .xlsx (as Massachusetts did).\n\n\nDeeper Looks\n\n\nCode\nglimpse(ppp_ohio)\n\n\nRows: 10,000\nColumns: 53\n$ LoanNumber                  <dbl> 8316258510, 2043838906, 2695968905, 786696…\n$ DateApproved                <chr> \"03/09/2021\", \"04/26/2021\", \"04/27/2021\", …\n$ SBAOfficeCode               <chr> \"0593\", \"0549\", \"0593\", \"0593\", \"0593\", \"0…\n$ ProcessingMethod            <chr> \"PPS\", \"PPP\", \"PPS\", \"PPP\", \"PPP\", \"PPS\", …\n$ BorrowerName                <chr> \"JASON MILLER\", \"JOHN HOLLY\", \"SHELLESSA D…\n$ BorrowerAddress             <chr> \"1008 Gateway Dr\", \"3977 E 186th St\", \"42 …\n$ BorrowerCity                <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ BorrowerState               <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ BorrowerZip                 <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ LoanStatusDate              <chr> \"11/19/2021\", \"01/31/2022\", \"09/25/2021\", …\n$ LoanStatus                  <chr> \"Paid in Full\", \"Paid in Full\", \"Paid in F…\n$ Term                        <dbl> 60, 60, 60, 24, 60, 60, 24, 60, 24, 60, 60…\n$ SBAGuarantyPercentage       <dbl> 100, 100, 100, 100, 100, 100, 100, 100, 10…\n$ InitialApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ CurrentApprovalAmount       <dbl> 61859.00, 20833.33, 11414.00, 57540.00, 20…\n$ UndisbursedAmount           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ FranchiseName               <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ServicingLenderLocationID   <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ ServicingLenderName         <chr> \"Civista Bank\", \"American Lending Center\",…\n$ ServicingLenderAddress      <chr> \"100 E Water St\", \"1 World Trade Center, S…\n$ ServicingLenderCity         <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ ServicingLenderState        <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ ServicingLenderZip          <chr> \"44870-2524\", \"90831\", \"92653\", \"37030-120…\n$ RuralUrbanIndicator         <chr> \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U…\n$ HubzoneIndicator            <chr> \"N\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ LMIIndicator                <chr> \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ BusinessAgeDescription      <chr> \"Existing or more than 2 years old\", \"Exis…\n$ ProjectCity                 <chr> \"Dayton\", \"Cleveland\", \"Dayton\", \"HARRISON…\n$ ProjectCountyName           <chr> \"MONTGOMERY\", \"CUYAHOGA\", \"MONTGOMERY\", \"H…\n$ ProjectState                <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", …\n$ ProjectZip                  <chr> \"45404-2281\", \"44122-6757\", \"45417-3723\", …\n$ CD                          <chr> \"OH-10\", \"OH-11\", \"OH-10\", \"OH-01\", \"OH-01…\n$ JobsReported                <dbl> 12, 1, 1, 14, 1, 1, 17, 1, 2, 1, 2, 1, 1, …\n$ NAICSCode                   <dbl> 722410, 722320, 624110, 721110, 531210, 56…\n$ Race                        <chr> \"White\", \"Unanswered\", \"Unanswered\", \"Unan…\n$ Ethnicity                   <chr> \"Not Hispanic or Latino\", \"Unknown/NotStat…\n$ UTILITIES_PROCEED           <dbl> 1, NA, NA, NA, NA, 1, NA, NA, NA, 1, 1, NA…\n$ PAYROLL_PROCEED             <dbl> 61853.00, 20833.33, 11414.00, 57540.00, 20…\n$ MORTGAGE_INTEREST_PROCEED   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ RENT_PROCEED                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ REFINANCE_EIDL_PROCEED      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ HEALTH_CARE_PROCEED         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ DEBT_INTEREST_PROCEED       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ BusinessType                <chr> \"Limited  Liability Company(LLC)\", \"Sole P…\n$ OriginatingLenderLocationID <dbl> 58322, 530223, 509316, 66851, 529472, 5803…\n$ OriginatingLender           <chr> \"Civista Bank\", \"American Lending Center\",…\n$ OriginatingLenderCity       <chr> \"SANDUSKY\", \"Long Beach\", \"Laguna Hills\", …\n$ OriginatingLenderState      <chr> \"OH\", \"CA\", \"CA\", \"TN\", \"TX\", \"OH\", \"PA\", …\n$ Gender                      <chr> \"Male Owned\", \"Unanswered\", \"Unanswered\", …\n$ Veteran                     <chr> \"Non-Veteran\", \"Unanswered\", \"Unanswered\",…\n$ NonProfit                   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ForgivenessAmount           <dbl> 62252.49, 20924.77, 11447.46, 57990.86, 20…\n$ ForgivenessDate             <chr> \"10/26/2021\", \"10/06/2021\", \"08/20/2021\", …\n\n\nForgivenessAmount, ForgivenessDate, and BorrowerName promise to be the most useful variables.\n\n\nCode\nprint(summarytools::dfSummary(ppp_ohio,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\n\n\nData Frame Summary\nppp_ohio\nDimensions: 10000 x 53\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      LoanNumber\n[numeric]\n      Mean (sd) : 5492418880 (2600653757)min ≤ med ≤ max:1000358703 ≤ 5564758604 ≤ 9997128810IQR (CV) : 4509922449 (0.5)\n      10000 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DateApproved\n[character]\n      1. 05/01/20202. 04/28/20203. 04/15/20204. 04/27/20205. 04/14/20206. 05/12/20217. 04/30/20208. 04/29/20209. 04/22/202110. 03/23/2021[ 212 others ]\n      724(7.2%)312(3.1%)265(2.6%)240(2.4%)179(1.8%)164(1.6%)155(1.6%)154(1.5%)150(1.5%)149(1.5%)7508(75.1%)\n      \n      0\n(0.0%)\n    \n    \n      SBAOfficeCode\n[character]\n      1. 01012. 05493. 0593\n      1(0.0%)4830(48.3%)5169(51.7%)\n      \n      0\n(0.0%)\n    \n    \n      ProcessingMethod\n[character]\n      1. PPP2. PPS\n      7835(78.3%)2165(21.6%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerName\n[character]\n      1. MICHAEL JOHNSON2. 16ELEVEN LLC3. ALL SERVICE AERATION4. ANDREW BRUSH5. ANTONIO JONES6. ASHLEY JACKSON7. BLUE FITNESS INC8. BRITTANY JONES9. BRITTNEY HAYWARD10. CAMERON JOHNSON[ 9924 others ]\n      3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9979(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerAddress\n[character]\n      1. 24 E North St2. 24455 Lake Shore Blvd3. 4503 Marburg Ave4. 4910 Tiedeman Dr5. 1 E Campus View Blvd6. 100 E Campus View Blvd St7. 100 E Wilson Bridge Rd8. 1005 W 3rd Ave9. 109 Elm St10. 11271 Reading Rd[ 9911 others ]\n      3(0.0%)3(0.0%)3(0.0%)3(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)2(0.0%)9976(99.8%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1299 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      BorrowerZip\n[character]\n      1. 441452. 441243. 432154. 450115. 430176. 430657. 452388. 440959. 4413910. 45242[ 8358 others ]\n      21(0.2%)20(0.2%)17(0.2%)17(0.2%)16(0.2%)16(0.2%)15(0.1%)14(0.1%)14(0.1%)14(0.1%)9836(98.4%)\n      \n      0\n(0.0%)\n    \n    \n      LoanStatusDate\n[character]\n      1. 03/22/20222. 11/17/20213. 01/06/20224. 08/17/20215. 09/28/20216. 09/25/20217. 10/21/20218. 11/20/20219. 10/20/202110. 10/15/2021[ 380 others ]\n      392(4.5%)258(2.9%)243(2.8%)240(2.7%)166(1.9%)159(1.8%)131(1.5%)130(1.5%)121(1.4%)102(1.2%)6844(77.9%)\n      \n      1214\n(12.1%)\n    \n    \n      LoanStatus\n[character]\n      1. Exemption 42. Paid in Full\n      1214(12.1%)8786(87.9%)\n      \n      0\n(0.0%)\n    \n    \n      Term\n[numeric]\n      Mean (sd) : 47.7 (17.2)min ≤ med ≤ max:0 ≤ 60 ≤ 75IQR (CV) : 36 (0.4)\n      28 distinct values\n      \n      0\n(0.0%)\n    \n    \n      SBAGuarantyPercentage\n[numeric]\n      1 distinct value\n      100:10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      InitialApprovalAmount\n[numeric]\n      Mean (sd) : 26700 (28548)min ≤ med ≤ max:105 ≤ 20207 ≤ 252947IQR (CV) : 16700 (1.1)\n      5789 distinct values\n      \n      0\n(0.0%)\n    \n    \n      CurrentApprovalAmount\n[numeric]\n      Mean (sd) : 26570.5 (28170.1)min ≤ med ≤ max:105 ≤ 20207 ≤ 149700IQR (CV) : 16654.9 (1.1)\n      5803 distinct values\n      \n      0\n(0.0%)\n    \n    \n      UndisbursedAmount\n[numeric]\n      1 distinct value\n      0:9998(100.0%)\n      \n      2\n(0.0%)\n    \n    \n      FranchiseName\n[character]\n      1. Subway2. Comfort Inn by Choice Hot3. H&R Block - Franchise Lic4. Hot Head Burritos5. Vision Source6. Dunkin' Donuts7. Holiday Inn Express (Lice8. Orange Leaf Frozen Yogurt9. Wild Birds Unlimited10. Zoup![ 79 others ]\n      9(8.2%)3(2.7%)3(2.7%)3(2.7%)3(2.7%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)2(1.8%)79(71.8%)\n      \n      9890\n(98.9%)\n    \n    \n      ServicingLenderLocationID\n[numeric]\n      Mean (sd) : 214404.3 (203525.1)min ≤ med ≤ max:4282 ≤ 79184 ≤ 538160IQR (CV) : 441813 (0.9)\n      327 distinct values\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderName\n[character]\n      1. The Huntington National B2. Harvest Small Business Fi3. Prestamos CDFI, LLC4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. Cross River Bank[ 314 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderAddress\n[character]\n      1. 17 S High St.2. 24422 Avenida de la Carlo3. 1024 East Buckeye Road Su4. 2247 Central Drive5. 38 Fountain Sq Plz6. 7000 SW 97th Avenue Suite7. 222 Delaware Ave8. 1111 Polaris Pkwy9. 425 Walnut St10. 885 Teaneck Rd[ 315 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Laguna Hills4. Phoenix5. Bedford6. Miami7. WILMINGTON8. TEANECK9. CLEVELAND10. LAKE MARY[ 255 others ]\n      1275(12.8%)975(9.8%)718(7.2%)624(6.2%)484(4.8%)427(4.3%)382(3.8%)306(3.1%)302(3.0%)248(2.5%)4259(42.6%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. NJ7. DE8. PA9. NY10. RI[ 29 others ]\n      5178(51.8%)907(9.1%)681(6.8%)625(6.2%)515(5.1%)379(3.8%)378(3.8%)362(3.6%)243(2.4%)216(2.2%)516(5.2%)\n      \n      0\n(0.0%)\n    \n    \n      ServicingLenderZip\n[character]\n      1. 43215-34132. 926533. 850344. 760215. 452636. 331737. 19801-16218. 43240-20319. 45202-395610. 07666-4546[ 316 others ]\n      846(8.5%)718(7.2%)624(6.2%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)306(3.1%)5060(50.6%)\n      \n      0\n(0.0%)\n    \n    \n      RuralUrbanIndicator\n[character]\n      1. R2. U\n      2381(23.8%)7619(76.2%)\n      \n      0\n(0.0%)\n    \n    \n      HubzoneIndicator\n[character]\n      1. N2. Y\n      6935(69.3%)3065(30.6%)\n      \n      0\n(0.0%)\n    \n    \n      LMIIndicator\n[character]\n      1. N2. Y\n      6823(68.2%)3177(31.8%)\n      \n      0\n(0.0%)\n    \n    \n      BusinessAgeDescription\n[character]\n      1. Existing or more than 2 y2. New Business or 2 years o3. Startup, Loan Funds will 4. Unanswered\n      9005(90.0%)433(4.3%)1(0.0%)561(5.6%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCity\n[character]\n      1. Cleveland2. Columbus3. Cincinnati4. Toledo5. Dayton6. CINCINNATI7. COLUMBUS8. Akron9. CLEVELAND10. Youngstown[ 1298 others ]\n      693(6.9%)622(6.2%)493(4.9%)311(3.1%)255(2.5%)247(2.5%)225(2.2%)206(2.1%)123(1.2%)112(1.1%)6713(67.1%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectCountyName\n[character]\n      1. CUYAHOGA2. FRANKLIN3. HAMILTON4. SUMMIT5. LUCAS6. MONTGOMERY7. STARK8. BUTLER9. MAHONING10. LORAIN[ 78 others ]\n      1810(18.1%)1277(12.8%)800(8.0%)491(4.9%)473(4.7%)473(4.7%)273(2.7%)237(2.4%)224(2.2%)197(2.0%)3745(37.5%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectState\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      ProjectZip\n[character]\n      1. 44145-00012. 43215-00013. 44124-00014. 45011-00015. 43065-00016. 45242-00017. 43017-00018. 44122-00019. 44139-000110. 43015-0001[ 8476 others ]\n      21(0.2%)19(0.2%)19(0.2%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)9847(98.5%)\n      \n      0\n(0.0%)\n    \n    \n      CD\n[character]\n      1. OH-112. OH-033. OH-054. OH-015. OH-096. OH-147. OH-128. OH-089. OH-1010. OH-13[ 7 others ]\n      1352(13.5%)832(8.3%)699(7.0%)679(6.8%)646(6.5%)613(6.1%)610(6.1%)555(5.6%)555(5.6%)541(5.4%)2918(29.2%)\n      \n      0\n(0.0%)\n    \n    \n      JobsReported\n[numeric]\n      Mean (sd) : 4.1 (11.9)min ≤ med ≤ max:1 ≤ 1 ≤ 500IQR (CV) : 3 (2.9)\n      69 distinct values\n      \n      0\n(0.0%)\n    \n    \n      NAICSCode\n[numeric]\n      Mean (sd) : 540071.1 (202394.1)min ≤ med ≤ max:111110 ≤ 541511 ≤ 999990IQR (CV) : 267779.5 (0.4)\n      686 distinct values\n      \n      113\n(1.1%)\n    \n    \n      Race\n[character]\n      1. American Indian or Alaska2. Asian3. Black or African American4. Native Hawaiian or Other 5. Unanswered6. White\n      23(0.2%)129(1.3%)1103(11.0%)7(0.1%)6905(69.0%)1833(18.3%)\n      \n      0\n(0.0%)\n    \n    \n      Ethnicity\n[character]\n      1. Hispanic or Latino2. Not Hispanic or Latino3. Unknown/NotStated\n      119(1.2%)3279(32.8%)6602(66.0%)\n      \n      0\n(0.0%)\n    \n    \n      UTILITIES_PROCEED\n[numeric]\n      Mean (sd) : 513.4 (3183.8)min ≤ med ≤ max:0 ≤ 1 ≤ 134700IQR (CV) : 0 (6.2)\n      345 distinct values\n      \n      6802\n(68.0%)\n    \n    \n      PAYROLL_PROCEED\n[numeric]\n      Mean (sd) : 26102.4 (27604.3)min ≤ med ≤ max:0 ≤ 20052 ≤ 149700IQR (CV) : 16222 (1.1)\n      6362 distinct values\n      \n      10\n(0.1%)\n    \n    \n      MORTGAGE_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 2901 (7033.4)min ≤ med ≤ max:0 ≤ 1210.7 ≤ 86712IQR (CV) : 2737.8 (2.4)\n      140 distinct values\n      \n      9805\n(98.0%)\n    \n    \n      RENT_PROCEED\n[numeric]\n      Mean (sd) : 5396.7 (5788.6)min ≤ med ≤ max:0 ≤ 3700 ≤ 36502IQR (CV) : 5746.7 (1.1)\n      263 distinct values\n      \n      9632\n(96.3%)\n    \n    \n      REFINANCE_EIDL_PROCEED\n[numeric]\n      Mean (sd) : 1550.9 (3835.3)min ≤ med ≤ max:0 ≤ 0 ≤ 25200IQR (CV) : 1000 (2.5)\n      13 distinct values\n      \n      9931\n(99.3%)\n    \n    \n      HEALTH_CARE_PROCEED\n[numeric]\n      Mean (sd) : 4122 (5865.4)min ≤ med ≤ max:0 ≤ 2070 ≤ 32900IQR (CV) : 4600 (1.4)\n      86 distinct values\n      \n      9871\n(98.7%)\n    \n    \n      DEBT_INTEREST_PROCEED\n[numeric]\n      Mean (sd) : 1349.6 (4310.4)min ≤ med ≤ max:0 ≤ 150 ≤ 36100IQR (CV) : 1186.5 (3.2)\n      34 distinct values\n      \n      9924\n(99.2%)\n    \n    \n      BusinessType\n[character]\n      1. Sole Proprietorship2. Limited  Liability Compan3. Corporation4. Independent Contractors5. Subchapter S Corporation6. Self-Employed Individuals7. Non-Profit Organization8. Single Member LLC9. Partnership10. Limited Liability Partner[ 7 others ]\n      3250(32.5%)2545(25.5%)1285(12.9%)865(8.7%)790(7.9%)760(7.6%)227(2.3%)89(0.9%)70(0.7%)60(0.6%)52(0.5%)\n      \n      7\n(0.1%)\n    \n    \n      OriginatingLenderLocationID\n[numeric]\n      Mean (sd) : 213535.4 (203054.8)min ≤ med ≤ max:4282 ≤ 78723 ≤ 531105IQR (CV) : 430853.2 (1)\n      331 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLender\n[character]\n      1. The Huntington National B2. Prestamos CDFI, LLC3. Harvest Small Business Fi4. Capital Plus Financial, L5. Fifth Third Bank6. Benworth Capital7. PNC Bank, National Associ8. JPMorgan Chase Bank, Nati9. U.S. Bank, National Assoc10. KeyBank National Associat[ 317 others ]\n      846(8.5%)624(6.2%)497(5.0%)484(4.8%)442(4.4%)427(4.3%)377(3.8%)364(3.6%)352(3.5%)292(2.9%)5295(52.9%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderCity\n[character]\n      1. COLUMBUS2. CINCINNATI3. Phoenix4. Laguna Hills5. Bedford6. Miami7. WILMINGTON8. CLEVELAND9. TEANECK10. LAKE MARY[ 259 others ]\n      1275(12.8%)975(9.8%)624(6.2%)497(5.0%)484(4.8%)427(4.3%)382(3.8%)302(3.0%)268(2.7%)247(2.5%)4519(45.2%)\n      \n      0\n(0.0%)\n    \n    \n      OriginatingLenderState\n[character]\n      1. OH2. CA3. FL4. AZ5. TX6. DE7. NJ8. PA9. RI10. UT[ 29 others ]\n      5183(51.8%)849(8.5%)681(6.8%)625(6.2%)519(5.2%)378(3.8%)349(3.5%)311(3.1%)216(2.2%)209(2.1%)680(6.8%)\n      \n      0\n(0.0%)\n    \n    \n      Gender\n[character]\n      1. Female Owned2. Male Owned3. Unanswered\n      1604(16.0%)2753(27.5%)5643(56.4%)\n      \n      0\n(0.0%)\n    \n    \n      Veteran\n[character]\n      1. Non-Veteran2. Unanswered3. Veteran\n      3701(37.0%)6096(61.0%)203(2.0%)\n      \n      0\n(0.0%)\n    \n    \n      NonProfit\n[character]\n      1. Y\n      251(100.0%)\n      \n      9749\n(97.5%)\n    \n    \n      ForgivenessAmount\n[numeric]\n      Mean (sd) : 27785.1 (29446.5)min ≤ med ≤ max:12.6 ≤ 20307.2 ≤ 150842.8IQR (CV) : 20463.8 (1.1)\n      8422 distinct values\n      \n      1145\n(11.5%)\n    \n    \n      ForgivenessDate\n[character]\n      1. 06/15/20212. 10/06/20213. 01/07/20214. 09/07/20215. 09/29/20216. 09/01/20217. 06/23/20218. 09/22/20219. 11/10/202110. 10/14/2021[ 432 others ]\n      164(1.9%)147(1.7%)132(1.5%)98(1.1%)96(1.1%)92(1.0%)91(1.0%)91(1.0%)82(0.9%)79(0.9%)7783(87.9%)\n      \n      1145\n(11.5%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-10-30\n\n\n\n(Data below from unsampled, larger dataset)\nFrom a first look, most applicants reported only one employee needing coverage under the PPP. In fact, “Sole Proprietorship” was the highest category of BusinessType, above even LLCs, with 33.3%.\n19% of applicants were white, leading the race category, and 68% were unreported.\nMedian ForgivenessAmount was $20,438.7, and most loans were forgiven in 2021."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#voter-registration",
    "href": "posts/Final_SteveONeill.html#voter-registration",
    "title": "Final Part 1",
    "section": "Voter Registration",
    "text": "Voter Registration\nThe Ohio voter registration data is easily read-in:\n\n\nCode\n#ohio_voters <- read_csv(\"_data/SWVF_1_22.txt\")\n#ohio_voters_sampled <- ohio_voters %>% sample_n(10000)\n#write_csv(ohio_voters_sampled, \"_data/ohio_voters_sampled.csv\")\n\nohio_voters <- read_csv(\"_data/ohio_voters_sampled.csv\")\n\n\nRows: 10000 Columns: 116\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (91): SOS_VOTERID, COUNTY_NUMBER, LAST_NAME, FIRST_NAME, MIDDLE_NAME, S...\ndbl   (4): COUNTY_ID, RESIDENTIAL_ZIP, MAILING_ZIP, STATE_REPRESENTATIVE_DIS...\nlgl  (19): RESIDENTIAL_COUNTRY, RESIDENTIAL_POSTALCODE, MAILING_SECONDARY_AD...\ndate  (2): DATE_OF_BIRTH, REGISTRATION_DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis is a 1gb+ file, which needs to be fully complete in order for the join of sampled PPP data. For the actual project, I will do that in a one-time process on my local computer.\nZip codes will let us know with high confidence that we are dealing with the same business owner even if names are duplicated.\nThe historical information tells us if they have changed their party affiliation from year-to-year, which can be useful in determining if political rent-seeking was successful - we can even see ‘D’ or ‘R’ going back 22 years:\n\n\nCode\nglimpse(ohio_voters)\n\n\nRows: 10,000\nColumns: 116\n$ SOS_VOTERID                   <chr> \"OH0010417183\", \"OH0024878858\", \"OH00159…\n$ COUNTY_NUMBER                 <chr> \"19\", \"18\", \"18\", \"12\", \"18\", \"03\", \"10\"…\n$ COUNTY_ID                     <dbl> 9500876, 2859748, 73683, 122609458, 2158…\n$ LAST_NAME                     <chr> \"COLLINS\", \"SAFO\", \"WHELAN\", \"OVERHOLT\",…\n$ FIRST_NAME                    <chr> \"SHANNON\", \"SHAREEF\", \"PATRICK\", \"ADDISO…\n$ MIDDLE_NAME                   <chr> \"AMY\", \"RESHAUN\", \"HOWARD\", \"ELIZABETH\",…\n$ SUFFIX                        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ DATE_OF_BIRTH                 <date> 1977-02-22, 1998-12-18, 1969-07-21, 200…\n$ REGISTRATION_DATE             <date> 1995-05-19, 2021-09-01, 2022-08-02, 202…\n$ VOTER_STATUS                  <chr> \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", …\n$ PARTY_AFFILIATION             <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ RESIDENTIAL_ADDRESS1          <chr> \"372 S MAIN ST\", \"1629 CARLYON RD\", \"232…\n$ RESIDENTIAL_SECONDARY_ADDR    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_CITY              <chr> \"NEW MADISON\", \"EAST CLEVELAND\", \"NORTH …\n$ RESIDENTIAL_STATE             <chr> \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\", \"OH\"…\n$ RESIDENTIAL_ZIP               <dbl> 45346, 44112, 44070, 45502, 44109, 44805…\n$ RESIDENTIAL_ZIP_PLUS4         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_COUNTRY           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ RESIDENTIAL_POSTALCODE        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ADDRESS1              <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_SECONDARY_ADDRESS     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_CITY                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_STATE                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP                   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_ZIP_PLUS4             <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_COUNTRY               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ MAILING_POSTAL_CODE           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CAREER_CENTER                 <chr> \"MIAMI VALLEY CAREER TECH\", NA, \"POLARIS…\n$ CITY                          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CITY_SCHOOL_DISTRICT          <chr> NA, \"EAST CLEVELAND CITY SD\", \"NORTH OLM…\n$ COUNTY_COURT_DISTRICT         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CONGRESSIONAL_DISTRICT        <chr> \"08\", \"11\", \"07\", \"15\", \"11\", \"04\", \"06\"…\n$ COURT_OF_APPEALS              <chr> \"02\", \"08\", \"08\", \"02\", \"08\", \"05\", \"07\"…\n$ EDU_SERVICE_CENTER_DISTRICT   <chr> \"DARKE COUNTY ESC\", NA, NA, \"CLARK COUNT…\n$ EXEMPTED_VILL_SCHOOL_DISTRICT <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LIBRARY                       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ LOCAL_SCHOOL_DISTRICT         <chr> \"TRI-VILLAGE LOCAL SD (DARKE)\", NA, NA, …\n$ MUNICIPAL_COURT_DISTRICT      <chr> \"DARKE-CO\", \"EAST-CLEVELAND\", \"ROCKY-RIV…\n$ PRECINCT_NAME                 <chr> \"PRECINCT HARRISON EAST & NEW MADISON\", …\n$ PRECINCT_CODE                 <chr> \"19AAZ\", \"18-P-BMR\", \"18-P-CDT\", \"12ADJ\"…\n$ STATE_BOARD_OF_EDUCATION      <chr> \"03\", \"10\", \"11\", \"10\", \"11\", \"05\", \"08\"…\n$ STATE_REPRESENTATIVE_DISTRICT <dbl> 80, 22, 16, 74, 15, 67, 79, 71, 47, 14, …\n$ STATE_SENATE_DISTRICT         <chr> \"05\", \"21\", \"24\", \"10\", \"24\", \"22\", \"33\"…\n$ TOWNSHIP                      <chr> \"HARRISON TWP\", NA, NA, \"BETHEL TOWNSHIP…\n$ VILLAGE                       <chr> \"NEW MADISON VILLAGE\", NA, NA, NA, NA, N…\n$ WARD                          <chr> NA, \"EAST CLEVELAND WARD 2\", \"NORTH OLMS…\n$ `PRIMARY-03/07/2000`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/07/2000`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, NA…\n$ `SPECIAL-05/08/2001`          <chr> NA, NA, NA, NA, NA, NA, NA, \"R\", \"X\", NA…\n$ `GENERAL-11/06/2001`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-05/07/2002`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/05/2002`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, \"X\", NA…\n$ `SPECIAL-05/06/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/04/2003`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-03/02/2004`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, \"X\", NA…\n$ `GENERAL-11/02/2004`          <chr> \"X\", NA, \"X\", NA, NA, NA, \"X\", NA, \"X\", …\n$ `SPECIAL-02/08/2005`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/03/2005`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/13/2005`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2005`          <chr> NA, NA, NA, NA, \"X\", NA, \"X\", NA, NA, NA…\n$ `SPECIAL-02/07/2006`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/02/2006`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `GENERAL-11/07/2006`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/11/2007`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-11/06/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-12/11/2007`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-03/04/2008`          <chr> NA, NA, NA, NA, \"D\", NA, \"D\", NA, \"R\", N…\n$ `PRIMARY-10/14/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/04/2008`          <chr> \"X\", NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\",…\n$ `GENERAL-11/18/2008`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-05/05/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/08/2009`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/15/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/29/2009`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2009`          <chr> NA, NA, \"X\", NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/04/2010`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n$ `PRIMARY-07/13/2010`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/07/2010`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2010`          <chr> \"X\", NA, NA, NA, \"X\", NA, \"X\", NA, \"X\", …\n$ `PRIMARY-05/03/2011`          <chr> \"X\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ `PRIMARY-09/13/2011`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2011`          <chr> \"X\", NA, \"X\", NA, NA, NA, NA, NA, \"X\", N…\n$ `PRIMARY-03/06/2012`          <chr> NA, NA, NA, NA, \"R\", NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/06/2012`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/07/2013`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/10/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-10/01/2013`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2013`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", NA, \"X\"…\n$ `PRIMARY-05/06/2014`          <chr> \"L\", NA, NA, NA, NA, NA, \"D\", NA, NA, NA…\n$ `GENERAL-11/04/2014`          <chr> NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", N…\n$ `PRIMARY-05/05/2015`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `PRIMARY-09/15/2015`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/03/2015`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-03/15/2016`          <chr> \"D\", NA, NA, NA, NA, \"R\", \"D\", NA, \"R\", …\n$ `GENERAL-06/07/2016`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/13/2016`          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/08/2016`          <chr> \"X\", NA, \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"X…\n$ `PRIMARY-05/02/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/12/2017`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/07/2017`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", NA, \"X\", …\n$ `PRIMARY-05/08/2018`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, NA, \"R…\n$ `GENERAL-08/07/2018`          <chr> NA, NA, NA, NA, NA, NA, \"X\", NA, NA, NA,…\n$ `GENERAL-11/06/2018`          <chr> \"X\", \"X\", NA, NA, NA, \"X\", \"X\", \"X\", \"X\"…\n$ `PRIMARY-05/07/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/10/2019`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/05/2019`          <chr> NA, NA, NA, NA, NA, NA, \"X\", \"X\", NA, \"X…\n$ `PRIMARY-03/17/2020`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, \"R\"…\n$ `GENERAL-11/03/2020`          <chr> \"X\", \"X\", \"X\", NA, \"X\", \"X\", \"X\", \"X\", \"…\n$ `PRIMARY-05/04/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"X\",…\n$ `PRIMARY-08/03/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `PRIMARY-09/14/2021`          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `GENERAL-11/02/2021`          <chr> \"X\", NA, NA, NA, NA, \"X\", \"X\", \"X\", NA, …\n$ `PRIMARY-05/03/2022`          <chr> \"D\", NA, NA, NA, NA, NA, \"D\", NA, \"R\", \"…\n$ `PRIMARY-08/02/2022`          <chr> NA, NA, NA, NA, NA, NA, \"D\", NA, NA, NA,…\n\n\n\n\nCode\nprint(summarytools::dfSummary(ohio_voters,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\nWarning in png(png_loc <- tempfile(fileext = \".png\"), width = 150 *\ngraph.magnif, : unable to open connection to X11 display ''\n\n\n\n\nData Frame Summary\nohio_voters\nDimensions: 10000 x 116\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      SOS_VOTERID\n[character]\n      1. OH00100003302. OH00100003443. OH00100008674. OH00100012975. OH00100022616. OH00100028027. OH00100028178. OH00100028989. OH001000297910. OH0010003129[ 9990 others ]\n      1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)1(0.0%)9990(99.9%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_NUMBER\n[character]\n      1. 182. 093. 214. 135. 126. 157. 028. 049. 2210. 07[ 12 others ]\n      4086(40.9%)1187(11.9%)685(6.9%)683(6.8%)392(3.9%)346(3.5%)326(3.3%)286(2.9%)249(2.5%)208(2.1%)1552(15.5%)\n      \n      0\n(0.0%)\n    \n    \n      COUNTY_ID\n[numeric]\n      Mean (sd) : 4804609 (14312683)min ≤ med ≤ max:30 ≤ 1939790 ≤ 122614854IQR (CV) : 2624441 (3)\n      9992 distinct values\n      \n      0\n(0.0%)\n    \n    \n      LAST_NAME\n[character]\n      1. SMITH2. JOHNSON3. WILLIAMS4. MILLER5. JONES6. BROWN7. THOMPSON8. DAVIS9. JACKSON10. MOORE[ 6190 others ]\n      91(0.9%)81(0.8%)72(0.7%)63(0.6%)59(0.6%)58(0.6%)38(0.4%)35(0.4%)33(0.3%)33(0.3%)9437(94.4%)\n      \n      0\n(0.0%)\n    \n    \n      FIRST_NAME\n[character]\n      1. MICHAEL2. ROBERT3. JOHN4. DAVID5. WILLIAM6. JAMES7. MARY8. THOMAS9. CHRISTOPHER10. RICHARD[ 2495 others ]\n      160(1.6%)143(1.4%)130(1.3%)126(1.3%)125(1.2%)123(1.2%)94(0.9%)88(0.9%)78(0.8%)74(0.7%)8859(88.6%)\n      \n      0\n(0.0%)\n    \n    \n      MIDDLE_NAME\n[character]\n      1. A2. M3. L4. J5. E6. D7. R8. C9. S10. ANN[ 1186 others ]\n      713(7.8%)696(7.6%)677(7.4%)496(5.4%)322(3.5%)314(3.4%)302(3.3%)227(2.5%)220(2.4%)216(2.4%)4975(54.3%)\n      \n      842\n(8.4%)\n    \n    \n      SUFFIX\n[character]\n      1. I2. II3. III4. IV5. JR6. SR7. V\n      2(0.5%)35(8.4%)57(13.6%)8(1.9%)259(61.8%)56(13.4%)2(0.5%)\n      \n      9581\n(95.8%)\n    \n    \n      DATE_OF_BIRTH\n[Date]\n      min : 1922-07-17med : 1972-01-18max : 2004-10-25range : 82y 3m 8d\n      8207 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REGISTRATION_DATE\n[Date]\n      min : 1900-01-01med : 2015-12-31max : 2022-10-07range : 122y 9m 6d\n      4484 distinct values\n      \n      0\n(0.0%)\n    \n    \n      VOTER_STATUS\n[character]\n      1. ACTIVE2. CONFIRMATION\n      8534(85.3%)1466(14.7%)\n      \n      0\n(0.0%)\n    \n    \n      PARTY_AFFILIATION\n[character]\n      1. D2. L3. R\n      1431(45.7%)2(0.1%)1697(54.2%)\n      \n      6870\n(68.7%)\n    \n    \n      RESIDENTIAL_ADDRESS1\n[character]\n      1. 2201 ACACIA PARK DR2. 1055 OLD RIVER RD3. 11050 FANCHER RD4. 112 S CLINTON ST5. 12900 LAKE AVE6. 16700 LAKE SHORE BLVD7. 18221 EUCLID AVE8. 19101 VAN AKEN BLVD9. 19201 EUCLID AVE10. 2020 TAYLOR RD[ 9834 others ]\n      4(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)3(0.0%)9969(99.7%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_SECONDARY_ADDR\n[character]\n      1. APT 12. UPPR3. APT 24. APT 35. LOWR6. APT 47. APT B8. APT 79. APT 20110. APT A[ 565 others ]\n      54(4.4%)43(3.5%)39(3.2%)34(2.8%)32(2.6%)27(2.2%)22(1.8%)17(1.4%)16(1.3%)16(1.3%)923(75.5%)\n      \n      8777\n(87.8%)\n    \n    \n      RESIDENTIAL_CITY\n[character]\n      1. CLEVELAND2. HAMILTON3. SPRINGFIELD4. PARMA5. MIDDLETOWN6. LIMA7. WEST CHESTER8. LAKEWOOD9. DELAWARE10. CLEVELAND HTS[ 324 others ]\n      1166(11.7%)429(4.3%)286(2.9%)244(2.4%)235(2.4%)225(2.2%)191(1.9%)181(1.8%)180(1.8%)177(1.8%)6686(66.9%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_STATE\n[character]\n      1. OH\n      10000(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP\n[numeric]\n      Mean (sd) : 44459.3 (713.5)min ≤ med ≤ max:43003 ≤ 44138 ≤ 45896IQR (CV) : 949 (0)\n      338 distinct values\n      \n      0\n(0.0%)\n    \n    \n      RESIDENTIAL_ZIP_PLUS4\n[character]\n      1. 11012. 16383. 96084. 10305. 10526. 11127. 12018. 12419. 132010. 1325[ 531 others ]\n      3(0.5%)3(0.5%)3(0.5%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)2(0.3%)568(96.1%)\n      \n      9409\n(94.1%)\n    \n    \n      RESIDENTIAL_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      RESIDENTIAL_POSTALCODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_ADDRESS1\n[character]\n      1. PO BOX 32. PO BOX 410523. 10 CHRISTOPHER WAY APT 44. 100 ELM AV5. 10072A HORIZON ST6. 10115 VANDERBILT CIRCLE7. 1012 PROSPECT AVE APT 1028. 10919 GLENVIEW AVENUE9. 11 N FISHER DR ROOM 22110. 1121 W COLUMBIA AVE APT  [ 201 others ]\n      2(0.9%)2(0.9%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)1(0.5%)201(94.4%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_SECONDARY_ADDRESS\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_CITY\n[character]\n      1. OXFORD2. CLEVELAND3. ANDOVER4. LIMA5. BETHESDA6. CLARKSVILLE7. DELAWARE8. FELICITY9. GENEVA10. GETTYSBURG[ 133 others ]\n      15(7.0%)8(3.8%)6(2.8%)4(1.9%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)3(1.4%)162(76.1%)\n      \n      9787\n(97.9%)\n    \n    \n      MAILING_STATE\n[character]\n      1. OH2. CA3. IL4. VA5. AE6. IN7. MA8. WA9. AL10. BC[ 4 others ]\n      189(89.2%)3(1.4%)3(1.4%)3(1.4%)2(0.9%)2(0.9%)2(0.9%)2(0.9%)1(0.5%)1(0.5%)4(1.9%)\n      \n      9788\n(97.9%)\n    \n    \n      MAILING_ZIP\n[numeric]\n      Mean (sd) : 44768.7 (10335.5)min ≤ med ≤ max:1821 ≤ 44731 ≤ 98433IQR (CV) : 1355.2 (0.2)\n      150 distinct values\n      \n      9790\n(97.9%)\n    \n    \n      MAILING_ZIP_PLUS4\n[character]\n      1. 00622. 01093. 01334. 01655. 01736. 01817. 02088. 02709. 028210. 0367[ 15 others ]\n      1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)1(4.0%)15(60.0%)\n      \n      9975\n(99.8%)\n    \n    \n      MAILING_COUNTRY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      MAILING_POSTAL_CODE\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CAREER_CENTER\n[character]\n      1. GREAT OAKS CAREER CAMPUSE2. POLARIS CAREER CENTER3. DELAWARE AREA CAREER CENT4. SPRINGFIELD-CLARK COUNTY 5. CUYAHOGA VALLEY CAREER CE6. APOLLO CAREER CENTER7. COLUMBIANA COUNTY CAREER 8. BELMONT-HARRISON CAREER C9. EHOVE CAREER CENTER10. ASHLAND COUNTY-WEST HOLME[ 21 others ]\n      691(15.9%)571(13.2%)553(12.7%)392(9.0%)356(8.2%)281(6.5%)197(4.5%)196(4.5%)162(3.7%)147(3.4%)794(18.3%)\n      \n      5660\n(56.6%)\n    \n    \n      CITY\n[character]\n      1. HAMILTON CITY2. SPRINGFIELD CITY3. MIDDLETOWN CITY4. FAIRFIELD CITY5. DELAWARE CITY6. LIMA CITY7. SANDUSKY CITY8. ASHLAND CITY9. ATHENS CITY10. OXFORD CITY[ 32 others ]\n      172(8.7%)171(8.7%)137(6.9%)124(6.3%)120(6.1%)95(4.8%)79(4.0%)67(3.4%)62(3.1%)56(2.8%)892(45.2%)\n      \n      8025\n(80.2%)\n    \n    \n      CITY_SCHOOL_DISTRICT\n[character]\n      1. CLEVELAND MUNICIPAL CITY 2. PARMA CITY SD3. FAIRFIELD CITY SD4. CLEVELAND HTS-UNIV HTS CI5. LAKEWOOD CITY SD6. HAMILTON CITY SD7. SPRINGFIELD CITY SD8. EUCLID CITY SD9. BEREA CITY SD10. STRONGSVILLE CITY SD[ 55 others ]\n      1167(18.4%)343(5.4%)218(3.4%)214(3.4%)181(2.8%)173(2.7%)166(2.6%)153(2.4%)148(2.3%)148(2.3%)3442(54.2%)\n      \n      3647\n(36.5%)\n    \n    \n      COUNTY_COURT_DISTRICT\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      CONGRESSIONAL_DISTRICT\n[character]\n      1. 112. 073. 084. 045. 026. 067. 128. 099. 1010. 14[ 2 others ]\n      2522(25.2%)1564(15.6%)1358(13.6%)1274(12.7%)1006(10.1%)641(6.4%)445(4.4%)374(3.7%)294(2.9%)286(2.9%)236(2.4%)\n      \n      0\n(0.0%)\n    \n    \n      COURT_OF_APPEALS\n[character]\n      1. 022. 033. 044. 055. 066. 077. 088. 119. 12\n      683(6.8%)725(7.2%)244(2.4%)961(9.6%)249(2.5%)641(6.4%)4086(40.9%)286(2.9%)2125(21.2%)\n      \n      0\n(0.0%)\n    \n    \n      EDU_SERVICE_CENTER_DISTRICT\n[character]\n      1. ESC OF CENTRAL OHIO2. BUTLER COUNTY ESC3. CLARK COUNTY ESC4. ALLEN COUNTY ESC5. ASHTABULA COUNTY ESC6. BROWN COUNTY ESC7. NORTH POINT ESC8. COLUMBIANA COUNTY ESC9. SOUTHERN OHIO ESC10. DARKE COUNTY ESC[ 23 others ]\n      503(18.8%)496(18.6%)226(8.5%)202(7.6%)124(4.6%)120(4.5%)120(4.5%)116(4.3%)116(4.3%)80(3.0%)569(21.3%)\n      \n      7328\n(73.3%)\n    \n    \n      EXEMPTED_VILL_SCHOOL_DISTRICT\n[character]\n      1. MILFORD EX VILL SD (CLERM2. CARROLLTON EX VILL SD (CA3. NEW RICHMOND EX VILL SD (4. COLUMBIANA EX VILL SD (CO5. BARNESVILLE EX VILL SD (B6. CHAGRIN FALLS EX VILL SD 7. VERSAILLES EX VILL SD (DA8. MECHANICSBURG EX VILL SD 9. BLUFFTON EX VILL SD (ALLE10. HICKSVILLE EX VILL SD (DE[ 7 others ]\n      176(32.8%)57(10.6%)43(8.0%)32(6.0%)28(5.2%)25(4.7%)24(4.5%)21(3.9%)20(3.7%)19(3.5%)91(17.0%)\n      \n      9464\n(94.6%)\n    \n    \n      LIBRARY\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      LOCAL_SCHOOL_DISTRICT\n[character]\n      1. LAKOTA LOCAL SD (BUTLER)2. OLENTANGY LOCAL SD (DELAW3. WEST CLERMONT LOCAL SD (C4. ELIDA LOCAL SD (ALLEN)5. ROSS LOCAL SD (BUTLER)6. SHAWNEE LOCAL SD (ALLEN)7. NORTHEASTERN LOCAL SD (CL8. WESTERN BROWN LOCAL SD (B9. BUCKEYE VALLEY LOCAL SD (10. BIG WALNUT LOCAL SD (DELA[ 103 others ]\n      347(11.2%)324(10.4%)220(7.1%)68(2.2%)67(2.2%)66(2.1%)63(2.0%)60(1.9%)58(1.9%)56(1.8%)1782(57.3%)\n      \n      6889\n(68.9%)\n    \n    \n      MUNICIPAL_COURT_DISTRICT\n[character]\n      1. CLEVELAND2. DELAWARE3. PARMA4. ROCKY-RIVER5. BEREA6. LIMA7. BEDFORD8. GARFIELD-HEIGHTS9. COLUMBIANA-CO10. SHAKER-HEIGHTS[ 16 others ]\n      1175(19.1%)685(11.1%)562(9.1%)427(6.9%)364(5.9%)326(5.3%)280(4.5%)276(4.5%)260(4.2%)200(3.2%)1605(26.1%)\n      \n      3840\n(38.4%)\n    \n    \n      PRECINCT_NAME\n[character]\n      1. PRECINCT GOSHEN2. HAM2WD33. PRECINCT TWIN TWP. GORDON4. PRECINCT E LIVERPOOL 2-A5. CLEVELAND-03-I6. FAIRVIEW PARK-05-A7. NORTH ROYALTON-06-D8. PRECINCT UNION CITY VILLA9. BATAVIA TOWNSHIP A10. BAY VILLAGE-02-C[ 2373 others ]\n      16(0.2%)15(0.1%)15(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9864(98.6%)\n      \n      0\n(0.0%)\n    \n    \n      PRECINCT_CODE\n[character]\n      1. 09-P-ACR2. 19ABP3. 11AAW4. 15AAF5. 18-P-AYX6. 18-P-BSM7. 18-P-CGK8. 19ABD9. 09-P-AIT10. 09-P-AJL[ 2378 others ]\n      15(0.1%)15(0.1%)14(0.1%)14(0.1%)13(0.1%)13(0.1%)13(0.1%)13(0.1%)12(0.1%)12(0.1%)9866(98.7%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_BOARD_OF_EDUCATION\n[character]\n      1. 012. 023. 034. 055. 076. 087. 098. 109. 11\n      959(9.6%)249(2.5%)1244(12.4%)1176(11.8%)685(6.9%)803(8.0%)406(4.1%)2119(21.2%)2359(23.6%)\n      \n      0\n(0.0%)\n    \n    \n      STATE_REPRESENTATIVE_DISTRICT\n[numeric]\n      Mean (sd) : 48.2 (28.4)min ≤ med ≤ max:13 ≤ 47 ≤ 99IQR (CV) : 55 (0.6)\n      40 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STATE_SENATE_DISTRICT\n[character]\n      1. 212. 243. 234. 045. 146. 197. 128. 109. 1810. 33[ 8 others ]\n      1259(12.6%)1192(11.9%)1173(11.7%)1134(11.3%)889(8.9%)791(7.9%)696(7.0%)509(5.1%)462(4.6%)433(4.3%)1462(14.6%)\n      \n      0\n(0.0%)\n    \n    \n      TOWNSHIP\n[character]\n      1. WEST CHESTER TWP2. MIAMI TWP3. UNION  TWP4. LIBERTY TOWNSHIP5. ORANGE TWP6. LIBERTY TWP7. GENOA TWP8. FAIRFIELD TOWNSHIP9. PERRY TWP10. BATAVIA TWP[ 232 others ]\n      224(4.9%)164(3.6%)157(3.4%)130(2.8%)121(2.6%)116(2.5%)101(2.2%)94(2.1%)86(1.9%)84(1.8%)3295(72.1%)\n      \n      5428\n(54.3%)\n    \n    \n      VILLAGE\n[character]\n      1. MT. ORAB VILLAGE2. EAST PALESTINE VILLAGE3. CRESTLINE VILLAGE4. UNION CITY VILLAGE5. BETHEL VILLAGE6. BLUFFTON VILLAGE7. NEW RICHMOND VILLAGE8. BARNESVILLE VILLAGE9. BELLAIRE VILLAGE10. CARROLLTON VILLAGE[ 119 others ]\n      15(2.9%)14(2.7%)13(2.5%)13(2.5%)12(2.3%)12(2.3%)12(2.3%)11(2.1%)10(1.9%)10(1.9%)400(76.6%)\n      \n      9478\n(94.8%)\n    \n    \n      WARD\n[character]\n      1. DELAWARE CITY - WARD2. HAMILTON CTY WARD 13. CLEVELAND WARD 34. CLEVELAND WARD 45. CLEVELAND WARD 116. CLEVELAND WARD 97. CLEVELAND WARD 88. CLEVELAND WARD 109. CLEVELAND WARD 1510. CLEVELAND WARD 16[ 262 others ]\n      120(2.4%)91(1.9%)84(1.7%)83(1.7%)76(1.5%)75(1.5%)74(1.5%)71(1.4%)70(1.4%)68(1.4%)4095(83.5%)\n      \n      5093\n(50.9%)\n    \n    \n      PRIMARY-03/07/2000\n[character]\n      1. D2. E3. R4. X\n      598(35.9%)1(0.1%)697(41.8%)371(22.3%)\n      \n      8333\n(83.3%)\n    \n    \n      GENERAL-11/07/2000\n[character]\n      1. X\n      3328(100.0%)\n      \n      6672\n(66.7%)\n    \n    \n      SPECIAL-05/08/2001\n[character]\n      1. D2. R3. X\n      6(1.6%)44(11.9%)320(86.5%)\n      \n      9630\n(96.3%)\n    \n    \n      GENERAL-11/06/2001\n[character]\n      1. X\n      1866(100.0%)\n      \n      8134\n(81.3%)\n    \n    \n      PRIMARY-05/07/2002\n[character]\n      1. D2. R3. X\n      401(36.5%)308(28.0%)390(35.5%)\n      \n      8901\n(89.0%)\n    \n    \n      GENERAL-11/05/2002\n[character]\n      1. X\n      2454(100.0%)\n      \n      7546\n(75.5%)\n    \n    \n      SPECIAL-05/06/2003\n[character]\n      1. D2. R3. X\n      44(12.4%)38(10.7%)272(76.8%)\n      \n      9646\n(96.5%)\n    \n    \n      GENERAL-11/04/2003\n[character]\n      1. X\n      1312(100.0%)\n      \n      8688\n(86.9%)\n    \n    \n      PRIMARY-03/02/2004\n[character]\n      1. D2. R3. X\n      924(45.5%)468(23.1%)637(31.4%)\n      \n      7971\n(79.7%)\n    \n    \n      GENERAL-11/02/2004\n[character]\n      1. X\n      4737(100.0%)\n      \n      5263\n(52.6%)\n    \n    \n      SPECIAL-02/08/2005\n[character]\n      1. X\n      304(100.0%)\n      \n      9696\n(97.0%)\n    \n    \n      PRIMARY-05/03/2005\n[character]\n      1. D2. R3. X\n      19(2.0%)33(3.5%)884(94.4%)\n      \n      9064\n(90.6%)\n    \n    \n      PRIMARY-09/13/2005\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2005\n[character]\n      1. X\n      2617(100.0%)\n      \n      7383\n(73.8%)\n    \n    \n      SPECIAL-02/07/2006\n[character]\n      1. X\n      71(100.0%)\n      \n      9929\n(99.3%)\n    \n    \n      PRIMARY-05/02/2006\n[character]\n      1. D2. R3. X\n      707(44.4%)572(35.9%)314(19.7%)\n      \n      8407\n(84.1%)\n    \n    \n      GENERAL-11/07/2006\n[character]\n      1. R2. X\n      2(0.1%)3559(99.9%)\n      \n      6439\n(64.4%)\n    \n    \n      PRIMARY-05/08/2007\n[character]\n      1. D2. R3. X\n      71(16.4%)73(16.8%)290(66.8%)\n      \n      9566\n(95.7%)\n    \n    \n      PRIMARY-09/11/2007\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/06/2007\n[character]\n      1. X\n      1346(100.0%)\n      \n      8654\n(86.5%)\n    \n    \n      PRIMARY-11/06/2007\n[character]\n      1. D2. R3. X\n      13(19.1%)19(27.9%)36(52.9%)\n      \n      9932\n(99.3%)\n    \n    \n      GENERAL-12/11/2007\n[character]\n      1. X\n      53(100.0%)\n      \n      9947\n(99.5%)\n    \n    \n      PRIMARY-03/04/2008\n[character]\n      1. D2. R3. X\n      2073(64.4%)965(30.0%)182(5.7%)\n      \n      6780\n(67.8%)\n    \n    \n      PRIMARY-10/14/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/04/2008\n[character]\n      1. X\n      5352(100.0%)\n      \n      4648\n(46.5%)\n    \n    \n      GENERAL-11/18/2008\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-05/05/2009\n[character]\n      1. D2. R3. X\n      41(7.3%)5(0.9%)517(91.8%)\n      \n      9437\n(94.4%)\n    \n    \n      PRIMARY-09/08/2009\n[character]\n      1. D2. R3. X\n      5(3.9%)2(1.6%)120(94.5%)\n      \n      9873\n(98.7%)\n    \n    \n      PRIMARY-09/15/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/29/2009\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2009\n[character]\n      1. D2. X\n      1(0.0%)3175(100.0%)\n      \n      6824\n(68.2%)\n    \n    \n      PRIMARY-05/04/2010\n[character]\n      1. C2. D3. G4. L5. R6. X\n      2(0.1%)756(43.3%)1(0.1%)3(0.2%)812(46.5%)172(9.9%)\n      \n      8254\n(82.5%)\n    \n    \n      PRIMARY-07/13/2010\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-09/07/2010\n[character]\n      1. D2. R3. X\n      319(65.5%)153(31.4%)15(3.1%)\n      \n      9513\n(95.1%)\n    \n    \n      GENERAL-11/02/2010\n[character]\n      1. X\n      3868(100.0%)\n      \n      6132\n(61.3%)\n    \n    \n      PRIMARY-05/03/2011\n[character]\n      1. D2. R3. X\n      89(12.9%)61(8.9%)539(78.2%)\n      \n      9311\n(93.1%)\n    \n    \n      PRIMARY-09/13/2011\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2011\n[character]\n      1. X\n      3639(100.0%)\n      \n      6361\n(63.6%)\n    \n    \n      PRIMARY-03/06/2012\n[character]\n      1. D2. G3. L4. R5. X\n      790(38.3%)1(0.0%)4(0.2%)1195(57.9%)73(3.5%)\n      \n      7937\n(79.4%)\n    \n    \n      GENERAL-11/06/2012\n[character]\n      1. X\n      5794(100.0%)\n      \n      4206\n(42.1%)\n    \n    \n      PRIMARY-05/07/2013\n[character]\n      1. D2. R3. X\n      24(5.2%)15(3.2%)423(91.6%)\n      \n      9538\n(95.4%)\n    \n    \n      PRIMARY-09/10/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      PRIMARY-10/01/2013\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/05/2013\n[character]\n      1. X\n      2347(100.0%)\n      \n      7653\n(76.5%)\n    \n    \n      PRIMARY-05/06/2014\n[character]\n      1. D2. G3. L4. R5. X\n      668(43.8%)2(0.1%)9(0.6%)703(46.1%)144(9.4%)\n      \n      8474\n(84.7%)\n    \n    \n      GENERAL-11/04/2014\n[character]\n      1. X\n      3433(100.0%)\n      \n      6567\n(65.7%)\n    \n    \n      PRIMARY-05/05/2015\n[character]\n      1. D2. R3. X\n      15(4.4%)43(12.6%)282(82.9%)\n      \n      9660\n(96.6%)\n    \n    \n      PRIMARY-09/15/2015\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/03/2015\n[character]\n      1. X\n      3543(100.0%)\n      \n      6457\n(64.6%)\n    \n    \n      PRIMARY-03/15/2016\n[character]\n      1. D2. G3. R4. X\n      1497(40.0%)2(0.1%)2199(58.7%)46(1.2%)\n      \n      6256\n(62.6%)\n    \n    \n      GENERAL-06/07/2016\n[character]\n      1. X\n      80(100.0%)\n      \n      9920\n(99.2%)\n    \n    \n      PRIMARY-09/13/2016\n[logical]\n      All NA's\n      \n      \n      10000\n(100.0%)\n    \n    \n      GENERAL-11/08/2016\n[character]\n      1. X\n      6412(100.0%)\n      \n      3588\n(35.9%)\n    \n    \n      PRIMARY-05/02/2017\n[character]\n      1. D2. R3. X\n      1(0.2%)13(3.2%)388(96.5%)\n      \n      9598\n(96.0%)\n    \n    \n      PRIMARY-09/12/2017\n[character]\n      1. D2. X\n      4(2.4%)163(97.6%)\n      \n      9833\n(98.3%)\n    \n    \n      GENERAL-11/07/2017\n[character]\n      1. X\n      2874(100.0%)\n      \n      7126\n(71.3%)\n    \n    \n      PRIMARY-05/08/2018\n[character]\n      1. D2. G3. R4. X\n      954(46.1%)6(0.3%)1006(48.6%)105(5.1%)\n      \n      7929\n(79.3%)\n    \n    \n      GENERAL-08/07/2018\n[character]\n      1. X\n      246(100.0%)\n      \n      9754\n(97.5%)\n    \n    \n      GENERAL-11/06/2018\n[character]\n      1. X\n      5287(100.0%)\n      \n      4713\n(47.1%)\n    \n    \n      PRIMARY-05/07/2019\n[character]\n      1. D2. R3. X\n      12(2.4%)105(21.0%)383(76.6%)\n      \n      9500\n(95.0%)\n    \n    \n      PRIMARY-09/10/2019\n[character]\n      1. D2. X\n      7(35.0%)13(65.0%)\n      \n      9980\n(99.8%)\n    \n    \n      GENERAL-11/05/2019\n[character]\n      1. X\n      2364(100.0%)\n      \n      7636\n(76.4%)\n    \n    \n      PRIMARY-03/17/2020\n[character]\n      1. D2. L3. R4. X\n      1152(52.2%)2(0.1%)937(42.5%)116(5.3%)\n      \n      7793\n(77.9%)\n    \n    \n      GENERAL-11/03/2020\n[character]\n      1. X\n      7230(100.0%)\n      \n      2770\n(27.7%)\n    \n    \n      PRIMARY-05/04/2021\n[character]\n      1. D2. R3. X\n      2(0.6%)25(7.9%)289(91.5%)\n      \n      9684\n(96.8%)\n    \n    \n      PRIMARY-08/03/2021\n[character]\n      1. D2. R\n      355(91.3%)34(8.7%)\n      \n      9611\n(96.1%)\n    \n    \n      PRIMARY-09/14/2021\n[character]\n      1. D2. X\n      7(2.4%)285(97.6%)\n      \n      9708\n(97.1%)\n    \n    \n      GENERAL-11/02/2021\n[character]\n      1. X\n      2546(100.0%)\n      \n      7454\n(74.5%)\n    \n    \n      PRIMARY-05/03/2022\n[character]\n      1. D2. R3. X\n      732(35.8%)1291(63.1%)23(1.1%)\n      \n      7954\n(79.5%)\n    \n    \n      PRIMARY-08/02/2022\n[character]\n      1. D2. R3. X\n      474(47.3%)525(52.4%)3(0.3%)\n      \n      8998\n(90.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-10-30\n\n\n\nThis sample of 10k is 46.3% and 53.7% Republican.\nOhio actually has open primaries which is a good reason to consider another state for this dataset. I’ll be looking for others."
  },
  {
    "objectID": "posts/Final_SteveONeill.html#next-steps",
    "href": "posts/Final_SteveONeill.html#next-steps",
    "title": "Final Part 1",
    "section": "Next Steps",
    "text": "Next Steps\nNext, I will find the most advantageous state with a closed primary. Ideally, it will be a battleground or solid-Republican state with freely available voter registration data and a downloadable Corporation Search database (or available upon request).\nI look forward to any feedback or refinements to the hypotheses above."
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html",
    "href": "posts/Buck_Yoon_finalpart1.html",
    "title": "finalpart1",
    "section": "",
    "text": "we are going to be using the National Longitudinal Study of Adolescent to Adult Health, 1994-2018 we are interested in exploring the relation between education levels and health.\n#Some of our research questions are:\nWhat is the correlation and relationship between someone’s education and health? Does the type and duration of education matter? Are there fields that may be “more healthy”? How does the relationship between education and health differ among the education levels/ is there a difference?\nWhat does this data set have to say to a possible causal link between education and health? Does the data set provide apt data to establish a causal link?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "href": "posts/Buck_Yoon_finalpart1.html#hypothesis",
    "title": "finalpart1",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are going to be using the hypothesis from researchers Eric R. Ride and Mark H. Showalter, but using the data from the National Longitudinal Study\nThere hypothesis was: ’The empirical link between education and health is firmly established. Numerous studies document that higher levels of education are positively associated with longer life and better health throughout the lifespan…But measuring the causal links between education and health is a more challenging task.” Estimating the relation between health and education: what do we know and what do we need to know?\nWe are hypothesizing that a positive correlation exists between education and health; the more education an individual receives, the better health the individual may have.\nWe want to look at the National Longitudinal Study of Adolescent to Adult Health 1992-2018 and observe what other factors beyond education there is that can affect the correlation to health. What are the potential moderating or mediating variables?"
  },
  {
    "objectID": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "href": "posts/Buck_Yoon_finalpart1.html#descriptive-statistics",
    "title": "finalpart1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThis is an overview of the entire data set we are still determining which specific sections we want to analyze for our final project.\nAccording to ICPSR:\nStudy Purpose: Add Health was developed in response to a mandate from the U.S. Congress to fund a study of adolescent health. Waves I and II focused on the forces that may influence adolescents’ health and risk behaviors, including personal traits, families, friendships, romantic relationships, peer groups, schools, neighborhoods, and communities. As participants aged into adulthood, the scientific goals of the study expanded and evolved. Wave III explored adolescent experiences and behaviors related to decisions, behavior, and health outcomes in the transition to adulthood. Wave IV expanded to examine developmental and health trajectories across the life course of adolescence into young adulthood, using an integrative study design which combined social, behavioral, and biomedical measures data collection. Wave V aimed to track the emergence of chronic disease as the cohort aged into their 30s and early 40s.\nStudy Design: Add health is a school-based longitudinal study of a nationally-representative sample of adolescents in grates 7-12 in the United States in 1945-45. Over more than 20 years of data collection, data have been collected from adolescents, their fellow students, school administrators, parents, siblings, friends, and romantic partners through multiple data collection components. In addition, existing databases with information about respondents’ neighborhoods and communities have been merged with Add Health data, including variables on income poverty, unemployment, availability and utilization of health services, crime, church membership, and social programs and policies.\nSample:\n\nWave I: The Stage 1 in-school sample was a stratified, random sample of all high schools in the United States. A school was eligible for the sample if it included an 11th grade and had a minimum enrollment of 30 students. A feeder school – a school that sent graduates to the high school and that included a 7th grade – was also recruited from the community. The in-school questionnaire was administered to more than 90,000 students in grades 7 through 12. The Stage 2 in-home sample of 27,000 adolescents consisted of a core sample from each community, plus selected special over samples. Eligibility for over samples was determined by an adolescent’s responses on the in-school questionnaire. Adolescents could qualify for more than one sample.\nWave II: The Wave II in-home interview surveyed almost 15,000 of the same students one year after Wave I.\nWave III: The in-home Wave III sample consists of over 15,000 Wave I respondents who could be located and re-interviewed six years later.\nWave IV: All original Wave I in-home respondents were eligible for in-home interviews at Wave IV. At Wave IV, the Add Health sample was dispersed across the nation with respondents living in all 50 states. Administrators were able to locate 92.5% of the Wave IV sample and interviewed 80.3% of eligible sample members.\nWave V: All Wave I respondents who were still living were eligible at Wave V, yielding a pool of 19,828 persons. This pool was split into three stratified random samples for the purposes of survey design testing.\nTime Method: Longitudinal:Panel\nUniverse: Adolescents in grades 7 through 12 during the 1994-1995 school year. Respondents were geographically located in the United States.\nUnits of Observation: Individual\nData Types: Survey Data\nTime periods: 1994 - 2018\nDate of Collections: Wave 1(1994-01 - 1995-12), Wave II(1996-04 - 1996-09), Wave III(2001-04 - 2002 -04), Wave IV(2007-04 - 2009-01), Wave V(2016-03 - 2018-11)\nResponse Rates: Wave 1(79%), Wave 2(88.6%), Wave III(77.4%), Wave IV(80.3%), Wave V(71.8%)."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html",
    "href": "posts/HW1_Solutions_OmerYalcin.html",
    "title": "Homework 1",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(magrittr)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap, xlab = 'Lung Capacity', main = '', freq = F)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap ~ Gender, data = df)\n\n\n\n\n\nThe shape of the distribution is similar for males and females. The median, first quartile, third quartile lung capacity values all seem to be somewhat higher for males.\n\n\n\n\n\nCode\ndf %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       7.77\n2 yes      8.65\n\n\nThe lung capacity for smokers seems to be higher than non-smokers. It goes against the common idea that smoking would hurt lung capacity.\n\n\n\n\nLess than or equal to 13\n\n\n\nCode\ndf %>%\n  filter(Age <= 13) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       6.36\n2 yes      7.20\n\n\n\n14 to 15\n\n\n\nCode\ndf %>%\n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       9.14\n2 yes      8.39\n\n\n\n16 to 17\n\n\n\nCode\ndf %>%\n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no      10.5 \n2 yes      9.38\n\n\n\nGreater than or equal to 18\n\n\n\nCode\ndf %>%\n  filter(Age >= 18) %>%\n  group_by(Smoke) %>%\n  summarize(LungCap = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke LungCap\n  <chr>   <dbl>\n1 no       11.1\n2 yes      10.5\n\n\n\n\n\nFor three out of the four groups, lung capacity if smaller for smokers. This makes another explanation plausible. Smoking is inversely related to lung capacity, but older people both smoke more and have more lung capacity. Thus, considering the relationship between smoking and lung capacity without looking at age makes the relationship look the opposite of what it is.\n\n\n\n\n\nCode\ncov(df$LungCap, df$Age)\n\n\n[1] 8.738289\n\n\nCode\ncor(df$LungCap, df$Age)\n\n\n[1] 0.8196749\n\n\nBoth the correlation and the covariance are positive (when one of them is the other has to). Positive values suggest that people who are older tend to have higher lung capacity, confirming what we found. Since correlation is standardized (needs to be between -1 and 1), its absolute value tells us about the strength of the relationship. 0.82 suggests a pretty strong relationship."
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#a-1",
    "title": "Homework 1",
    "section": "a",
    "text": "a\n\n\nCode\ntb %>%\n  filter(X == 2) %>%\n  pull(Frequency) %>%\n  divide_by(n)\n\n\n[1] 0.1975309"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#b-1",
    "title": "Homework 1",
    "section": "b",
    "text": "b\n\n\nCode\ntb %>%\n  filter(X < 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.6938272"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#c-1",
    "title": "Homework 1",
    "section": "c",
    "text": "c\n\n\nCode\ntb %>%\n  filter(X <= 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.891358"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#d-1",
    "title": "Homework 1",
    "section": "d",
    "text": "d\n\n\nCode\ntb %>%\n  filter(X > 2) %>%\n  pull(Frequency) %>%\n  sum() %>%\n  divide_by(n)\n\n\n[1] 0.108642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#e-1",
    "title": "Homework 1",
    "section": "e",
    "text": "e\nExpected number of prior convictions is just a weighted average of the number of prior convictions.\n\nMethod 1: Multiply every value with their frequency, then divide by total frequency i.e. (0 * 128 + 1 * 434 + 2 * 160 ……) / 810.\n\n\n\nCode\nsum(tb$X * tb$Frequency) / n\n\n\n[1] 1.28642\n\n\n\nMethod 2: Multiply every value with their probility, sum them up.\n\n\n\nCode\ntb %>%\n  mutate(probability = Frequency / n) -> tb\n\nprint(tb)\n\n\n# A tibble: 5 × 3\n      X Frequency probability\n  <dbl>     <dbl>       <dbl>\n1     0       128      0.158 \n2     1       434      0.536 \n3     2       160      0.198 \n4     3        64      0.0790\n5     4        24      0.0296\n\n\n\n\nCode\nsum(tb$X * tb$probability)\n\n\n[1] 1.28642\n\n\n\nMethod 3: Recreate the whole sample (a vector that has 128 zeroes, 434 ones, 160 twos, ….) with a total length/size of 810. Take the mean.\n\n\n\nCode\nsample <- c(rep(0, 128), rep(1, 434), rep(2, 160), rep(3, 64), rep(4, 24))\nmean(sample)\n\n\n[1] 1.28642"
  },
  {
    "objectID": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "href": "posts/HW1_Solutions_OmerYalcin.html#f-1",
    "title": "Homework 1",
    "section": "f",
    "text": "f\n\nMethod 1: Let’s start from the end: we have the sample, just call var() and sd()\n\n\n\nCode\ncat('Variance:', var(sample))\n\n\nVariance: 0.8572937\n\n\nCode\ncat('\\nStandard Deviation:', sd(sample))\n\n\n\nStandard Deviation: 0.9259016\n\n\nMethod 2: Manually apply the formula using weights.\nStandard deviation is square root of variance. So let’s calculate variance first. For that we need the mean. Let’s pull the expected value from the previous section:\n\n\nCode\nm <- sum(tb$X * tb$Frequency) / n\n\n\nFor every observation, we’ll need the squared difference from mean (squared deviation from mean).\n\n\nCode\ntb %>%\n  mutate(sq_deviation = (X - m)^2) -> tb \nprint(tb)\n\n\n# A tibble: 5 × 4\n      X Frequency probability sq_deviation\n  <dbl>     <dbl>       <dbl>        <dbl>\n1     0       128      0.158        1.65  \n2     1       434      0.536        0.0820\n3     2       160      0.198        0.509 \n4     3        64      0.0790       2.94  \n5     4        24      0.0296       7.36  \n\n\nThen, we can now multiply them with probability.\n\n\nCode\nsum(tb$sq_deviation * tb$probability)\n\n\n[1] 0.8562353\n\n\nThis gives us the ‘population’ variance. If we wanted the ‘sample’ variance, what the var() function does, we could manually apply the Bessel’s correction:\n\n\nCode\nvariance <- sum(tb$sq_deviation * tb$probability) * (n / (n-1))\nprint(variance)\n\n\n[1] 0.8572937\n\n\nStandard deviation is then just the square root:\n\n\nCode\nsqrt(variance)\n\n\n[1] 0.9259016\n\n\nThis replicated what we found directly using the sample."
  },
  {
    "objectID": "posts/Project_Yakub Rabiutheen.html",
    "href": "posts/Project_Yakub Rabiutheen.html",
    "title": "Project Rough Draft Proposal",
    "section": "",
    "text": "Hypopthesis\nThis research project will be testing two hypothesis regarding Britain and France.\n#Colonial Powers Hypopthesis. 1. The Years that France and Britain had more Exports is when the rate of colonization increased. 2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\nCode\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\n\n\nError: `path` does not exist: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls'\n\n\nCode\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv' does not exist.\n\n\nCode\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n\n\nError: 'C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv' does not exist.\n\n\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\n\n\nCode\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\n\n\nError in colnames(Colonial_Years)[3] <- \"Colonizing Country\": object 'Colonial_Years' not found\n\n\nCode\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\n\n\nError in colnames(Colonial_Years)[4] <- \"Year_Colonization_Began\": object 'Colonial_Years' not found\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\n\n\nError in select(Colonial_Years, \"Colonizing Country\", \"Year_Colonization_Began\"): object 'Colonial_Years' not found\n\n\nCode\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\n\n\nError in filter(Colonial_Years, `Colonizing Country` %in% c(\"F\", \"UK\")): object 'Colonial_Years' not found\n\n\nCode\ntable(Colonial_Years)\n\n\nError in table(Colonial_Years): object 'Colonial_Years' not found\n\n\n\n\nCode\nImports_Exports%>% filter(year < '1960') \n\n\nError in filter(., year < \"1960\"): object 'Imports_Exports' not found\n\n\n\n\nCode\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\n\n\nError in filter(Imports_Exports, stateabb %in% c(\"FRN\", \"UKG\")): object 'Imports_Exports' not found\n\n\n\n\nCode\noptions(scipen = 999)    \n\n\nCreated a Forumula to calculate Trade Surplus and Deficits.\n\n\nCode\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\n\n\nError in eval(expr, envir, enclos): object 'colonial_trade' not found\n\n\nFound a better way to find years that France and Britain were running Trade Deficits.\n\n\nCode\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports < colonial_trade$imports, : object 'colonial_trade' not found\n\n\nI did the inverse to find that the UK has always had a Trade Deficit\n\n\nCode\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\n\n\nError in print(colonial_trade[colonial_trade$exports > colonial_trade$imports, : object 'colonial_trade' not found\n\n\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-sidebar-title\"}\n[Project Rough Draft Proposal]{.hidden render-id=\"quarto-int-navbar-title\"}\n[Fall 2022 Posts]{.hidden render-id=\"quarto-int-navbar:Fall 2022 Posts\"}\n[Contributors]{.hidden render-id=\"quarto-int-navbar:Contributors\"}\n[DACSS]{.hidden render-id=\"quarto-int-navbar:DACSS\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-metatitle\"}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-twittercardtitle\"}\n[ - Project Rough Draft Proposal]{.hidden render-id=\"quarto-ogcardtitle\"}\n[International Trade's influence on War]{.hidden render-id=\"quarto-metadesc\"}\n:::\n\n\n\n\n<!-- -->\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Project Rough Draft Proposal\"\nauthor: \"Yakub Rabiutheen\"\ndescription: \"International Trade's influence on War\"\ndate: \"10/11/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - finalproject1\n  - desriptive statistics \n  - probability\n---\n\n# Research Question\n\nHow has international trade influenced how countries interact with each other? This research project looks specifically at  France and Britain which  are grouped together as Colonial Powers to explore the relationship of Colonialism and international trade. This research project will be looking at data from the Correlates Of War Project, which has international trade data from 1870 to 2015. The cut-off year for this research project will be 1960, as  on December 14,1960, the UN declared Colonialism was a human's right's violation and legally declared Colonialism was over(McWhinney,1960).   \n\n\n# Hypopthesis\n\nThis research project will be testing two hypothesis regarding Britain and France.\n\n#Colonial Powers Hypopthesis.\n1. The Years that France and Britain had more Exports is when the rate of colonization increased.\n2. The Years that France and Britain had more Iron Production correlates to the years France and Britain increased levels of colonization.\n\n\n\nquarto-executable-code-5450563D\n\n```r\nlibrary(readxl)\nlibrary(readr)\nColonial_Years <- read_excel(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Colonial_transformation_data.xls\")\nImports_Exports<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/Countries_Imports_Exports.csv\")\nmilitary_raw_metals<-read_csv(\"C:/Users/yakub/Documents/GitHub/603_Fall_2022/posts/_data/NMC_v4_0.csv\")\n#Descriptive Statistics.\nAs shown below, I tried finding Data regarding when colonialism began by France and Uk and seeing whether France and UK had more Trade Surpluses as they expanded their colonial empire. However, I was proven wrong as it appears that the UK has been running a Trade Deficit and has never had a Trade Surplus during their Colonial era pre-1960s. As such, I will have to change the approach of this research study. It appears the Balance of Trade has no relationship to Colonialism.\nquarto-executable-code-5450563D\ncolnames(Colonial_Years)[3] <- \"Colonizing Country\"\ncolnames(Colonial_Years)[4]<- \"Year_Colonization_Began\"\nquarto-executable-code-5450563D\nlibrary(dplyr)\nColonial_Years<-select(Colonial_Years,\"Colonizing Country\",\"Year_Colonization_Began\")\nColonial_Years<-filter(Colonial_Years,`Colonizing Country` %in% c(\"F\",\"UK\"))\ntable(Colonial_Years)\nquarto-executable-code-5450563D\nImports_Exports%>% filter(year < '1960') \nquarto-executable-code-5450563D\ncolonial_trade<-filter(Imports_Exports,`stateabb` %in% c(\"FRN\",\"UKG\"))\nquarto-executable-code-5450563D\noptions(scipen = 999)    \nCreated a Forumula to calculate Trade Surplus and Deficits.\nquarto-executable-code-5450563D\ncolonial_trade$trade_balance<-(colonial_trade$exports-colonial_trade$imports)\nFound a better way to find years that France and Britain were running Trade Deficits. quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports < colonial_trade$imports,] )\nI did the inverse to find that the UK has always had a Trade Deficit quarto-executable-code-5450563D\nprint(colonial_trade[colonial_trade$exports > colonial_trade$imports,] )\nMy finding has found that there is no relationship between Trade Deficits and Colonialism as the UK has never had a positive trade balance.\n##Conclusion\nI think that the approach of my research has to be changed as my initial theory about trade deficits and Colonialism has been disapprove. As such, I think I will shift this project towards a different approach. I will try exploring the historical prices of commodity goods when France and U.K. were colonial powers.\n\n\nReferences\nMcWhinney, E. (1960, December 14). Declaration on the granting of Independence to colonial countries and Peoples. United Nations. Retrieved October 10, 2022, from https://legal.un.org/avl/ha/dicc/dicc.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/KarenDetter_HW2.html",
    "href": "posts/KarenDetter_HW2.html",
    "title": "HW 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.1",
    "href": "posts/KarenDetter_HW2.html#q.1",
    "title": "HW 2",
    "section": "Q.1",
    "text": "Q.1\n- Construct the 90% confidence interval to estimate the actual mean wait time for each of the two procedures:\nBypass -\n\n\nCode\n#assign values\nBs_sd <- 10\nBs_size <- 539\nBs_mean <- 19\n#calculate standard error\nBstandard_error <- Bs_sd / sqrt(Bs_size)\nBstandard_error\n\n\n[1] 0.4307305\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nBtail_area <- (1-confidence_level)/2\nBtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nBt_score <- qt(p = 1-Btail_area, df = Bs_size-1)\nBt_score\n\n\n[1] 1.647691\n\n\n\n\nCode\n#calculate confidence interval\nBCI <- c(Bs_mean - Bt_score * Bstandard_error, Bs_mean + Bt_score * Bstandard_error)\nprint(BCI)\n\n\n[1] 18.29029 19.70971\n\n\nAngiography-\n\n\nCode\n#assign values\nAs_sd <- 9\nAs_size <- 847\nAs_mean <- 18\n#calculate standard error\nAstandard_error <- As_sd / sqrt(As_size)\nAstandard_error\n\n\n[1] 0.3092437\n\n\n\n\nCode\n#calculate area of two tails\nconfidence_level <- 0.90\nAtail_area <- (1-confidence_level)/2\nAtail_area\n\n\n[1] 0.05\n\n\n\n\nCode\n#calculate t-score\nAt_score <- qt(p = 1-Atail_area, df = As_size-1)\nAt_score\n\n\n[1] 1.646657\n\n\n\n\nCode\n#calculate confidence interval\nACI <- c(As_mean - At_score * Astandard_error, As_mean + At_score * Astandard_error)\nprint(ACI)\n\n\n[1] 17.49078 18.50922\n\n\n- Is the confidence interval narrower for angiography or bypass surgery?\n\n\nCode\n#calculate differences in upper and lower bounds of both confidence intervals\n(Bs_mean + Bt_score * Bstandard_error) - (Bs_mean - Bt_score * Bstandard_error)\n\n\n[1] 1.419421\n\n\nCode\n(As_mean + At_score * Astandard_error) - (As_mean - At_score * Astandard_error)\n\n\n[1] 1.018436\n\n\nAngiography has a narrower confidence interval."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.2",
    "href": "posts/KarenDetter_HW2.html#q.2",
    "title": "HW 2",
    "section": "Q.2",
    "text": "Q.2\n- Find the point estimate, p, of the proportion of all adult Americans who believe that a college education is essential for success.\n\n\nCode\n#assign values\nk <- 567\nn <- 1031\n#calculate sample proportion\np <- k/n\np\n\n\n[1] 0.5499515\n\n\n- Construct and interpret a 95% confidence interval for p\n\n\nCode\n#calculate margin of error\nmargin <- qnorm(0.975) * sqrt(p*(1-p)/n)\n#calculate lower and upper bounds of confidence interval\nlow <- p - margin\nhigh <- p + margin\nprint(low)\n\n\n[1] 0.5195839\n\n\nCode\nprint(high)\n\n\n[1] 0.5803191\n\n\nThe 95% confidence interval for the population proportion is [.52, .58]. Since 95% of confidence intervals calculated from point estimates of population proportions would contain the true mean population proportion, we can be reasonably confident that the true mean proportion of adult Americans who believe a college education is essential for success lies somewhere between 52 and 58%."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.3",
    "href": "posts/KarenDetter_HW2.html#q.3",
    "title": "HW 2",
    "section": "Q.3",
    "text": "Q.3\n- Assuming the significance level to be 5%, what should be the size of the sample?\n\n\nCode\n#assign values\nz_score <- qnorm(.975) #assuming normal distribution and 95% confidence level\nmargin_error <- 5 #half of confidence interval\n#calculate population standard deviation (one quarter of the range)\npop_sd <- (200-30) / 4\n\n\n\n\nCode\n#calculate sampling size of population mean\nsamp_size <- z_score^2 * pop_sd^2 / margin_error^2\nsamp_size\n\n\n[1] 277.5454\n\n\nThe sample size should be 278."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.4",
    "href": "posts/KarenDetter_HW2.html#q.4",
    "title": "HW 2",
    "section": "Q.4",
    "text": "Q.4\nA. Test whether the mean income of female employees differs from $500 per week. Include assumptions, hypotheses, test statistic, and P-value. Interpret the result.\nassumptions: random sampling, normally distributed data, adequate sample size; hypotheses: \\(H_{0}\\) : \\(\\bar{y}\\) = \\(\\mu\\) ; \\(H_{\\alpha}\\) : \\(\\bar{y}\\) \\(\\neq\\) \\(\\mu\\) ; test statistic: t-statistic\n\n\nCode\n#calculate t-statistic\nt_stat <- (410 - 500) / (90 / (sqrt(9)))\n#calculate two-tailed p-value\np_val <- 2 * (pt(q = t_stat, df=8))\np_val\n\n\n[1] 0.01707168\n\n\nAssuming \\(\\alpha\\) = .05, we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\).\nB. Report the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) < 500. Interpret.\n\n\nCode\n#calculate lower-tail p-value\np_low <- pt(t_stat, df = 8, lower.tail = TRUE)\np_low\n\n\n[1] 0.008535841\n\n\nThis p-value is significantly lower than the .05 significance level, which means that we can reject \\(H_{0}\\) because there is evidence to support \\(H_{\\alpha}\\) : \\(\\mu\\) < 500.\nC. Report and interpret the P-value for \\(H_{\\alpha}\\) : \\(\\mu\\) > 500.\n\n\nCode\n#calculate lower-tail p-value\np_high <- pt(t_stat, df = 8, lower.tail = FALSE)\np_high\n\n\n[1] 0.9914642\n\n\n\n\nCode\n#double-check p-values\ncheck <- p_high + p_low\ncheck\n\n\n[1] 1\n\n\nThis p-value is significantly higher than the .05 significance level, so in this case we fail to reject \\(H_{0}\\) in favor of \\(H_{\\alpha}\\) : \\(\\mu\\) > 500."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.5",
    "href": "posts/KarenDetter_HW2.html#q.5",
    "title": "HW 2",
    "section": "Q.5",
    "text": "Q.5\nA. Show that t = 1.95 and P-value = 0.051 for Jones Show that t = 1.97 and P-value = 0.049 for Smith\n\n\nCode\n#calculate t-statistics\nJones_t <- (519.5 - 500) / 10\nJones_t\n\n\n[1] 1.95\n\n\nCode\nSmith_t <- (519.7 - 500) / 10\nSmith_t\n\n\n[1] 1.97\n\n\n\n\nCode\n#calculate p-values\nJones_p <- 2 * (pt(q = Jones_t, df=999, lower.tail = FALSE))\nJones_p\n\n\n[1] 0.05145555\n\n\nCode\nSmith_p <- 2 * (pt(q = Smith_t, df=999, lower.tail = FALSE))\nSmith_p\n\n\n[1] 0.04911426\n\n\nB. Using α = 0.05, for each study indicate whether the result is “statistically significant.”\nAt this significance level, Smith’s study would be considered significant and allow for rejection of the null hypothesis. Jones’ study, however, would fail to reject the null.\nC. Using this example, explain the misleading aspects of reporting the result of a test as “P ≤ 0.05” versus “P > 0.05”, or as “reject H0” versus “Do not reject H0”, without reporting the actual P-value.\nThis example shows the importance of being specific and thorough in reporting the “significance” of study findings. Both Smith and Jones produced results very near the cutoff point for statistical significance, so it would be critical to know both the actual p-value AND the exact standard, \\(\\leq\\) or <, being used to interpret the results in order to assess the actual impact of the findings. Reporting only “reject” or “do not reject” the null hypothesis would also not provide the information needed to make a judgment of the meaning of the findings, as it would not provide any evidence in support of the claim."
  },
  {
    "objectID": "posts/KarenDetter_HW2.html#q.6",
    "href": "posts/KarenDetter_HW2.html#q.6",
    "title": "HW 2",
    "section": "Q.6",
    "text": "Q.6\nIs there enough evidence to conclude at a 95% confidence level that the average tax per gallon of gas in the US in 2005 was less than 45 cents? Explain.\n\n\nCode\n#assign values\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n#run one sample t-test\nt.test(gas_taxes, mu = 45, alternative = 'less')\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nAt the 95% confidence level, the p-value of \\(H_{\\alpha}\\) : \\(\\mu\\) < 45 is .04, indicating that we can reject \\(H_{0}\\). Additionally, 45 is above the upper bound of the confidence interval, which also supports the alternative hypothesis."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html",
    "href": "posts/HW1_PrahithaMovva.html",
    "title": "Homework 1 - Prahitha Movva",
    "section": "",
    "text": "First, let’s read in the data from the Excel file:\n\n\nCode\nlibrary(readxl)\ndf <- read_excel(\"_data/LungCapData.xls\")\n\n\nThe distribution of LungCap looks as follows:\n\n\nCode\nhist(df$LungCap)\n\n\n\n\n\nThe histogram suggests that the distribution is close to a normal distribution. Most of the observations are close to the mean. Very few observations are close to the margins (0 and 15).\n\n\n\n\n\nCode\nboxplot(LungCap~Gender, data = df)\n\n\n\n\n\nFrom the boxplots for gender above, we can see that males seem to have (slightly) higher lung capacity than females.\n\n\n\n\n\nCode\naggregate(data = df, LungCap~Smoke, mean)\n\n\n  Smoke  LungCap\n1    no 7.770188\n2   yes 8.645455\n\n\nThe mean lung capacity for smokers and nonsmokers seems to be higher for smokers. This does not make sense as we generally expect smokers to have a reduced lung capacity due to the damage from smoking.\n\n\n\n\n\nCode\ndf_ageGroups <- mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == 14 | Age == 15 ~ \"14 to 15\", Age == 16 | Age == 17 ~ \"16 to 17\", Age >= 18 ~ \"18 and above\"))\n\n\nError in mutate(df, AgeGroup = case_when(Age <= 13 ~ \"13 and below\", Age == : could not find function \"mutate\"\n\n\nCode\nggplot(df_ageGroups, aes(x = LungCap)) +\n  geom_histogram() +\n  facet_grid(AgeGroup~Smoke)\n\n\nError in ggplot(df_ageGroups, aes(x = LungCap)): could not find function \"ggplot\"\n\n\nWe see non-smokers to have a higher lung capacity than smokers, as expected.\n\n\n\nLung capacity seems to be directly proportional to age and after breaking down the data by age groups, we see that the lung capacities for non-smokers are higher than those of smokers in the same age group (except for less than or equal to 13). This could be because of the total number of observations in each age group. The age group less than or equal to 13 has the highest number of observations - thereby skewing the results (here, mean) for the entire distribution.\n\n\n\n\n\nCode\ncor(x= df$LungCap, y = df$Age)\n\n\n[1] 0.8196749\n\n\nCode\ncov(x= df$LungCap, y = df$Age)\n\n\n[1] 8.738289\n\n\nLung capacity seems to be positively correlated with age i.e., as age increases, lung capacity increases. Same is the case with covariance."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#a-1",
    "href": "posts/HW1_PrahithaMovva.html#a-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "a",
    "text": "a\n\n\nCode\na <- 160/810\n\n\nThe probability that a randomly selected inmate has exactly 2 prior convictions is 0.1975309."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#b-1",
    "href": "posts/HW1_PrahithaMovva.html#b-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "b",
    "text": "b\n\n\nCode\nb <- (128+434)/810\n\n\nThe probability that a randomly selected inmate has fewer than 2 prior convictions is 0.6938272."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#c-1",
    "href": "posts/HW1_PrahithaMovva.html#c-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "c",
    "text": "c\n\n\nCode\nc <- (128+434+160)/810\n\n\nThe probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891358."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#d-1",
    "href": "posts/HW1_PrahithaMovva.html#d-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "d",
    "text": "d\n\n\nCode\nd <- (64+24)/810\n\n\nThe probability that a randomly selected inmate has more than 2 prior convictions is 0.108642."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#e-1",
    "href": "posts/HW1_PrahithaMovva.html#e-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "e",
    "text": "e\n\n\nCode\ne <- (0*(128/810)) + (1*(434/810)) + (2*(160/810)) + (3*(64/810)) + (4*(24/810))\n\n\nThe expected value for the number of prior convictions is 1.2864198 or 1, as the number of convictions cannot be a float."
  },
  {
    "objectID": "posts/HW1_PrahithaMovva.html#f-1",
    "href": "posts/HW1_PrahithaMovva.html#f-1",
    "title": "Homework 1 - Prahitha Movva",
    "section": "f",
    "text": "f\n\n\nCode\nvar_0 <- ((0-e)^2) * (128/810)\nvar_1 <- ((1-e)^2) * (434/810)\nvar_2 <- ((2-e)^2) * (160/810)\nvar_3 <- ((3-e)^2) * (64/810)\nvar_4 <- ((4-e)^2) * (24/810)\n\nvar <- var_0 + var_1 + var_2 + var_3 + var_4\n\nsd <- sqrt(var)\n\n\nFor prior convictions, the variance is 0.8562353 and the standard deviation is 0.9253298."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html",
    "href": "posts/HW2_EmmaRasmussen.html",
    "title": "Homework 2",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section",
    "href": "posts/HW2_EmmaRasmussen.html#section",
    "title": "Homework 2",
    "section": "1.",
    "text": "1.\n\n\nCode\n#Bypass\n#calculating t-score for 90% confidence interval\ntscoreb<- qt(p=1-.05, df=539-1)\n\n#calculating standard error\nseb<- 10/sqrt(539)\n\nmeanb<- 19\n\nCIb<- c(meanb- (tscoreb*seb), meanb+ (tscoreb*seb))\nCIb\n\n\n[1] 18.29029 19.70971\n\n\nCode\n#Angiography\n#calculating t-score for 90% confidence interval\ntscorea<- qt(p=1-.05, df=847-1)\n\n#calculating standard error\nsea<- 9/sqrt(847)\n\nmeana<- 18\n\nCIa<- c(meana- (tscorea*sea), meana+ (tscorea*sea))\nCIa\n\n\n[1] 17.49078 18.50922\n\n\nThe 90% confidence interval for bypass is [18.29, 19.71] days. The 90% confidence interval for angiography is [17.49, 18.51] days. The confidence interval for angiography is narrower, which makes sense given it has a (slightly) smaller standard deviation and a larger sample size (larger sample size reduces margin of error)."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-1",
    "href": "posts/HW2_EmmaRasmussen.html#section-1",
    "title": "Homework 2",
    "section": "2.",
    "text": "2.\n\n\nCode\n#assigning n= number of trials\nn<- 1031\n#assigning k= number agree\nk<- 567\n\n#calculating point estimate\np<- 567/1031\np\n\n\n[1] 0.5499515\n\n\nCode\n#calculating margin of error for 95% CI. I have no idea how to calculate a confidence interval without a sd. I found this formula online.\nmargin<- qnorm(0.975)*sqrt(p*(1-p)/n)\nmargin\n\n\n[1] 0.03036761\n\n\nCode\nCI<- c(p-margin, p+ margin)\nCI\n\n\n[1] 0.5195839 0.5803191\n\n\nThe 95% confidence interval for American’s agreeing that college education is essential for success is [51.96, 58.03]%. The point estimate for this value based on the survey is 55%."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-2",
    "href": "posts/HW2_EmmaRasmussen.html#section-2",
    "title": "Homework 2",
    "section": "3.",
    "text": "3.\n\n\nCode\n#estimating population standard deviation\n(200-30)/4\n\n\n[1] 42.5\n\n\nCode\n#solving for n in the equation for confidence intervals 5=1.96*(42.5/sqrt(n))\n#n= 277.56 or 278\n\n\nBy plugging in 5 to the formula for confidence intervals (the t-value for 95%, and the standard deviation estimate of 42.5) we get a value of n=277.56 or 278 need to be in the sample to retrieve a confidence interval of width 10."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-3",
    "href": "posts/HW2_EmmaRasmussen.html#section-3",
    "title": "Homework 2",
    "section": "4.",
    "text": "4.\n\na.\n\nAssume data is normally distributed\nHo: μ =500\nHa: μ not equal to 500 μ < 500 μ > 500\nalpha level =0.05\n\n\n\nCode\n#calculating the standard error\nsef<- 90/sqrt(9)\nsef\n\n\n[1] 30\n\n\nCode\n#calculating t-score\ntf<-(410-500)/(sef)\ntf \n\n\n[1] -3\n\n\nCode\n#calculating the p-value from the test statistic (multiply times two because we are doing a two-sided test)\n(pt(q=-3, df=8))*2\n\n\n[1] 0.01707168\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or lower, as the default calculates the lower tail)\npt(-3, 8)\n\n\n[1] 0.008535841\n\n\nCode\n#this represents the probability of getting a random sample from the population with a mean of 410 or higher, as we included lower.tail=FALSE)\npt(-3, 8, lower.tail=FALSE)\n\n\n[1] 0.9914642\n\n\nCode\n# since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than 500 a week\n\n\nTwo-sided: Since our p-value is 0.017 we can reject the null hypothesis at alpha level=0.05. We have evidence that the mean weekly earnings for women at this company is different from $500. ### b. Lower tail: Since our p-value (0.0085) is less than the alpha level of 0.05, we can reject the null. We have evidence that the mean weekly earnings at this company for women is less than $500. ### c.  Upper tail: Since our p-value is 0.99 we do not have evidence that the mean income of female employees is greater than $500 a week"
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-4",
    "href": "posts/HW2_EmmaRasmussen.html#section-4",
    "title": "Homework 2",
    "section": "5.",
    "text": "5.\n\na.\n\n\nCode\n#Jones\n#calculating t-score\n(519.5-500)/(10)\n\n\n[1] 1.95\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.95, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.05145555\n\n\nCode\n#Smith\n##calculating t-score\n(519.7-500)/(10)\n\n\n[1] 1.97\n\n\nCode\n#Calculating from p value from t-score. Because it is a two sided test, we multiply the result times two.\n(pt(q=1.97, df=999, lower.tail=FALSE))*2\n\n\n[1] 0.04911426\n\n\n\n\nb.\nAt the .05 significance level, Jones’ findings are not significant but Smith’s findings are.\n\n\nc.\nThis example shows that there is a very find line between rejecting and not rejecting the null hypothesis. Their findings were extremely similar, the means are different by only 0.2. In this way, reporting the p-value retrieved is actually really important to make this distinction. Similarly, Jones’ findings would have been significant at the 0.1 significance level, so rejecting or not rejecting the null hypothesis based on a p-value can be fairly arbitrary."
  },
  {
    "objectID": "posts/HW2_EmmaRasmussen.html#section-5",
    "href": "posts/HW2_EmmaRasmussen.html#section-5",
    "title": "Homework 2",
    "section": "6.",
    "text": "6.\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\nt.test(gas_taxes, mu=45.0, alternative=\"less\")\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = -1.8857, df = 17, p-value = 0.03827\nalternative hypothesis: true mean is less than 45\n95 percent confidence interval:\n     -Inf 44.67946\nsample estimates:\nmean of x \n 40.86278 \n\n\nUsing a 95% confidence level we get a p-value of 0.038. We reject the null hypothesis. We have evidence that the average tax per gallon of gas in the U.S. is less than 45 cents."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html",
    "href": "posts/FinalPart1_CalebHill.html",
    "title": "Final Part 1",
    "section": "",
    "text": "Multiple research reports state that there is a relationship between re-hospitalization rates and social characteristics, such as demographic and economic identifiers, (Barnett, Hsu & McWilliams, 2015; Murray, Allen, Clark, Daly & Jacobs, 2021). Specifically, racial characteristics play a large role in predicting re-hospitalization in a population (Li, Cai & Glance, 2015). While some articles examine economic and health factors contributing to these disparities, very few dig deep into environmental factors that influence this phenomenon, (Spatz, Bernheim, Horwitz & Herrin, 2020). With your zipcode affecting up to 60% of your health outcomes, this research is relevant to better improving one of our most costly health expenditures: hospitalization.\nThis paper aims to explore how different environmental variables impact re-hospitalization rates on a county-by-county level, controlling for racial, ethnic, and sex variables (maybe). These environmental factors will include both common environmental concerns, such as heat index, average temperature, precipitation, and natural disasters, along with the built environment, population density.\nThe data-set chosen for this analysis is taken from the Agency for Healthcare Research and Quality, Social Determinants of Health (SDOH) Database. This data-set has over 300 variables to explore each SDOH domain: social context, economic context, education, healthcare, and the environment. We shall pull data from three of these five domains: social, economic, and environmental.\nTo further reduce data bloat, we shall limit the geographic review to Texas counties – my home state! That should provide us with 200+ observations.\nThe hypothesis for this research report is: *Environmental factors increase rates of re-hospitalization in Texas counties.\nTherefore, the null hypothesis is: *Environmental factors do not increase rates of re-hospitalization in Texas counties.\nMultiple regression analyses shall be employed to determine the relationship – or lack thereof – between these variables.\nFirst I’ll import the relevant libraries.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(corrplot)\n\n\nError in library(corrplot): there is no package called 'corrplot'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n\n\nThen I’ll import the dataset and view the first six rows.\n\n\nCode\ndf <- SDOH_2020_COUNTY_1_0 <- read_excel(\"_data/SDOH_2020_COUNTY_1_0.xlsx\", sheet = \"Data\")\n\n\nWarning: Expecting logical in OA1673 / R1673C391: got '46123'\n\n\nWarning: Expecting logical in OA1765 / R1765C391: got '32510'\n\n\nWarning: Expecting logical in OB1765 / R1765C392: got '41025'\n\n\nWarning: Expecting logical in OC1765 / R1765C393: got '41037'\n\n\nWarning: Expecting logical in OA2799 / R2799C391: got '49017'\n\n\nWarning: Expecting logical in OB2799 / R2799C392: got '49019'\n\n\nWarning: Expecting logical in OC2799 / R2799C393: got '49025'\n\n\nWarning: Expecting logical in OD2799 / R2799C394: got '49055'\n\n\nWarning: Expecting logical in OA2844 / R2844C391: got '51760'\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 685\n   YEAR COUNTYFIPS STATEFIPS STATE COUNTY REGION TERRI…¹ ACS_T…² ACS_T…³ ACS_T…⁴\n  <dbl> <chr>      <chr>     <chr> <chr>  <chr>    <dbl>   <dbl>   <dbl>   <dbl>\n1  2020 01001      01        Alab… Autau… South        0   55639   54929   52404\n2  2020 01003      01        Alab… Baldw… South        0  218289  216518  206329\n3  2020 01005      01        Alab… Barbo… South        0   25026   24792   23694\n4  2020 01007      01        Alab… Bibb … South        0   22374   22073   21121\n5  2020 01009      01        Alab… Bloun… South        0   57755   57164   54250\n6  2020 01011      01        Alab… Bullo… South        0   10173   10143    9579\n# … with 675 more variables: ACS_TOT_POP_ABOVE15 <dbl>,\n#   ACS_TOT_POP_ABOVE16 <dbl>, ACS_TOT_POP_16_19 <dbl>,\n#   ACS_TOT_POP_ABOVE25 <dbl>, ACS_TOT_CIVIL_POP_ABOVE18 <dbl>,\n#   ACS_TOT_CIVIL_VET_POP_ABOVE25 <dbl>, ACS_TOT_OWN_CHILD_BELOW17 <dbl>,\n#   ACS_TOT_WORKER_NWFH <dbl>, ACS_TOT_WORKER_HH <dbl>,\n#   ACS_TOT_CIVILIAN_LABOR <dbl>, ACS_TOT_CIVIL_EMPLOY_POP <dbl>,\n#   ACS_TOT_POP_POV <dbl>, ACS_TOT_CIVIL_NONINST_POP_POV <dbl>, …\n\n\nNext I want to verify the class is a dataframe. Otherwise, I’ll need to transform the data to make it easier to work with.\n\n\nCode\nclass(df)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nAll good here.\nNow on to data transformation. We will need to select only the relevant columns for this analysis and filter by Texas, bringing the observations (rows) down to 254.\n\n\nCode\ndf_new <- df %>%\n  select(COUNTYFIPS,\n         STATE,\n         COUNTY,\n         ACS_TOT_POP_WT,\n         ACS_PCT_MALE,\n         ACS_PCT_FEMALE,\n         ACS_PCT_AIAN,\n         ACS_PCT_ASIAN,\n         ACS_PCT_BLACK,\n         ACS_PCT_HISPANIC,\n         ACS_PCT_MULT_RACE,\n         ACS_PCT_NHPI,\n         ACS_PCT_OTHER_RACE,\n         ACS_PCT_WHITE,\n         CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  filter(STATE == \"Texas\")\nhead(df_new)\n\n\n# A tibble: 6 × 22\n  COUNTYF…¹ STATE COUNTY ACS_T…² ACS_P…³ ACS_P…⁴ ACS_P…⁵ ACS_P…⁶ ACS_P…⁷ ACS_P…⁸\n  <chr>     <chr> <chr>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 48001     Texas Ander…   57917    61.2    38.8    0.41    0.6    20.9    17.9 \n2 48003     Texas Andre…   18227    49.6    50.4    0       0.31    2.29   56.9 \n3 48005     Texas Angel…   87119    48.9    51.1    0.31    1.11   15.1    22.3 \n4 48007     Texas Arans…   24220    49.5    50.5    0.9     1.14    0.33   28.0 \n5 48009     Texas Arche…    8754    50.2    49.8    1.46    0.17    1.1     8.21\n6 48011     Texas Armst…    1950    45.7    54.3    0.77    0       0.72    8.46\n# … with 12 more variables: ACS_PCT_MULT_RACE <dbl>, ACS_PCT_NHPI <dbl>,\n#   ACS_PCT_OTHER_RACE <dbl>, ACS_PCT_WHITE <dbl>, CEN_POPDENSITY_COUNTY <dbl>,\n#   NEPHTN_HEATIND_105 <dbl>, NOAAC_AVG_TEMP_YEARLY <dbl>,\n#   NOAAC_PRECIPITATION_AVG_YEARLY <dbl>, NOAAS_TOT_NATURAL_DISASTERS <dbl>,\n#   SAIPE_MEDIAN_HH_INCOME <dbl>, SAIPE_PCT_POV <dbl>,\n#   LTC_AVG_OBS_REHOSP_RATE <dbl>, and abbreviated variable names ¹​COUNTYFIPS,\n#   ²​ACS_TOT_POP_WT, ³​ACS_PCT_MALE, ⁴​ACS_PCT_FEMALE, ⁵​ACS_PCT_AIAN, …\n\n\nOut of 300+ variables, we’ve whittled them down to 22. Of those 22, we have three (3) that are unique identifiers (FIPS, State, and County), 11 that are potential control variables (population, gender, and race / ethnicity), and eight (8) that we can explore (Population Density to Re-hospitalization Rate).\nBefore we launch into exploring these eight variables via descriptive statistics, first we need to determine where the NAs are and see if any of the variables will have a substantial amount of missing data.\n\n\nCode\ncolSums(is.na(df_new))\n\n\n                    COUNTYFIPS                          STATE \n                             0                              0 \n                        COUNTY                 ACS_TOT_POP_WT \n                             0                              0 \n                  ACS_PCT_MALE                 ACS_PCT_FEMALE \n                             0                              0 \n                  ACS_PCT_AIAN                  ACS_PCT_ASIAN \n                             0                              0 \n                 ACS_PCT_BLACK               ACS_PCT_HISPANIC \n                             0                              0 \n             ACS_PCT_MULT_RACE                   ACS_PCT_NHPI \n                             0                              0 \n            ACS_PCT_OTHER_RACE                  ACS_PCT_WHITE \n                             0                              0 \n         CEN_POPDENSITY_COUNTY             NEPHTN_HEATIND_105 \n                             0                              0 \n         NOAAC_AVG_TEMP_YEARLY NOAAC_PRECIPITATION_AVG_YEARLY \n                             0                              0 \n   NOAAS_TOT_NATURAL_DISASTERS         SAIPE_MEDIAN_HH_INCOME \n                             0                              0 \n                 SAIPE_PCT_POV        LTC_AVG_OBS_REHOSP_RATE \n                             0                             44 \n\n\nThis is not ideal, as that’s our dependent variable. However, 44 / 254 is not bad. That still leaves us with plenty of counties to review.\n\n\nCode\ndf_new %>%\n  drop_na() %>%\n  print(nrow(df_new))\n\n\n# A tibble: 210 × 22\n   COUNTYFIPS STATE COUNTY           ACS_TOT_POP_WT ACS_PCT_MALE ACS_PCT_FEMALE\n   <chr>      <chr> <chr>                     <dbl>        <dbl>          <dbl>\n 1 48001      Texas Anderson County           57917         61.2           38.8\n 2 48003      Texas Andrews County            18227         49.6           50.4\n 3 48005      Texas Angelina County           87119         48.9           51.1\n 4 48007      Texas Aransas County            24220         49.5           50.5\n 5 48011      Texas Armstrong County           1950         45.7           54.3\n 6 48013      Texas Atascosa County           50194         50.2           49.8\n 7 48015      Texas Austin County             29892         49.9           50.1\n 8 48017      Texas Bailey County              6916         50.0           50.0\n 9 48019      Texas Bandera County            22770         49.8           50.2\n10 48021      Texas Bastrop County            86839         50.8           49.2\n   ACS_PCT_AIAN ACS_PCT_ASIAN ACS_PCT_BLACK ACS_PCT_HISPANIC ACS_PCT_MULT_RACE\n          <dbl>         <dbl>         <dbl>            <dbl>             <dbl>\n 1         0.41          0.6          20.9             17.9               4.46\n 2         0             0.31          2.29            56.9               5.76\n 3         0.31          1.11         15.1             22.3               3.21\n 4         0.9           1.14          0.33            28.0               6.2 \n 5         0.77          0             0.72             8.46              5.33\n 6         0.08          0.5           1.08            64.7              11.4 \n 7         0.14          0.55          8.77            27.2               2.96\n 8         1             0.68          0.29            65.8               0.49\n 9         1.2           0.34          0.75            19.3               5.97\n10         0.53          0.84          7.83            38.8               6.98\n   ACS_PCT_NHPI ACS_PCT_OTHER_RACE ACS_PCT_WHITE CEN_POPDENSITY_COUNTY\n          <dbl>              <dbl>         <dbl>                 <dbl>\n 1         0.02               2.35          71.2                 54.5 \n 2         0.14              10.2           81.2                 12.2 \n 3         0.01               2.78          77.4                109.  \n 4         0                  3.57          87.9                 96.1 \n 5         0                  1.59          91.6                  2.14\n 6         0                  2.2           84.8                 41.2 \n 7         0                 12.1           75.5                 46.2 \n 8         0                  4.38          93.2                  8.36\n 9         0                  2.7           89.0                 28.8 \n10         0                 18.4           65.4                 97.8 \n   NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_Y…¹ NOAAC…² NOAAS…³ SAIPE…⁴ SAIPE…⁵ LTC_A…⁶\n                <dbl>              <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1                 28               66.5   4.36       46   50879    20.9    0.21\n 2                  0               64.6   0.595      26   76600     9.2    0.2 \n 3                 26               67.6   4.05       15   49943    17      0.18\n 4                  7               73.2   2.24       36   51461    17.1    0.09\n 5                  0               60.6   1.09       90   62256     9.3    0.33\n 6                 47               71.8   2.09       38   60594    14.9    0.16\n 7                 33               70.4   3.41       19   60593    11.4    0.08\n 8                  0               59.6   0.692      38   48259    14.4    0   \n 9                  7               68.2   2.03       29   64389    11      0.08\n10                 44               70.0   2.89       30   74612    10.8    0.14\n# … with 200 more rows, and abbreviated variable names ¹​NOAAC_AVG_TEMP_YEARLY, ²​NOAAC_PRECIPITATION_AVG_YEARLY, ³​NOAAS_TOT_NATURAL_DISASTERS, ⁴​SAIPE_MEDIAN_HH_INCOME, ⁵​SAIPE_PCT_POV, ⁶​LTC_AVG_OBS_REHOSP_RATE\n\n\n210 x 22 is a good place to start. We’ll need to re-do this step for the descriptive statistics section, but we can carry over this object when we fit the linear models."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "href": "posts/FinalPart1_CalebHill.html#descriptive-statistics",
    "title": "Final Part 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nFor our preliminary analysis, we’re going to provide summary statistics analyzing the 8 variables relevant to our research question, from Population Density to the end of the data-set, and a visualization for each. Re-hospitalization rates will be the dependent variable in future models, with the 11 demographic variables as potential controls for the regression(s).\n\n\nCode\ndata <- df_new %>%\n  select(CEN_POPDENSITY_COUNTY,\n         NEPHTN_HEATIND_105,\n         NOAAC_AVG_TEMP_YEARLY,\n         NOAAC_PRECIPITATION_AVG_YEARLY,\n         NOAAS_TOT_NATURAL_DISASTERS,\n         SAIPE_MEDIAN_HH_INCOME,\n         SAIPE_PCT_POV,\n         LTC_AVG_OBS_REHOSP_RATE) %>%\n  drop_na()\nsummary(data)\n\n\n CEN_POPDENSITY_COUNTY NEPHTN_HEATIND_105 NOAAC_AVG_TEMP_YEARLY\n Min.   :   0.78       Min.   : 0.00      Min.   :56.52        \n 1st Qu.:  12.53       1st Qu.: 7.00      1st Qu.:64.89        \n Median :  30.48       Median :24.00      Median :66.53        \n Mean   : 143.53       Mean   :22.14      Mean   :67.06        \n 3rd Qu.:  78.46       3rd Qu.:34.00      3rd Qu.:69.71        \n Max.   :3003.99       Max.   :59.00      Max.   :76.47        \n NOAAC_PRECIPITATION_AVG_YEARLY NOAAS_TOT_NATURAL_DISASTERS\n Min.   :0.4583                 Min.   :  0.00             \n 1st Qu.:1.6135                 1st Qu.: 14.25             \n Median :2.5933                 Median : 28.50             \n Mean   :2.7027                 Mean   : 32.75             \n 3rd Qu.:3.8808                 3rd Qu.: 40.00             \n Max.   :5.4558                 Max.   :186.00             \n SAIPE_MEDIAN_HH_INCOME SAIPE_PCT_POV   LTC_AVG_OBS_REHOSP_RATE\n Min.   : 33513         Min.   : 4.80   Min.   :0.0000         \n 1st Qu.: 48455         1st Qu.:11.45   1st Qu.:0.1100         \n Median : 54536         Median :14.50   Median :0.1500         \n Mean   : 57028         Mean   :14.75   Mean   :0.1528         \n 3rd Qu.: 61901         3rd Qu.:17.40   3rd Qu.:0.2000         \n Max.   :106225         Max.   :28.70   Max.   :1.0000         \n\n\n\nPopulation Density\n\n\nCode\nggplot(data, aes(CEN_POPDENSITY_COUNTY)) +\n  geom_histogram(binwidth = 50)\n\n\n\n\n\nWe see quite a number of counties have a low population density. This is no surprise, as over 80% of counties in Texas are labeled as “rural” by multiple federal agencies – dependent upon low population density.\nThis is further attested and we see a wide range between this variable’s median (21.8) and mean (119.4). Lots of out-liers. If we had a urban/rural classification code, we could filter on only rural counties to help mitigate this spread. I may need to merge a data-set due to this wide range.\n\n\nHeat Index Over 105F\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_boxplot()\n\n\n\n\n\nTexas is a hot state, and this visualization is evidence of that. The median number of days Texas’ counties experience a heat index of over 105F each year is 20 days per year. One county even reached 59 days!\n\n\nCode\nggplot(data, aes(NEPHTN_HEATIND_105)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe data-set has a very normal distribution, centered around the 25/30 mark – if the number of counties at 0 were removed. Yet because that’s not so, this variable has a sharp bimodal distribution. We may have to separate the data into two bins: those with less than 10 days over 105F and those with more than 10 days over 105F. That’s yet to be determined.\n\n\nAverage Yearly Temperature\n\n\nCode\nggplot(data, aes(NOAAC_AVG_TEMP_YEARLY)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere’s a good distribution. Average temperature each month is between 65 to 67 for most of the counties. The range (20) is also fairly small for a state with such a large area and multiple climates within its borders.\n\n\nAverage Yearly Precipitation\n\n\nCode\nggplot(data, aes(NOAAC_PRECIPITATION_AVG_YEARLY)) +\n  geom_boxplot()\n\n\n\n\n\nAverage precipitation each month is fairly uniform, with the mean at 2.5 inches of rain, on average, each month. This variable will most likely provide less variation in the analysis compared to others, such as population density and heat index. This can be both a good and a bad thing, as variations in precipitation was one of the variables I was most interested in exploring for this project. Oh well.\n\n\nTotal Natural Disasters\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_boxplot()\n\n\n\n\n\nMany high out-liers over 75. Let’s plot a histogram to get a better look at the data’s distribution.\n\n\nCode\nggplot(data, aes(NOAAS_TOT_NATURAL_DISASTERS)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA right skewed variable, with observations dropping off dramatically once we reach 50 total recorded natural disasters.\n\n\nMedian Household Income\n\n\nCode\nggplot(data, aes(SAIPE_MEDIAN_HH_INCOME)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA couple of high out-liers, hovering around $90,000+ in median household income, but the mean holds at $57,291.\n\n\nPercent in Poverty\n\n\nCode\nggplot(data, aes(SAIPE_PCT_POV)) +\n  geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother close to normal distribution. Most counties have poverty rates ranging from 10% to 20%. There are of course out-liers, especially a good number below 10%, but those are rare.\n\n\nRe-hospitalization Rate\n\n\nCode\nggplot(data, aes(LTC_AVG_OBS_REHOSP_RATE)) +   geom_histogram()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnother right skewed variable. Lots of counties with 0.00 rates of re-hospitalization, and few, if any, above 0.25 per 100,000 people. From a health perspective, this is good news! From a research perspective, that’s going to make analysis a little trickier. However, the somewhat normal and/or bimodal distribution should be fairly easy to work with, needing little to no transformation for a linear regression.\n\n\nCorrelation\nFinally, let’s plot a brief correlation matrix to see if there’s any relationships we can explore as a simple linear regression in the next section.\n\n\nCode\ndata %>%\n  cor(data) %>%\n  corrplot(is.corr = FALSE, method=\"number\", tl.cex = .4)\n\n\nError in corrplot(., is.corr = FALSE, method = \"number\", tl.cex = 0.4): could not find function \"corrplot\"\n\n\nThe closer a box is to 1, the higher the correlation. Not particularly exciting news, as it shows there’s not a high correlation between re-hospitalization rates and any of the explanatory variables. This may throw a kink in our analysis – and explain why others haven’t delved deeply into this research!\nPerhaps this step should have been completed first, but nonetheless, we shall continue on with the report. I may pull two more environmental variables, to see if we can find a correlation somewhere. Even so, the sum total of all environmental variables might contribute to re-hospitalization rates as well. I’m just not sure if that – along with control variables – is outside the scope of this report.\nFor Part 2, I’d like to rename the variables to more digestible phrases, and I would like to overhaul the code outputs, to make the tables and visualizations a little easier on the eyes. That’s just polish work, though, and won’t affect the analysis.\nLooking over the Spatz et. al. (2020) article again, the two most significant Built Environment variables (with the highest R2 value) are 1) Long Commute, Driving Alone and 2) Severe Housing Problems. I’m going to scour the SDOH data-set to see what relevant variables match these two and add them into Part 2."
  },
  {
    "objectID": "posts/FinalPart1_CalebHill.html#references",
    "href": "posts/FinalPart1_CalebHill.html#references",
    "title": "Final Part 1",
    "section": "References",
    "text": "References\nBarnett, M., Hsu, J. & McWilliams, M. (2015). “Patient Characteristics and Differences in Hospital Readmission Rates.” JAMA Intern Med., 175(11): 1803-1812.\nLi, Y., Cai, X. & Glance, L. (2015). “Disparities in 30-day rehospitalization rates among Medicare skilled nursing facility residents by race and site of care.” Med Care, 53(12): 1058-1065.\nMurray, F., Allen, M., Clark, C., Daly, C. & Jacobs, D. (2021). “Socio-demographic and -economic factors associated with 30-day readmission for conditions targeted by the hospital readmissions reduction program: a population-based study.” BMC Public Health, 21.\nSpatz, E., Bernheim, S., Horwitz, L. & Herrin, J. (2020). Community factors and hospital wide readmission rates: Does context matter? PLoS One, 15(10)."
  },
  {
    "objectID": "posts/Homework1QH.html",
    "href": "posts/Homework1QH.html",
    "title": "Homework 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/Homework1QH.html#a",
    "href": "posts/Homework1QH.html#a",
    "title": "Homework 1",
    "section": "1a",
    "text": "1a\n\n\nCode\nggplot(LungCapData, mapping = aes(LungCap)) +\n  geom_histogram(color = \"black\", fill = \"grey\")+\n  geom_density()+\n  labs(title = \"Distribution of Lung Capacity\", x = \"Lung Capacity\", y = \"Count\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nplot(x = LungCapData$LungCap, y = lungcap_prob_dense)\n\n\n\n\n\nWith these two functions I can see the distribution is normal with both a histogram and regular graph. The second graph more clearly depicts a normal distribution with the probability density points laid throughout. ## 1b\n\n\nCode\nggplot(LungCapData, mapping = aes(x = Gender, y = LungCap)) +\n  geom_boxplot() \n\n\n\n\n\nIt looks like men, on average, have a higher lung capacity than females, but only by a slim margin. Overall, lung capacity is relatively similar among genders. The real comparison will come with smokers and nonsmokers. ## 1c\n\n\nCode\nLungCapData %>% \n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             7.77\n2 yes            8.65\n\n\nAbove is the lung capacity mean for smokers and nonsmokers. I’m actually a little surprised the mean lung capacity for nonsmokers is slightly higher than that of nonsmokers. I would think the opposite to be true, but I suspect because there is a range of ages under 18 and the body is not fully developed yet, I imagine a 6 year old nonsmoker will not have the same lung capacity as a 17 year old smoker."
  },
  {
    "objectID": "posts/Homework1QH.html#d",
    "href": "posts/Homework1QH.html#d",
    "title": "Homework 1",
    "section": "1d",
    "text": "1d\nBelow I created a bunch of variables to separate people into certain age groups. I imagine there would be an easier way to separate them.\n\n\nCode\n#LungCapData %>% \n  #group_by(Age) %>% \n  #summarise(lungcap = mean(LungCap))\n  \nage13 <- LungCapData %>% \n  filter(Age <= 13) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1415 <- LungCapData %>% \n  filter(Age == 14 | Age == 15) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage1617 <- LungCapData %>% \n  filter(Age == 16 | Age == 17) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage18 <- LungCapData %>% \n  filter(Age >= 18) %>%\n  group_by(Smoke) %>% \n  summarise(lung_cap_mean = mean(LungCap))\n\nage13\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             6.36\n2 yes            7.20\n\n\nCode\nage1415\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             9.14\n2 yes            8.39\n\n\nCode\nage1617\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no            10.5 \n2 yes            9.38\n\n\nCode\nage18\n\n\n# A tibble: 2 × 2\n  Smoke lung_cap_mean\n  <chr>         <dbl>\n1 no             11.1\n2 yes            10.5"
  },
  {
    "objectID": "posts/Homework1QH.html#e",
    "href": "posts/Homework1QH.html#e",
    "title": "Homework 1",
    "section": "1e",
    "text": "1e\nBased on the variables I created above, it appears the lung capacity for people under 13, and that smoke, is higher than people who do not smoke. As the age brackets increase, so does lung capacity overall, but it begins to show that those who do smoke, generally have a lower lung capacity than those who choose not to smoke. This is what I would expect to happen since a 13 year old still has plenty of growing to do, therefore the lung capacity will be much lower than a grown teenager."
  },
  {
    "objectID": "posts/Homework1QH.html#f",
    "href": "posts/Homework1QH.html#f",
    "title": "Homework 1",
    "section": "1f",
    "text": "1f\n\n\nCode\ncor(LungCapData$LungCap, LungCapData$Age)\n\n\n[1] 0.8196749\n\n\nWith a correlation of 0.81, lung capacity and age have a fairly strong positive relationship. This is what I figured would be the case. As people age, their lung capacities grow larger. A 17 year old will be more developed and most likely have a larger lung capacity than, say, a child the age of 8.\nI created a table of the data frame in question 2\n\n\nCode\nxx <- c(0:4)\n\nfreq <- c(128, 434, 160, 64, 24)\n\ndf <- tibble(xx, freq)"
  },
  {
    "objectID": "posts/Homework1QH.html#a-1",
    "href": "posts/Homework1QH.html#a-1",
    "title": "Homework 1",
    "section": "2a",
    "text": "2a\n\n\nCode\n160/810\n\n\n[1] 0.1975309\n\n\nThe probability of selecting inmates with 2 prior convictions is 19.7%."
  },
  {
    "objectID": "posts/Homework1QH.html#b",
    "href": "posts/Homework1QH.html#b",
    "title": "Homework 1",
    "section": "2b",
    "text": "2b\n\n\nCode\n562/810\n\n\n[1] 0.6938272\n\n\nThe probability of selecting inmates with less than 2 prior convictions is 69%."
  },
  {
    "objectID": "posts/Homework1QH.html#c",
    "href": "posts/Homework1QH.html#c",
    "title": "Homework 1",
    "section": "2c",
    "text": "2c\n\n\nCode\n722/810\n\n\n[1] 0.891358\n\n\nThe probability of selecting inmates with 2 or less prior convictions is 89%."
  },
  {
    "objectID": "posts/Homework1QH.html#d-1",
    "href": "posts/Homework1QH.html#d-1",
    "title": "Homework 1",
    "section": "2d",
    "text": "2d\n\n\nCode\n88/810\n\n\n[1] 0.108642\n\n\nThe probability of selecting inmates with more than 2 prior convictions is 10.8%."
  },
  {
    "objectID": "posts/Homework1QH.html#e-1",
    "href": "posts/Homework1QH.html#e-1",
    "title": "Homework 1",
    "section": "2e",
    "text": "2e\nThe expected value for number of prior convictions is 291.4.\n\n\nCode\ntest <- c(128, 434, 160, 64, 24)\n\ntestprobs <- c(0.15, 0.54, 0.2, 0.08, 0.03)\n\nsum(test*testprobs)\n\n\n[1] 291.4"
  },
  {
    "objectID": "posts/Homework1QH.html#f-1",
    "href": "posts/Homework1QH.html#f-1",
    "title": "Homework 1",
    "section": "2f",
    "text": "2f\nuse rep()\n\n\nCode\nconvictions <- c(rep(0,128), rep(1, 434), rep(2,160), rep(3,64), rep(4,24))\n\nsd(convictions)\n\n\n[1] 0.9259016\n\n\nCode\nvar(convictions)\n\n\n[1] 0.8572937"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html",
    "href": "posts/RahulGundeti_DACSS603_HW2.html",
    "title": "DACSS603_HW2",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stats)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-1",
    "title": "DACSS603_HW2",
    "section": "Question 1",
    "text": "Question 1"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#creating-the-table",
    "title": "DACSS603_HW2",
    "section": "Creating the table",
    "text": "Creating the table\n\n\nCode\nprocedure <- c(\"Bypass\", \"Angiography\")\nsample_size <- c(539, 847)\nmwt <- c(19, 18)\ns_stddev <- c(10, 9)\n\nsurgery <- data.frame(procedure, sample_size, mwt, s_stddev)\nsurgery\n\n\n\n\n  \n\n\n\n\n\nCode\nstd_error <- s_stddev / sqrt(sample_size)\nstd_error\n\n\n[1] 0.4307305 0.3092437\n\n\n\n\nCode\nconfidence_level <- 0.90\ntail_area <- (1-confidence_level)/2\ntail_area\n\n\n[1] 0.05\n\n\n\n\nCode\nt_score <- qt(p = 1-tail_area, df = sample_size-1)\nt_score\n\n\n[1] 1.647691 1.646657\n\n\n\n\nCode\nConfidence_Interval <- c(mwt - t_score * std_error,\n        mwt + t_score * std_error)\nConfidence_Interval\n\n\n[1] 18.29029 17.49078 19.70971 18.50922\n\n\nThe above results are obtained by fitting the 90% confidence interval level for the sample mean wait time for both the bypass surgery and the angiograph.\nBypass Surgery mean wait time : 18.29029 and 19.70971 days\nAngiograph mean wait time: 17.49078 and 18.50922 days\nThe comparision shows that the wait time for Angiograph is shorter than that of Bypass Surgery."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-2",
    "title": "DACSS603_HW2",
    "section": "Question 2",
    "text": "Question 2\n\n\nCode\nprop.test(567, 1031, conf.level = .95)\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  567 out of 1031, null probability 0.5\nX-squared = 10.091, df = 1, p-value = 0.00149\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5189682 0.5805580\nsample estimates:\n        p \n0.5499515 \n\n\n95 percent confidence interval: 0.5189682 0.5805580\nThe sample estimate for the point p, from the sample who believes that college is necessary for success is: 0.5499515"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-3",
    "title": "DACSS603_HW2",
    "section": "Question 3",
    "text": "Question 3\n\n\nCode\nME <- 5\nz <- 1.96\ns_sd <- (200-30)/4\n\ns_size <- ((z*s_sd)/ME)^2\ns_size\n\n\n[1] 277.5556\n\n\nThe necessary sample size is: 278."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-4",
    "title": "DACSS603_HW2",
    "section": "Question 4",
    "text": "Question 4"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than or equal to 0.05\n\n\nCode\ns_mean <- 410\nμ <- 500\ns_sd <- 90\ns_size <- 9"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-test-statistic",
    "title": "DACSS603_HW2",
    "section": "Calculating test-statistic",
    "text": "Calculating test-statistic\n\n\nCode\nt_score <- (s_mean-μ)/(s_sd/sqrt(s_size))\nt_score\n\n\n[1] -3"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-p-value",
    "title": "DACSS603_HW2",
    "section": "Calculating p-value",
    "text": "Calculating p-value\n\n\nCode\np <- 2*pt(t_score, s_size-1)\np\n\n\n[1] 0.01707168\n\n\nThe test-statistic is -3 and p-value is 0.01707168. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is not equal to $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ < 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = TRUE)\np\n\n\n[1] 0.008535841\n\n\nThe p-value is 0.008535841. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is less than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ > 500\nWe will reject the null hypothesis at a p-value less than 0.05\n\n\nCode\np <- pt(t_score, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.9914642\n\n\nThe p-value is 0.9914642. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the mean income of female employees is greater than $500."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-5",
    "title": "DACSS603_HW2",
    "section": "Question 5",
    "text": "Question 5"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#a-1",
    "title": "DACSS603_HW2",
    "section": "A",
    "text": "A\nWe assume that the sample is random and that the population has a normal distribution.\nNull hypothesis: H0: μ = 500\nAlternative hypothesis: Ha: μ ≠ 500\nWe will reject the null hypothesis at a p-value less than 0.05"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-jones",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Jones",
    "text": "Calculating t-statistic and p-value for Jones\n\n\nCode\ns_mean <- 519.5\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.95\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.05145555"
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#calculating-t-statistic-and-p-value-for-smith",
    "title": "DACSS603_HW2",
    "section": "Calculating t-statistic and p-value for Smith",
    "text": "Calculating t-statistic and p-value for Smith\n\n\nCode\ns_mean <- 519.7\nμ <- 500\nse <- 10\ns_size <- 1000\n\njt <- (s_mean-μ)/se\njt\n\n\n[1] 1.97\n\n\nCode\np <- 2*pt(jt, s_size-1, lower.tail = FALSE)\np\n\n\n[1] 0.04911426\n\n\nThe test-statistic is 1.95, p-value is 0.05145555 for Jones and the test-statistic is 1.97, p-value is 0.05145555 for Smith."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#b-1",
    "title": "DACSS603_HW2",
    "section": "B",
    "text": "B\nThe p-value is 0.05145555 for Jones. As p-value is greater than the 0.05, we fail to reject the null hypothesis. The p-value is 0.04911426 for Jones. As p-value is less than the 0.05, we reject the null hypothesis. Therefore, the result is statistically significant for Smith, but not Jones."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#c-1",
    "title": "DACSS603_HW2",
    "section": "C",
    "text": "C\nIf we fail to report the P-value and simply state whether the P-value is less than/equal to or greater than the defined significance level of the test, one cannot determine the strength of the conclusion. In the Jones/Smith example, reporting the results only as P ≤ 0.05 versus P > 0.05 will lead to different conclusions about very similar results."
  },
  {
    "objectID": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "href": "posts/RahulGundeti_DACSS603_HW2.html#question-6",
    "title": "DACSS603_HW2",
    "section": "Question 6",
    "text": "Question 6\n\n\nCode\ngas_taxes <- c(51.27, 47.43, 38.89, 41.95, 28.61, 41.29, 52.19, 49.48, 35.02, 48.13, 39.28, 54.41, 41.66, 30.28, 18.49, 38.72, 33.41, 45.02)\n\nt.test(gas_taxes, mu = 18.4, conf.level = .95)\n\n\n\n    One Sample t-test\n\ndata:  gas_taxes\nt = 10.238, df = 17, p-value = 1.095e-08\nalternative hypothesis: true mean is not equal to 18.4\n95 percent confidence interval:\n 36.23386 45.49169\nsample estimates:\nmean of x \n 40.86278 \n\n\nThe 95% confidence interval for the mean tax per gallon is 36.23386 through 45.49169. We cannot conclude with 95% confidence that the mean tax is less than 45 cents, since the 95% confidence interval contains values above 45 cents."
  },
  {
    "objectID": "posts/hw1.html",
    "href": "posts/hw1.html",
    "title": "Homework #1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary (ggplot)\n\n\nError in library(ggplot): there is no package called 'ggplot'\n\n\nCode\nlungcap<- read_excel(\"LungCapData.xls\") \n\n\nError: `path` does not exist: 'LungCapData.xls'\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/hw1.html#lungcapdata",
    "href": "posts/hw1.html#lungcapdata",
    "title": "Homework #1",
    "section": "LungCapData",
    "text": "LungCapData\n\n1a. What does the distribution of LungCap look like?\n\n\nCode\nggplot(lungcap, aes(x=LungCap))+ geom_histogram()\n``\nThis is not normally distributed as there are far more observations of lower lung capacity than higher suggesting the distribution is negatively skewed.\n \n### 1b. Compare the probability distribution of the LungCap with respect to Males and Females? \n\n\nError: attempt to use zero-length variable name\n\n\n\n\nCode\nlungcap %>%\ngroup_by(Gender)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Gender): object 'lungcap' not found\n\n\nThe average lung capacity for females is 7.41, lower than the average for males at 8.31.\n\n\n1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\n\nCode\nlungcap %>%\ngroup_by(Smoke)%>%\nsummarise(mean(LungCap))\n\n\nError in group_by(., Smoke): object 'lungcap' not found\n\n\nThe mean lung capacity for non-smokers is 7.77, lower than the mean for smokers at 8.65. At first glance, this seems contradictory as one would guess smokers to have a lower lung capacity than non-smokers.The following grid displays non-smokers as having overall higher lung capacity, conflicting with the mean above.\n\n\nCode\nggplot(lungcap, aes(x = LungCap)) +\nfacet_grid(Gender ~ Smoke)+\n  geom_histogram()\n\n\nError in ggplot(lungcap, aes(x = LungCap)): object 'lungcap' not found\n\n\n\n\n1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n\n1e. Compare the lung capacities for smokers and non-smokers within each age group.\nLung capacity for those under age 13 is 6.36 for non-smokers and 7.20 for smokers.\n\n\nCode\nlungcap %>%\n+ filter(Age <= 13)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 14 to 15\nlungcap%>%\n+ filter(Age=<15 & Age >=14)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those between the age of 16 to 17\nlungcap%>%\n+     filter(Age=<17>=16)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\nLung capacity for those 18 and older\nlungcap%>%\n+ filter(Age>=18)%>%\n+ group_by(Smoke)%>%\n+ summarise(mean(LungCap))\n\n\nError: <text>:6:6: unexpected symbol\n5: \n6: Lung capacity\n        ^\n\n\n\n\nIs your answer different from the one in part c? What could possibly be going on here?\n\n\n1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret results.\n\n\nCode\ncov(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nCode\ncor(lungcap$LungCap, lungcap$Age)\n\n\nError in is.data.frame(y): object 'lungcap' not found\n\n\nThe covariance between lung capacity and age is 8.74 suggesting a positive relationship in which both variables move in the same direction (i.e. for this data set an increase in lung capacity would suggest an increase in age as well).\nThe correlation between lung capacity and age is 0.82 suggesting a strong positive correlation (0.82 of a potential -1 to +1)."
  },
  {
    "objectID": "posts/hw1.html#inmate-data",
    "href": "posts/hw1.html#inmate-data",
    "title": "Homework #1",
    "section": "Inmate Data",
    "text": "Inmate Data\n\n\nCode\nx<- c(0, 1, 2, 3, 4)\ny<- c(128, 434, 160, 64, 24)\nprison <-data.frame(x,y)\nView(prison)\n\n\nWarning in View(prison): unable to open display\n\n\nError in .External2(C_dataviewer, x, title): unable to start data viewer\n\n\n2a. What is the probability that a randomly selected inmate has exactly 2 prior convictions? 20% 2b. What is the probability that a randomly selected inmate has fewer than 2 prior convictions? 69% 2c. What is the probability that a randomly selected inmate has 2 or fewer prior convictions? 89% 2d. What is the probability that a randomly selected inmate has more than 2 prior convictions? 11% 2e. What is the expected value for the number of prior convictions? 84% 2f. Calculate the variance and the standard deviation for the Prior Convictions.\n\n\nCode\nvar(prison, y= NULL)\nsd(rnorm(810))\nThe standard deviation is 1.02.\n\n\nError: <text>:3:5: unexpected symbol\n2: sd(rnorm(810))\n3: The standard\n       ^"
  },
  {
    "objectID": "posts/KPopiela_HW1.html",
    "href": "posts/KPopiela_HW1.html",
    "title": "HW1",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(lsr)\n\nError in library(lsr): there is no package called 'lsr'\n#Question 1"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "href": "posts/KPopiela_HW1.html#a1b.-what-does-the-distribution-of-lungcap-look-like",
    "title": "HW1",
    "section": "1a/1b. What does the distribution of LungCap look like?",
    "text": "1a/1b. What does the distribution of LungCap look like?\n\nHint: Plot a histogram with probability density on the y axis\n\n\nHint: make boxplots separated by gender using the boxplot() function\n\nLungCap <- read_xls(\"_data/LungCapData.xls\")\nhead(LungCap)\n\n# A tibble: 6 × 6\n  LungCap   Age Height Smoke Gender Caesarean\n    <dbl> <dbl>  <dbl> <chr> <chr>  <chr>    \n1    6.48     6   62.1 no    male   no       \n2   10.1     18   74.7 yes   female no       \n3    9.55    16   69.7 no    female yes      \n4   11.1     14   71   no    male   no       \n5    4.8      5   56.9 no    male   no       \n6    6.22    11   58.7 no    female no       \n\n\n\nhist(LungCap$LungCap)\n\n\n\n\n\nLungCap_MF <- LungCap %>%\n  arrange(LungCap, Gender) %>%\n  group_by(Gender)\nboxplot(LungCap_MF$LungCap ~ LungCap_MF$Gender)\n\n\n\n#I wanted to change the axis labels to \"Gender\" (x) and \"Lung Capacity\" (y), but after an hour and a half of trying to no avail, I had to call it for my own sanity.\n\n\ncolnames(LungCap)\n\n[1] \"LungCap\"   \"Age\"       \"Height\"    \"Smoke\"     \"Gender\"    \"Caesarean\""
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "href": "posts/KPopiela_HW1.html#c.-compare-the-mean-lung-capacities-for-smokers-and-non-smokers.-does-it-make-sense",
    "title": "HW1",
    "section": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?",
    "text": "1c. Compare the mean lung capacities for smokers and non-smokers. Does it make sense?\n\nLungCap_smoke <- LungCapData %>%\n  select(LungCap, Smoke) %>%\n  group_by(Smoke)\n\nError in select(., LungCap, Smoke): object 'LungCapData' not found\n\nhead(LungCap_smoke)\n\nError in head(LungCap_smoke): object 'LungCap_smoke' not found\n\n\n\nsummarise(LungCap_smoke, mean(LungCap))\n\nError in summarise(LungCap_smoke, mean(LungCap)): object 'LungCap_smoke' not found\n\n#The mean lung capacities for non-smokers and smokers is 7.77 and 8.65 respectively. Does this make sense? No. One would expect that the mean lung capacity for non-smokers would be higher, but that is not the case here. Let's do some digging to see what the range of values for smokers' and non-smokers' lung capacity. I also want to look at how many people voted \"yes\" or \"no\"; it could be that fewer people (with higher lung capacity) voted \"yes,\" contributing to the higher mean.\n\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"yes\")\n\nError in filter(., Smoke == \"yes\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\nLCS2 <- LungCap_smoke %>%\n  filter(Smoke == \"no\")\n\nError in filter(., Smoke == \"no\"): object 'LungCap_smoke' not found\n\nrange(LCS2$LungCap)\n\nError in eval(expr, envir, enclos): object 'LCS2' not found\n\n##Lung capacity for smokers ranges from 3.850 to 13.325, while the range for non-smokers is 0.507 to 14.675. Right off the bat, smokers have a higher minimum value, which prevents the mean from being dragged down during calculation. Non-smokers' minimum value is 0.507, an outlier which does seem to have an effect on this category's mean. \n\n\nLungCap_smoke %>%\n  count(Smoke)\n\nError in count(., Smoke): object 'LungCap_smoke' not found\n\n#Out of 725 respondents only 77 voted yes and 648 voted no, so I was right with my guess as to what caused the difference in mean lung capacity between smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "href": "posts/KPopiela_HW1.html#d.-examine-the-relationship-between-smoking-and-lung-capacity-within-age-groups-less-than-or-equal-to-13-14-to-15-16-to-17-and-greater-than-or-equal-to-18.",
    "title": "HW1",
    "section": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.",
    "text": "1d. Examine the relationship between Smoking and Lung Capacity within age groups: “less than or equal to 13”, “14 to 15”, “16 to 17”, and “greater than or equal to 18”.\n\n#To start, I'm going to calculate the range and mean of each of the above age groups, as well as a tally of how many are and aren't smokers.\n\n#a) Less than or equal to 13\nLC_Age13 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age <= 13)\nrange(LC_Age13$LungCap)\n\n[1]  0.507 12.050\n\n#The range of lung capacity values for children under the age of 13 is 0.507 to 12.050. The mean is 6.412.\n\n\nsummarise(LC_Age13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.41\n\n\n\nLC_Age13 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no      401\n2 yes      27\n\n#401 individuals 13 and under responded that they don't smoke, while 27 said they do. Compared to the initial calculations for the whole survey, the mean value is slightly lower, which is likely indicative of the fact that children have smaller lungs than adults and therefore have less lung capacity. Something important to note, however, is that this age group accounts for 428 of the total 725 responses (about 59%).\n\n\n#b) 14 to 15 \nLC_Age145 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 14:15)\n\nWarning in Age == 14:15: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age145$LungCap)\n\n[1]  5.625 12.900\n\n#The minimum and maximum lung capacity values for individuals aged 14-15 are 5.625 and 12.900. The mean is 8.842.\n\n\nsummarise(LC_Age145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.84\n\n\n\nLC_Age145 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       44\n2 yes       8\n\n#Out of the 52 respondents in this age group, 44 stated that they don't smoke and 8 said that they do. The 14-15y/o age group is MUCH smaller than the \"13 and under\" one (it makes up only 12% of total responses). The percentage of smokers to non-smokers in each of the above age groups,is 7% and 18% respectively. If you were to take these percentages at face value without taking sample size into account, it would look as if the 14-15 y/o age group makes up 18% of the total 725 responses. In reality, this sample accounts for 6% of the total, making its impact relatively low.\n\n\n#c) 16 to 17\nLC_Age167 <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age == 16:17)\n\nWarning in Age == 16:17: longer object length is not a multiple of shorter\nobject length\n\nrange(LC_Age167$LungCap)\n\n[1]  5.675 13.375\n\n#The minimum and maximum lung capacity values for individuals ages 16 to 17 are 5.675 and 13.375. The mean is 10.058.\n\n\nsummarise(LC_Age167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.1\n\n\n\nLC_Age167 %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       40\n2 yes       8\n\n#This sample is similar in size to the previous (14-15 year olds) with only 48 responses, and smokers make up 20% of the responses. Now lets discuss the other figures.  \n\n#The mean lung capacity for 16-17 year olds is 10.058, 1.216 units higher than the previous age group, and 3.646 units higher than the \"13 and under\" age group. As of this point in my calculations, the only relationship seems to be between age and lung capacity rather than smoking and lung capacity; this is due to the facts that: 1) not that many people ages 0-17 smoke, and 2) the sample sizes for the 14-17 age group is 100 compared to the 428 responses in the \"13 and under\" group.\n\n\nLC_Age18p <- LungCap %>%\n  select(LungCap, Age, Smoke) %>%\n  filter(Age >= 18)\nrange(LC_Age18p$LungCap)\n\n[1]  7.750 14.675\n\n#The minimum and maximum range values for the 18+ age group are 7.750 and 14.675. The mean is 10.965.\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\nLC_Age18p %>%\n  count(Smoke)\n\n# A tibble: 2 × 2\n  Smoke     n\n  <chr> <int>\n1 no       65\n2 yes      15\n\n#Ok so what we've learned here is that this survey was HEAVILY focused on kids 13 and younger; although the sample for this age group is larger than the previous 2, it's still only 80 out of 725 responses (about 11% of total respondents). The mean lung capacity for this group is the highest of all of them at 10.965, but this still doesn't seem to show a relationship between smoking and lung capacity. Rather, at least to me, it shows a relationship between age and lung capacity (i.e. stage of lung development and lung capacity).\n\n##1e. Compare the lung capacities for smokers and non-smokers within each age group. Is your answer different from the one in part d. What could possibly be going on here?\n\n#I will place a comparison of all values produced by the following calculations at the bottom. (I will go through the smoker and non-smoker calculations first)\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"yes\")\nrange(LCu13$LungCap)\n\n[1]  3.850 10.275\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            7.20\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"yes\")\nrange(LCu145$LungCap)\n\n[1]  6.225 11.025\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.36\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"yes\")\nrange(LCu167$LungCap)\n\n[1]  7.550 11.775\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            9.06\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"yes\")\nrange(LCu18p$LungCap)\n\n[1]  8.200 13.325\n\n\n\nsummarise(LCu18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.5\n\n\n\n#Now for the non-smoker calculations.\n\nLCu13 <- LC_Age13 %>%\n  filter(Smoke == \"no\")\nrange(LCu13$LungCap)\n\n[1]  0.507 12.050\n\n\n\nsummarise(LCu13, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            6.36\n\n\n\nLCu145 <- LC_Age145 %>%\n  filter(Smoke == \"no\")\nrange(LCu145$LungCap)\n\n[1]  5.625 12.900\n\n\n\nsummarise(LCu145, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            8.93\n\n\n\nLCu167 <- LC_Age167 %>%\n  filter(Smoke == \"no\")\nrange(LCu167$LungCap)\n\n[1]  5.675 13.375\n\n\n\nsummarise(LCu167, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            10.3\n\n\n\nLCu18p <- LC_Age18p %>%\n  filter(Smoke == \"no\")\nrange(LCu18p$LungCap)\n\n[1]  7.750 14.675\n\n\n\nsummarise(LC_Age18p, mean(LungCap))\n\n# A tibble: 1 × 1\n  `mean(LungCap)`\n            <dbl>\n1            11.0\n\n\n\n#Comparison!!  \n  # 13 and under smokers: range = 3.850 to 10.275, mean = 7.202\n  # 13 and under non-smokers: range = 0.507 and 12.050, mean = 6.359 \n    \n  # 14-15 smokers: range = 6.225 and 11.025, mean = 8.359\n  # 14-15 non-smokers: range = 5.625 and 12.900, mean = 8.930  \n  \n  # 16-17 smokers: range = 7.550 and 11.775, mean = 9.063\n  # 16-17 non-smokers: range = 5.675 and 13.375, mean = 10.257  \n    \n  # 18+ smokers: range = 8.200 and 13.325,mean = 10.513 \n  # 18+ non-smokers: range = 7.750 and 14.675, mean = 10.965\n\n\n#The answers I got for smokers vs. non-smokers are obviously different, but I wouldn't say they necessarily convey something different to what I interpreted from 1d. I don't see a relationship between smoking and lung capacity, and I certainly don't see a massive difference in the values comparing the lung capacity of smokers and non-smokers."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "href": "posts/KPopiela_HW1.html#f.-calculate-the-correlation-and-covariance-between-lung-capacity-and-age.-use-the-cov-and-cor-functions-in-r.-interpret-your-results.",
    "title": "HW1",
    "section": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.",
    "text": "1f. Calculate the correlation and covariance between Lung Capacity and Age. (use the cov() and cor() functions in R). Interpret your results.\n\ncov(LungCap$LungCap, LungCap$Age)\n\n[1] 8.738289\n\n\n\ncor(LungCap$LungCap, LungCap$Age)\n\n[1] 0.8196749\n\n\n\n#Covariance between lung capacity and age: 8.738.  \n#Correlation between lung capacity and age: 0.820  \n\n#I'm not totally confident in my understanding of covariance yet, but from what I know, it's the positive or negative relationship between two variables and the further the value is from 0, the stronger the relationship is. And the covariance between lung capacity and age is 8.738. Correlation gets stronger the closer the value gets to 1 or -1; the correlation between lung capacity and age for 'LungCap' dataset is 0.820, a figure relatively close to 1, so I would say there is a moderate to strong correlation between the two variables in question here.\n\n#Question 2\n###Let X=number of prior convictions for prisoners at a state prison at which there are 810 inmates."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#a.-what-is-the-probability-that-a-randomly-selected-inmate-has-fewer-than-2-prior-convictions",
    "title": "HW1",
    "section": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?",
    "text": "2a. What is the probability that a randomly selected inmate has fewer than 2 prior convictions?\n\nconrange <- rep(c(0,1,2,3,4),times=c(128,434,160,64,24))\nconrange\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[445] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[519] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[556] 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[593] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[630] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[667] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[704] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[741] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[778] 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n\n#To start I just wanted to present a visualization of the frequency of each categorical variable.  \n  # 0 prior convictions = 128  \n  # 1 prior conviction = 434  \n  # 2 prior convictions = 160  \n  # 3 prior convictions = 64  \n  # 4 prior convictions = 24\n\n\nprop.table(table(conrange))[0:2]\n\nconrange\n        0         1 \n0.1580247 0.5358025 \n\n#By combining the probability values for 0 and 1, we can see that the probability of a randomly selected inmate having fewer than 2 prior convictions is 0.694."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "href": "posts/KPopiela_HW1.html#b.-what-is-the-probability-that-a-randomly-selected-inmate-has-2-or-fewer-prior-convictions",
    "title": "HW1",
    "section": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?",
    "text": "2b. What is the probability that a randomly selected inmate has 2 or fewer prior convictions?\n\nprop.table(table(conrange))[0:3]\n\nconrange\n        0         1         2 \n0.1580247 0.5358025 0.1975309 \n\n#By using the same math above, the probability that a randomly selected inmate has 2 or fewer prior convictions is 0.891."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "href": "posts/KPopiela_HW1.html#c.-what-is-the-probability-that-a-randomly-selected-inmate-has-more-than-2-prior-convictions",
    "title": "HW1",
    "section": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?",
    "text": "2c. What is the probability that a randomly selected inmate has more than 2 prior convictions?\n\nprop.table(table(conrange))[4:5]\n\nconrange\n         3          4 \n0.07901235 0.02962963 \n\n#The probability that a randomly selected inmate has more than 2 prior convictions is 0.108."
  },
  {
    "objectID": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "href": "posts/KPopiela_HW1.html#d.-what-is-the-expected-value-for-the-number-of-prior-convictions",
    "title": "HW1",
    "section": "2d. What is the expected value for the number of prior convictions?",
    "text": "2d. What is the expected value for the number of prior convictions?\n\nprior_con_range <- c(0,1,2,3,4)\nprobs <- c(0.158,0.535,0.197,0.079,0.029)\nc(prior_con_range %*% probs)\n\n[1] 1.282\n\n#The expected value for the number of prior convictions is 1.282"
  },
  {
    "objectID": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "href": "posts/KPopiela_HW1.html#e.-calculate-the-variance-and-the-standard-deviation-for-the-prior-convictions.",
    "title": "HW1",
    "section": "2e. Calculate the variance and the standard deviation for the Prior Convictions.",
    "text": "2e. Calculate the variance and the standard deviation for the Prior Convictions.\n\nvar(conrange)\n\n[1] 0.8572937\n\nsd(conrange)\n\n[1] 0.9259016\n\n#The variance and standard deviation for prior convictions are 0.857 and 0.925 respectively."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 603 Introduction to Quantitative Analysis",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 603, Introduction to Quantitative Analysis.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\nHW1 Quat\n\n\nAmy Quarkume\n\n\n\n\n\n\nGraphs and Probz\n\n\n\n\n\n\n\n\nKPopiela_final p1\n\n\n\n\n\n\n\n\nKPopiela HW2\n\n\n\n\n\n\n\n\ntheme: default\n\n\n\n\n\n\n\n\nResearch Question\n\n\n\n\n\n\n\n\nHW1\n\n\nKatie Popiela\n\n\n\n\nOct 28, 2022\n\n\nHomework 3\n\n\nSaaradhaa M\n\n\n\n\nOct 27, 2022\n\n\nHomework 3\n\n\nEthan Campbell\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nDonny Snyder\n\n\n\n\nOct 16, 2022\n\n\nHomework 2 Solution\n\n\nDane Shelton\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Final Part 1\n\n\nTory Bartelloni\n\n\n\n\nOct 18, 2022\n\n\nHomework 2\n\n\nRoy Yoon\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nLindsay Jones\n\n\n\n\nOct 17, 2022\n\n\nKimble HW 2\n\n\nKaren Kimble\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nKalimah Muhammad\n\n\n\n\nOct 15, 2022\n\n\nHomework 2\n\n\nEthan Campbell\n\n\n\n\nOct 14, 2022\n\n\nHomework 2\n\n\nCaleb Hill\n\n\n\n\nOct 12, 2022\n\n\nFinal Project: Part 1\n\n\nDane Shelton\n\n\n\n\nOct 11, 2022\n\n\nProject Proposal\n\n\nMEGHA JOSEPH\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Submission 1\n\n\nKaushika Potluri\n\n\n\n\nOct 11, 2022\n\n\nFinal_Project_1\n\n\nMani Kanta Gogula & Rahul Gundeti\n\n\n\n\nOct 11, 2022\n\n\nfinalpart1\n\n\nNiharika Pola\n\n\n\n\nOct 11, 2022\n\n\nFinal project part 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 10, 2022\n\n\nFinal Project\n\n\nEthan Campbell\n\n\n\n\nOct 10, 2022\n\n\nFinal Project Proposal\n\n\nEmily Duryea\n\n\n\n\nOct 9, 2022\n\n\nFinal Project Proposal\n\n\nSaaradhaa M\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Proposal\n\n\nLindsay Jones\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project - Proposal\n\n\n\n\n\n\nOct 5, 2022\n\n\nHomework 1\n\n\nNick Boonstra\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603: Homework 1\n\n\nTory Bartelloni\n\n\n\n\nOct 3, 2022\n\n\nShoshanaBuck-HW1\n\n\nShoshana Buck\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nSteph Roberts\n\n\n\n\nOct 3, 2022\n\n\nDuryea Homework 1\n\n\nEmily Duryea\n\n\n\n\nOct 2, 2022\n\n\nDACSS603_HW1\n\n\nRahul Gundeti\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nLindsay Jones\n\n\n\n\nOct 1, 2022\n\n\nHomework 1\n\n\nSaaradhaa M\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 - Donny Snyder\n\n\nDonny Snyder\n\n\n\n\nOct 1, 2022\n\n\nHomework 1 Solution\n\n\nDane Shelton\n\n\n\n\nSep 29, 2022\n\n\nHomework 1\n\n\nEmma Rasmussen\n\n\n\n\nSep 21, 2022\n\n\nHomework 1\n\n\nEthan Campbell\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nSteve O’Neill\n\n\n\n\nSep 20, 2022\n\n\nHomework 2\n\n\nSteve O’Neill\n\n\n\n\nAug 2, 2022\n\n\nHomework 1\n\n\nKaushika Potluri\n\n\n\n\nAug 2, 2022\n\n\nHW3\n\n\nKaren Detter\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nMegha joseph\n\n\n\n\nOct 17, 2022\n\n\nHW2\n\n\nKen Docekal\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Emily Duryea\n\n\nEmily Duryea\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nMani Kanta Gogula\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nShoshana Buck\n\n\n\n\nOct 17, 2022\n\n\nHomework 2 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 17, 2022\n\n\nHomework 2\n\n\nNiyati Sharma\n\n\n\n\nOct 17, 2022\n\n\nHW 2\n\n\nKaren Detter\n\n\n\n\nOct 17, 2022\n\n\nDACSS603_HW2\n\n\nRahul Gundeti\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nMani Shanker Kamarapu\n\n\n\n\nOct 16, 2022\n\n\nHomework 2\n\n\nKaushika Potluri\n\n\n\n\nOct 13, 2022\n\n\nHomework 2\n\n\nEmma Rasmussen\n\n\n\n\nOct 12, 2022\n\n\nVoter Turnout and Partisan Bias in U.S. Presidential Elections\n\n\nNicholas Boonstra\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Part 1\n\n\nEmma Rasmussen\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nNiyati Sharma\n\n\n\n\nOct 11, 2022\n\n\nFinal Project Proposal\n\n\nKaren Detter\n\n\n\n\nOct 11, 2022\n\n\nfinalpart1\n\n\nShoshana Buck & Roy Yoon\n\n\n\n\nOct 11, 2022\n\n\nProject Rough Draft Proposal\n\n\nYakub Rabiutheen\n\n\n\n\nOct 10, 2022\n\n\nHomework 2\n\n\nSaaradhaa M\n\n\n\n\nOct 10, 2022\n\n\nFinal Project 1\n\n\nKen Docekal\n\n\n\n\nOct 7, 2022\n\n\nFinal Project Part 1\n\n\nDonny Snyder\n\n\n\n\nOct 7, 2022\n\n\nDACSS 603 Final Project Pt 1\n\n\nKaren Kimble\n\n\n\n\nOct 5, 2022\n\n\nHomework 1 - Prahitha Movva\n\n\nPrahitha Movva\n\n\n\n\nOct 4, 2022\n\n\nHomework 1\n\n\nOmer Yalcin\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKaren Detter\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nMani Kanta Gogula\n\n\n\n\nOct 3, 2022\n\n\nHW1\n\n\nNiyati Sharma\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 3, 2022\n\n\nHOME WORK1 603\n\n\nMegha Joseph\n\n\n\n\nOct 3, 2022\n\n\nDACSS 603 HW 1 Kimble\n\n\nKaren Kimble\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nQuinn He\n\n\n\n\nOct 3, 2022\n\n\nHomework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nCaleb Hill\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nMani Shanker Kamarapu\n\n\n\n\nInvalid Date\n\n\nDACSS 603: Homework 2\n\n\nTory Bartelloni\n\n\n\n\nOct 3, 2022\n\n\nHomework 1\n\n\nKen Docekal\n\n\n\n\nOct 2, 2022\n\n\nHomework 1\n\n\nNiharika Pola\n\n\n\n\nInvalid Date\n\n\nHomework 2\n\n\nNick Boonstra\n\n\n\n\nOct 3, 2022\n\n\nResubmission: Homework #1\n\n\nKalimah Muhammad\n\n\n\n\nOct 2, 2022\n\n\nFinal Part 1\n\n\nCaleb Hill\n\n\n\n\nOct 1, 2022\n\n\nFinal Part 1\n\n\nSteve O’Neill\n\n\n\n\nSep 20, 2022\n\n\nHomework 1\n\n\nYakub Rabiutheen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/AboutTemplate_mani.html",
    "href": "about/AboutTemplate_mani.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#educationwork-background",
    "href": "about/AboutTemplate_mani.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#r-experience",
    "href": "about/AboutTemplate_mani.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#research-interests",
    "href": "about/AboutTemplate_mani.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hometown",
    "href": "about/AboutTemplate_mani.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#hobbies",
    "href": "about/AboutTemplate_mani.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "about/AboutTemplate_mani.html#fun-fact",
    "href": "about/AboutTemplate_mani.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Your Name\n\n\n\n\n\n\n\nNo matching items"
  }
]