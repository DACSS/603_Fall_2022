{
  "hash": "ed9aa1709cad1330fc3c667fd5461ced",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Niyati Sharma\"\ndescription: \"Homework 5 \"\ndate: \"12/9/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - Niyati Sharma\n  \n---\n\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(smss)\nlibrary(alr4)\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## Answer1\n\n# A\n\nOf all the four predictors listed, Beds would be deleted first because it has the highest p-value\n\n# B\n\nFor forward elimination, we would add Size to the model first. Since Size and New have similar p-values, the correlation matrix reinforces the selection of Size first since it has a higher correlation to price compared to New.\n\n# c\n\nI think that Beds has such a large p-value in the regression model because it is somewhat redundant information. Size its capturing the overall size of the house, including bedrooms,meaning Beds isn’t really adding anything new to the model. Beds could be highly correlated with price since more beds means larger size, but we already have Size accounting for a large portion of the variation.\n\n# D\n::: {.cell}\n\n```{.r .cell-code}\n library(smss)\n    data(\"house.selling.price.2\")\n    reg <- house.selling.price.2\n\n    hw1_lm_full <- lm(P ~ S + Be + Ba + New, data=reg)\n\n    model_terms <- c('New', 'Ba', 'Be', 'S', 'Be + Ba + New', 'S + Be + New', 'S + Ba + New', 'S + Be + Ba', 'Ba + New', 'Be + New', 'Be + Ba', 'S + New', 'S + Be', 'S + Ba', 'S + Be + Ba + New')\n\n    hw1_model_stats <- data.frame(model = character(),\n                                r2 = numeric(),\n                                adj_r2 = numeric(),\n                                PRESS = numeric(),\n                                AIC = numeric(),\n                                BIC = numeric(),\n                                stringsAsFactors = FALSE)\n\n    PRESS <- function(model) {\n        i <- residuals(model)/(1 - lm.influence(model)$hat)\n        sum(i^2)\n    }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n attach(hw1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in attach(hw1): object 'hw1' not found\n```\n:::\n\n```{.r .cell-code}\n    for(i in 1:length(model_terms)){\n      lm.i <- lm(paste(\"P ~ \", model_terms[i], sep = \"\"))\n      sul = summary(lm.i)\n      rowsss <- c(model_terms[i],\n                  signif(sul$r.squared, 4),\n                  signif(sul$adj.r.squared, 4), \n                  signif(PRESS(lm.i), 4),\n                  signif(AIC(lm.i), 4),\n                  signif(BIC(lm.i))\n      )\n      hw1_model_stats <- rbind(hw1_model_stats, rowsss)\n    }\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(predvars, data, env): object 'P' not found\n```\n:::\n\n```{.r .cell-code}\n    colnames(hw1_model_stats)<-c(\"Model\", \"R2\", \"Adj R2\", \"PRESS\", \"AIC\", \"BIC\")\n\n    kbl <- knitr::kable(hw1_model_stats)\n```\n:::\n\n* R2 - In terms of R2, using all the predictors wins out by a slim margin (.8681)\n\n* Adjusted R2 - In terms of Adjusted R2, using Size, Baths, and New wins out.\n\n* PRESS - In terms of using PRESS, again, using Size, Baths, and New wins out.\n\n* AIC - In terms of using AIC, again, using Size, Baths, and New wins out.\n\n* BIC - In terms of using BIC, again, again,using Size, Baths, and New wins out.\n\n# E\nBased off the five criterion listed in the previous question, the model that uses (size, baths, new) as predictors won out on four of the five criteria, thus I would use this model.\n\n## Answer 2\n\n# a\n\n::: {.cell}\n\n```{.r .cell-code}\nblack_cherry <- trees\nblack_cherry_lm <- lm(Volume ~ Height + Girth, data = black_cherry)\npar(mfrow = c(2,3)); plot(black_cherry_lm, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](NiyatiSharma_HW5_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n# b\n\nThe following regression assumptions are being violated:\n\n* Homoskedasticity - According the the residuals vs fitted plot, the residuals seemed to be grouped at various spots within the plot. This seems to suggest some heteroskedasticity going on.\n\n* Independence of Errors - Judging by the Cook’s Distance and Residuals vs Leverage plots, observation 31 seems to be unusually influential. Given its distance from observations, as well as the observation falling outside of the bound in the Residuals vs Leverage plot, it be said that due to this observation, this assumption is being violated.\n\n## Answer 3\n\n# a\n\n::: {.cell}\n\n```{.r .cell-code}\nvt <- lm(Buchanan ~ Bush, data=florida)\npar(mfrow = c(2,3)); plot(vt, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](NiyatiSharma_HW5_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n# b\nI would absolutely classify Palm Beach County as an outlier considering for a majority of the graphs, it is well outside of the rest of the observations. Palm Beach County as a single observation cannot be reasonably grouped with any of the other observations.\n\n\n",
    "supporting": [
      "NiyatiSharma_HW5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}