{
  "hash": "735588becf3d836b2f4f4de44ce88369",
  "result": {
    "markdown": "---\ntitle: \"Homework 5 - Prahitha Movva\"\nauthor: \"Prahitha Movva\"\ndescription: \"The fifth homework\"\ndate: \"12/08/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - model selection\n  - regression\n  - transformation\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\nlibrary(smss)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'smss' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo=TRUE, warning=FALSE)\n```\n:::\n\n\n\n\n## Question 1\n\n### A\n\nWith backward elimination, the variable 'Beds' would be deleted first as it has the largest p-value (0.487) and is therefore the weakest indicator of house price.\n\n\n### B\n\nWith forward selection, the variable 'Size' would be added first as it has the least p-value (0) and is therefore the strongest indicator of house price.\n\n\n### C\n\nBeds has a substantial correlation with not only Price but also Size (which is a statistically significant predictor of house price). This suggests that the variables Beds and Size maybe multicollinear owing to the large p-value for the variables Beds.\n\n\n### D\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(house.selling.price.2)\nm1 <- lm(P ~ S, data = house.selling.price.2)\nm2 <- lm(P ~ S + New, data = house.selling.price.2)\nm3 <- lm(P ~ . -Be, data = house.selling.price.2)\nm4 <- lm(P ~ ., data = house.selling.price.2)\n```\n:::\n\n\n\n#### a. R^2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.807866\n```\n:::\n\n```{.r .cell-code}\nsummary(m2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8483699\n```\n:::\n\n```{.r .cell-code}\nsummary(m3)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8681361\n```\n:::\n\n```{.r .cell-code}\nsummary(m4)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.868863\n```\n:::\n:::\n\n\nThe model using all the variables (m4), since it has the highest R^2\n\n#### b. Adjusted R^2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m1)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8057546\n```\n:::\n\n```{.r .cell-code}\nsummary(m2)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8450003\n```\n:::\n\n```{.r .cell-code}\nsummary(m3)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8636912\n```\n:::\n\n```{.r .cell-code}\nsummary(m4)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8629022\n```\n:::\n:::\n\n\nThe model using all the variables except Beds (m3), since it has the highest adjusted R^2\n\n#### c. PRESS\n\nUsed the code from this discussion on Stack Exchange: https://stats.stackexchange.com/questions/248603/how-can-one-compute-the-press-diagnostic\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS <- function(linear.model) {\n    pr <- residuals(linear.model)/(1 - lm.influence(linear.model)$hat)\n    sum(pr^2)\n}\n\nPRESS(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 38203.29\n```\n:::\n\n```{.r .cell-code}\nPRESS(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31066\n```\n:::\n\n```{.r .cell-code}\nPRESS(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n\n```{.r .cell-code}\nPRESS(m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n:::\n\n\nThe model using all the variables except Beds (m3), since it has the least error\n\n#### d. AIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 820.1439\n```\n:::\n\n```{.r .cell-code}\nAIC(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n\n```{.r .cell-code}\nAIC(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\nAIC(m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n:::\n\n\nThe model using all the variables except Beds (m3), since it has the least AIC, meaning it's a better fit\n\n#### d. BIC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 827.7417\n```\n:::\n\n```{.r .cell-code}\nBIC(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n\n```{.r .cell-code}\nBIC(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\nBIC(m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n:::\n\n\nThe model using all the variables except Beds (m3), since it has the least BIC, meaning it's a better fit\n\n\n### E\n\nI would prefer the model using all the variables except Beds (m3) because the other criterion estimate the fit better than R^2 (we know that using R^2, the value increases with the number of variables even though they are not necessarily good predictors).\n\n\n\n## Question 2\n\n\n### A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"trees\")\nmodel <- lm(Volume ~ Girth + Height, data = trees)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n### B\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_PrahithaMovva_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nResiduals vs Fitted: We can see that it's not a horizontal line (has distinct U-shaped pattern) which is a good indication for a non-linear relationship. So we can say that the 'Linearity' assumption is violated.\n\nNormal Q-Q: Most of the residuals seem to be following the straight dashed line, which means that the data is normally distributed.\n\nScale-Location: A horizontal line with equally spread points is a good indication of homoscedasticity but this is not the case with our data, so we can say that the 'Homoscedasticity' assumption is violated.\n\nResiduals vs Leverage: For it to have a linear relationship (between the predictors and the outcome variables), the red line must be approximately horizontal to zero. Also, the observations 31, 3 and 18 are outside of the dotted line which means they influence the results of the regression (outliers).\n\n\n\n## Question 3\n\n\n## A\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\nmodel <- lm(Buchanan ~ Bush, data = florida)\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_PrahithaMovva_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nYes, Palm Beach County is outside the dotted line in all the plots so I think it is an outlier.\n\n## B\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(log(Buchanan) ~ log(Bush), data = florida)\npar(mfrow = c(2, 3)); plot(model, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_PrahithaMovva_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nNo, Palm Beach County is still an outlier but the Scale-Location and the Residuals vs Leverage plots look better (horizontal line) than in the earlier model.\n\n",
    "supporting": [
      "HW5_PrahithaMovva_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}