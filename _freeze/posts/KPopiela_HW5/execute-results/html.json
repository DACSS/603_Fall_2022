{
  "hash": "3bbde61e2cfc3b69f76dc40e24a690d9",
  "result": {
    "markdown": "---\ntitle: \"HW5\"\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: effects\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\nlibrary(smss)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'smss' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(stats)\nlibrary(tidyr)\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.2\n```\n:::\n:::\n\n\n\n## Question 1\n\n(Data file: house.selling.price.2 from smss R package)\n\nFor the house.selling.price.2 data the tables below (not included) show a correlation matrix and a model fit using four predictors of selling price: size (home and lot), # of bedrooms, # of bathrooms, and whether or not the house is new. \n\n(Hint 1: You should be able to answer A, B, C just using the tables, although you should feel free to load the data in R and work with it if you so choose. They will be consistent with what you see on the tables.\n\nHint 2: The p-value of a variable in a simple linear regression is the same p-value one would get from a Pearson’s correlation (cor.test). The p-value is a function of the magnitude of the correlation coefficient (the higher the coefficient, the lower the p-value) and of sample size (larger samples lead to smaller p-values). For the correlations shown in the tables, they are between variables of the same length.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"house.selling.price.2\")\nnames(house.selling.price.2) <- c('Price','Size','Bedrooms','Bathrooms','New')\n\nsummary(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Price             Size         Bedrooms       Bathrooms    \n Min.   : 17.50   Min.   :0.40   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 72.90   1st Qu.:1.33   1st Qu.:3.000   1st Qu.:2.000  \n Median : 96.00   Median :1.57   Median :3.000   Median :2.000  \n Mean   : 99.53   Mean   :1.65   Mean   :3.183   Mean   :1.957  \n 3rd Qu.:115.00   3rd Qu.:1.98   3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :309.40   Max.   :3.85   Max.   :5.000   Max.   :3.000  \n      New        \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3011  \n 3rd Qu.:1.0000  \n Max.   :1.0000  \n```\n:::\n:::\n\n\n#### a. For backward elimination, which variable would be deleted first? Why?  \nIn backward elimination, the variable 'bedrooms' would be removed first as it has the highest p-value of the variables.\n\n#### b. For forward selection, which variable would be added first? Why?   \nIn forward selection 'size' would be added first as it has the lowest p-value at 0.\n\n#### c. Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE? \n'BEDS' may have such a large p-value in the multiple regression model because of its relationship to 'SIZE' - the price of a house goes up as its size increases. The extent of the price increase would probably be affected by other variables such as 'NEW', since the age of a house also affects its price.\n\n#### d. Using software with these four predictors, find the model that would be selected using each criterion: \n      i. R^2 \n      ii. Adjusted R^2  \n      iii. PRESS  \n      iv. AIC\n      v. BIC  \n\nBackward elimination, forward selection, and stepwise regression point to one model: the one that excludes the variable ‘Bedrooms’. To confirm these findings, I’m going to compare the model without ‘Bedrooms’ to one that does.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_bedrooms <- lm(Price~.,data=house.selling.price.2)\nfit_no_bedrooms <- lm(Price~.-Bedrooms,data=house.selling.price.2)\n```\n:::\n\n\nLets create functions to gather the information needed to move forward with the problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_squared <- function(fit)+\n  summary(fit)$r.squared\nadjusted_rsq <- function(fit)+\n  summary(fit)$adj.r.squared\npress <- function(fit) {\n  pr <- residuals(fit)/(1-lm.influence(fit)$hat)\n  sum(pr^2)\n}\n```\n:::\n\n\nNow lets apply these to the two models (fit_bedrooms and fit_no_bedrooms).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbed_models <- list(fit_bedrooms, fit_no_bedrooms)\ndata.frame(bed_models = c('fit_bedrooms','fit_no_bedrooms'),\n           r_squared=sapply(bed_models,r_squared),\n           adjusted_rsq=sapply(bed_models, adjusted_rsq),\n           press=sapply(bed_models,press),\n           AIC=sapply(bed_models, AIC),\n           BIC=sapply(bed_models,BIC))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       bed_models r_squared adjusted_rsq    press      AIC      BIC\n1    fit_bedrooms 0.8688630    0.8629022 28390.22 790.6225 805.8181\n2 fit_no_bedrooms 0.8681361    0.8636912 27860.05 789.1366 801.7996\n```\n:::\n:::\n\n\nFor R^2 and Adjusted R^2, the larger the value, the more favorable it is. But for Press, AIC, and BIC the opposite is true: we want the smallest values we can get. With that in mind, the model 'fit_no_bedrooms' is more fitting as it has a higher Adjustred R^2 and lower values for Press, AIC, and BIC.\n\n#### e. Explain which model you prefer and why.\n\nBased on the comparison above, I prefer the model with 'BEDS' excluded.\n\n\n## Question 2\n\n(Data file: 'trees' from base R)\n\n(Data file: trees from base R)\nFrom the documentation:\n“This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.”\n\nTree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular:\n\n#### a. fit a multiple regression model with  the Volume as the outcome and Girth  and Height as the explanatory variables  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\ntree_mrm <- lm(Volume~Girth+Height,data=trees)\nsummary(tree_mrm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Girth + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nGirth         4.7082     0.2643  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### b. Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,3))\nplot(tree_mrm,which=1:6)\n```\n\n::: {.cell-output-display}\n![](KPopiela_HW5_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLooking at the above matrix, I do think it has some violations. For example, the red line in the first plot violates that linearity assumption since its u-shaped rather than straight. Let's compare with a similar plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_d2 <- lm(Volume~Girth+I(Girth^2)+Height,data=trees)\nplot(tree_d2,which=1)\n```\n\n::: {.cell-output-display}\n![](KPopiela_HW5_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nUnlike the red line in the first plot of the initial matrix, this one does NOT violate the linearity assumption; this line is much straighter.\n\n\n## Question 3  \n\n(Data file: florida in alr R package) \n\nIn the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.\n\nThe data has variables for the number of votes for each candidate—Gore, Bush, and Buchanan. \n\n#### a. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(florida)\nfl_lrm <- lm(Buchanan~Bush,data=florida)\npar(mfrow=c(2,3))\nplot(fl_lrm, which=1:6)\n```\n\n::: {.cell-output-display}\n![](KPopiela_HW5_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nIn every plot in this matrix Palm Beach County is an outlier; it sits pretty far from all the other data points.\n\n#### b. Run a simple linear regression model where the Buchanan vote is the outcome and the Bush vote is the explanatory variable. Produce the regression diagnostic plots. Is Palm Beach County an outlier based on the diagnostic plots? Why or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfl_log <- lm(log(Buchanan)~log(Bush), data=florida)\npar(mfrow=c(2,3))\nplot(fl_log, which=1:6)\n```\n\n::: {.cell-output-display}\n![](KPopiela_HW5_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis model didn't change my initial opinion about Palm Beach County being an outlier, since that's still the case. The only difference is that Palm Beach County appears to be less of an ourlier than it was in the first model.\n\n\n",
    "supporting": [
      "KPopiela_HW5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}