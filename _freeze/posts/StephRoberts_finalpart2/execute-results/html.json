{
  "hash": "09e98e57589a9e250425b9399436b5c0",
  "result": {
    "markdown": "---\ntitle: \"Final Project: Diabetes Prediction - Part 2\"\nauthor: \"Steph Roberts\"\ndesription: \"Final Project Draft 2\"\ndate: \"10/9/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - finalproject\n  - Steph Roberts\n  - dataset\n  - ggplot2\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(Hmisc)\nlibrary(pscl)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in library(pscl): there is no package called 'pscl'\n```\n:::\n\n```{.r .cell-code}\nlibrary(magrittr)\n\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n### Diabetes risk factors\n\nAccording to the World Health Organization (WHO), an estimated 537 million people worldwide are living with diabetes. It is a leading cause of health complications and even death. The WHO states close to 1.5 million people died due to diabetes and its complication in 2019 alone. It is a growing problem that requires dedicated research to aim at the slowdown and prevention of future cases. \n\n## Research Questions\n1. What risk factors are most predictive of diabetes? \n\nResearch on Diabetes is ongoing and in-depth within the medical field. The prevalence and incidence of diabetes mellitus type 2 (DQ2) have increased consistently for decades, giving way to an increase in mortality related to diabetes. Commonly in the medical field, many risk factors are used to measure a patient’s risk of developing DM2, such as obesity, family history, hypertension and changes in fasting blood sugar levels. \nMoreno et al. (2018) studied risk parameters for diabetes and concluded \"risk of being diabetic rises in patients whose father has suffered an acute myocardial infarction, in those whose mother or father is diabetic and in patients with a high waist perimeter.\" \nTheir focus on family history leaves room for studies more focused on individual medical factors, such as blood pressure, BMI, number of pregnancies, etc. That is the aim of this project.\n\n\nM. L. M. V. J. A. (2018, June 11). Predictive risk model for the diagnosis of diabetes mellitus type 2 in a follow-up study 15 Years on: Prodi2 study. European journal of public health. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/29897477/ \n\n\n2. Can the use of regression analysis help predict risk of diabetes based on several medical variables?\n\nOther research, such as a Edlitz & Segal (2022) study titled Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards, does focus on using individual medical factors to predict risk of diabetes through regression. This project aims to conduct similar analysis on different data. \n\nE. Y. S. E. (2022, June 22). Prediction of type 2 diabetes mellitus onset using logistic regression-based scorecards. eLife. Retrieved October 9, 2022, from https://pubmed.ncbi.nlm.nih.gov/35731045/ \n\n## Hypothesis\n\nBody mass index (BMI), glucose, and age are positive predictors of diabetes mellitus type 2.  \n\n\nBoth hypothesis have been tested in the above mentioned studies. The contribution from this project will be the additional support for or against the hypotheses from the analysis of different data. \n\n## Descriptive Statistics\n\nThe data was collected by the “National Institute of Diabetes and Digestive and Kidney Diseases” as part of the Pima Indians Diabetes Database (PIDD). A total of 768 cases are available in PIDD. However, 5 patients had a glucose of 0, 11 patients had a body mass index of 0, 28 others had a diastolic blood pressure of 0, 192 others had skin fold thickness readings of 0, and 140 others had serum insulin levels of 0. After cleaning the data by removing the cases with numbers that are incompatible with life, 392 cases remained. All patients belong to the Pima Indian heritage (subgroup of Native Americans), and are females aged 21 years and above.\n\nThe datasets consists of 9 medical predictor (independent) variables and one target (dependent) variable, outcome.  \n\n**Pregnancies:** Number of times a woman has been pregnant\n**Glucose:** Plasma Glucose concentration of 2 hours in an oral glucose tolerance test\n**BloodPressure:** Diastolic Blood Pressure (mm hg)\n**SkinThickness:** Triceps skin fold thickness(mm)\n**Insulin:** 2 hour serum insulin(mu U/ml)\n**BMI:** Body Mass Index ((weight in kg/height in m)^2)\n**Age:** Age(years)\n**DiabetesPedigreeFunction:** scores likelihood of diabetes based on family history\n**Outcome:** 0(doesn't have diabetes) or 1 (has diabetes)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf<- read_csv(\"diabetes2.csv\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: 'diabetes2.csv' does not exist in current working directory ('C:/Users/srika/OneDrive/Desktop/DACSS/603_Fall_2022/posts').\n```\n:::\n\n```{.r .cell-code}\ndim(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\nsummary(df)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in object[[i]]: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n6 }                                           \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#check for null entries\nis.null(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Check number of 0s in each column\ncolSums(df==0)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in df == 0: comparison (1) is possible only for atomic and list types\n```\n:::\n:::\n\n\nGlucose, blood pressure, skin thickness, insulin, BMI and Age are not variables that should logically have 0s. Those values, if true, are likely incompatible with life. To reduce the amount of data we exclude, we will only remove observations that have 0s for our explanatory variables. We will remove those cases from analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Remove rows with 0 in respective columns\ndf <- df[apply(df[c(2,6,8)],1,function(z) !any(z==0)),] \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in df[c(2, 6, 8)]: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\n#Verify 0s are gone in selected rows\ncolSums(df==0)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in df == 0: comparison (1) is possible only for atomic and list types\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Check cleaned data frame\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (x, df1, df2, ncp, log = FALSE)  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize df\nsummary(df)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in object[[i]]: object of type 'closure' is not subsettable\n```\n:::\n:::\n\nAt a glance, this summary is more fitting after having cleaned our data. An average of 3 pregnancies, considering our 21+ female population, makes sense. A mean glucose of 122, blood pressure of 70.66, a BMI of 33, and age of 30.86 are reasonably accurate of our population. The data is clean and ready for analysis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Rename columns\ncolnames(df) <- c(\"pregnancies\", \"glucose\", \"bp\", \"skin_thickness\", \"insulin\", \"bmi\", \"dpf\", \"age\", \"diabetes\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `colnames<-`(`*tmp*`, value = c(\"pregnancies\", \"glucose\", \"bp\", : attempt to set 'colnames' on an object with less than two dimensions\n```\n:::\n\n```{.r .cell-code}\ndim(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\nWe have 752 observations and 9 variables with which to work. \n\nLet's get familiar with the data set by observing the scatterplot matrix and histograms for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Scatterplot matrix\npairs(df)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.data.frame.default(x): cannot coerce class '\"function\"' to a data.frame\n```\n:::\n\n```{.r .cell-code}\n#Histograms of each variable\nhist.data.frame(df)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in for (v in x) {: invalid for() loop sequence\n```\n:::\n:::\n\nThe scatterplot matix shows that our variables generally lack linear relationships. Logistics regression may be best for analyzing this type of data. \n\nThe histograms show us the distribution of all our variables. Pregnancies is right skewed with more women having a smaller number of pregnancies and a small number having over 10. Glucose appears to follow a close-to-normal distribution with the mean just over 100. BMI follows a similar pattern with a central value of just over 30. Age is right skewed with the majority of the sample appearing to be in their 20s and slower tapering off to ages in their 50s. This is a good place to start. The only variable we cannot view in a histogram is the outcome variable due to its binary nature as a dummy variable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbgap <- as.data.frame(df[, c(1,2,6,8,9)])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in df[, c(1, 2, 6, 8, 9)]: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\npairs(bgap)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in pairs(bgap): object 'bgap' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot outcome variable\nggplot(df, aes(x=factor(diabetes))) +\n  geom_bar(fill = \"#0073C2FF\")+\n  scale_x_discrete(labels=c('No Diabetes','Diabetes'))+\n  xlab(\"Outcome\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `ggplot()`:\n!   You're passing a function as global data.\n  Have you misspelled the `data` argument in `ggplot()`\n```\n:::\n:::\n\nAbout a third of our sample have diabetes. \n\n## Hypothesis Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Create training and test data\nset.seed(1)\nbgap %>% \n  nrow() %>% \n  multiply_by(0.7) %>% \n  round() -> training_set_size\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(.): object 'bgap' not found\n```\n:::\n\n```{.r .cell-code}\ntrain_indices <- sample(1:nrow(df), training_set_size)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in 1:nrow(df): argument of length 0\n```\n:::\n\n```{.r .cell-code}\ntrain <- bgap[train_indices,]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bgap' not found\n```\n:::\n\n```{.r .cell-code}\ntest <- bgap[-train_indices,]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'bgap' not found\n```\n:::\n\n```{.r .cell-code}\nnrow(train)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(train): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\nnrow(test)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(test): object 'test' not found\n```\n:::\n\n```{.r .cell-code}\nhead(train)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(train): object 'train' not found\n```\n:::\n:::\n\nAs a reminder, pregnancies, age, and BMI are all right skewed. So, we will use the logarithm of those variables in our regressions. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Model using all independent variables\nall_var <- glm(diabetes ~ ., data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\n#Model using BMI, glucose, and age\nbga <- glm(diabetes ~ . -pregnancies, data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\n#Model using BMI and glucose\nbg <- glm(diabetes ~ .-age -pregnancies, data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\n#Model using BMI, glucose, age, and an interaction between pregnancies and age.\nia <- glm(diabetes ~ glucose + bmi + age*pregnancies, data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\n#Model with BMI, glucose, and pregnancies (without age)\nbgp <- glm(diabetes ~ . -age, data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n:::\n\n\nOur hypothesis involves the BMI, glucose, and age variables. First we can take a look at them individually. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Generalized linear model can be best for predicting categorical outcome\n#Regression model on BMI\nmbmi <- glm(diabetes ~ bmi, data = train)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(mbmi)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(mbmi): object 'mbmi' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(data = train, aes(x =bmi, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(data = train, aes(x = bmi, y = diabetes)): object 'train' not found\n```\n:::\n:::\n\nInterpretation: The residuals are relatively small. The coefficient estimate suggests that for every 1 unit increase of BMI, the chance of diabetes increases 0.019. The standard errors are small. Our t-values are large compared to our standard error and relatively far from 0. All that, combined with a very small p-value of < 5.15e-11, indicate we can reject the null hypothesis and conclude a relationship between BMI and diabetes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Regression model on glucose \nmgluc <- glm(diabetes ~ glucose, data = train)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(mgluc)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(mgluc): object 'mgluc' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(data = train, aes(x = glucose, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(data = train, aes(x = glucose, y = diabetes)): object 'train' not found\n```\n:::\n:::\n\nInterpretation: Interestingly, the values of this regression mirror that of the BMI regression. With a very small p-value, < 2e-16, we can see that the plasma glucose concentrations of the 2 hour oral glucose tolerance test (GGT) are related to our outcome variable, diabetes. This makes sense as GGT is sometimes used as a diagnostic tool for diabetes. \n\nThe similarities is regression model values between BMI and glucose sparked a curiosity. Let's see if BMI and glucose are specifically correlated to one another. \n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(train$bmi, train$glucose, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in cor.test(train$bmi, train$glucose, method = \"pearson\"): object 'train' not found\n```\n:::\n:::\n\nThis shows a very small p-value of 8.477e-09, which is well under the significance level alpha = 0.5. However, the correlation coefficient suggests a weak positive correlation. This suggests the variables are only weakly related.\n\nLet's continue with our regression model hypothesis testing of individual variables. \n\n::: {.cell}\n\n```{.r .cell-code}\n#Regression model on age\nmage <- glm(diabetes ~ age, data = train)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(data): object 'train' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(mage)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(mage): object 'mage' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(data = train, aes(x = age, y = diabetes)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(data = train, aes(x = age, y = diabetes)): object 'train' not found\n```\n:::\n:::\n\nInterpretation: Similar to the previous variable, age also has small residuals and standard error. It shows a p-value of 1.12e-06, indicating a relationship between age and diabetes. \n\nThe hypothesis that BMI, glucose tolerance results, and age are positively associated with diabetes is true. Next, we can investigate models that could help predict the outcome using these variables. \n\n## Model Comparisons\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize model including BMI, glucose, age, and pregnancies\nsummary(all_var)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(all_var): object 'all_var' not found\n```\n:::\n:::\n\nWe can see with the all variable model that collectively, independent variables including pregnancies, glucose, bmi, dpf and even blood pressure have low p-values. That suggests those variables have a relationship with the outcome variable. We can use AIC as an indicator of model fitness, with this model being 517.97. Let's compare this with the following models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize BMI, glucose, and age model\nsummary(bga)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(bga): object 'bga' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(train, aes(x=age, y=bmi, color=glucose)) + geom_point() + facet_grid(~diabetes)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(train, aes(x = age, y = bmi, color = glucose)): object 'train' not found\n```\n:::\n:::\n\nThe model looking at BMI, age, and glucose as explanatory variables for diabetes has an AIC of 526.6, which is a worse fit than the all variables model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize BMI and glucose model\nsummary(bg)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(bg): object 'bg' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(train, aes(x=glucose, y=bmi, color=diabetes)) + geom_point() + facet_grid(~diabetes)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(train, aes(x = glucose, y = bmi, color = diabetes)): object 'train' not found\n```\n:::\n:::\n\nThis BMI and glucose only model has a worse (higher) AIC, with 536.8, than the 2 previous models. So far, the all variable model appears the best fit. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize model including interaction between age and pregnancies \n#This was chosen to see if the impact of age on diabetes is different depending on the number of pregnancies\nsummary(ia)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(ia): object 'ia' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(train, aes(x=age, y=bmi, color=pregnancies)) + geom_point() + facet_grid(~diabetes)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(train, aes(x = age, y = bmi, color = pregnancies)): object 'train' not found\n```\n:::\n:::\n\nWith an AIC of 510.41, this model appears to be the best fit model of those we have compared. This indicated there is an interaction between age and pregnancies that has a relationship with the outcome variable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summarize model including without age\nsummary(bgp)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in summary(bgp): object 'bgp' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(train, aes(x=pregnancies, y=bmi, color=glucose)) + geom_point() + facet_grid(~diabetes)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(train, aes(x = pregnancies, y = bmi, color = glucose)): object 'train' not found\n```\n:::\n:::\n\nRemoving age entirely increased the AIC of this model. So, the interaction model seems to be the best fit. That is, the model including BMI and glucose and controlling for an interaction between age and pregnancy has the best fit of all models tested. \n\nWe can use ANOVA to analyze the table of deviance of our regression model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANOVA test on interaction model\nanova(ia, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in anova(ia, test = \"Chisq\"): object 'ia' not found\n```\n:::\n:::\n\nAll of our variables have small p-values and appear good predictors of the diabetes outcome, with the exception of age. However, age is needed for the interaction variable. \n\nLet's check one more thing to ensure we have a good fit. \n\n::: {.cell}\n\n```{.r .cell-code}\n#Check pseudo R2 for interaction model\npR2(ia)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in pR2(ia): could not find function \"pR2\"\n```\n:::\n:::\n\nWith a McFadden R-squared of 0.27, this model can be considered an excellent fit. \n\n## Diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Apply model to test data\nfitted.results <- predict(ia ,test,type='response')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in predict(ia, test, type = \"response\"): object 'ia' not found\n```\n:::\n\n```{.r .cell-code}\nfitted.results <- ifelse(fitted.results > 0.5,1,0)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ifelse(fitted.results > 0.5, 1, 0): object 'fitted.results' not found\n```\n:::\n\n```{.r .cell-code}\npred <- mean(fitted.results != test$diabetes)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mean(fitted.results != test$diabetes): object 'fitted.results' not found\n```\n:::\n\n```{.r .cell-code}\noutput <- cbind(test, fitted.results)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in cbind(test, fitted.results): object 'test' not found\n```\n:::\n\n```{.r .cell-code}\nprint(paste('Accuracy',1-pred))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in paste(\"Accuracy\", 1 - pred): object 'pred' not found\n```\n:::\n\n```{.r .cell-code}\nhead(output)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(output): object 'output' not found\n```\n:::\n:::\n\nTesting the interaction model on the test data produces the correct outcome 79% of the time. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Diagnostic plot of model\nplot(ia)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(ia): object 'ia' not found\n```\n:::\n:::\n\n**Residuals vs Fitted**: The residuals have 2 distinct patterns of behavior. They start at close to 0 and taper off to a negative slope. Another group of residuals starts large and tapers on a negative slope toward 0. \n\n**Normal Q-Q**: The Q-Q plot, which indicates level of asymmetry, tells us our data is not a normal distribution. It appears to have a right tail (see the points drifting from the line at the top), which aligns with our individual variable histogram interpretations. It follows that a model combining variables with right skews may also be skewed to the right. However, it also has values away from the line on the bottom left, away from the intercept. This suggests our model also includes a left tail. This distribution seems to have \"fat tails.\"\n\n**Scale-Location**: This plots the fitted values of the model along the x-axis and the square root of the standardized residuals along the y-axis. The red line does roughly travel horizontal across the plot. However, there is a clear X pattern among residuals. This suggests our model has heteroscedasticity, which is not ideal. \n\n**Residuals vs Leverage**: The points on this graph all fall within Cook's distance, indicating there are not any influential points in our regression model. \n\n\n\n*\nPlan to work on fixing the heteroscedasticity by log transforming BMI, pregnancies, and age when those variables are included in models because they are right skewed data. However, it is taking time figuring out how to do a multiple logistics regression without errors..\n*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}