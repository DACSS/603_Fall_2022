{
  "hash": "f26ff0f87c0f1150e2816ac051688241",
  "result": {
    "markdown": "---\ntitle: \"Homework 5\"\nauthor: \"Steve O'Neill\"\ndescription: \"Homework 5\"\ndate: \"11/27/2022\"\ndf-paged: true\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smss)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\ndata(house.selling.price.2)\nlibrary(alr4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: car\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nLoading required package: effects\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\ndata(\"florida\")\n```\n:::\n\n\n*For the house.selling.price.2 data the tables below show a correlation matrix and a model fit using four predictors of selling price.*\n\nIn this data, the variables are meant as:\n\n`P`: selling price\n\n`Be`: number of bedrooms\n\n`Ba`: number of bathrooms\n\n`New`: whether new (1 = yes, 0 = no)\n\nHere is my impression of the correlation matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(house.selling.price.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            P         S        Be        Ba       New\nP   1.0000000 0.8988136 0.5902675 0.7136960 0.3565540\nS   0.8988136 1.0000000 0.6691137 0.6624828 0.1762879\nBe  0.5902675 0.6691137 1.0000000 0.3337966 0.2672091\nBa  0.7136960 0.6624828 0.3337966 1.0000000 0.1820651\nNew 0.3565540 0.1762879 0.2672091 0.1820651 1.0000000\n```\n:::\n:::\n\n\nAnd regression output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(P ~ ., data=house.selling.price.2)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ ., data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.212  -9.546   1.277   9.406  71.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -41.795     12.104  -3.453 0.000855 ***\nS             64.761      5.630  11.504  < 2e-16 ***\nBe            -2.766      3.960  -0.698 0.486763    \nBa            19.203      5.650   3.399 0.001019 ** \nNew           18.984      3.873   4.902  4.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 88 degrees of freedom\nMultiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \nF-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Automated variable selection\n\n*For backward elimination, which variable would be deleted first? Why?*\n\nIf I was doing backward elimination, I would pick a significance level (let's say alpha = .05) and, at each stage, delete the variable with the largest p-value. I would stop when all variables are significant.\n\nIn this example, I would delete the `Be` (bedroom) variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(P ~ . - Be, data=house.selling.price.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ . - Be, data = house.selling.price.2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.804  -9.496   0.917   7.931  73.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\nS             62.263      4.335  14.363  < 2e-16 ***\nBa            20.072      5.495   3.653 0.000438 ***\nNew           18.371      3.761   4.885 4.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.31 on 89 degrees of freedom\nMultiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \nF-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n*For forward selection, which variable would be added first? Why?*\n\nLike backward elimination, I would also predetermine a significance level (say, 5%). But here I would begin with no explanatory variable.\n\nThe `Size` variable would be added first in forward selection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintercept_only <- lm(P ~ 1, data=house.selling.price.2)\n\nstep(intercept_only, direction = \"forward\", scope=~ S + Be + Ba + New)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=705.63\nP ~ 1\n\n       Df Sum of Sq    RSS    AIC\n+ S     1    145097  34508 554.22\n+ Ba    1     91484  88121 641.41\n+ Be    1     62578 117028 667.79\n+ New   1     22833 156772 694.99\n<none>              179606 705.63\n\nStep:  AIC=554.22\nP ~ S\n\n       Df Sum of Sq   RSS    AIC\n+ New   1    7274.7 27234 534.20\n+ Ba    1    4475.6 30033 543.30\n<none>              34508 554.22\n+ Be    1      40.4 34468 556.11\n\nStep:  AIC=534.2\nP ~ S + New\n\n       Df Sum of Sq   RSS    AIC\n+ Ba    1    3550.1 23684 523.21\n+ Be    1     588.8 26645 534.17\n<none>              27234 534.20\n\nStep:  AIC=523.21\nP ~ S + New + Ba\n\n       Df Sum of Sq   RSS    AIC\n<none>              23684 523.21\n+ Be    1    130.55 23553 524.70\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = P ~ S + New + Ba, data = house.selling.price.2)\n\nCoefficients:\n(Intercept)            S          New           Ba  \n     -47.99        62.26        18.37        20.07  \n```\n:::\n:::\n\n\n*Why do you think that BEDS has such a large P-value in the multiple regression model, even though it has a substantial correlation with PRICE?*\n\nAs pointed out, `Be` does have a substantial correlation with `P` at .59. However, the large P-values in multiple regression indicate that while holding other variables fixed, it does not 'explain' the response variable of `P`, price.\n\n*Using software with these four predictors, find the model that would be selected using each criterion:*\n\nI'm not sure if I exactly get the question, but I will arbitrarily compare some models that I have made from combinations of the predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(P ~ S, data=house.selling.price.2)\nmod2 <- lm(P ~ S + New, data=house.selling.price.2)\nmod3 <- lm(P ~ S + New + Ba, data=house.selling.price.2)\nmod4 <- lm(P ~ .,data=house.selling.price.2)\n\n#A few with interaction variables\n\nmod5 <- lm(P ~ S + New + S*New, data=house.selling.price.2)\nmod6 <- lm(P ~ S + New + Ba + S*New, data=house.selling.price.2)\nmod7 <- lm(P ~ . + S * New, data=house.selling.price.2)\n```\n:::\n\n\n###R2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.807866\n```\n:::\n\n```{.r .cell-code}\nsummary(mod2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8483699\n```\n:::\n\n```{.r .cell-code}\nsummary(mod3)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8681361\n```\n:::\n\n```{.r .cell-code}\nsummary(mod4)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.868863\n```\n:::\n\n```{.r .cell-code}\nsummary(mod5)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8675196\n```\n:::\n\n```{.r .cell-code}\nsummary(mod6)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8891938\n```\n:::\n\n```{.r .cell-code}\nsummary(mod7)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8906193\n```\n:::\n:::\n\nUsing 'highest R-squared' as our criteria, `P ~ . + S * New` is the winner. That one includes all the predictor variables in the equation, with size and newness as interaction variables.\n\n###Adjusted R2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8057546\n```\n:::\n\n```{.r .cell-code}\nsummary(mod2)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8450003\n```\n:::\n\n```{.r .cell-code}\nsummary(mod3)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8636912\n```\n:::\n\n```{.r .cell-code}\nsummary(mod4)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8629022\n```\n:::\n\n```{.r .cell-code}\nsummary(mod5)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8630539\n```\n:::\n\n```{.r .cell-code}\nsummary(mod6)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8841571\n```\n:::\n\n```{.r .cell-code}\nsummary(mod7)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8843331\n```\n:::\n:::\n\nAdjusted R-squared penalizes for adding more explanatory variables to the regression. However, it is still a virtual tie between `P ~ S + New + Ba + S*New` and `P ~ . + S * New`. Still, the latter wins. \n\n###PRESS\n\nThis elegant function is found on [Github](https://gist.github.com/tomhopper/8c204d978c4a0cbcb8c0) and requires no additional libraries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS <- function(linear.model) {\n  #' calculate the predictive residuals\n  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)\n  #' calculate the PRESS\n  PRESS <- sum(pr^2)\n  \n  return(PRESS)\n}\n```\n:::\n\n\nAccording to a comparison of the PRESS statistics, the best model is `P ~ S + New + Ba + S*New` with a PRESS of 27501.78\n\n::: {.cell}\n\n```{.r .cell-code}\nPRESS(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 38203.29\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31066\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27860.05\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28390.22\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31899.8\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27501.78\n```\n:::\n\n```{.r .cell-code}\nPRESS(mod7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27665.14\n```\n:::\n:::\n\n###AIC\n\nAccording to AIC, the best (lowest) score is 774.9558, associated with the model `P ~ S + New + Ba + S*New`\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 820.1439\n```\n:::\n\n```{.r .cell-code}\nAIC(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n\n```{.r .cell-code}\nAIC(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\nAIC(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\nAIC(mod5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.5704\n```\n:::\n\n```{.r .cell-code}\nAIC(mod6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 774.9558\n```\n:::\n\n```{.r .cell-code}\nAIC(mod7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 775.7515\n```\n:::\n:::\n\n###BIC\n\nAccording to BIC, the best model is the same - `P ~ S + New + Ba + S*New`, with a statistic of 790.1514.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 827.7417\n```\n:::\n\n```{.r .cell-code}\nBIC(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n\n```{.r .cell-code}\nBIC(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\nBIC(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\nBIC(mod5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 802.2334\n```\n:::\n\n```{.r .cell-code}\nBIC(mod6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.1514\n```\n:::\n\n```{.r .cell-code}\nBIC(mod7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 793.4797\n```\n:::\n:::\n\n\n*Explain which model you prefer and why.*\n\nI prefer the model favored by AIC and BIC, `P ~ S + New + Ba + S*New`. Intuitively, it makes sense that bedrooms are not a significant driver of a house's price, although bathrooms are. And the size of a house is more consequential on the outcome of price when the building is newer.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod2, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod5, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod6, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod7, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-13-4.png){width=672}\n:::\n:::\n\n\n# Question 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\ntrees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n11  11.3     79   24.2\n12  11.4     76   21.0\n13  11.4     76   21.4\n14  11.7     69   21.3\n15  12.0     75   19.1\n16  12.9     74   22.2\n17  12.9     85   33.8\n18  13.3     86   27.4\n19  13.7     71   25.7\n20  13.8     64   24.9\n21  14.0     78   34.5\n22  14.2     80   31.7\n23  14.5     74   36.3\n24  16.0     72   38.3\n25  16.3     77   42.6\n26  17.3     81   55.4\n27  17.5     82   55.7\n28  17.9     80   58.3\n29  18.0     80   51.5\n30  18.0     80   51.0\n31  20.6     87   77.0\n```\n:::\n:::\n\n\n*From the documentation: \"This data set provides measurements of the diameter, height and volume of timber in 31 felled black cherry trees. Note that the diameter (in inches) is erroneously labeled Girth in the data. It is measured at 4 ft 6 in above the ground.\"*\n\n*Tree volume estimation is a big deal, especially in the lumber industry. Use the trees data to build a basic model of tree volume prediction. In particular, *\n\n*fit a multiple regression model with  the Volume as the outcome and Girth and Height as the explanatory variables*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod8 <- lm(Volume ~ Girth + Height, data = trees)\n```\n:::\n\n\n*Run regression diagnostic plots on the model. Based on the plots, do you think any of the regression assumptions is violated?*\n\nThese plots show some significant issues with the regression `Volume ~ Girth + Height`. \n\n1. The 'Residuals vs Fitted' graph is U-shaped, indicating an issue with linearity. \n\n2. The Normal Q-Q graph actually looks pretty good except for a wild outlier - this shows an issue with normality.\n\n3. The Scale-Location graph should also be flat, but isn't. This indicates an issue with homoscedasticity.\n\n4. The Cook's distance graph shows a clear outlier in one of the observations. A 'high leverage' observation (above the benchmark of 1 or n/4) may effect the regression if it were taken out.\n\n5. The Residuals vs Leverage plot and Cook's dist vs Leverage plot also show the influence of this outlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod8, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n# Question 3\n\n*In the 2000 election for U.S. president, the counting of votes in Florida was controversial. In Palm Beach County in south Florida, for example, voters used a so-called butterfly ballot. Some believe that the layout of the ballot caused some voters to cast votes for Buchanan when their intended choice was Gore.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflorida\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Gore   Bush Buchanan\nALACHUA       47300  34062      262\nBAKER          2392   5610       73\nBAY           18850  38637      248\nBRADFORD       3072   5413       65\nBREVARD       97318 115185      570\nBROWARD      386518 177279      789\nCALHOUN        2155   2873       90\nCHARLOTTE     29641  35419      182\nCITRUS        25501  29744      270\nCLAY          14630  41745      186\nCOLLIER       29905  60426      122\nCOLUMBIA       7047  10964       89\nDADE         328702 289456      561\nDE SOTO        3322   4256       36\nDIXIE          1825   2698       29\nDUVAL        107680 152082      650\nESCAMBIA      40958  73029      504\nFLAGLER       13891  12608       83\nFRANKLIN       2042   2448       33\nGADSDEN        9565   4750       39\nGILCHRIST      1910   3300       29\nGLADES         1420   1840        9\nGULF           2389   3546       71\nHAMILTON       1718   2153       24\nHARDEE         2341   3764       30\nHENDRY         3239   4743       22\nHERNANDO      32644  30646      242\nHIGHLANDS     14152  20196       99\nHILLSBOROUGH 166581 176967      836\nHOLMES         2154   4985       76\nINDIAN RIVER  19769  28627      105\nJACKSON        6868   9138      102\nJEFFERSON      3038   2481       29\nLAFAYETTE       788   1669       10\nLAKE          36555  49963      289\nLEE           73560 106141      305\nLEON          61425  39053      282\nLEVY           5403   6860       67\nLIBERTY        1011   1316       39\nMADISON        3011   3038       29\nMANATEE       49169  57948      272\nMARION        44648  55135      563\nMARTIN        26619  33864      108\nMONROE        16483  16059       47\nNASSAU         6952  16404       90\nOKALOOSA      16924  52043      267\nOKEECHOBEE     4588   5058       43\nORANGE       140115 134476      446\nOSCEOLA       28177  26216      145\nPALM BEACH   268945 152846     3407\nPASCO         69550  68581      570\nPINELLAS     199660 184312     1010\nPOLK          74977  90101      538\nPUTNAM        12091  13439      147\nST. JOHNS     19482  39497      229\nST. LUCIE     41559  34705      124\nSANTA ROSA    12795  36248      311\nSARASOTA      72854  83100      305\nSEMINOLE      58888  75293      194\nSUMTER         9634  12126      114\nSUWANNEE       4084   8014      108\nTAYLOR         2647   4051       27\nUNION          1399   2326       26\nVOLUSIA       97063  82214      396\nWAKULLA        3835   4511       46\nWALTON         5637  12176      120\nWASHINGTON     2796   4983       88\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod9 <- lm(Buchanan ~ Bush, data = florida)\nsummary(mod9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Buchanan ~ Bush, data = florida)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.50  -46.10  -29.19   12.26 2610.19 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.529e+01  5.448e+01   0.831    0.409    \nBush        4.917e-03  7.644e-04   6.432 1.73e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 353.9 on 65 degrees of freedom\nMultiple R-squared:  0.3889,\tAdjusted R-squared:  0.3795 \nF-statistic: 41.37 on 1 and 65 DF,  p-value: 1.727e-08\n```\n:::\n\n```{.r .cell-code}\npar(mfrow = c(2,3)); plot(mod9, which = 1:6)\n```\n\n::: {.cell-output-display}\n![](HW5_SteveONeill_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\nThe normal Q-Q shows a strong outlier: Palm Beach county. It is very high in the Cook's distance plot (over 1) and outside of the second dotted gray line in the Residuals vs Leverage plot. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod10 <- lm(log(Buchanan) ~ log(Bush), data = florida)\nsummary(mod10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Buchanan) ~ log(Bush), data = florida)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.96075 -0.25949  0.01282  0.23826  1.66564 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.57712    0.38919  -6.622 8.04e-09 ***\nlog(Bush)    0.75772    0.03936  19.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4673 on 65 degrees of freedom\nMultiple R-squared:  0.8508,\tAdjusted R-squared:  0.8485 \nF-statistic: 370.6 on 1 and 65 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nLogging both variables increased the coefficient, lowered the p-value of log(Bush), and increased the multiple R-squared by almost half. The findings of the first model are better-supported.\n\n",
    "supporting": [
      "HW5_SteveONeill_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}