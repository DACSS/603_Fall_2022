{
  "hash": "eaca02588e7d8fb8b11ec17717fd73d7",
  "result": {
    "markdown": "---\ntitle: \"Homework 5 Solution\"\nauthor: \"Dane Shelton\"\ndesription: \"Model Selection and Evalutaion\"\ndate: \"12/09/22\"\nformat:\n  html:\n    toc: true\n    df-print: paged\n    callout-appearance: \"simple\"\n    callout-icon: FALSE\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - hw5\n  - shelton\n---\n\n\n\n\n:::{.callout-note collapse=\"true\"}\n\n## Q1\n\n**a)** For backwards elimination, `Beds` would be removed first as it has the highest p-value, indicating it is least important in predicting `Price`.\n\n**b)** For forwards selection, `New` would be the first variable added with the model as it has the lowest p-value, indicating it is the most important/ strongest predictor of `Price` in the model.\n\n**c)** `Beds` likely has a high p-value because it has high collinearity with  `Size`, which is highly correlated with the response `Price`.\n\n**d)**\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(summary(fit1))\n## \n## Call:\n## lm(formula = P ~ S + Ba + New + Be, data = house)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -36.212  -9.546   1.277   9.406  71.953 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -41.795     12.104  -3.453 0.000855 ***\n## S             64.761      5.630  11.504  < 2e-16 ***\n## Ba            19.203      5.650   3.399 0.001019 ** \n## New           18.984      3.873   4.902  4.3e-06 ***\n## Be            -2.766      3.960  -0.698 0.486763    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.36 on 88 degrees of freedom\n## Multiple R-squared:  0.8689,\tAdjusted R-squared:  0.8629 \n## F-statistic: 145.8 on 4 and 88 DF,  p-value: < 2.2e-16\n\n(summary(fit2))\n## \n## Call:\n## lm(formula = P ~ S + Ba + New, data = house)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -34.804  -9.496   0.917   7.931  73.338 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -47.992      8.209  -5.847 8.15e-08 ***\n## S             62.263      4.335  14.363  < 2e-16 ***\n## Ba            20.072      5.495   3.653 0.000438 ***\n## New           18.371      3.761   4.885 4.54e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.31 on 89 degrees of freedom\n## Multiple R-squared:  0.8681,\tAdjusted R-squared:  0.8637 \n## F-statistic: 195.3 on 3 and 89 DF,  p-value: < 2.2e-16\n\n(summary(fit3))\n## \n## Call:\n## lm(formula = P ~ S + New, data = house)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -47.207  -9.763  -0.091   9.984  76.405 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -26.089      5.977  -4.365 3.39e-05 ***\n## S             72.575      3.508  20.690  < 2e-16 ***\n## New           19.587      3.995   4.903 4.16e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 17.4 on 90 degrees of freedom\n## Multiple R-squared:  0.8484,\tAdjusted R-squared:  0.845 \n## F-statistic: 251.8 on 2 and 90 DF,  p-value: < 2.2e-16\n```\n:::\n\n\n### Models\n**fit1:** Price ~ Size + Bath + New + Beds\n\n**fit2:** Price ~ Size + Bath + New \n\n**fit3:** Price ~ Size + New \n\n\n#### Model Evaluation\n\n**R^2^:** \nfit1: .8689\n\nfit2: .8681\n\nfit3: .8484\n\nUsing R^2^ as the model selection criterion would result in `fit`, the full model, being selected as it has the highest R^2^ value.\n\n**Adjusted R^2^:** \nfit1: .8629\n\nfit2: .8637\n\nfit3: .845\n\nNow, with Adjusted R^2^ penalizing models for additional variables, `fit 2` would be selected as the best fitting model.\n\n**PRESS**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(PRESS(fit1)$stat)\n## .........10.........20.........30.........40.........50\n## .........60.........70.........80.........90...\n## [1] 28390.22\n(PRESS(fit2)$stat)\n## .........10.........20.........30.........40.........50\n## .........60.........70.........80.........90...\n## [1] 27860.05\n(PRESS(fit3)$stat)\n## .........10.........20.........30.........40.........50\n## .........60.........70.........80.........90...\n## [1] 31066\n```\n:::\n\nUsing the PRESS statistic for our model selection criteria would select `fit2`, the model with the lowest value. \n\n**AIC**\n\n\n::: {.cell collaspe='true'}\n\n```{.r .cell-code}\n(AIC(fit1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 790.6225\n```\n:::\n\n```{.r .cell-code}\n(AIC(fit2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 789.1366\n```\n:::\n\n```{.r .cell-code}\n(AIC(fit3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 800.1262\n```\n:::\n:::\n\n\nSelecting a model using AIC as our evaluation criterion, `fit 2` is determined to be the best model (lowest value).\n\n**BIC**\n\n\n::: {.cell collaspe='true'}\n\n```{.r .cell-code}\n(BIC(fit1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 805.8181\n```\n:::\n\n```{.r .cell-code}\n(BIC(fit2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 801.7996\n```\n:::\n\n```{.r .cell-code}\n(BIC(fit3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 810.2566\n```\n:::\n:::\n\n\nSelecting a model using BIC as our evaluation criterion, `fit 2` is determined to be the best model (lowest value).\n\n**d**: I prefer `fit2`, it satisfies all relevant model selection criterion and avoids extraneous variables. Because `Beds` is highly correlated with `Size`, it does not need to be included in the model, and `Baths` provides completeness rather than predicting price from only the `Size` of a house and whether it is `New`.\n\n:::\n\n\n:::{.callout-note collapse=\"true\"}\n\n## Q2\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#A\ndata(\"trees\")\n#trees\n\nfit1 <-  lm(formula=`Volume` ~ `Girth` + `Height`, data= trees)\n\n#B\ndiag1 <- autoplot(fit1, 1:6, ncol=3)\n## Error in `autoplot()`:\n## ! Objects of type lm not supported by autoplot.\ndiag1\n## Error in eval(expr, envir, enclos): object 'diag1' not found\n```\n:::\n\n\nEvaluating the `Residuals vs Fitted Values` plot, we see a clear pattern in the residuals. This indicates that the model violates the homoskedasticity assumption, all residuals share the same variance regardless of fitted value.\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Q3\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflorida <- alr4::florida\n\nbuch_bush <- lm(`Buchanan` ~ `Bush`, data=florida)\n\ndiag2 <- autoplot(buch_bush, 1:6, ncol=3)\n## Error in `autoplot()`:\n## ! Objects of type lm not supported by autoplot.\ndiag2\n## Error in eval(expr, envir, enclos): object 'diag2' not found\n\nlog_buch_bush <- florida %>% \n                  mutate('Buchanan' = log(`Buchanan`),\n                         'Bush'=log(`Bush`))\n\nlog_fit <- lm(`Buchanan` ~ `Bush`, log_buch_bush)\n\ndiag3 <- autoplot(log_fit, 1:6, ncol=3)\n## Error in `autoplot()`:\n## ! Objects of type lm not supported by autoplot.\ndiag3\n## Error in eval(expr, envir, enclos): object 'diag3' not found\n```\n:::\n\n\n**a:** Yes, we see Palm Beach county identified as an outlier in all diagnostic plots, with `Fitted vs Residual Values` and `Cooks Distance` providing the most telling evidence.\n\n**b:** After taking the natural log of both variables, diagnostic plots improve but PBC is still identified as an outlier. \n\n\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}