{
  "hash": "fcc1cc8d482db52f1a12a8e0c4316c82",
  "result": {
    "markdown": "---\ntitle: \"Final Project\"\nauthor: \"Megha Joseph\"\ndescription: Final project\ndate: \"12/15/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Finalpart\n  - Megha Joseph\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cowplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'cowplot' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'caret' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(ROCR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ROCR' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rpart.plot' was built under R version 4.2.2\n```\n:::\n\n```{.r .cell-code}\nlibrary(rattle)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rattle' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: bitops\nRattle: A free graphical interface for data science with R.\nVersion 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.\nType 'rattle()' to shake, rattle, and roll your data.\n```\n:::\n\n```{.r .cell-code}\nlibrary(plotly)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'plotly' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid)\nlibrary(gridExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'gridExtra' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'corrplot' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ncorrplot 0.92 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2) #for data visualization\nlibrary(grid) # for grids\nlibrary(gridExtra) # for arranging the grids\nknitr::opts_chunk$set(echo = TRUE)\n```\n:::\n\n\n## Introduction\n\nDiabetes is a chronic disease characterized by a high amount of glucose in the blood and can cause too many complications also in the body, such as internal organ failure, retinopathy, and neuropathy. According to the predictions made by WHO, the figure may reach approximately 642 million by 2040, which means one in a ten may suffer from diabetes due to unhealthy lifestyle and lack of exercise. \n\n## Research Question\n\nQ1. How diabetes depends on the BMI of the person?\n\nThe purpose of this project is to take a data set of diabetes patients and build a predictive model for making a decision as to predict whether a person with certain medical diagnostic attributes is likely to have a diabetes or not. \n\n\n## Hypothesis\n\nH0: There is a no relationship between BMI and Diabetes.\nHA: There is a relationship between BMI and Diabetes.\n\n\n## Dataset\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nThe data can be downloaded from Kaggle database, https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n\n### Importing Dataset\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\ndbp <- read_csv(\"_data/DiabetesPrediction.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 768 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nstr(dbp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [768 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Pregnancies             : num [1:768] 6 1 8 1 0 5 3 10 2 8 ...\n $ Glucose                 : num [1:768] 148 85 183 89 137 116 78 115 197 125 ...\n $ BloodPressure           : num [1:768] 72 66 64 66 40 74 50 0 70 96 ...\n $ SkinThickness           : num [1:768] 35 29 0 23 35 0 32 0 45 0 ...\n $ Insulin                 : num [1:768] 0 0 0 94 168 0 88 0 543 0 ...\n $ BMI                     : num [1:768] 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...\n $ DiabetesPedigreeFunction: num [1:768] 0.627 0.351 0.672 0.167 2.288 ...\n $ Age                     : num [1:768] 50 31 32 21 33 30 26 29 53 54 ...\n $ Outcome                 : num [1:768] 1 0 1 0 1 0 1 0 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Pregnancies = col_double(),\n  ..   Glucose = col_double(),\n  ..   BloodPressure = col_double(),\n  ..   SkinThickness = col_double(),\n  ..   Insulin = col_double(),\n  ..   BMI = col_double(),\n  ..   DiabetesPedigreeFunction = col_double(),\n  ..   Age = col_double(),\n  ..   Outcome = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n:::\n:::\n\nThe following features have been provided to help us predict whether a person is diabetic or not:\n\nPregnancies: Number of times pregnant\nGlucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test\nBlood Pressure: Diastolic blood pressure (mm Hg)\nSkin Thickness: Triceps skin fold thickness (mm)\nInsulin: 2-Hour serum insulin (mu U/ml)\nBMI: Body mass index (weight in kg/(height in m)2)\nDiabetes Pedigree Function: Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\nAge: Age (years)\nOutcome: Class variable (0 if non-diabetic, 1 if diabetic)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dbp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 768\nColumns: 9\n$ Pregnancies              <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, …\n$ Glucose                  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125…\n$ BloodPressure            <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74…\n$ SkinThickness            <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, …\n$ Insulin                  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, …\n$ BMI                      <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.…\n$ DiabetesPedigreeFunction <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.2…\n$ Age                      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 3…\n$ Outcome                  <dbl> 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, …\n```\n:::\n:::\n\n\n\n\n# Data cleaning\n\nChecking for missing value and NULL value in the given dataset is one of the crucial steps in data cleaning\n\n::: {.cell}\n\n```{.r .cell-code}\n any(is.na(dbp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n any(is.null(dbp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\nThe results are False indicating that our data set does not contain neither missing value nor NULL value. So, we can do further analysis.\n\nNext step, we will look for duplicate records in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndp<-unique(dbp)\ndim(dbp)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 768\n```\n:::\n:::\n\nUsing unique function, we can sort out the unique records within the original dataset. The new dataset’s dimension has 768 unique rows which has the same number of records with our original dataset. Both contain 768 rows. Thus, we come to the conclusion that there is no duplicate value.\n\n## Exploratory analysis\n\nChecking for missing values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolSums(is.na(dbp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Pregnancies                  Glucose            BloodPressure \n                       0                        0                        0 \n           SkinThickness                  Insulin                      BMI \n                       0                        0                        0 \nDiabetesPedigreeFunction                      Age                  Outcome \n                       0                        0                        0 \n```\n:::\n:::\n\nWe need to check the class proportion of the target variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- prop.table(table(dbp$Outcome))\nb <- barplot(x,col=\"lightGreen\", main = \"Target Class Proportion Diagram\")\ntext(x=b, y= x, labels=round(x,2), pos = 1)\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nThe target variable class is still relatively balanced\n\n## Plotting Histograms of Numeric Values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nph1 <- ggplot(dbp, aes(x=Pregnancies)) + ggtitle(\"Number of times pregnant\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 1, colour=\"black\", fill=\"blue\") + ylab(\"Percentage\")\nph2 <- ggplot(dbp, aes(x=Glucose)) + ggtitle(\"Glucose\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 5, colour=\"black\", fill=\"orange\") + ylab(\"Percentage\")\nph3 <- ggplot(dbp, aes(x=BloodPressure)) + ggtitle(\"Blood Pressure\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 2, colour=\"black\", fill=\"green\") + ylab(\"Percentage\")\nph4 <- ggplot(dbp, aes(x=SkinThickness)) + ggtitle(\"Skin Thickness\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 2, colour=\"black\", fill=\"pink\") + ylab(\"Percentage\")\nph5 <- ggplot(dbp, aes(x=Insulin)) + ggtitle(\"Insulin\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 20, colour=\"black\", fill=\"red\") + ylab(\"Percentage\")\nph6 <- ggplot(dbp, aes(x=BMI)) + ggtitle(\"Body Mass Index\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth = 1, colour=\"black\", fill=\"yellow\") + ylab(\"Percentage\")\nph7 <- ggplot(dbp, aes(x=DiabetesPedigreeFunction)) + ggtitle(\"Diabetes Pedigree Function\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), colour=\"black\", fill=\"purple\") + ylab(\"Percentage\")\nph8 <- ggplot(dbp, aes(x=Age)) + ggtitle(\"Age\") +\n  geom_histogram(aes(y = 100*(..count..)/sum(..count..)), binwidth=1, colour=\"black\", fill=\"lightblue\") + ylab(\"Percentage\")\ngrid.arrange(ph1, ph2, ph3, ph4, ph5, ph6, ph7, ph8, ncol=2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n```{.r .cell-code}\ngrid.rect(width = 1, height = 1, gp = gpar(lwd = 1, col = \"black\", fill = NA))\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\nAll the variables seem to have reasonable broad distribution, therefore, will be kept for the regression analysis.\n\n## Correlation between Numeric Variables\n\nHere, sapply() function will return the columns from the diabetes dataset which have numeric values. cor() function will produce correlation matrix of all those numeric columns returned by sapply(). corrplot() provides a visual representation of correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnv <- sapply(dbp, is.numeric)\ncorr.matrix <- cor(dbp[,nv])\ncorrplot(corr.matrix, main=\"\\n\\nCorrelation Plot for Numerical Variables\", order = \"hclust\", tl.col = \"black\", tl.srt=45, tl.cex=0.8, cl.cex=0.8)\nbox(which = \"outer\", lty = \"solid\")\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\nThe numeric variables are almost not correlated.\n\n## Correlation between Numeric Variables and Outcomes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(dbp)\npar(mfrow=c(2,4))\nboxplot(Pregnancies~Outcome, main=\"No. of Pregnancies vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Pregnancies\",col=\"red\")\nboxplot(Glucose~Outcome, main=\"Glucose vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Glucose\",col=\"pink\")\nboxplot(BloodPressure~Outcome, main=\"Blood Pressure vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Blood Pressure\",col=\"green\")\nboxplot(SkinThickness~Outcome, main=\"Skin Thickness vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Skin Thickness\",col=\"orange\")\nboxplot(Insulin~Outcome, main=\"Insulin vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Insulin\",col=\"yellow\")\nboxplot(BMI~Outcome, main=\"BMI vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"BMI\",col=\"purple\")\nboxplot(DiabetesPedigreeFunction~Outcome, main=\"Diabetes Pedigree Function vs. Diabetes\", xlab=\"Outcome\", ylab=\"DiabetesPedigreeFunction\",col=\"lightgreen\")\nboxplot(Age~Outcome, main=\"Age vs. Diabetes\", \n        xlab=\"Outcome\", ylab=\"Age\",col=\"lightblue\")\nbox(which = \"outer\", lty = \"solid\")\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\nBlood pressure and skin thickness show little variation with diabetes, they will be excluded from the model. Other variables show more or less correlation with diabetes, so will be kept.\n\n\n\n\n\n# Linear Regression with BMI :\n\nA linear regression is a statistical model that analyzes the relationship between a response variable (often called y) and one or more variables and their interactions (often called x or explanatory variables).\nBMI is a independent variable (x) and Diabetes is the dependent variable (y)\n\n1. Perform OLS Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBI <- dbp$BMI\nDiabetes_P <- dbp$Outcome\nplot(BI,Diabetes_P)\n#fit simple linear regression model\nmodel <- lm(Diabetes_P~BI)\nabline(model, col = 'blue')\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nFrom the plot we can see that the relationship doesn't appear to be linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#view model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Diabetes_P ~ BI)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7971 -0.3579 -0.2278  0.5451  1.2175 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.21752    0.06886  -3.159  0.00165 ** \nBI           0.01771    0.00209   8.472  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4564 on 766 degrees of freedom\nMultiple R-squared:  0.08567,\tAdjusted R-squared:  0.08448 \nF-statistic: 71.77 on 1 and 766 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n*Here is how to interpret the rest of the model summary*:\n\n*Pr(>|t|)*: This is the p-value associated with the model coefficients. Since the p-value for BMI (0.00106) is significantly less than .05, we can say that there is a statistically significant association between BMI and diabetes.\n\n*Multiple R-squared*: This number tells us the percentage of the variation in the diabetes outcome can be explained by the BMI. In general, the larger the R-squared value of a regression model the better the predictor variables are able to predict the value of the response variable. In this case, 1% of the variation in diabetes outcome can be explained using BMI.\n\n2. Histogram: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(resid(model))\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nHistogram for a dataset is not bell-shaped,it’s likely that the data is not normally distributed.\n\n3.Residual Plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(BI, resid(model))\nabline(h=0, col = 'red')\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n4.Normal Q-Q plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create Q-Q plot for residuals\nqqnorm(resid(model))\n\n#add a straight diagonal line to the plot\nqqline(resid(model)) \n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThe residuals of a model follow a normal distribution. The points on the plot not forming a straight diagonal line, then the normality assumption is not satisfying the condition.\n\nBecause the above reasons Linear regression is not a good model to predict the relation between BMI and diabetes that's we are implementing logistic regression. \n\n# Logistic regression\n\nLogistic regression (aka logit regression or logit model)is a regression model where the response variable Y is categorical. Logistic regression allows us to estimate the probability of a categorical response based on one or more predictor variables (X). It allows one to say that the presence of a predictor increases (or decreases) the probability of a given outcome by a specific percentage. When Y is binary — that is, where it can take only two values, “0” and “1”, which represent outcomes such as pass/fail, win/lose, good/bad or healthy/sick. Cases where the dependent variable has more than two outcome categories may be analysed with multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression. \n\nThe first step before applying models is to create training and test data sets. The data will be split 70/30 and spread evenly between having diabetes and not having diabetes using the Create Data Partition function in the caret package.\n\n##Cross Validation\n\nBefore we make the model, we need to split the data into train data set and test data set. We will take 70% of the data as the training data and the rest of it as the testing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#RNGkind(sample.kind = \"Rounding\")\nset.seed(23)\n\nintrain <- sample(nrow(dbp),nrow(dbp)*.7)\n\ndbp_train <- dbp[intrain,]\ndbp_test <- dbp[-intrain,]\n```\n:::\n\nWe need to check again the proportion of our train dataset, wheter it is still balanced or not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.table(table(dbp_train$Outcome))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n        0         1 \n0.6405959 0.3594041 \n```\n:::\n:::\n\n##Modelling\n\nWe will try to create several models the Logistic Regression using Outcome as the target value. The models that we will create come from several ways, some from the my understanding or estimation and from stepwise selection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbp_model<- glm(formula = Outcome ~ ., data = dbp_train, family = \"binomial\")\n\nsummary(dbp_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ ., family = \"binomial\", data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5377  -0.7075  -0.4178   0.7169   2.9257  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -8.2667332  0.8441183  -9.793  < 2e-16 ***\nPregnancies               0.1524586  0.0383131   3.979 6.91e-05 ***\nGlucose                   0.0340854  0.0043550   7.827 5.01e-15 ***\nBloodPressure            -0.0093631  0.0061683  -1.518 0.129025    \nSkinThickness             0.0011132  0.0081187   0.137 0.890942    \nInsulin                  -0.0009446  0.0010377  -0.910 0.362702    \nBMI                       0.0724637  0.0172859   4.192 2.76e-05 ***\nDiabetesPedigreeFunction  1.4680400  0.3797685   3.866 0.000111 ***\nAge                       0.0127825  0.0110580   1.156 0.247704    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 508.13  on 528  degrees of freedom\nAIC: 526.13\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\nHigh BMI has a high corelation with developing diabetes\n\n*Model 2*\nThe first model of logistic regression using the glm() function where we are predicting diabetes with all variables of the dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbp_model<- glm(formula = Outcome ~ BMI, data = dbp_train, family = \"binomial\")\n\nsummary(dbp_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ BMI, family = \"binomial\", data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7723  -0.9471  -0.7332   1.2751   2.5160  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.12194    0.46071  -6.776 1.23e-11 ***\nBMI          0.07783    0.01361   5.719 1.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 662.94  on 535  degrees of freedom\nAIC: 666.94\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2800)\nlmModel_all <- glm(Outcome ~ ., family = binomial, data = dbp_train)\n\n# Fit model to test set\nlmFit_all <- predict(lmModel_all, type = \"response\", dbp_test)\n\n# Compare predictions to test set\nlmPred_all <- prediction(lmFit_all, dbp_test$Outcome)\n\n# Create Area Under the Curve (AUC) plot\nAUC <- performance(lmPred_all, 'tpr', 'fpr')\nplot(AUC,main = 'Outcome Vs All Independent Variables')\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'pROC' was built under R version 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\nroccurve <- roc(dbp_test$Outcome~lmFit_all)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = 0, case = 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nprint(auc(roccurve))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.8379\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmModel_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ ., family = binomial, data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5377  -0.7075  -0.4178   0.7169   2.9257  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              -8.2667332  0.8441183  -9.793  < 2e-16 ***\nPregnancies               0.1524586  0.0383131   3.979 6.91e-05 ***\nGlucose                   0.0340854  0.0043550   7.827 5.01e-15 ***\nBloodPressure            -0.0093631  0.0061683  -1.518 0.129025    \nSkinThickness             0.0011132  0.0081187   0.137 0.890942    \nInsulin                  -0.0009446  0.0010377  -0.910 0.362702    \nBMI                       0.0724637  0.0172859   4.192 2.76e-05 ***\nDiabetesPedigreeFunction  1.4680400  0.3797685   3.866 0.000111 ***\nAge                       0.0127825  0.0110580   1.156 0.247704    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 508.13  on 528  degrees of freedom\nAIC: 526.13\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance(lmPred_all, measure = 'auc')@y.values[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8379487\n```\n:::\n:::\n\n\n*Model 3*\nThe second model of logistic regression using the glm() function where we are predicting diabetes outcome with BMI and other factors such as age and glucose variables of the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbp_model<- glm(formula = Outcome ~ BMI+Age+Glucose, data = dbp_train, family = \"binomial\")\n\nsummary(dbp_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ BMI + Age + Glucose, family = \"binomial\", \n    data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3275  -0.7750  -0.4556   0.7983   2.7874  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -7.918601   0.769379 -10.292  < 2e-16 ***\nBMI          0.069436   0.015343   4.526 6.02e-06 ***\nAge          0.032844   0.008947   3.671 0.000242 ***\nGlucose      0.031356   0.003862   8.119 4.71e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 541.98  on 533  degrees of freedom\nAIC: 549.98\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2800)\nlmModel_mul <- glm(Outcome ~ BMI + Age+ Glucose, family = binomial, data = dbp_train)\n# Fit model to test set\nlmFit_mul <- predict(lmModel_mul, type = \"response\", dbp_test)\n\n# Compare predictions to test set\nlmPred_mul <- prediction(lmFit_mul, dbp_test$Outcome)\n\n# Create Area Under the Curve (AUC) plot\nAUC <- performance(lmPred_mul, 'tpr', 'fpr')\nplot(AUC,main = 'Outcome Vs BMI + Age + Glucose')\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmModel_mul)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ BMI + Age + Glucose, family = binomial, \n    data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3275  -0.7750  -0.4556   0.7983   2.7874  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -7.918601   0.769379 -10.292  < 2e-16 ***\nBMI          0.069436   0.015343   4.526 6.02e-06 ***\nAge          0.032844   0.008947   3.671 0.000242 ***\nGlucose      0.031356   0.003862   8.119 4.71e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 541.98  on 533  degrees of freedom\nAIC: 549.98\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance(lmPred_mul, measure = 'auc')@y.values[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8457265\n```\n:::\n:::\n\n\n*Model 3*\nLogistic Regression with BMI :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmModel_age <- glm(Outcome ~ BMI, family = binomial, data = dbp)\nsummary(lmModel_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ BMI, family = binomial, data = dbp)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9209  -0.9178  -0.6838   1.2351   2.7244  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.68641    0.40896  -9.014  < 2e-16 ***\nBMI          0.09353    0.01205   7.761 8.45e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 920.71  on 766  degrees of freedom\nAIC: 924.71\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2800)\nlmModel_age <- glm(Outcome ~ BMI, family = binomial, data = dbp_train)\n# Fit model to test set\nlmFit_age <- predict(lmModel_age, type = \"response\", dbp_test)\n\n# Compare predictions to test set\nlmPred_age <- prediction(lmFit_age, dbp_test$Outcome)\n\n# Create Area Under the Curve (AUC) plot\nAUC <- performance(lmPred_age, 'tpr', 'fpr')\nplot(AUC,main = 'Diabetes Outcome Vs BMI')\n```\n\n::: {.cell-output-display}\n![](MEGHA-JOSEPH_Final_Project_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmModel_age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Outcome ~ BMI, family = binomial, data = dbp_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7723  -0.9471  -0.7332   1.2751   2.5160  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -3.12194    0.46071  -6.776 1.23e-11 ***\nBMI          0.07783    0.01361   5.719 1.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.40  on 536  degrees of freedom\nResidual deviance: 662.94  on 535  degrees of freedom\nAIC: 666.94\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\nLet us take a closer look on the coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmModel_age)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate Std. Error   z value     Pr(>|z|)\n(Intercept) -3.12194374 0.46070522 -6.776445 1.231686e-11\nBMI          0.07782564 0.01360763  5.719266 1.069854e-08\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(coefficients(lmModel_age)[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     BMI \n1.080934 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance(lmPred_age, measure = 'auc')@y.values[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7407265\n```\n:::\n:::\n\n\n#Interpreting the results of our logistic regression model.\n\n*P-Value*\nNow we can analyze the fitting and interpret what the models are telling us.\nFirst of all, we can see that BMI is statistically significant variable as the p-value is less than 0.05 suggesting a strong association of the BMI with diabetes outcome. So we can reject null hypothesis.   \n\n\n*Coefficient*\nThe coefficient of the BMI has a positive sign of this value indicates that the chance of Diabetes increases with BMI. The magnitude of the coefficient implies that for every degree increase in BMI the log-odds increases by a constant 0.009393367 units, on average. By taking the exponent of the coefficient value, we get the odds ratio. In all three models Diabetes is increasing with BMI. So we can conclude On comparing first model and second model based on AUC it shows Model one has a accuracy of 80% while Model second has a accuracy of 61%. Based on these two models , We can conclude that Model one is the best model with predictor variables to predict the diabetes.\n\n## References\n\n1. Collins, Gary S, et al. “Developing Risk Prediction Models for Type 2 Diabetes: A Systematic Review of Methodology and Reporting - BMC Medicine.” BioMed Central, BioMed Central, 8 Sept. 2011, https://bmcmedicine.biomedcentral.com/articles/10.1186/1741-7015-9-103\n\n2. panelAishwaryaMujumdaraEnvelopeVVaidehiDr.b, Author links open overlay, et al. “Diabetes Prediction Using Machine Learning Algorithms.” Procedia Computer Science, Elsevier, 27 Feb. 2020, https://www.sciencedirect.com/science/article/pii/S1877050920300557. \n\n3. “Sign In.” RPubs, https://rpubs.com/niamzaki/diabetics_prediction_lr_knn. \n",
    "supporting": [
      "MEGHA-JOSEPH_Final_Project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}